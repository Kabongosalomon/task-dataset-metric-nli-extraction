<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-Scale Image Retrieval with Attentive Deep Local Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Araujo</surname></persName>
							<email>andrearaujo@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
							<email>jacksim@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
							<email>weyand@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
							<email>bhhan@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Postech</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large-Scale Image Retrieval with Attentive Deep Local Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an attentive local feature descriptor suitable for large-scale image retrieval, referred to as DELF (DEep Local Feature). The new feature is based on convolutional neural networks, which are trained only with image-level annotations on a landmark image dataset. To identify semantically useful local features for image retrieval, we also propose an attention mechanism for keypoint selection, which shares most network layers with the descriptor. This framework can be used for image retrieval as a drop-in replacement for other keypoint detectors and descriptors, enabling more accurate feature matching and geometric verification. Our system produces reliable confidence scores to reject false positives-in particular, it is robust against queries that have no correct match in the database. To evaluate the proposed descriptor, we introduce a new large-scale dataset, referred to as Google-Landmarks dataset, which involves challenges in both database and query such as background clutter, partial occlusion, multiple landmarks, objects in variable scales, etc. We show that DELF outperforms the state-of-the-art global and local descriptors in the large-scale setting by significant margins. Code and dataset can be found at the project webpage: https://github.com/tensorflow/models/ tree/master/research/delf.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large-scale image retrieval is a fundamental task in computer vision, since it is directly related to various practical applications, e.g., object detection, visual place recognition, and product recognition. The last decades have witnessed tremendous advances in image retrieval systems-from handcrafted features and indexing algorithms <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16]</ref> to, more recently, methods based on convolutional neural networks (CNNs) for global descriptor learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Despite the recent advances in CNN-based global descriptors for image retrieval in small or medium-size datasets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, their performance may be hindered by a wide variety</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DELF Pipeline</head><p>Large-Scale Index On the left, we illustrate the pipeline for extraction and selection of DELF. The portion highlighted in yellow represents an attention mechanism that is trained to assign high scores to relevant features and select the features with the highest scores. Feature extraction and selection can be performed with a single forward pass using our model. On the right, we illustrate our large-scale feature-based retrieval pipeline. DELF for database images are indexed offline. The index supports querying by retrieving nearest neighbor (NN) features, which can be used to rank database images based on geometrically verified matches.</p><p>of challenging conditions observed in large-scale datasets, such as clutter, occlusion, and variations in viewpoint and illumination. Global descriptors lack the ability to find patchlevel matches between images. As a result, it is difficult to retrieve images based on partial matching in the presence of occlusion and background clutter. In a recent trend, CNNbased local features are proposed for patch-level matching <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b39">40]</ref>. However, these techniques are not optimized specifically for image retrieval since they lack the ability to detect semantically meaningful features, and show limited accuracy in practice. Most existing image retrieval algorithms have been evaluated in small to medium-size datasets with few query images, i.e., only 55 in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> and 500 in <ref type="bibr" target="#b15">[16]</ref>, and the images in the datasets have limited diversity in terms of landmark locations and types. Therefore, we believe that the image retrieval community can benefit from a large-scale dataset, comprising more comprehensive and challenging examples, to improve algorithm performance and evaluation methodology by deriving more statistically meaningful results.</p><p>The main goal of this work is to develop a large-scale image retrieval system based on a novel CNN-based feature descriptor. To this end, we first introduce a new large-scale dataset, Google-Landmarks, which contains more than 1M landmark images from almost 13K unique landmarks. This dataset covers a wide area in the world, and is consequently more diverse and comprehensive than existing ones. The query set is composed of an extra 100K images with diverse characteristics; in particular, we include images that have no match in the database, which makes our dataset more challenging. This allows to assess the robustness of retrieval systems when queries do not necessarily depict landmarks.</p><p>We then propose a CNN-based local feature with attention, which is trained with weak supervision using imagelevel class labels only, without the need of object-and patchlevel annotations. This new feature descriptor is referred to as DELF (DEep Local Feature), and <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the overall procedure of feature extraction and image retrieval. In our approach, the attention model is tightly coupled with the proposed descriptor; it reuses the same CNN architecture and generates feature scores using very little extra computation (in the spirit of recent advances in object detection <ref type="bibr" target="#b29">[30]</ref>). This enables the extraction of both local descriptors and keypoints via one forward pass over the network. We show that our image retrieval system based on DELF achieves the stateof-the-art performance with significant margins compared to methods based on existing global and local descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are standard datasets commonly used for the evaluation of image retrieval techniques. Oxford5k <ref type="bibr" target="#b26">[27]</ref> has 5,062 building images captured in Oxford with 55 query images. Paris6k <ref type="bibr" target="#b27">[28]</ref> is composed of 6,412 images of landmarks in Paris, and also has 55 query images. These two datasets are often augmented with 100K distractor images from Flickr100k dataset <ref type="bibr" target="#b26">[27]</ref>, which constructs Oxford105k and Paris106k datasets, respectively. On the other hand, Holidays dataset <ref type="bibr" target="#b15">[16]</ref> provides 1,491 images including 500 query images, which are from personal holiday photos. All these three datasets are fairly small, especially having a very small number of query images, which makes it difficult to generalize the performance tested in these datasets. Although Pitts250k <ref type="bibr" target="#b34">[35]</ref> is larger, it is specialized to visual places with repetitive patterns and may not be appropriate for the general image retrieval task.</p><p>Instance retrieval has been a popular research problem for more than a decade. See <ref type="bibr" target="#b42">[43]</ref> for a recent survey. Early systems rely on hand-crafted local features <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref>, coupled with approximate nearest neighbor search methods using KD trees or vocabulary trees <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>. Still today, such featurebased techniques combined with geometric re-ranking provide strong performance when retrieval systems need to operate with high precision.</p><p>More recently, many works have focused on aggregation methods of local features, which include popular techniques such as VLAD <ref type="bibr" target="#b17">[18]</ref> and Fisher Vector (FV) <ref type="bibr" target="#b18">[19]</ref>. The main advantage of such global descriptors is the ability to provide high-performance image retrieval with a compact index.</p><p>In the past few years, several global descriptors based on CNNs have been proposed to use pretrained <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref> or learned networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11]</ref>. These global descriptors are most commonly trained with a triplet loss, in order to preserve the ranking between relevant and irrelevant images. Some retrieval algorithms using these CNN-based global descriptors make use of deep local features as a drop-in replacement for hand-crafted features in conventional aggregation techniques such as VLAD or FV <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. Other works have re-evaluated and proposed different feature aggregation methods using such deep local features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>CNNs have also been used to detect, represent and compare local image features. Verdie et al. <ref type="bibr" target="#b36">[37]</ref> learned a regressor for repeatable keypoint detection. Yi et al. <ref type="bibr" target="#b40">[41]</ref> proposed a generic CNN-based technique to estimate the canonical orientation of a local feature and successfully deployed it to several different descriptors. MatchNet <ref type="bibr" target="#b11">[12]</ref> and DeepCompare <ref type="bibr" target="#b41">[42]</ref> have been proposed to jointly learn patch representations and associated metrics. Recently, LIFT <ref type="bibr" target="#b39">[40]</ref> proposed an end-to-end framework to detect keypoints, estimate orientation, and compute descriptors. Different from our work, these techniques are not designed for image retrieval applications since they do not learn to select semantically meaningful features.</p><p>Many visual recognition problems employ visual attention based on deep neural networks, which include object detection <ref type="bibr" target="#b44">[45]</ref>, semantic segmentation <ref type="bibr" target="#b13">[14]</ref>, image captioning <ref type="bibr" target="#b37">[38]</ref>, visual question answering <ref type="bibr" target="#b38">[39]</ref>, etc. However, visual attention has not been explored actively to learn visual features for image retrieval applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Google-Landmarks Dataset</head><p>Our dataset is constructed based on the algorithm described in <ref type="bibr" target="#b43">[44]</ref>. Compared to the existing datasets for image retrieval <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref>, the new dataset is much larger, contains diverse landmarks, and involves substantial challenges. It contains 1, 060, 709 images from 12, 894 landmarks, and 111, 036 additional query images. The images in the dataset are captured at various locations in the world, and each image is associated with a GPS coordinate. Example images and their geographic distribution are presented in <ref type="figure" target="#fig_2">Fig. 2</ref> and <ref type="figure" target="#fig_3">Fig. 3</ref>, respectively. While most images in the existing datasets are landmark-centric, which makes global feature descriptors work well, our dataset contains more realistic im-   ages with wild variations including foreground/background clutter, occlusion, partially out-of-view objects, etc. In particular, since our query images are collected from personal photo repositories, some of them may not contain any landmarks and should not retrieve any image from the database. We call these query images distractors, which play a critical role to evaluate robustness of algorithms to irrelevant and noisy queries.</p><p>We use visual features and GPS coordinates for groundtruth construction. All images in the database are clustered using the two kinds of information, and we assign a landmark identifier to each cluster. If physical distance between the location of a query image and the center of the cluster associated with the retrieved image is less than a threshold, we assume that the two images belong to the same landmark. Note that ground-truth annotation is extremely challenging, especially considering the facts that it is hard to predefine what landmarks are, landmarks are not clearly noticeable sometimes, and there might be multiple instances in a single image. Obviously, this approach for ground-truth construction is noisy due to GPS errors. Also, photos can be captured from a large distance for some landmarks (e.g., Eiffel Tower, Golden Gate Bridge), and consequently the photo location might be relatively far from the actual landmark location. However, we found very few incorrect annotations with the threshold of 25km when checking a subset of data manually. Even though there are few minor errors, it is not problematic, especially in relative evaluation, because algorithms are unlikely to be confused between landmarks anyway if their visual appearances are sufficiently discriminative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Image Retrieval with DELF</head><p>Our large-scale retrieval system can be decomposed into four main blocks: (i) dense localized feature extraction, (ii) keypoint selection, (iii) dimensionality reduction and (iv) indexing and retrieval. This section describes DELF feature extraction and learning algorithm followed by our indexing and retrieval procedure in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dense Localized Feature Extraction</head><p>We extract dense features from an image by applying a fully convolutional network (FCN), which is constructed by using the feature extraction layers of a CNN trained with a classification loss. We employ an FCN taken from the ResNet50 <ref type="bibr" target="#b12">[13]</ref> model, using the output of the conv4 x convolutional block. To handle scale changes, we explicitly construct an image pyramid and apply the FCN for each level independently. The obtained feature maps are regarded as a dense grid of local descriptors. Features are localized based on their receptive fields, which can be computed by considering the configuration of convolutional and pooling layers of the FCN. We use the pixel coordinates of the center of the receptive field as the feature location. The receptive field size for the image at the original scale is 291 × 291. Using the image pyramid, we obtain features that describe image regions of different sizes.</p><p>We use the original ResNet50 model trained on Ima-geNet <ref type="bibr" target="#b30">[31]</ref> as a baseline, and fine-tune for enhancing the discriminativeness of our local descriptors. Since we consider a landmark recognition application, we employ annotated datasets of landmark images <ref type="bibr" target="#b3">[4]</ref> and train the network with a standard cross-entropy loss for image classification as illustrated in <ref type="figure" target="#fig_5">Fig. 4(a)</ref>. The input images are initially centercropped to produce square images and rescaled to 250 × 250. Random 224 × 224 crops are then used for training. As a  result of training, local descriptors implicitly learn representations that are more relevant for the landmark retrieval problem. In this manner, neither object-nor patch-level labels are necessary to obtain improved local descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Attention-based Keypoint Selection</head><p>Instead of using densely extracted features directly for image retrieval, we design a technique to effectively select a subset of the features. Since a substantial part of the densely extracted features are irrelevant to our recognition task and likely to add clutter, distracting the retrieval process, keypoint selection is important for both accuracy and computational efficiency of retrieval systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Learning with Weak Supervision</head><p>We propose to train a landmark classifier with attention to explicitly measure relevance scores for local feature descriptors. To train the function, features are pooled by a weighted sum, where the weights are predicted by the attention network. The training procedure is similar to the one described in Sec. 4.1 including the loss function and datasets, and is illustrated in <ref type="figure" target="#fig_5">Fig. 4(b)</ref>, where the attention network is highlighted in yellow. This generates an embedding for the whole input image, which is then used to train a softmax-based landmark classifier.</p><p>More precisely, we formulate the training as follows. Denote by f n ∈ R d , n = 1, ..., N the d-dimensional features to be learned jointly with the attention model. Our goal is to learn a score function α(f n ; θ) for each feature, where θ denotes the parameters of function α(·). The output logit y of the network is generated by a weighted sum of the feature vectors, which is given by</p><formula xml:id="formula_0">y = W n α(f n ; θ) · f n ,<label>(1)</label></formula><p>where W ∈ R M ×d represents the weights of the final fullyconnected layer of the CNN trained to predict M classes. For training, we use cross entropy loss, which is given by</p><formula xml:id="formula_1">L = −y * · log exp (y) 1 T exp (y) ,<label>(2)</label></formula><p>where y * is ground-truth in one-hot representation and 1 is one vector. The parameters in the score function α(·) are trained by backpropagation, where the gradient is given by</p><formula xml:id="formula_2">∂L ∂θ = ∂L ∂y n ∂y ∂α n ∂α n ∂θ = ∂L ∂y n W f n ∂α n ∂θ ,<label>(3)</label></formula><p>where the backpropagation of the output score α n ≡ α(f n ; θ) with respect to θ is same as the standard multi-layer perceptron.</p><p>We restrict α(·) to be non-negative, to prevent it from learning negative weighting. The score function is designed using a 2-layer CNN with a softplus <ref type="bibr" target="#b8">[9]</ref> activation at the top. For simplicity, we employ the convolutional filters of size 1 × 1, which work well in practice. Once the attention model is trained, it can be used to assess the relevance of features extracted by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Training Attention</head><p>In the proposed framework, both the descriptors and the attention model are implicitly learned with image-level labels. Unfortunately, this poses some challenges to the learning process. While the feature representation and the score function can be trained jointly by backpropagation, we found that this setup generates weak models in practice. Therefore, we employ a two-step training strategy. First, we learn descriptors with fine-tuning as described in Sec. 4.1, and then the score function is learned given the fixed descriptors.</p><p>Another improvement to our models is obtained by random image rescaling during attention training process. This is intuitive, as the attention model should be able to generate effective scores for features at different scales. In this case, the input images are initially center-cropped to produce square images, and rescaled to 900 × 900. Random 720 × 720 crops are then extracted and finally randomly scaled with a factor γ ≤ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Characteristics</head><p>One unconventional aspect of our system is that keypoint selection comes after descriptor extraction, which is different from the existing techniques (e.g., SIFT <ref type="bibr" target="#b21">[22]</ref> and LIFT <ref type="bibr" target="#b39">[40]</ref>), where keypoints are first detected and later described. Traditional keypoint detectors focus on detecting keypoints repeatably under different imaging conditions, based only on their low-level characteristics. However, for a high-level recognition task such as image retrieval, it is also critical to select keypoints that discriminate different object instances. The proposed pipeline achieves both goals by training a model that encodes higher level semantics in the feature map, and learning to select discriminative features for the classification task. This is in contrast to recently proposed techniques for learning keypoint detectors, i.e., LIFT <ref type="bibr" target="#b39">[40]</ref>, which collect training data based on SIFT matches. Although our model is not constrained to learn invariances to pose and viewpoint, it implicitly learns to do so-similar to CNN-based image classification techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Dimensionality Reduction</head><p>We reduce the dimensionality of selected features to obtain improved retrieval accuracy, as common practice <ref type="bibr" target="#b14">[15]</ref>. First, the selected features are L 2 normalized, and their dimensionality is reduced to 40 by PCA, which presents a good trade-off between compactness and discriminativeness. Finally, the features once again undergo L 2 normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image Retrieval System</head><p>We extract feature descriptors from query and database images, where a predefined number of local features with the highest attention scores per image are selected. Our image retrieval system is based on nearest neighbor search, which is implemented by a combination of KD-tree <ref type="bibr" target="#b6">[7]</ref> and Product Quantization (PQ) <ref type="bibr" target="#b16">[17]</ref>. We encode each descriptor to a 50bit code using PQ, where each 40D feature descriptor is split into 10 subvectors with equal dimensions, and we identify 2 5 centroids per subvector by k-means clustering to achieve 50bit encoding. We perform asymmetric distance calculation, where the query descriptors are not encoded to improve the accuracy of nearest neighbor retrieval. To speed up the nearest neighbor search, we construct an inverted index for descriptors, using a codebook of size 8K. To reduce encoding errors, a KD-tree is used to partition each Voronoi cell, and a Locally Optimized Product Quantizer <ref type="bibr" target="#b19">[20]</ref> is employed for each subtree with fewer than 30K features.</p><p>When a query is given, we perform approximate nearest neighbor search for each local descriptor extracted from a query image. Then for the top K nearest local descriptors retrieved from the index, we aggregate all the matches per database image. Finally, we perform geometric verification using RANSAC <ref type="bibr" target="#b9">[10]</ref> and employ the number of inliers as the score for retrieved images. Many distractor queries are rejected by this geometric verification step because features from distractors may not be consistently matched with the ones from landmark images.</p><p>This pipeline requires less than 8GB memory to index 1 billion descriptors, which is sufficient to handle our largescale landmark dataset. The latency of the nearest neighbor search is less than 2 seconds using a single CPU under our experiment setup, where we soft-assign 5 centroids to each query and search up to 10K leaf nodes within each inverted index tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section mainly discusses the performance of DELF compared to existing global and local feature descriptors in our dataset. In addition, we also show how DELF can be employed to achieve good accuracy in the existing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Multi-scale descriptor extraction We construct image pyramids by using scales that are a √ 2 factor apart. For the set of scales with range from 0.25 to 2.0, 7 different scales are used. The size of receptive field is inversely proportional to the scale; for example, for the 2.0 scale, the receptive field of the network covers 146 × 146 pixels.</p><p>Training We employed landmarks dataset <ref type="bibr" target="#b3">[4]</ref> for finetuning descriptors and training keypoint selection. In the dataset, there are the "full" version, referred to as LF (after removal of overlapping classes with Oxf5k/Par6k, by <ref type="bibr" target="#b10">[11]</ref>), containing 140,372 images from 586 landmarks, and the "clean" version (LC) obtained by a SIFT-based matching procedure <ref type="bibr" target="#b10">[11]</ref>, with 35,382 images of 586 landmarks. We use LF to train our attention model, and LC is employed to fine-tune the network for image retrieval.</p><p>Parameters We identify the top K(= 60) nearest neighbors for each feature in a query and extract up to 1000 local features from each image-each feature is 40-dimensional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Compared Algorithms</head><p>DELF is compared with several recent global and local descriptors. Although there are various research outcomes related to image retrieval, we believe that the following methods are either relevant to our algorithm or most critical to evaluation due to their good performance.</p><p>Deep Image Retrieval (DIR) <ref type="bibr" target="#b10">[11]</ref> This is a recent global descriptor that achieves the state-of-the-art performance in several existing datasets. DIR feature descriptors are 2, 048 dimensional and multi-resolution descriptors are used in all cases. We also evaluate with query expansion (QE), which typically improves accuracy in the standard datasets. We use the released source code that implements the version with ResNet101 <ref type="bibr" target="#b12">[13]</ref>. For retrieval, a parallelized implementation of brute-force search is employed to avoid penalization by the error from approximate nearest neighbor search.</p><p>siaMAC <ref type="bibr" target="#b28">[29]</ref> This is a recent global descriptor that obtains high performance in existing datasets. We use the released source code with parallelized implementation of brute-force search. The CNN based on VGG16 <ref type="bibr" target="#b31">[32]</ref> extracts 512 dimensional global descriptor. We also experiment with query expansion (QE) as in DIR.</p><p>CONGAS <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> CONGAS is a 40D hand-engineered local feature, which has been widely used for instance-level image matching and retrieval <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">44]</ref>. This feature descriptor is extracted by collecting Gabor wavelet responses at the detected keypoint scale and orientation, and is known to have very similar performance and characteristic to other gradient-based local descriptors like SIFT. A Laplacian-of-Gaussian keypoint detector is used.</p><p>LIFT LIFT <ref type="bibr" target="#b39">[40]</ref> is a recently proposed feature matching pipeline, where keypoint detection, orientation estimation and keypoint description are jointly learned. Features are 128 dimensional. We use the source code publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation</head><p>Image retrieval systems have typically been evaluated based on mean average precision (mAP), which is computed by sorting images in descending order of relevance per query and averaging AP of individual queries. However, for datasets with distractor queries, such evaluation method is not representative since it is important to determine whether each image is relevant to the query or not. In our case, the absolute retrieval score is used to estimate the relevance of each image. For performance evaluation, we employ a modified version of precision (PRE) and recall (REC) by considering all query images at the same time, which are given by</p><formula xml:id="formula_3">PRE = q |R TP q | q |R q | and REC = q |R TP q |,<label>(4)</label></formula><p>where R q denotes a set of retrieved images for query q given a threshold, and R TP q (⊆ R q ) is a set of true positives. This is similar to the micro-AP metric introduced in <ref type="bibr" target="#b25">[26]</ref>. Note that in our case only the top-scoring image per landmark is considered in the final scoring. We prefer unnormalized recall values, which present the number of retrieved true positives. Instead of summarizing our result in a single number, we present a full precision-recall curve to inspect operating points with different retrieval thresholds. <ref type="figure" target="#fig_6">Fig. 5</ref> presents the precision-recall curve of DELF (denoted by DELF+FT+ATT), compared to other methods. The results of LIFT could not be shown because feature extraction is extremely slow and large-scale experiment is infeasible <ref type="bibr" target="#b0">1</ref> . DELF clearly outperforms all other techniques significantly. Global feature descriptors, such as DIR, suffer in our challenging dataset. In particular, due to a large number of distractors in the query set, DIR with QE degrades accuracy significantly. CONGAS does a reasonably good job, but is still worse than DELF with substantial margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Quantitative Results</head><p>To analyze the benefit of fine-tuning and attention for image retrieval, we compare our full model (DELF+FT+ATT) <ref type="bibr" target="#b0">1</ref> LIFT feature extraction approximately takes 2 min/image using a GPU. with its variations: DELF-noFT, DELF+FT and DELF-noFT+ATT. DELF-noFT means that extracted features are based on the pretrained CNN on ImageNet without finetuning and attention learning. DELF+FT denotes the model with fine-tuning but without attention modeling while DELF-noFT+ATT corresponds to the model without fine-tuning but using attention. As illustrated in <ref type="figure" target="#fig_6">Fig. 5</ref>, both fine-tuning and attention modeling make substantial contributions to performance improvement. In particular, note that the use of attention is more important than fine-tuning. This demonstrates that the proposed attention layers effectively learn to select the most discriminative features for the retrieval task, even if the features are simply pretrained on ImageNet.</p><p>In terms of memory requirement, DELF, CONGAS and DIR are almost equally complex. DELF and CONGAS adopt the same feature dimensionality and maximum number of features per image; they require approximately 8GB of memory. DIR descriptors need 8KB per image, summing up to approximately 8GB to index the entire dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Results</head><p>We present qualitative results to illustrate performance of DELF compared to two competitive algorithms based on global and local features-DIR and CONGAS, respectively. Also, we analyze our attention-based keypoint detection algorithm by visualization. <ref type="figure" target="#fig_7">Fig. 6</ref> shows retrieval results, where DELF outperforms DIR. DELF obtains matches between specific local regions in images, which helps significantly to find the same object in different imaging conditions. Common failure cases of DIR happen when the database contains similar objects or scenes, e.g., obelisks, mountains, harbors, as illustrated in <ref type="figure" target="#fig_7">Fig. 6</ref>. In many cases, DIR cannot distinguish these specific objects or scenes; although it finds semantically similar images, they often do not correspond to the instance of interest. Another weakness of DIR and other global descriptors is that they are not good at identifying small objects of interest. <ref type="figure" target="#fig_8">Fig. 7</ref> shows the cases that DIR outperforms DELF. While DELF is able to match localized patterns across different images, this leads to errors when the floor tiling or vegetation is similar across different landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DELF vs. DIR</head><p>DELF vs. CONGAS The main advantage of DELF over CONGAS is its recall; it retrieves more relevant landmarks than CONGAS, which suggests that DELF descriptors are more discriminative. We did not observe significant examples where CONGAS outperforms DELF. <ref type="figure">Fig. 8</ref> shows pairs of images from query and database, which are successfully matched by DELF but missed by CONGAS, where feature correspondences are presented by connecting the center of the receptive fields for matching features. Since the receptive fields can be fairly large, some features seem to be localized in undiscriminative regions, e.g., ocean or sky. However, in these cases, the features take into account more discriminative regions in the neighborhood. <ref type="figure" target="#fig_9">Fig. 9</ref> visualizes three variations of keypoint detection, where the benefit of our attention model is clearly illustrated qualitatively while the L 2 norm of fine-tuned features is marginally different from the one without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of keypoint detection methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Results in Existing Datasets</head><p>We demonstrate the performance of DELF in existing datasets such as Oxf5k, Par6k and their extensions, Oxf105k and Par106k, for completeness. For this experiment, we simply obtain the score per image using the proposed method, and make a late fusion with the score from DIR by computing a weighted mean of two normalized scores, where the weight for DELF is set to 0.25. The results are presented in Tab. 1. We present accuracy of existing methods in their original papers and our reproductions using public source codes, which are very close. DELF improves accuracy nontrivially in the datasets when combined with DIR, although it does not show the best performance by itself. This fact indicates that DELF has capability to encode complementary information that is not available in global feature descriptors. <ref type="figure">Figure 8</ref>: Visualization of feature correspondences between images in query and database using DELF+FT+ATT. For each pair, query and database images are presented side-by-side. DELF successfully matches landmarks and objects in challenging environment including partial occlusion, distracting objects, and background clutter. Both ends of the red lines denote the centers of matching features. Since the receptive fields are fairly large, the centers may be located outside landmark object areas. For the same queries, CONGAS fails to retrieve any image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented DELF, a new local feature descriptor that is designed specifically for large-scale image retrieval applications. DELF is learned with weak supervision, using image-level labels only, and is coupled with our new attention mechanism for semantic feature selection. In the proposed CNN-based model, one forward pass over the network is sufficient to obtain both keypoints and descriptors. To properly evaluate performance of large-scale image retrieval algorithm, we introduced Google-Landmarks dataset, which consists of more than 1M database images, 13K unique landmarks, and 100K query images. The evaluation in such a large-scale setting shows that DELF outperforms existing global and local descriptors by substantial margins. We also present results on existing datasets, and show that DELF achieves excellent performance when combined with global descriptors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall architecture of our image retrieval system, using DEep Local Features (DELF) and attention-based keypoint selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Example database and query images from Google-Landmarks. They have a lot of variations and challenges including background clutter, small objects, and multiple landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Image geolocation distribution of our Google-Landmarks dataset. The landmarks are located in 4,872 cities in 187 countries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Descriptor Fine-tuning (b) Attention-based Training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>The network architectures used for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Precision-recall curve for the large-scale retrieval experiment on the Google-Landmarks dataset, where recall is presented in absolute terms, as in Eq. (4). DELF shows outstanding performance compared with existing global and local features. Fine-tuning and attention model in DELF are critical to performance improvement. The accuracy of DIR drops significantly with query expansion, due to many distractor queries in our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Examples where DELF+FT+ATT outperforms DIR: (a) query image, (b) top-1 image of DELF+FT+ATT, (c) top-1 image of DIR. The green borders denote correct results while the red ones mean incorrect retrievals. Note that DELF deals with clutter in query and database images and small landmarks effectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Examples where DIR outperforms DELF+FT+ATT: (a) query image, (b) top-1 image of DELF+FT+ATT, (c) top-1 image of DIR. The green and red borders denotes correct and incorrect results, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Comparison of keypoint selection methods. (a) Input image (b) L2 norm scores using the pretrained model (DELF-noFT) (c) L2 norm scores using fine-tuned descriptors (DELF+FT) (d) Attention-based scores (DELF+FT+ATT). Our attention-based model effectively disregards clutter compared to other options.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance evaluation on existing datasets in mAP (%). All results of existing methods are based on our reproduction using public source codes. We tested LIFT only on Oxf5k and Par6k due to its slow speed. (* denotes the results from the original papers.)</figDesc><table><row><cell>Dataset</cell><cell cols="3">Oxf5k Oxf105k Par6k</cell><cell>Par106k</cell></row><row><cell>DIR [11]</cell><cell>86.1</cell><cell>82.8</cell><cell>94.5</cell><cell>90.6</cell></row><row><cell>DIR+QE [11]</cell><cell>87.1</cell><cell>85.2</cell><cell>95.3</cell><cell>91.8</cell></row><row><cell>siaMAC [29]</cell><cell>77.1</cell><cell>69.5</cell><cell>83.9</cell><cell>76.3</cell></row><row><cell>siaMAC+QE [29]</cell><cell>81.7</cell><cell>76.6</cell><cell>86.2</cell><cell>79.8</cell></row><row><cell>CONGAS [8]</cell><cell>70.8</cell><cell>61.1</cell><cell>67.1</cell><cell>56.8</cell></row><row><cell>LIFT [40]</cell><cell>54.0</cell><cell>-</cell><cell>53.6</cell><cell>-</cell></row><row><cell>DIR+QE* [11]</cell><cell>89.0</cell><cell>87.8</cell><cell>93.8</cell><cell>90.5</cell></row><row><cell>siaMAC+QE* [29]</cell><cell>82.9</cell><cell>77.9</cell><cell>85.6</cell><cell>78.3</cell></row><row><cell>DELF+FT+ATT (ours)</cell><cell>83.8</cell><cell>82.6</cell><cell>85.0</cell><cell>81.7</cell></row><row><cell cols="2">DELF+FT+ATT+DIR+QE (ours) 90.0</cell><cell>88.5</cell><cell>95.7</cell><cell>92.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was performed while the first and last authors were in Google Inc., CA. It was partly supported by the ICT R&amp;D program of MSIP/IITP [2016-0-00563] and the NRF grant [NRF-2011-0031648] in Korea.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video2text: Learning to Annotate Video Content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Data Mining Workshops</title>
		<meeting>IEEE International Conference on Data Mining Workshops</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN Architecture for Weakly Supervised Place Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aggregating Local Deep Features for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Codes for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
		</imprint>
	</monogr>
	<note>Speeded-Up Robust Features (SURF)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shape Indexing Using Approximate Nearest-Neighbour Search in High-Dimensional Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Beis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multidimensional Binary Search Trees Used for Associative Searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<idno>1975. 5</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Systems and Methods for Descriptor Vector Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Buddemeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Patent 8,098,938. 2, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating Second-Order Functional Knowledge for Better Option Pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Image Retrieval: Learning Global Representations for Image Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MatchNet: Unifying Feature and Metric Learning for Patch-Based Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Negative Evidences and Co-Occurences in Image Retrieval: The Benefit of PCA and Whitening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hamming Embedding and Weak Geometric Consistency for Large Scale Image Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Product Quantization for Nearest Neighbor Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aggregating Local Descriptors into a Compact Image Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aggregating Local Image Descriptors into Compact Codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Locally Optimized Product Quantization for Approximate Nearest Neighbor Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-Dimensional Weighting for Aggregated Deep Convolutional Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mellina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV Workshops</title>
		<meeting>ECCV Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Image Recognition with an Adiabatic Quantum Computer I. Mapping to Quadratic Unconstrained Binary Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0804.4457</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting Local Features from Deep Networks for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable Recognition with a Vocabulary Tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Family of Contextual Measures of Similarity between Distributions with Application to Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object Retrieval with Large Vocabularies and Fast Spatial Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lost in Quantization: Improving Particular Object Retrieval in Large Scale Image Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Im-ageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video Google: A Text Retrieval Approach to Object Matching in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Particular Object Retrieval with Integral Max-Pooling of CNN Activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual Place Recognition with Repetitive Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fisher Encoded Convolutional Bag-of-Windows for Efficient Image Retrieval and Social Image Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshops</title>
		<meeting>ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">TILDE: A Temporally Invariant Learned Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked Attention Networks for Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LIFT: Learned Invariant Feature Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to Assign Orientations to Feature Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to Compare Image Patches via Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01807</idno>
		<title level="m">SIFT Meets CNN: A Decade Survey of Instance Retrieval</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tour the World: Building a Web-Scale Landmark Recognition Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Buddemeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
