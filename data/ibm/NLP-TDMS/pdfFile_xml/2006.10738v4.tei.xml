<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentiable Augmentation for Data-Efficient GAN Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IIIS</orgName>
								<orgName type="institution" key="instit2">Tsinghua University and MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><forename type="middle">Zhu</forename><surname>Adobe</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Cmu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Differentiable Augmentation for Data-Efficient GAN Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128×128 and 2-4× reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore, with only 20% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at https://github.com/mit-han-lab/data-efficient-gans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Big data has enabled deep learning algorithms achieve rapid advancements. In particular, stateof-the-art generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11]</ref> are able to generate high-fidelity natural images of diverse categories <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>. Many computer vision and graphics applications have been enabled <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54</ref>]. However, this success comes at the cost of a tremendous amount of computation and data. Recently, researchers have proposed promising techniques to improve the computational efficiency of model inference <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, while the data efficiency remains to be a fundamental challenge.</p><p>GANs heavily rely on vast quantities of diverse and high-quality training examples. To name a few, the FFHQ dataset [17] contains 70,000 selective post-processed high-resolution images of human faces; the ImageNet dataset [6] annotates more than a million of images with various object categories. Collecting such large-scale datasets requires months or even years of considerable human efforts along with prohibitive annotation costs. In some cases, it is not even possible to have that many examples, e.g., images of rare species or photos of a specific person or landmark. Thus, it is of critical importance to eliminate the need of immense datasets for GAN training. However, reducing the amount of training data results in drastic degradation in the performance. For example in <ref type="figure">Figure 1</ref>, given only 10% or 20% of the CIFAR-10 data, the training accuracy of the discriminator saturates quickly (to nearly 100%); however, its validation accuracy keeps decreasing (to lower than 30%), suggesting that the discriminator is simply memorizing the entire training set. This severe over-fitting problem disrupts the training dynamics and leads to degraded image quality.</p><p>A widely-used strategy to reduce overfitting in image classification is data augmentation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>, which can increase the diversity of training data without collecting new samples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: BigGAN heavily deteriorates given a limited amount of data. left: With 10% of CIFAR-10 data, FID increases shortly after the training starts, and the model then collapses (red curve). middle: the training accuracy of the discriminator D quickly saturates. right: the validation accuracy of D dramatically falls, indicating that D has memorized the exact training set and fails to generalize.  <ref type="figure">Figure 2</ref>: Unconditional generation results on CIFAR-10. StyleGAN2's performance drastically degrades given less training data. With DiffAugment, we are able to roughly match its FID and outperform its Inception Score (IS) using only 20% training data. FID and IS are measured using 10k samples; the validation set is used as the reference distribution for FID calculation.</p><p>as cropping, flipping, scaling, color jittering <ref type="bibr" target="#b19">[20]</ref>, and region masking (Cutout) <ref type="bibr" target="#b7">[8]</ref> are commonly-used augmentations for vision models. However, applying data augmentation to GANs is fundamentally different. If the transformation is only added to the real images, the generator would be encouraged to match the distribution of the augmented images. As a consequence, the outputs suffer from distribution shift and the introduced artifacts (e.g., a region being masked, unnatural color, see <ref type="figure">Figure 5a</ref>). Alternatively, we can augment both the real and generated images when training the discriminator; however, this would break the subtle balance between the generator and discriminator, leading to poor convergence as they are optimizing completely different objectives (see <ref type="figure">Figure 5b</ref>).</p><p>To combat it, we introduce a simple but effective method, DiffAugment, which applies the same differentiable augmentation to both real and fake images for both generator and discriminator training. It enables the gradients to be propagated through the augmentation back to the generator, regularizes the discriminator without manipulating the target distribution, and maintains the balance of training dynamics. Experiments on a variety of GAN architectures and datasets consistently demonstrate the effectiveness of our method. With DiffAugment, we improve BigGAN and achieve a Fréchet Inception Distance (FID) of 6.80 with an Inception Score (IS) of 100.8 on ImageNet 128×128 without the truncation trick <ref type="bibr" target="#b1">[2]</ref> and reduce the StyleGAN2 baseline's FID by 2-4× given 1,000 images on the FFHQ and LSUN datasets. We also match the top performance on CIFAR-10 and CIFAR-100 using only 20% training data (see <ref type="figure">Figure 2</ref> and <ref type="figure">Figure 10</ref>). Furthermore, our method can generate high-quality images with only 100 examples (see <ref type="figure">Figure 3</ref>). Without any pre-training, we achieve competitive performance with existing transfer learning algorithms that used to require tens of thousands of training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generative Adversarial Networks. Following the pioneering work of GAN <ref type="bibr" target="#b10">[11]</ref>, researchers have explored different ways to improve its performance and training stability. Recent efforts are centered on more stable objective functions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>, more advanced architectures <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">48]</ref>,</p><formula xml:id="formula_0">StyleGAN2 (baseline)</formula><p>StyleGAN2 + DiffAugment (ours) and better training strategy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49]</ref>. As a result, both the visual fidelity and diversity of generated images have increased significantly. For example, BigGAN <ref type="bibr" target="#b1">[2]</ref> is able to synthesize natural images with a wide range of object classes at high resolution, and StyleGAN <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> can produce photorealistic face portraits with large varieties, often indistinguishable from natural photos. However, the above work paid less attention to the data efficiency aspect. A recent attempt <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref> leverages semi-and self-supervised learning to reduce the amount of human annotation required for training.</p><p>In this paper, we study a more challenging scenario where both data and labels are limited.</p><p>Regularization for GANs. GAN training often requires additional regularization as they are highly unstable. To stabilize the training dynamics, researchers have proposed several techniques including the instance noise <ref type="bibr" target="#b38">[39]</ref>, Jensen-Shannon regularization <ref type="bibr" target="#b33">[34]</ref>, gradient penalties <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref>, spectral normalization <ref type="bibr" target="#b27">[28]</ref>, adversarial defense regularization <ref type="bibr" target="#b52">[53]</ref>, and consistency regularization <ref type="bibr" target="#b49">[50]</ref>. All of these regularization techniques implicitly or explicitly penalize sudden changes in the discriminator's output within a local region of the input. In this paper, we provide a different perspective, data augmentation, and we encourage the discriminator to perform well under different types of augmentation. In Section 4, we show that our method is complementary to the regularization techniques in practice.</p><p>Data Augmentation. Many deep learning models adopt label-preserving transformations to reduce overfitting: e.g., color jittering <ref type="bibr" target="#b19">[20]</ref>, region masking <ref type="bibr" target="#b7">[8]</ref>, flipping, rotation, cropping <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref>, data mixing <ref type="bibr" target="#b46">[47]</ref>, and local and affine distortion <ref type="bibr" target="#b37">[38]</ref>. Recently, AutoML <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b54">55]</ref> has been used to explore adaptive augmentation policies for a given dataset and task <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">23]</ref>. However, applying data augmentation to generative models, such as GANs, remains an open question. Different from the classifier training where the label is invariant to transformations of the input, the goal of generative models is to learn the data distribution itself. Directly applying augmentation would inevitably alter the distribution. We present a simple strategy to circumvent the above concern. Concurrent with our work, several methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">52]</ref> independently proposed data augmentation for training GANs. We urge the readers to check out their work for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Generative adversarial networks (GANs) aim to model the distribution of a target dataset via a generator G and a discriminator D. The generator G maps an input latent vector z, typically drawn from a Gaussian distribution, to its output G(z). The discriminator D learns to distinguish generated samples G(z) from real observations x. The standard GANs training algorithm alternately optimizes the discriminator's loss L D and the generator's loss L G given loss functions f D and f G :  <ref type="table">Table 1</ref>: DiffAugment vs. vanilla augmentation strategies on CIFAR-10 with 100% training data. "Augment reals only" applies augmentation T to (i) only (see <ref type="figure">Figure 4</ref>) and corresponds to Equations (3)-(4); "Augment D only" applies T to both reals (i) and fakes (ii), but not G (iii), and corresponds to Equations (5)-(6); "DiffAugment" applies T to reals (i), fakes (ii), and G (iii).</p><formula xml:id="formula_1">L D = Ex∼p data (x) [f D (−D(x))] + Ez∼p(z)[fD(D(G(z)))],<label>(1)</label></formula><formula xml:id="formula_2">L G = Ez∼p(z)[fG(−D(G(z)))].<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>z G D D(T(x)) vs. D(T(G(z))) z G D D(T(G(z))) x T(x) T(G(z)) T(G(z))</head><p>(iii) requires T to be differentiable since gradients should be back-propagated through T to G. DiffAugment corresponds to Equations (7)- <ref type="bibr" target="#b7">(8)</ref>. IS and FID are measured using 10k samples; the validation set is the reference distribution. We select the snapshot with the best FID for each method.</p><p>Results are averaged over 5 evaluation runs; all standard deviations are less than 1% relatively.</p><p>Here, different loss functions can be used, such as the non-saturating loss <ref type="bibr" target="#b10">[11]</ref>, where f D (x) = f G (x) = log (1 + e x ), and the hinge loss <ref type="bibr" target="#b27">[28]</ref>, where f D (x) = max(0, 1 + x) and f G (x) = x.</p><p>Despite extensive ongoing efforts of better GAN architectures and loss functions, a fundamental challenge still exists: the discriminator tends to memorize the observations as the training progresses. An overfitted discriminator penalizes any generated samples other than the exact training data points, provides uninformative gradients due to poor generalization, and usually leads to training instability.</p><p>Challenge: Discriminator Overfitting. Here we analyze the performance of BigGAN <ref type="bibr" target="#b1">[2]</ref> with different amounts of data on CIFAR-10. As plotted in <ref type="figure">Figure 1</ref>, even given 100% data, the gap between the discriminator's training and validation accuracy keeps increasing, suggesting that the discriminator is simply memorizing the training images. This happens not only on limited data but also on the large-scale ImageNet dataset, as observed by Brock et al. <ref type="bibr" target="#b1">[2]</ref>. BigGAN already adopts Spectral Normalization <ref type="bibr" target="#b27">[28]</ref>, a widely-used regularization technique for both generator and discriminator architectures, but still suffers from severe overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisiting Data Augmentation</head><p>Data augmentation is a commonly-used strategy to reduce overfitting in many recognition tasks -it has an irreplaceable role and can also be applied in conjunction with other regularization techniques: e.g., weight decay. We have shown that the discriminator suffers from a similar overfitting problem as the binary classifier. However, data augmentation is seldom used in the GAN literature compared to the explicit regularizations on the discriminator <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. In fact, a recent work <ref type="bibr" target="#b49">[50]</ref> observes that directly applying data augmentation to GANs does not improve the baseline. So, we would like to ask the questions: what prevents us from simply applying data augmentation to GANs? Why is augmenting GANs not as effective as augmenting classifiers?</p><p>Augment reals only. The most straightforward way of augmenting GANs would be directly applying augmentation T to the real observations x, which we call "Augment reals only":</p><formula xml:id="formula_3">L D = Ex∼p data(x) [f D (−D(T (x)))] + Ez∼p(z)[fD(D(G(z)))],<label>(3)</label></formula><formula xml:id="formula_4">L G = Ez∼p(z)[fG(−D(G(z)))].</formula><p>(4) However, "Augment reals only" deviates from the original purpose of generative modeling, as the model is now learning a different data distribution of T (x) instead of x. This prevents us from Transla�on + Cutout Transla�on Color + Transla�on + Cutout (a) "Augment reals only": the same augmentation artifacts appear on the generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High accuracy on T(x) and T(G(z))</head><p>Low accuracy on G(z) (b) "Augment D only": the unbalanced optimization between G and D cripples training. <ref type="figure">Figure 5</ref>: Understanding why vanilla augmentation strategies fail: (a) "Augment reals only" mimics the same data distortion as introduced by the augmentations, e.g., the translation padding, the Cutout square, and the color artifacts; (b) "Augment D only" diverges because of the unbalanced optimization -D perfectly classifies the augmented images (both T (x) and T (G(z)) but barely recognizes G(z) (i.e., fake images without augmentation) from which G receives gradients.</p><p>applying any augmentation that significantly alters the distribution of the real images. The choices that meet this requirement, although strongly dependent on the specific dataset, can only be horizontal flips in most cases. We find that applying random horizontal flips does increase the performance moderately, and we use it in all our experiments to make our baselines stronger. We demonstrate the side effects of enforcing stronger augmentations quantitatively in <ref type="table">Table 1</ref> and qualitatively in <ref type="figure">Figure 5a</ref>. As expected, the model learns to produce unwanted color and geometric distortion (e.g., unnatural color, cutout holes) as introduced by these augmentations, resulting in a significantly worse performance (see "Augment reals only" in <ref type="table">Table 1</ref>).</p><p>Augment D only. Previously, "Augment reals only" applies one-sided augmentation to the real samples, and hence the convergence can be achieved only if the generated distribution matches the manipulated real distribution. From the discriminator's perspective, it may be tempting to augment both real and fake samples when we update D:</p><formula xml:id="formula_5">L D = Ex∼p data (x) [f D (−D(T (x)))] + Ez∼p(z)[fD(D(T (G(z))))],<label>(5)</label></formula><formula xml:id="formula_6">L G = Ez∼p(z)[fG(−D(G(z)))].<label>(6)</label></formula><p>Here, the same function T is applied to both real samples x and fake samples G(z). If the generator successfully models the distribution of x, T (G(z)) and T (x) should be indistinguishable to the discriminator as well as G(z) and x. However, this strategy leads to even worse results (see "Augment D only" in <ref type="table">Table 1</ref>). <ref type="figure">Figure 5b</ref> plots the training dynamics of "Augment D only" with Translation applied. Although D classifies the augmented images (both T (G(z)) and T (x)) perfectly with an accuracy of above 90%, it fails to recognize G(z), the generated images without augmentation, with an accuracy of lower than 10%. As a result, the generator completely fools the discriminator by G(z) and cannot obtain useful information from the discriminator. This suggests that any attempts that break the delicate balance between the generator G and discriminator D are prone to failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Differentiable Augmentation for GANs</head><p>The failure of "Augment reals only" motivates us to augment both real and fake samples, while the failure of "Augment D only" warns us that the generator should not neglect the augmented samples. Therefore, to propagate gradients through the augmented samples to G, the augmentation T must be differentiable as depicted in <ref type="figure">Figure 4</ref>. We call this Differentiable Augmentation (DiffAugment):</p><formula xml:id="formula_7">L D = Ex∼p data (x)[f D (−D(T (x)))] + Ez∼p(z)[fD(D(T (G(z))))],<label>(7)</label></formula><formula xml:id="formula_8">L G = Ez∼p(z)[fG(−D(T (G(z))))].<label>(8)</label></formula><p>Note that T is required to be the same (random) function but not necessarily the same random seed across the three places illustrated in <ref type="figure">Figure 4</ref>. We demonstrate the effectiveness of DiffAugment using three simple choices of transformations and its composition, throughout the paper: Translation   <ref type="table">Table 2</ref>: ImageNet 128×128 results without the truncation trick <ref type="bibr" target="#b1">[2]</ref>. IS and FID are measured using 50k samples; the validation set is used as the reference distribution for FID. We select the snapshot with the best FID for each method. We report means and standard deviations over 3 evaluation runs.  <ref type="table">Table 1</ref>, BigGAN can be improved using the simple Translation policy and further boosted using a composition of Cutout and Translation; it is also robust to the strongest policy when Color is used in combined. <ref type="figure">Figure 6</ref> analyzes that stronger DiffAugment policies generally maintain a higher discriminator's validation accuracy at the cost of a lower training accuracy, alleviate the overfitting problem, and eventually achieve better convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments on ImageNet <ref type="bibr" target="#b5">[6]</ref>, CIFAR-10 <ref type="bibr" target="#b18">[19]</ref>, CIFAR-100, FFHQ <ref type="bibr" target="#b16">[17]</ref>, and LSUN-Cat <ref type="bibr" target="#b45">[46]</ref> based on the leading class-conditional BigGAN <ref type="bibr" target="#b1">[2]</ref> and unconditional Style-GAN2 <ref type="bibr" target="#b17">[18]</ref>. We use the common evaluation metrics Fréchet Inception Distance (FID) <ref type="bibr" target="#b12">[13]</ref> and Inception Score (IS) <ref type="bibr" target="#b34">[35]</ref>. In addition, we apply our method to low-shot generation both with and without pre-training in Section 4.4. Finally, we perform analysis studies in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ImageNet</head><p>We follow the top-performing model BigGAN <ref type="bibr" target="#b1">[2]</ref> on ImageNet dataset at 128×128 resolution. Additionally, we augment real images with random horizontal flips, yielding the best reimplementation of BigGAN to our knowledge (FID: ours 7.6 vs. 8.7 in the original paper <ref type="bibr" target="#b1">[2]</ref>). We use the simple Translation DiffAugment for all the data percentage settings. In <ref type="table">Table 2</ref>, our method achieves significant gains especially under the 25% data setting, in which the baseline model undergoes an early collapse, and advances the state-of-the-art FID and IS with 100% data available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">FFHQ and LSUN-Cat</head><p>We further experiment with StyleGAN2 <ref type="bibr" target="#b17">[18]</ref> on the FFHQ portrait dataset <ref type="bibr" target="#b16">[17]</ref> and the LSUN-Cat dataset <ref type="bibr" target="#b45">[46]</ref> at 256×256 resolution. We investigate different limited data settings, with 1k, 5k, 10k, and 30k training images available. We apply the strongest Color + Translation + Cutout DiffAugment to all the StyleGAN2 baselines without any hyperparameter changes. The real images are also augmented with random horizontal flips as commonly applied in StyleGAN2 <ref type="bibr" target="#b17">[18]</ref>. Results are shown in <ref type="table" target="#tab_6">Table 3</ref>. Our performance gains are considerable under all the data percentage settings. Moreover, with the fixed policies used in DiffAugment, our performance is on par with ADA <ref type="bibr" target="#b15">[16]</ref>, a concurrent work based on the adaptive augmentation strategy.   <ref type="table">Table 4</ref>: CIFAR-10 and CIFAR-100 results. We select the snapshot with the best FID for each method. Results are averaged over 5 evaluation runs; all standard deviations are less than 1% relatively. We use 10k samples and the validation set as the reference distribution for FID calculation, as done in prior work <ref type="bibr" target="#b49">[50]</ref>. Concurrent works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CIFAR-10 and CIFAR-100</head><p>We experiment on the class-conditional BigGAN <ref type="bibr" target="#b1">[2]</ref> and CR-BigGAN <ref type="bibr" target="#b49">[50]</ref> and unconditional StyleGAN2 <ref type="bibr" target="#b17">[18]</ref> models. For a fair comparison, we also augment real images with random horizontal flips for all the baselines. The baseline models already adopt advanced regularization techniques, including Spectral Normalization <ref type="bibr" target="#b27">[28]</ref>, Consistency Regularization <ref type="bibr" target="#b49">[50]</ref>, and R 1 regularization <ref type="bibr" target="#b26">[27]</ref>; however, none of them achieves satisfactory results under the 10% data setting. For DiffAugment, we adopt Translation + Cutout for the BigGAN models, Color + Cutout for StyleGAN2 with 100% data, and Color + Translation + Cutout for StyleGAN2 with 10% or 20% data. As summarized in <ref type="table">Table 4</ref>, our method improves all the baselines independently of the baseline architectures, regularizations, and loss functions (hinge loss in BigGAN and non-saturating loss in StyleGAN2) without any hyperparameter changes. We refer the readers to the appendix <ref type="table" target="#tab_11">(Tables 6-7)</ref> for the complete tables with IS. The improvements are considerable especially when limited data is available. This is, to our knowledge, the new state of the art on CIFAR-10 and CIFAR-100 for both class-conditional and unconditional generation under all the 10%, 20%, and 100% data settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Low-Shot Generation</head><p>For a certain person, an object, or a landmark, it is often tedious, if not completely impossible, to collect a large-scale dataset. To address this, researchers recently exploit few-shot learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref> in the setting of image generation. Wang et al. <ref type="bibr" target="#b44">[45]</ref> use fine-tuning to transfer the knowledge of models pre-trained on external large-scale datasets. Several works propose to fine-tune only part of the model <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>. Below, we show that our method not only produces competitive results without using external datasets or models but also is orthogonal to the existing transfer learning methods.</p><p>We replicate the recent transfer learning algorithms <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>   <ref type="table">Table 5</ref>: Low-shot generation results. With only 100 (Obama, Grumpy cat, Panda), 160 (Cat), or 389 (Dog) training images, our method is on par with the transfer learning algorithms that are pre-trained with 70,000 images. FID is measured using 5k generated samples; the training set is the reference distribution. We select the snapshot with the best FID for each method. <ref type="figure">Figure 7</ref>: Style space interpolation of our method for low-shot generation without pre-training. The smooth interpolation results suggest little overfitting of our method even given small datasets.</p><p>we collect the 100-shot Obama, grumpy cat, and panda datasets, and train the StyleGAN2 model on each dataset using only 100 images without pre-training. For DiffAugment, we adopt Color + Translation + Cutout for StyleGAN2, Color + Cutout for both the vanilla fine-tuning algorithm TransferGAN <ref type="bibr" target="#b44">[45]</ref> and FreezeD <ref type="bibr" target="#b29">[30]</ref> that freezes the first several layers of the discriminator. <ref type="table">Table 5</ref> shows that DiffAugment achieves consistent gains independently of the training algorithm on all the datasets. Without any pre-training, we still achieve results on par with the existing transfer learning algorithms that require tens of thousands of images, with an exception on the 100-shot Obama dataset where pre-training with human faces clearly leads to better generalization. See <ref type="figure">Figure 3</ref> and the appendix <ref type="figure" target="#fig_3">(Figures 18-22</ref>) for qualitative comparisons. While there might be a concern that the generator is likely to overfit the tiny datasets (i.e., generating identical training images), <ref type="figure">Figure 7</ref> suggests little overfitting of our method via linear interpolation in the style space <ref type="bibr" target="#b16">[17]</ref> (see also <ref type="figure">Figure 15</ref> in the appendix); please refer to the appendix <ref type="figure">(Figures 16-17</ref>) for the nearest neighbor tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head><p>Below, we investigate whether smaller model or stronger regularization would similarly reduce overfitting and whether DiffAugment still helps. Finally, we analyze additional choices of DiffAugment.</p><p>(a) Impact of model size.</p><p>(b) Impact of R1 regularization γ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Size Matters?</head><p>We reduce the model capacity of BigGAN by progressively halving the number of channels for both G and D. As plotted in <ref type="figure" target="#fig_3">Figure 8a</ref>, the baseline heavily overfits on CIFAR-10 with 10% training data when using the full model and achieves a minimum FID of 29.02 at 1/4 channels. However, it is surpassed by our method over all model capacities. With 1/4 channels, our model achieves a significantly better FID of 21.57, while the gap is monotonically increasing as the model becomes larger. We refer the readers to the appendix <ref type="figure">(Figure 11</ref>) for the IS plot.</p><p>Stronger Regularization Matters? As StyleGAN2 adopts the R 1 regularization <ref type="bibr" target="#b26">[27]</ref> to stabilize training, we increase its strength from γ = 0.1 to up to 10 4 and plot the FID curves in <ref type="figure" target="#fig_3">Figure 8b</ref>. While we initially find that γ = 0.1 works best under the 100% data setting, the choice of γ = 10 3 boosts its performance from 34.05 to 26.87 under the 10% data setting. When γ = 10 4 , within 750k iterations, we only observe a minimum FID of 29.14 at 440k iteration and the performance deteriorates after that. However, its best FID is still 1.8× worse than ours (with the default γ = 0.1). This shows that DiffAugment is more effective compared to explicitly regularizing the discriminator.  . While all these policies consistently outperform the baseline, we find that the Color + Translation + Cutout DiffAugment is especially effective. The simplicity also makes it easier to deploy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present DiffAugment for data-efficient GAN training. DiffAugment reveals valuable observations that augmenting both real and fake samples effectively prevents the discriminator from over-fitting, and that the augmentation must be differentiable to enable both generator and discriminator training. Extensive experiments consistently demonstrate its benefits with different network architectures (StyleGAN2 and BigGAN), supervision settings, and objective functions, across multiple datasets (ImageNet, CIFAR, FFHQ, LSUN, and 100-shot datasets). Our method is especially effective when limited data is available. Our code, datasets, and models are available for future comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>In this paper, we investigate GANs from the data efficiency perspective, aiming to make generative modeling accessible to more people (e.g., visual artists and novice users) and research fields who have no access to abundant data. In the real-world scenarios, there could be various reasons that lead to limited amount of data available, such as rare incidents, privacy concerns, and historical visual data <ref type="bibr" target="#b9">[10]</ref>. DiffAugment provides a promising way to alleviate the above issues and make AI more accessible to everyone.  <ref type="figure">Figure 10</ref>: Unconditional generation results on CIFAR-100. We are able to roughly match Style-GAN2's FID and outperform its IS using only 20% training data. <ref type="figure">Figure 11</ref>: Analysis of smaller models or stronger regularization on CIFAR-10 with 10% training data. left: Smaller models reduce overfitting for the BigGAN baseline, while our method outperforms it at all the model capacities. right: Over a wide sweep of the R 1 regularization γ for the baseline StyleGAN2, its best IS (7.75) is still 12% worse than ours <ref type="bibr">(8.84</ref>   <ref type="table">Table 7</ref>: CIFAR-100 results. IS and FID are measured using 10k samples; the validation set is the reference distribution for FID calculation. We select the snapshot with the best FID for each method. Results are averaged over 5 evaluation runs; all standard deviations are less than 1% relatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 FFHQ and LSUN-Cat Experiments</head><p>We use the official TensorFlow implementation of StyleGAN2 † and the default network configuration at 256×256 resolution with an R 1 regularization γ of 1, but without the path length regularization and the lazy regularization since they do not improve FID <ref type="bibr" target="#b17">[18]</ref>. The number of feature maps at shallow layers (64×64 resolution and above) is halved to match the architecture of ADA <ref type="bibr" target="#b15">[16]</ref>. All the models in our experiments are augmented with random horizontal flips, trained on 8 GPUs with a maximum training length of 25,000k images.</p><p>See <ref type="figure">Figure 13</ref>-14 for qualitative comparisons between StyleGAN2 and StyleGAN2 + DiffAugment. Our method considerably improves the image quality with limited data available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 CIFAR-10 and CIFAR-100 Experiments</head><p>We replicate BigGAN and CR-BigGAN baselines on CIFAR using the PyTorch implementation ‡ . All hyperparameters are kept unchanged from the default CIFAR-10 configuration, including the batch size <ref type="bibr" target="#b49">(50)</ref>, the number of D steps (4) per G step, and a learning rate of 2 × 10 −4 for both G and D.</p><p>The hyperparameter λ of Consistency Regularization (CR) is set to 10 as recommended <ref type="bibr" target="#b49">[50]</ref>. All the models are run on 2 GPUs with a maximum of 250k training iterations on CIFAR-10 and 500k iterations on CIFAR-100.</p><p>For StyleGAN2, we use the official TensorFlow implementation § but include some changes to make it work better on CIFAR. The number of channels is 128 at 32×32 resolution and doubled at each coarser level with a maximum of 512 channels. We set the half-life of the exponential moving average of the generator's weights to 1, 000k instead of 10k images since it stabilizes the FID curve and leads to consistently better performance. We set γ = 0.1 instead of 10 for the R 1 regularization, which significantly improves the baseline's performance under the 100% data setting on CIFAR. The path length regularization and the lazy regularization are also disabled. The baseline model can already achieve the best FID and IS to our knowledge for unconditional generation on the CIFAR datasets. All StyleGAN2 models are trained on 4 GPUs with the default batch size (32) and a maximum training length of 25,000k images.</p><p>We apply DiffAugment to BigGAN, CR-BigGAN, and StyleGAN2 without changes to the baseline settings. There are several things to note when applying DiffAugment in conjunction with gradient penalties <ref type="bibr" target="#b11">[12]</ref> or CR <ref type="bibr" target="#b49">[50]</ref>. The R 1 regularization penalizes the gradients of D(x) w.r.t. the input x.</p><p>With DiffAugment, the gradients of D(T (x)) can be calculated w.r.t. either x or T (x). We choose to penalize the gradients of D(T (x)) w.r.t. T (x) for the CIFAR, FFHQ, and LSUN experiments since it slightly outperforms the other choice in practice; for the low-shot generation experiments, we penalize the gradients of D(T (x)) w.r.t. x instead from which we observe better diversity of the generated images. As CR has already used image translation to calculate the consistency loss, we only apply Cutout DiffAugment on top of CR under the 100% data setting. For the 10% and 20% data settings, we exploit stronger regularization by directly applying CR between x and T (x), i.e., before and after the Translation + Cutout DiffAugment.</p><p>We match the top performance for unconditional generation on CIFAR-100 as well as CIFAR-10 using only 20% data (see <ref type="figure">Figure 10</ref>). See <ref type="figure">Figure 11</ref> for the analysis of smaller models or stronger regularization in terms of IS. See <ref type="table" target="#tab_11">Table 6</ref> and <ref type="table">Table 7</ref> for quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Low-Shot Generation Experiments</head><p>We compare our method to transfer learning algorithms using the FreezeD's codebase ¶ (for Transfer-GAN <ref type="bibr" target="#b44">[45]</ref>, Scale/shift <ref type="bibr" target="#b30">[31]</ref>, and FreezeD <ref type="bibr" target="#b29">[30]</ref>) and the newly released MineGAN <ref type="bibr" target="#b43">[44]</ref>  When training the StyleGAN2 model from scratch, we use their default network configuration at 256×256 resolution with an R 1 regularization γ of 10 but without the path length regularization and the lazy regularization. We use a smaller batch size of 16, which improves the performance of both the StyleGAN2 baseline and ours, compared to the default batch size of 32. All the models are trained on 4 GPUs with a maximum training length of 300k images on our 100-shot datasets and 500k images on the AnimalFace datasets.</p><p>See <ref type="figure">Figure 15</ref> for the additional interpolation results, <ref type="figure">Figure 16</ref> and <ref type="figure">Figure 17</ref> for the nearest neighbor tests of our method without pre-training both in pixel space and in the LPIPIS feature space <ref type="bibr" target="#b50">[51]</ref>. See <ref type="figure" target="#fig_3">Figures 18-22</ref>   <ref type="figure">Figure 14</ref>: Qualitative comparison on LSUN-cat at 256×256 resolution with 1k, 5k, 10k, and 30k training images. Our method consistently outperforms the StyleGAN2 baselines <ref type="bibr" target="#b17">[18]</ref> under different data percentage settings. <ref type="figure">Figure 15</ref>: Style space interpolation of our method on the 100-shot Obama, grumpy cat, panda, the Bridge of Sighs, the Medici Fountain, and the Temple of Heaven datasets without pre-training. The smooth interpolation results suggest little overfitting of our method even given small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Top-3 nearest neighbors Query Top-3 nearest neighbors <ref type="figure">Figure 16</ref>: Nearest neighbors in pixel space measured by the pixel-wise L 1 distance. Each query (on the left of the dashed lines) is a generated image of our method without pre-training (StyleGAN2 + DiffAugment) on the 100-shot or AnimalFace generation datasets. Each nearest neighbor (on the right of the dashed lines) is an original image queried from the training set with horizontal flips. The generated images are different from the training set, indicating that our model does not simply memorize the training images or overfit even given small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Top-3 nearest neighbors Query Top-3 nearest neighbors <ref type="figure">Figure 17</ref>: Nearest neighbors in feature space measured by the Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b50">[51]</ref>. Each query (on the left of the dashed lines) is a generated image of our method without pre-training (StyleGAN2 + DiffAugment) on the 100-shot or AnimalFace generation datasets. Each nearest neighbor (on the right of the dashed lines) is an original image queried from the training set with horizontal flips. The generated images are different from the training set, indicating that our model does not simply memorize the training images or overfit even given small datasets. pre-trained with 70,000 images only 160 images <ref type="figure" target="#fig_3">Figure 18</ref>: Qualitative comparison on the AnimalFace-cat <ref type="bibr" target="#b36">[37]</ref> dataset. pre-trained with 70,000 images only 389 images <ref type="figure" target="#fig_4">Figure 19</ref>: Qualitative comparison on the AnimalFace-dog <ref type="bibr" target="#b36">[37]</ref> dataset.   pre-trained with 70,000 images only 100 images <ref type="figure">Figure 21</ref>: Qualitative comparison on the 100-shot grumpy cat dataset.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(within [−1/8, 1/8] of the image size, padded with zeros), Cutout [8] (masking with a random square of half image size), and Color (including random brightness within [−0.5, 0.5], contrast within [0.5, 1.5], and saturation within [0, 2]). As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>use a different protocol: 50k samples and the training set as the reference distribution. If we adopt this evaluation protocol, our BigGAN + DiffAugment achieves an FID of 4.61, CR-BigGAN + DiffAugment achieves an FID of 4.30, and StyleGAN2 + DiffAugment achieves an FID of 5.79.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Analysis of smaller models or stronger regularization on CIFAR-10 with 10% training data. (a) Smaller models reduce overfitting for the BigGAN baseline, while our method dominates its performance at all model capacities. (b) Over a wide sweep of the R 1 regularization γ for the baseline StyleGAN2, its best FID (26.87) is still much worse than ours(14.50).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Various types of DiffAugment consistently outperform the baseline. We report Style-GAN2's FID on CIFAR-10 with 10% training data. Choice of DiffAugment Matters? We investigate additional choices of DiffAugment in Figure 9, including random 90°rotations ({−90°, 0°, 90°} each with 1/3 probability), Gaussian noise (with a standard deviation of 0.1), and general geometry transformations that involve bilinear interpolation, such as bilinear translation (within [−0.25, 0.25]), bilinear scaling (within [0.75, 1.25]), bilinear rotation (within [−30°, 30°]), and bilinear shearing (within [−0.25, 0.25])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 20 :</head><label>20</label><figDesc>Qualitative comparison on the 100-shot Obama dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 22 :</head><label>22</label><figDesc>Qualitative comparison on the 100-shot panda dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Overview of DiffAugment for updating D (left) and G (right). DiffAugment applies the augmentation T to both the real samples x and the generated output G(z). When we update G, gradients need to be back-propagated through T , which requires T to be differentiable w.r.t. the input.</figDesc><table><row><cell></cell><cell>update</cell><cell></cell><cell>update</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(i)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(ii)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(ⅲ)</cell><cell></cell></row><row><cell>Figure 4: Method</cell><cell>Where T ?</cell><cell cols="4">Color + Transl. + Cutout Transl. + Cutout</cell><cell cols="2">Translation</cell></row><row><cell></cell><cell>(i) (ii) (iii)</cell><cell>IS</cell><cell>FID</cell><cell>IS</cell><cell>FID</cell><cell>IS</cell><cell>FID</cell></row><row><cell>BigGAN (baseline)</cell><cell></cell><cell>9.06</cell><cell>9.59</cell><cell>9.06</cell><cell>9.59</cell><cell>9.06</cell><cell>9.59</cell></row><row><cell>Aug. reals only</cell><cell></cell><cell>5.94</cell><cell>49.38</cell><cell>6.51</cell><cell>37.95</cell><cell>8.40</cell><cell>19.16</cell></row><row><cell>Aug. reals + fakes (D only)</cell><cell></cell><cell>3.00</cell><cell>126.96</cell><cell>3.76</cell><cell>114.14</cell><cell cols="2">3.50 100.13</cell></row><row><cell>DiffAugment (D + G, ours)</cell><cell></cell><cell>9.25</cell><cell>8.59</cell><cell>9.16</cell><cell>8.70</cell><cell>9.07</cell><cell>9.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Analysis of different types of DiffAugment on CIFAR-10 with 100% training data. A stronger DiffAugment can dramatically reduce the gap between the discriminator's training accuracy (middle) and validation accuracy (right), leading to a better convergence (left). ± 0.4 25.37 ± 0.07 + DiffAugment 100.8 ± 0.2 6.80 ± 0.02 91.9 ± 0.5 8.88 ± 0.06 74.2 ± 0.5 13.28 ± 0.07</figDesc><table><row><cell cols="2">BigGAN (baseline) DiffAugment (Trans. + Cutout) DiffAugment (Color + Trans. + Cutout) DiffAugment (Translation)</cell><cell>0.8 0.9 1.0</cell><cell></cell><cell cols="3">D's Training Accuracy</cell><cell>0.6 0.8 1.0</cell><cell>D's Validation Accuracy</cell></row><row><cell></cell><cell></cell><cell>0.5 0.6 0.7</cell><cell>0</cell><cell cols="3">50 BigGAN (baseline) 100 150 200 250 ×10 3 iterations DiffAugment (Translation) DiffAugment (Trans. + Cutout) DiffAugment (Color + Trans. + Cutout)</cell><cell>0.4 0.0 0.2</cell><cell>0</cell><cell>50 BigGAN (baseline) ×10 3 iterations 100 150 200 250 DiffAugment (Translation) DiffAugment (Trans. + Cutout) DiffAugment (Color + Trans. + Cutout)</cell></row><row><cell>Figure 6: Method</cell><cell cols="3">100% training data</cell><cell></cell><cell cols="2">50% training data</cell><cell>25% training data</cell></row><row><cell></cell><cell>IS</cell><cell cols="2">FID</cell><cell></cell><cell>IS</cell><cell>FID</cell><cell>IS</cell><cell>FID</cell></row><row><cell>BigGAN [2]</cell><cell>94.5 ± 0.4</cell><cell cols="3">7.62 ± 0.02</cell><cell>89.9 ± 0.2</cell><cell>9.64 ± 0.04</cell><cell>46.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>FFHQ and LSUN-Cat results with 1k, 5k, 10k, and 30k training samples. With the fixed Color + Translation + Cutout DiffAugment, our method improves the StyleGAN2 baseline and is on par with a concurrent work ADA<ref type="bibr" target="#b15">[16]</ref>. FID is measured using 50k generated samples; the full training set is used as the reference distribution. We select the snapshot with the best FID for each method. Results are averaged over 5 evaluation runs; all standard deviations are less than 1% relatively.</figDesc><table><row><cell>Method</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell></row><row><cell></cell><cell>100% data</cell><cell>20% data</cell><cell>10% data</cell><cell>100% data</cell><cell>20% data</cell><cell>10% data</cell></row><row><cell>BigGAN [2]</cell><cell>9.59</cell><cell>21.58</cell><cell>39.78</cell><cell>12.87</cell><cell>33.11</cell><cell>66.71</cell></row><row><cell>+ DiffAugment</cell><cell>8.70</cell><cell>14.04</cell><cell>22.40</cell><cell>12.00</cell><cell>22.14</cell><cell>33.70</cell></row><row><cell>CR-BigGAN [50]</cell><cell>9.06</cell><cell>20.62</cell><cell>37.45</cell><cell>11.26</cell><cell>36.91</cell><cell>47.16</cell></row><row><cell>+ DiffAugment</cell><cell>8.49</cell><cell>12.84</cell><cell>18.70</cell><cell>11.25</cell><cell>20.28</cell><cell>26.90</cell></row><row><cell>StyleGAN2 [18]</cell><cell>11.07</cell><cell>23.08</cell><cell>36.02</cell><cell>16.54</cell><cell>32.30</cell><cell>45.87</cell></row><row><cell>+ DiffAugment</cell><cell>9.89</cell><cell>12.15</cell><cell>14.50</cell><cell>15.22</cell><cell>16.65</cell><cell>20.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>CIFAR-10 results. IS and FID are measured using 10k samples; the validation set is the reference distribution for FID calculation. We select the snapshot with the best FID for each method. Results are averaged over 5 evaluation runs; all standard deviations are less than 1% relatively.</figDesc><table><row><cell>Method</cell><cell cols="2">100% training data</cell><cell cols="2">20% training data</cell><cell cols="2">10% training data</cell></row><row><cell></cell><cell>IS</cell><cell>FID</cell><cell>IS</cell><cell>FID</cell><cell>IS</cell><cell>FID</cell></row><row><cell>BigGAN [2]</cell><cell>10.92</cell><cell>12.87</cell><cell>9.11</cell><cell>33.11</cell><cell>5.94</cell><cell>66.71</cell></row><row><cell>+ DiffAugment</cell><cell>10.66</cell><cell>12.00</cell><cell>9.47</cell><cell>22.14</cell><cell>8.38</cell><cell>33.70</cell></row><row><cell>CR-BigGAN [50]</cell><cell>10.95</cell><cell>11.26</cell><cell>8.44</cell><cell>36.91</cell><cell>7.91</cell><cell>47.16</cell></row><row><cell>+ DiffAugment</cell><cell>10.81</cell><cell>11.25</cell><cell>9.12</cell><cell>20.28</cell><cell>8.70</cell><cell>26.90</cell></row><row><cell>StyleGAN2 [18]</cell><cell>9.51</cell><cell>16.54</cell><cell>7.86</cell><cell>32.30</cell><cell>7.01</cell><cell>45.87</cell></row><row><cell>+ DiffAugment</cell><cell>10.04</cell><cell>15.22</cell><cell>9.82</cell><cell>16.65</cell><cell>9.06</cell><cell>20.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>code || . All the models are transferred from a pre-trained StyleGAN model from the FFHQ dataset<ref type="bibr" target="#b16">[17]</ref> at 256×256 resolution. FreezeD reports the best performance when freezing the first 4 layers of D<ref type="bibr" target="#b29">[30]</ref>; when applying DiffAugment to FreezeD, we only freeze the first 2 layers of D. All other hyperparameters are kept unchanged from the default settings. All the models are trained on 1 GPU with a maximum of 10k training iterations on our 100-shot datasets and 20k iterations on the AnimalFace<ref type="bibr" target="#b36">[37]</ref> datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>for qualitative comparisons.</figDesc><table><row><cell></cell><cell>BigGAN (baseline) StyleGAN2 (baseline) StyleGAN2 (baseline)</cell><cell>+ DiffAugment (ours) + DiffAugment (ours) + DiffAugment (ours)</cell></row><row><cell>30k samples 30k samples</cell><cell></cell><cell></cell></row><row><cell>100% training data 10k samples (3× fewer) 10k samples (3× fewer)</cell><cell>FID: 6.16 FID: 10.12</cell><cell>FID: 5.05 FID: 9.68</cell></row><row><cell></cell><cell>FID: 14.75 FID: 17.93</cell><cell>FID: 7.86 FID: 12.07</cell></row><row><cell></cell><cell>IS: 94.5 FID: 7.62</cell><cell>IS: 100.8 FID: 6.80</cell></row><row><cell>5k samples (6× fewer) 5k samples (6× fewer) 25% training data 1k samples (30× fewer) 1k samples (30× fewer)</cell><cell>FID: 26.60 FID: 34.69</cell><cell>FID: 10.45 FID: 16.11</cell></row><row><cell></cell><cell>FID: 62.16 FID: 182.85</cell><cell>FID: 25.66 FID: 42.26</cell></row><row><cell></cell><cell>IS: 46.5 FID: 25.37 StyleGAN2 (baseline) StyleGAN2 (baseline)</cell><cell>IS: 74.2 FID: 13.28 + DiffAugment (ours) + DiffAugment (ours)</cell></row><row><cell cols="3">BigGAN (baseline) Figure 13: Qualitative comparison on FFHQ at 256×256 resolution with 1k, 5k, 10k, and 30k + DiffAugment (ours)</cell></row><row><cell cols="3">training images. Our method consistently outperforms the StyleGAN2 baselines [18] under different</cell></row><row><cell cols="2">data percentage settings.</cell><cell></cell></row></table><note>Figure 12: Qualitative comparison on ImageNet 128×128 without the truncation trick [2].</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* https://github.com/google/compare_gan</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank NSF Career Award #1943349, MIT-IBM Watson AI Lab, Google, Adobe, and Sony for supporting this research. Research supported with Cloud TPUs from Google's TensorFlow Research Cloud (TFRC). We thank William S. Peebles and Yijun Li for helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Hyperparameters and Training Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 ImageNet Experiments</head><p>The Compare GAN codebase * suffices to replicate BigGAN's FID on ImageNet dataset at 128×128 resolution but has some small differences to the original paper <ref type="bibr" target="#b1">[2]</ref>. First, the codebase uses a learning rate of 10 −4 for G and 5 × 10 −4 for D. Second, it processes the raw images into 128×128 resolution with random scaling and random cropping. Since we find that random cropping leads to a worse IS, we process the raw images with random scaling and center cropping instead. We additionally augment the images with random horizontal flips, yielding the best re-implementation of BigGAN to our knowledge. With DiffAugment, we find that D's learning rate of 5 × 10 −4 often makes D's loss stuck at a high level, so we reduce D's learning rate to 4 × 10 −4 for the 100% data setting and 2 × 10 −4 for the 10% and 20% data settings. However, we note that the baseline model does not benefit from this reduced learning rate: if we reduce D's learning rate from 5 × 10 −4 to 2 × 10 −4 under the 50% data setting, its performance degrades from an FID/IS of 9.64/89.9 to 10.79/75.7. All the models achieve the best FID within 200k iterations and deteriorate after that, taking up to 3 days on a TPU v2/v3 Pod with 128 cores.</p><p>See <ref type="figure">Figure 12</ref> for a qualitative comparison between BigGAN and BigGAN + DiffAugment. Our method improves the image quality of the samples in both 25% and 100% data settings. The visual difference is more clear under the 25% data setting.</p><p>Notes on CR-BigGAN <ref type="bibr" target="#b49">[50]</ref>. CR-BigGAN <ref type="bibr" target="#b49">[50]</ref> reports an FID of 6.66, which is slightly better than ours 6.80 (BigGAN + DiffAugment) with 100% data. However, the code and pre-trained models of CR-BigGAN <ref type="bibr" target="#b49">[50]</ref> are not available, while its IS is not reported either. Our reimplemented CR-BigGAN only achieves an FID of 7.95 with an IS of 82.0, even worse than the baseline BigGAN. Nevertheless, our CIFAR experiments suggest the potential of applying DiffAugment on top of CR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Evaluation Metrics</head><p>We measure FID and IS using the official Inception v3 model in TensorFlow for all the methods and datasets. Note that some papers using PyTorch implementations, including FreezeD <ref type="bibr" target="#b29">[30]</ref>, report different numbers from the official TensorFlow implementation of FID and IS. On ImageNet, CIFAR-10, and CIFAR-100, we inherit the setting from the Compare GAN codebase that the number of samples of generated images equals the number of real images in the validation set, and the validation set is used as the reference distribution for FID calculation. For the low-shot generation experiments, we sample 5k generated images and we use the training set as the reference distribution. For the FFHQ and LSUN experiments, we use the same evaluation setting as ADA <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C 100-Shot Generation Benchmark</head><p>We collect the 100-shot datasets from the Internet. We then manually filter and crop each image as a pre-processing step. The full datasets are available here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D Changelog</head><p>v1 Initial preprint release.</p><p>v2 (a) Add StyleGAN2 results on FFHQ <ref type="table">(Table 3</ref> and <ref type="figure">Figure 13</ref>). (b) For low-shot generation, we rerun the StyleGAN2 and StyleGAN + DiffAugment models with a batch size of 16 <ref type="table">(Table 5</ref>). Both our method and the baseline are improved with a batch size of 16 over 32 (used in v1). (c) Add interpolation results on the additional 100-shot landmark datasets ( <ref type="figure">Figure 15</ref>). (d) Update the CR-BigGAN notes in Appendix A.1.</p><p>v3 (a) Add StyleGAN2 results on the LSUN-Cat dataset ( <ref type="table">Table 3</ref> and <ref type="figure">Figure 14</ref>). (b) Rerun the MineGAN models using their newly released code <ref type="table">(Table 5)</ref>, and add qualitative samples <ref type="figure">(Figures 18-22</ref>). (c) Analyze additional choices of DiffAugment <ref type="figure">(Figure 9</ref>).</p><p>v4 Add notes in <ref type="table">Table 4</ref> regarding a different evaluation protocal for FID calculation as used in concurrent works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning Augmentation Policies from Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<title level="m">Practical Automated Data Augmentation with a Reduced Search Space. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<title level="m">Improved regularization of convolutional neural networks with cutout. arXiv</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A century of portraits: A visual historical record of american high school yearbooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Analyzing and Improving the Image Quality of StyleGAN. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GAN Compression: Efficient Architectures for Interactive Conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast AutoAugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diverse image generation via self-conditioned gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-Fidelity Image Generation With Fewer Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">cgans with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Freeze Discriminator: A Simple Baseline for Fine-tuning GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image generation from small datasets via batch statistics adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatiallyadaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stabilizing training of generative adversarial networks through regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2018" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Co-Evolutionary Compression for Unpaired Image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Hybrid Image Templates (HIT) by Information Projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangzhang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patrice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Document Analysis and Recognition</title>
		<meeting>International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Amortised map inference for image super-resolution. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Casper Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Trung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Hung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Bao</forename><surname>Nguyen</surname></persName>
		</author>
		<idno>arXiv, 2020. 3</idno>
		<title level="m">Trung-Kien Nguyen, and Ngai-Man Cheung. Towards good practices for data augmentation in gan training</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Minegan: effective knowledge transfer from gans to target domains with few images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transferring gans: generating images from limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenshen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raducanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">mixup: Beyond Empirical Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-Attention Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Image augmentations for gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Don&apos;t let your discriminator be fooled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
