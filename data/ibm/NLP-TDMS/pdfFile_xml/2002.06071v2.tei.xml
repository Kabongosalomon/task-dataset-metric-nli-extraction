<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FQuAD: French Question Answering Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>D&amp;apos;hoffschmidt</surname></persName>
							<email>martin@illuin.tech</email>
							<affiliation key="aff0">
								<orgName type="institution">Illuin Technology Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wacim</forename><surname>Belblidia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Illuin Technology Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brendlé</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Illuin Technology Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Heinrich</surname></persName>
							<email>quentin@illuin.tech</email>
							<affiliation key="aff0">
								<orgName type="institution">Illuin Technology Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Vidal</surname></persName>
							<email>mvidal@student.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Illuin Technology Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Illuin Technology Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FQuAD: French Question Answering Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, Reading Comprehension has made significant progress over the past few years. However, most results are reported in English since labeled resources available in other languages, such as French, remain scarce. In the present work, we introduce the French Question Answering Dataset (FQuAD). FQuAD is a French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version. We train a baseline model which achieves an F1 score of 92.2 and an exact match ratio of 82.1 on the test set. In order to track the progress of French Question Answering models we propose a leader-board and we have made the 1.0 version of our dataset freely available at https://illuin-tech. github.io/FQuAD-explorer/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current progress in language modeling has led to increasingly successful results on various Natural Language Processing (NLP) tasks such as Part of Speech Tagging (PoS), Named Entity Recognition (NER) and Natural Language Inference (NLI). Large amounts of unstructured text data available for most languages have facilitated the development of language models. Therefore, the releases of language specific models in Japanese, Chinese, German and Dutch <ref type="bibr" target="#b6">[de Vries et al., 2019]</ref>, amongst other languages, are now thriving as well as multilingual models <ref type="bibr" target="#b18">[Pires et al., 2019]</ref> and . Recently, two French language models, CamemBERT <ref type="bibr" target="#b15">[Martin et al., 2019]</ref> and FlauBERT , were released.</p><p>However, language specific datasets are costly and difficult to collect. This is especially the case with the Reading Comprehension task <ref type="bibr" target="#b25">[Richardson et al., 2013]</ref>. On one hand, numerous English datasets have been released such as SQuAD1.1 <ref type="bibr">[Rajpurkar et al., 2016]</ref>, SQuAD2.0 <ref type="bibr" target="#b23">[Rajpurkar et al., 2018]</ref> or CoQA <ref type="bibr" target="#b24">[Reddy et al., 2018]</ref> that fostered important and impressive progress for English Question Answering models over the past few years. On the other hand, the lack of native language annotated datasets apart from English is one of the main reasons why the development of language specific Question Answering models is slower. This is namely the case for French.</p><p>To tackle this problem, substantial efforts have been carried out recently to come up with native Reading Comprehension datasets in for instance Korean <ref type="bibr" target="#b13">[Lim et al., 2019]</ref>, Russian <ref type="bibr" target="#b8">[Efimov et al., 2019]</ref> and Chinese <ref type="bibr">[Cui et al., 2019]</ref>. A more appealing solution in terms of cost and time efficiency relies on leveraging the advances in Neural Machine Translation (NMT) to translate the English datasets in target languages to fine-tune the language model on the translated dataset. This is for instance the case of Carrino et al. where SQuAD1.1 is translated in Spanish in order to train a multilingual model to answer Spanish questions. An alternative is proposed by <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref> and  where the authors provide a cross-lingual evaluation benchmark to enhance the development of cross-lingual Question Answering models that can transfer to a target language without requiring training data in that language. However, in both cases, the reported performances fail to reach English comparable results on other languages.</p><p>In order to fill the gap for the French language, we release a French Reading Comprehension dataset similar to SQuAD1.1. The dataset consists of French native questions and answers samples annotated by a team of university students. The dataset comes in two versions. First FQuAD1.0, containing over 25,000+ samples. Second, FQuAD1.1 containing over 60,000+ samples. The 35k+ additional samples have been annotated with more demand-ing guidelines to strengthen complexity of the data and model to make the task harder. More specifically, the training, development and test sets of FQuAD1.0 contain respectively 20703, 3188 and 2189 samples. And the training, development and test sets of FQuAD1.1 contain respectively 50741, 5668 and 5594 samples.</p><p>In order to evaluate the FQuAD dataset, we perform various experiments by fine-tuning BERT based Question Answering models on both versions of the FQuAD dataset. Our experiments cover not only the recently released French pre-trained language models CamemBERT <ref type="bibr" target="#b15">[Martin et al., 2019]</ref> and FlauBERT  but also multilingual models such as mBERT <ref type="bibr" target="#b18">[Pires et al., 2019]</ref>, XLM-RoBERTa  in order to better understand how multilingual models perform on native datasets other than English.</p><p>Finally, we perform two types of cross-lingual Reading Comprehension experiences. First, we evaluate the performance of the zero-shot cross-lingual transfer learning approach as stated in <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref> and  on our newly obtained native French dataset. Second, we evaluate the performance of the translation approach by fine-tuning models on the French translated version of SQuAD1.1. The results of these two experiments help to better understand how the two cross-lingual approaches actually perform on a native dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The Reading Comprehension task (RC) <ref type="bibr" target="#b25">[Richardson et al., 2013]</ref>, <ref type="bibr">[Rajpurkar et al., 2016]</ref> attempts to solve the Question Answering (QA) problem by finding the text span in one or several documents or paragraphs that answers a given question <ref type="bibr" target="#b26">[Ruder, 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reading Comprehension in English</head><p>Many Reading Comprehension datasets have been built in English. Among them SQuAD1.1 <ref type="bibr">[Rajpurkar et al., 2016]</ref>, then later SQuAD2.0 <ref type="bibr" target="#b23">[Rajpurkar et al., 2018]</ref> has become one of the major reference dataset for training question answering models. Later, similar initiatives such as NewsQA <ref type="bibr" target="#b28">[Trischler et al., 2016]</ref>, CoQA <ref type="bibr" target="#b24">[Reddy et al., 2018]</ref>, QuAC <ref type="bibr" target="#b3">[Choi et al., 2018]</ref>, HotpotQA  have broadened the research area for English Question Answering.</p><p>These datasets are similar but each of them introduces its own subtleties. For instance, SQuAD2.0 <ref type="bibr" target="#b23">[Rajpurkar et al., 2018]</ref> develops unanswerable adversarial questions. CoQA <ref type="bibr" target="#b24">[Reddy et al., 2018]</ref> focuses on Conversation Question Answering in order to measure the ability of algorithms to understand a document and answer series of interconnected questions that appear in a conversation. QuAC <ref type="bibr" target="#b3">[Choi et al., 2018]</ref> focuses on Question An-swering in Context developed for Information Seeking Dialog (ISD). The benchmark established by <ref type="bibr" target="#b3">[Yatskar, 2018]</ref> offers a qualitative comparison of these datasets. Finally, HotPotQA  attempts to extend the Reading Comprehension task to more complex reasoning by introducing Multi Hop Questions (MHQ) where the answer must be found among multiple documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reading Comprehension in other languages</head><p>Native Reading Comprehension datasets other than English remain rare. Among them, some initiatives have been carried out in Chinese, Korean and Russian and all of them have been built in a similar way to SQuAD1.1. The SberQuAD dataset <ref type="bibr" target="#b8">[Efimov et al., 2019</ref>] is a Russian native Reading Comprehension dataset and is made up of 50,000+ samples. The CMRC 2018 <ref type="bibr">[Cui et al., 2019]</ref> dataset is a Chinese native Reading Comprehension dataset that gathers 20,000+ question and answer pairs. The KorQuAD dataset <ref type="bibr" target="#b13">[Lim et al., 2019</ref>] is a Korean native Reading Comprehension dataset that is made up of 70,000+ samples. Note that following our work, the PIAF project <ref type="bibr" target="#b19">[Rachel et al., 2020]</ref> has released a native French Dataset of 3835 question and answer pairs. As language specific datasets are costly and challenging to obtain, an alternative consists in developing cross-lingual models that can transfer to a target language without requiring training data in that language . It has indeed been shown that these unsupervised multilingual models generalize well in a zero-shot cross-lingual setting <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref>. For this reason, crosslingual Question Answering has recently gained traction and two cross-lingual benchmarks have been released, i.e XQuAD <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref> and MLQA . The XQuAD dataset <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref> is obtained by translating 1190 question and answer pairs from the SQuAD1.1 development set by professionals translators in 10 foreign languages. The MLQA dataset  consists of over 12000 question and answer samples in English and 5000 samples in 6 other languages such as Arabic, German and Spanish. Note that the two aforementioned datasets do not cover French.</p><p>Another alternative consists in translating the training dataset into the target language and finetuning a language model on the translated dataset. This is namely the case of <ref type="bibr" target="#b2">[Carrino et al., 2019]</ref> where the authors develop a specific translation method called Translate-Align-Retrieve (TAR) to translate the English SQuAD1.1 dataset into Spanish. The resulting Spanish SQuAD1.1 dataset is used to fine-tune a multilingual model that reaches a performance of respectively 68.1/48.3% F1/EM and 77.6/61.8% F1/EM on MLQA cross-lingual benchmark  and XQuAD <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref>. Note that a similar approach has been adopted for French and Japanese in <ref type="bibr" target="#b1">[Asai et al., 2018]</ref> and <ref type="bibr" target="#b27">[Siblini et al., 2019]</ref>. In <ref type="bibr" target="#b27">[Siblini et al., 2019]</ref> a multilingual BERT is trained on English texts of SQuAD1.1, and evaluated on the small translated Asai et al. French corpus. This set-up reaches a promising score of 76.7/61.8 % F1/EM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Language modeling for Reading Comprehension</head><p>Increasingly efficient language models have been released recently such as GPT-2 <ref type="bibr">[Radford et al., 2018]</ref>, BERT <ref type="bibr" target="#b7">[Devlin et al., 2018]</ref>, XLNet <ref type="bibr" target="#b31">[Yang et al., 2019]</ref> and RoBERTa <ref type="bibr" target="#b14">[Liu et al., 2019]</ref>. They have indeed disrupted the Reading Comprehension task and most of NLP fields: pre-training a language model on a generic corpus, eventually fine-tuning it on a domain specific corpus and then training it on a downstream task is the de facto state-ofthe-art approach for optimizing both performances and annotated data volumes <ref type="bibr" target="#b7">[Devlin et al., 2018]</ref>, <ref type="bibr" target="#b14">[Liu et al., 2019]</ref>. For instance, the top performing models on the SQuAD1.1 and SQuAD2.0 leaderboards 1 are essentially transformer based models. Unfortunately, the aforementioned models are pretrained on English corpora and their use for French is therefore limited.</p><p>Multilingual models pre-trained on large multilingual datasets attempt to alleviate the language specific shortcoming characteristic of the former models such as <ref type="bibr" target="#b10">[Lample and Conneau, 2019]</ref>, <ref type="bibr" target="#b18">[Pires et al., 2019]</ref> and more recently XLM-R . It has been shown in , <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref> and  that multilingual models are flexible and perform reasonably well on other languages than English. However, they do not appear to perform better than specific language models .</p><p>Regarding French, few resources were available until recently. First, the CamemBERT models <ref type="bibr" target="#b15">[Martin et al., 2019]</ref> were trained on 138 GB of French text from the Oscar dataset . Second, the FlauBERT models  were trained on 71 GB of text. Note that both models were pre-trained with the Masked Language Modeling task only <ref type="bibr" target="#b15">[Martin et al., 2019]</ref>, . Both models reach similar performances on French NLP tasks such as PoS, NER and NLI. However, their performance has not yet been evaluated on the Reading Comprehension task as no French dataset is available.</p><p>Finally, <ref type="table">Table 1</ref> lists some of the available datasets along with the number of samples they contain 2 . By means of comparison, <ref type="table">Table 1</ref>  includes FQuAD, whose collection is presented in the upcoming sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Collection</head><p>The collection procedure for our dataset follows the same standards and guidelines as SQuAD1.1 <ref type="bibr">[Rajpurkar et al., 2016]</ref>. First, paragraphs among diverse articles are collected. Second, question and answer pairs are crowd-sourced on the collected paragraphs. Third, additional answers are collected for the development and test sets. The Dataset Collection was conducted in two distinct steps: the first one resulted in FQuAD1.0 with 25k+ question and answer pairs, and the second one resulted in FQuAD1.1 with 60k+ question and answer pairs. FQuAD1.1, the number of collected paragraphs for the same sets are respectively 12123, 1387 and 1398.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Paragraphs collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question and answer pairs collection</head><p>A specific annotation platform was developed to collect the question and answer pairs. The workers were hired in collaboration with the Junior Enterprise of CentraleSupélec 4 . <ref type="figure" target="#fig_0">Figure 1</ref>: The interface used to collect the question/answers encourages workers to write difficult questions.</p><p>The guidelines for writing question and answer pairs for each paragraph are the same as for SQuAD1.1 <ref type="bibr">[Rajpurkar et al., 2016]</ref>. First, the paragraph is presented to the student on the platform and the student reads it. Second, the student thinks of a question whose answer is a span of text within the context. Third, the student selects the smallest span in the paragraph which contains the answer. The process is then repeated until 3 to 5 questions are generated and correctly answered. The students were asked to spend on average 1 minute on each question and answer pair. This amounts to an average of 3-5 minutes per annotated paragraph. Final dataset metrics are shared in table 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Additional answers collection</head><p>Additional answers are collected to decrease the annotation bias similarly to <ref type="bibr">[Rajpurkar et al., 2016]</ref>. For each question in the development and test sets, two additional answers are collected, resulting in three answers per question for these sets. The crowd-workers were asked to spend on average 30 seconds to answer each question.</p><p>For the same question, several answers may be correct: for instance the question Quand fut couronné Napoléon ? would have several possible answers such as mai 1804, en mai 1804 or 1804. As all those answers are admissible, enriching the test set with several annotations for the same question, with different annotators, is a way to decrease annotation bias. The additional answers are useful to get an indication of the human performance on FQuAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">FQuAD1.0</head><p>The results for the first annotation process resulting in the FQuAD1.0 dataset are reported in table 2. The number of collected question and answer pairs amounts to 26108. Diverse analysis to measure the difficulty of the resulting dataset are performed as described in the next section. A complete annotated paragraph is displayed in figure 2.</p><p>Article: Cérès Paragraph: Des observations de 2015 par la sonde Dawn ont confirmé qu'elle possède une forme sphérique, à la différence des corps plus petits qui ont une forme irrégulière. Sa surface est probablement composée d'un mélange de glace d'eau et de divers minéraux hydratés (notamment des carbonates et de l'argile), et de la matière organique a été décelée. Il semble que Cérès possède un noyau rocheux et un manteau de glace. Elle pourrait héberger un océan d'eau liquide, ce qui en fait une piste pour la recherche de vie extraterrestre. Cérès est entourée d'une atmosphère ténue contenant de la vapeur d'eau, dont deux geysers, ce qui a été confirmé le 22 janvier 2014 par l'observatoire spatial Herschel de l'Agence spatiale européenne.   <ref type="table" target="#tab_3">Train  117  4921  20731  Development  18  768  3188  Test  10  532  2189   Table 2</ref>: The number of articles, paragraphs and questions for FQuAD1.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">FQuAD1.1</head><p>The first dataset is extended with additional annotation samples to build the FQuAD1.1 dataset reported in table 3. The total number of questions amounts to 62003. The FQuAD1.1 training, development and test sets are then respectively composed of 271 articles (83%), 30 (9%) and 25 (8%). The difference with the first annotation process is that the workers were specifically asked to come up with complex questions by varying style and question types in order to increase difficulty. The additional answer collection process remains the same. <ref type="table" target="#tab_3">Train  271  12123  50741  Development  30  1387  5668  Test  25  1398  5594   Table 3</ref>: The number of articles, paragraphs and questions for FQuAD1.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Articles Paragraphs Questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Adversarial samples</head><p>The present dataset does not contain adversarial samples as in SQuAD2.0 by <ref type="bibr" target="#b23">[Rajpurkar et al., 2018]</ref>. However, this will hopefully be released in a future version of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset Analysis</head><p>In order to understand the diversity of the dataset, we perform various analysis. First, a mix of PoStagging and patterns is used to analyse the frequency of different kinds of answers (see <ref type="table" target="#tab_3">table 4</ref>). Second, a keyword based approach is used to analyse the frequency of the corresponding questions (see <ref type="table" target="#tab_4">table 5</ref>). Finally, we present the result of our analysis on the question-answer differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Answer analysis</head><p>To analyse the collected answers, a combination of rule-based regular expressions and entity extraction using spaCy <ref type="bibr" target="#b9">[Honnibal and Montani, 2017]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question analysis</head><p>The second analysis aims at understanding the question types of the dataset. The present analysis is performed rule-based only.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Question-answer differences</head><p>The difficulty in finding the answer given a particular question lies in the linguistic variation between the two. This can come in different ways, which are listed in <ref type="table">table 6</ref> The categories are taken from <ref type="bibr">[Rajpurkar et al., 2016]</ref>: Synonymy implies key question words are changed to a synonym in the context; World knowledge implies key question words require world knowledge to find the correspondence in the context; Syntactic variation implies a difference in the structure between the question and the answer; Multiple sentence reasoning implies knowledge requirement from multiple sentences in order to answer the question. We randomly sampled 6 questions from each article in the development set and manually labeled them. Note that samples can belong to multiple categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dataset Evaluation</head><p>We present the evaluation metrics for the FQuAD dataset. First, altough the evaluation metrics remain essentially the same as for SQuAD, some modifications must be taken into account regarding the French nature of the dataset. Second, we evaluate the human performance on the FQuAD development and test datasets. Third, we compare the FQuAD1.1 and SQuAD1.1 development datasets with several metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation metrics</head><p>The Exact Match (EM) and F1-score metrics are common metrics being computed to evaluate the performances of a model. The former measures  <ref type="table">Table 6</ref>: Question-answer relationships in 108 randomly selected samples from the FQuAD development set. In bold the elements needed for the corresponding reasoning, in italics the selected answer.</p><p>the percentage of predictions matching exactly one of the ground truth answers. The later computes the average overlap between the predicted tokens and the ground truth answer. The prediction and ground truth are processed as bags of tokens. For questions labeled with multiple answers, the F1 score is the maximum F1 over all the ground truth answers.</p><p>The evaluation process in <ref type="bibr">[Rajpurkar et al., 2016]</ref> for both the F1 and EM ignores some English punctuation, i.e. the a, an, the articles. In order to remain consistent with the former approach, the French evaluation process ignores the following articles: le, la, les, l', du, des, au, aux, un, une.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human performance</head><p>Similarly to SQuAD, human performances are evaluated on the development and test sets in order to assess how humans agree on answering questions. This score gives a comparison baseline when assessing the performance of a model. To measure the human performance, for each question, two of the three answers are considered as the ground truth, and the third as the prediction. In order not to bias this choice, the three answers are successively considered as the prediction, so that three human scores are calculated. The three runs are then averaged to obtain the final human performance. Both the F1 and EM score are computed based on this setup.</p><p>The   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer length</head><p>To compare the answer lengths for the FQuAD1.1 and SQuAD1.1 datasets, we first remove every punctuation signs as well as respectively french words le, la, les, l', du, des, au, aux, un, une and english words a, an, the. Then answers are split on white spaces to compute the number of tokens for each answer. The results are reported in <ref type="figure" target="#fig_2">figure 3</ref>. It appears clearly that FQuAD answers are generally longer than SQuAD answers. Furthermore, to highlight this important difference it is interesting to realise that the average number of tokens per answer for SQuAD1.1 is equal to 2.72 while it is equal to 4.24 for FQuAD1.1. This indicates that reaching a high Exact Match score on FQuAD is more difficult than on SQuAD.</p><p>Human performance as a function of the answer length To understand if the answer length can impact the difficulty of the Reading Comprehension task, we group question and answer pairs in FQuAD and SQuAD by the number of tokens for each answer. The <ref type="figure" target="#fig_3">figure 4</ref> shows the human performance as a function of the answer length. On one hand, it is straightforward to notice that the Exact Match quickly declines with an increasing answer length for both FQuAD and SQuAD. On the other hand, the F1 score is a lot less affected by answer length for both datasets. We conclude from these distributions that the difference in answers lengths between FQuAD and SQuAD may explain part of the difference in human performance regarding EM metric, while it does not seem to have an impact   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We present the experiments that are carried out in order to evaluate both the quality of the new French Reading Comprehension dataset and the resulting fine-tuned models. First we present the experimental set-up. Second, the French monolingual language models and multilingual language models fine-tuning experiments are performed. Finally, we investigate on one hand how zero-shot learning from English SQuAD1.1 performs on our dataset and on the other we evaluate the results with cross-lingual approaches based on the French translation of SQuAD1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental set-up</head><p>The experimental set-up is kept the same across all the experiments. The number of epochs is set to 3, with a learning rate equal to 3.0 · 10 −5 . The learning rate is scheduled according to a warm-up linear scheduler where the percentage ratio for the warm-up is consistently set to 6%. The batch size is kept constant across the training and is equal to 8 for the base models and 4 for the large ones. The optimizer that is being used is AdamW with its default parameters. All the experiments were carried out with the HuggingFace transformers library <ref type="bibr" target="#b29">[Wolf et al., 2019</ref>] on a single V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Native French Reading Comprehension</head><p>Monolingual vs. multilingual language models The goal of these experiments is two fold. First, we want to evaluate and compare how the French language models CamemBERT <ref type="bibr" target="#b15">[Martin et al., 2019]</ref> and FlauBERT ] perform on FQuAD. Second, we want to evaluate how multilingual models perform when they are fine-tuned on the French dataset. For this purpose we train two multilingual models, i.e mBERT <ref type="bibr" target="#b18">[Pires et al., 2019]</ref> and the XLM-RoBERTa model . Finally, we will be able to compare the results for both the monolingual and multilingual models to understand how they perform on the French dataset. Note that for each experiment, the fine-tuning is performed on the training sets of both FQuAD1.0 and FQuAD1.1 but are evaluated only on the development and test sets of FQuAD.1.1.</p><p>Performance analysis An analysis of the predictions for the best trained model is carried out. We have explored the distribution of answer and questions types in section 4 and we report now the performance of the model in terms of F1 score and Exact Match for each category. This analysis aims at understanding how the model performs on the various question and answer types.</p><p>Learning curve The question of how much data is needed to train a question answering model remains relatively unexplored. In our effort of annotating FQuAD1.0 and FQuAD1.1 we have consistently monitored the scores to know if the annotation process must be continued or stopped. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Cross-lingual Reading Comprehension</head><p>Cross-lingual Reading comprehension follows mainly two approaches as explained in 2. On one hand, experiments carried out in  and <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref> evaluate how multilingual models fine-tuned on the English SQuAD1.1 dataset perform on other languages such as Spanish, Chinese or Arabic. On the other hand, initiatives such as <ref type="bibr" target="#b2">[Carrino et al., 2019]</ref> attempt to translate the dataset in the target language to fine-tune a model. The newly obtained FQuAD dataset makes it now possible to test both approaches on the English-French cross-lingual set-up. Note however that French is unfortunately not supported by the cross-lingual benchmark proposed by <ref type="bibr" target="#b12">[Lewis, Oguz, Rinott, Riedel, and Schwenk, 2019]</ref>, <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref>. First, we perform several experiments with a so called zero-shot learning approach. In other words, we fine-tune several multilingual models on the English SQuAD1.1 dataset and we evaluate them on the FQuAD1.1 development set. In addition to that, the opposite approach is also carried out, i.e. fine-tuned models on FQuAD1.1 are evaluated on the SQuAD1.1 development set.</p><p>Second, we fine-tune CamemBERT on the SQuAD1.1 training dataset translated into French. For this purpose, the SQuAD1.1 training set is translated using NMT <ref type="bibr" target="#b17">[Ott et al., 2018]</ref>. Note that the translation process makes it difficult to keep all the samples from the original dataset and, for the sake of simplicity, we discard the translated answers that do not align with the start/end positions of the translated paragraphs. The resulting translated dataset SQuAD1.1-fr-train contains about 40.7k question and answer pairs. The fine-tuned model is then evaluated on the native French FQuAD1.1 development set. This experiment helps us to understand how the translation process ultimately affects the performance of the model on native data rather than on the translated development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>In the present section, we present the results for the aforementioned evaluation experiments. First, we present the results for the native French Reading Comprehension experiments along with the performance analysis for the best obtained model and a learning curve. Second, we present the results for the cross-lingual Reading Comprehension experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Native French Reading Comprehension</head><p>The training experiments on FQuAD1.1-train are summed up in    <ref type="bibr" target="#b31">[Yang et al., 2019]</ref>. Note that while the size of FQuAD1.1 remains smaller than its english counterpart, the aforementioned results yield a very promising baseline. Note further that the same model reaches a performance of 93.3% F1 and 84.6% EM on the test set of FQuAD1.0, hereby supporting the fact that FQuAD1.1 includes more difficult question 5. The FlauBERT BASE and FlauBERT LARGE model fine-tuned on the FQuAD1.1 training dataset yield a surprisingly low performance of respectively 77.6/66.5% and 80.6/70.3% F1/EM score. Indeed, it is reported that FlauBERT rivals or even surpasses CamemBERT performances on several downstream tasks such as Text Classification, Natural Language Inference (NLI) or Paraphrasing .</p><p>Multilingual models The results of the experiments carried out for the multilingual models reported in 9 and 10 show that they perform also very well when evaluated on the test and development sets of FQuAD1. These experiments show that monolingual language models reach stronger performances than multilingual models overall. Nevertheless, it is important to note that XLM-R LARGE model performs better than CamemBERT BASE on both the test and development sets and even surpasses the Human Performance in terms of Exact Match on the test set.</p><p>Performance analysis Our best model CamemBERT LARGE is used to run the performance analysis on the question and answer types. Tables 11 and 12 present the results sorted by F1 score. The model performs very well on structured data such as Date, Numeric or Location. Similarly, the model performs well on questions seeking for structured information, such as How many, Where, When. The Person answer type human score is very high on EM metric, meaning that these answers are easier to detect exactly probably because the answer is in general short. On the other end, the How and Why questions that probably expect a long and wordy answer are among the least well addressed. Note that Verb answers EM score is also quite low. This is probably due to either the variety of forms a verb can take, or to the fact that verbs are often part of long and wordy answers, which are by definition difficult to match exactly. Some prediction examples are available in the appendix. Selected samples are not part of FQuAD, but were sourced from Wikipedia.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>The release of a native French Reading Comprehension dataset is motivated by the release of recent French monolingual models <ref type="bibr" target="#b15">(Martin et al. [2019]</ref>, ) and by industrial opportunities. In addition to that, we think that a French dataset opens up a wide range of possible experiments at the research level. First, while it is generally accepted that monolingual models perform better than multilingual models we find that the gap is narrower than expected for the Reading Comprehension task. Second, to fine-tune a model on a target language, translated datasets have been extensively used but the lack of native data to evaluate the approach, at least in French, makes it difficult to evaluate it. Third, apart from Question Answering models for French applications, cross-lingual applications have found significant interest recently with <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref> and  where the need for quality annotated data on other languages than English are important to evaluate how models transfer across languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Monolingual vs. multilingual language models</head><p>Through our language models benchmark on FQuAD, we have evaluated several monolingual and multilingual models. The CamemBERT BASE and CamemBERT LARGE models reach a very promising baseline and the large model even outperforms the Human Performance consistently across the development and test datasets. Surprisingly we find very poor results for the FlauBERT BASE and FlauBERT LARGE models. For comparable model sizes we find that the monolingual models outperform multilingual models on the Reading Comprehension task. However, we find that multilingual models such as mBERT <ref type="bibr" target="#b18">[Pires et al., 2019]</ref> or XLM-R BASE and XLM-R LARGE  reach very promising scores. We find that XLM-R LARGE performs consistently better than the monolingual model CamemBERT BASE on both the development and test sets of FQuAD1.1. Let us further highlight that XLM-R LARGE reaches 79% EM on FQuADtest which is better than Human Performance, while the F1 score remains only 2% below it. As such a model is pre-trained on a multilingual corpus, we can hope that it could be used with reasonable performances on other languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Translated Reading Comprehension</head><p>Fine-tuning CamemBERT BASE on a French translated dataset yields 81.8/67.8% F1/EM on the FQuAD1.1 dev set. By means of comparison, CamemBERT BASE scores 88.1/78.1% F1/EM on the same set when trained with native French data. We find here that there exists an important gap between both approaches. Indeed, models that are fine-tuning on native data outperform models finetuned on translated data by an order of magnitude of 10% for the Exact Match.</p><p>In <ref type="bibr" target="#b2">[Carrino et al., 2019]</ref>, the authors report a performance of 77.6/61.8% F1/EM score when mBERT is trained on a Spanish-translated SQuAD1.1 and evaluated on XQuAD <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref>. While the two approaches differ in terms of evaluation dataset, i.e. XQuAD is not a native Spanish dataset, and model, mBERT vs. CamemBERT, and although French and Spanish are different languages, they are close enough in their construction and structure, so that comparing these two approaches is relevant to us. Given the level of effort put into the translation process in <ref type="bibr" target="#b2">[Carrino et al., 2019]</ref>, we think that both translation-based approaches, although using very recent language models, reach a performance ceiling with translated data. We observe also that enriching native French training data with the translated samples does not improve the performances on the native evaluation set. Given our experiments, we conclude therefore that there exist a significant gap between the native French and the French translated data in terms on quality and indicates that approaches based on translated data reach ceiling performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Cross-lingual Reading Comprehension</head><p>The zero-shot experiments show that multilingual models can reach strong performances on the Reading Comprehension task in French or English when the model has not encountered labels of the target language. For example, the XLM-R LARGE model fine-tuned solely on FQuAD1.1 reaches a performance on SQuAD just a few points below the English Human Performance. The same is also observed while fine-tuning solely on SQuAD1.1 and evaluating on the development set of FQuAD1.1. We conclude here in agreement with <ref type="bibr" target="#b0">[Artetxe et al., 2019]</ref> and  that the transfer of models from French to English and vice versa relevant approach when no annotated samples are available in the target language.</p><p>The experiments also show that the zero-shot performances are better for SQuAD than for FQuAD. This phenomenon can be explained by structural differences between French and English or an increased difficulty of FQuAD compared to SQuAD. It is also possible that the XLM-R language models used are capturing English language specifics better than for other languages because the dataset used for pre-training these models contains more English data. Further experiments aiming at training multilingual models on both FQuAD1.1 and SQuAD1.1 may improve the results further. This possibility is left for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In the present work, we introduce the French Question Answering Dataset. To our knowledge, it is the first dataset for native French Reading Comprehension. The contexts are collected from the set of high quality Wikipedia articles. With the help of French college students, 60k+ questions have been manually annotated. The FQuAD dataset is the result of two different annotation processes. First, FQuAD1.0 is collected to build a 25k+ questions dataset. Second, the dataset is enriched to reach 60k+ questions resulting in FQuAD1.1. The development and test sets have both been enriched with additional answers for the evaluation process.</p><p>We find that the Human performances for FQuAD1.1 on the test and development sets reach respectively a F1-score of 91.2% and an Exact Match of 75.9% and a F1-score of 92.1% and an Exact Match of 78.3%. Furthermore, we find that the Human performances on FQuAD1.1 reach comparable scores to SQuAD1.1.</p><p>Various experiments were carried out to evaluate the performances of fine-tuned monolingual and multilingual language models. Our best model, CamemBERT LARGE , achieves a F1-score and an Exact Match of respectively 92.2% and 82.1%, surpassing the established Human performance in terms of F1-Score and Exact Match. The experiments show that multilingual models reach promising results but monolingual models of comparable sizes perform better.</p><p>The FQuAD1.0 training and FQuAD1.1 development sets are made publicly available at https: //illuin-tech.github.io/FQuAD-explorer/ in order to foster research in the French NLP area. The extension of the dataset to adversarial questions similarly to SQuAD2.0 is left for future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Question 1 :</head><label>1</label><figDesc>A quand remonte les observations faites par la sonde Dawn ? Answer: 2015 Question 2: Qu'ont montré les observations faites en 2015 ? Answer: elle possède une forme sphérique, à la différence des corps plus petits qui ont une forme irrégulière Question 3: Quelle caractéristique possède Cérès qui rendrait la vie extraterrestre possible ? Answer: un océan d'eau liquide</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Question answer pairs for a sample passage in FQuADDatasetArticles Paragraphs Questions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Answers lengths distribution for FQuAD and SQuAD on human performance regarding F1 metric. And indeed, human performance regarding F1 metric is very similar between FQuAD and SQuAD. It is possible that these variations in answers lengths distributions are due to structural differences between French and English languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Evolution of the F1 and EM human scores for the answers length of the development sets of FQuAD1.1 and SQuAD1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>are used. First, a set of regular expression rules are applied to isolate dates and other numerical answers. Second, person and location entities are extracted using Named Entity Recognition. Third, a rule based approach is adopted to extract the remaining proper nouns. Finally, the remaining answers are labeled into common noun, verb and adjective phrases, or other if no labels were found. Answer type distribution is shown in table 4.</figDesc><table><row><cell>Answer type</cell><cell cols="2">Freq [%] Example</cell></row><row><cell>Common noun</cell><cell>26.6</cell><cell>rencontres</cell></row><row><cell>Person</cell><cell>14.6</cell><cell>John More</cell></row><row><cell>Other proper nouns</cell><cell>13.8</cell><cell>Grand Prix d'Italie</cell></row><row><cell>Other numeric</cell><cell>13.6</cell><cell>1,65 m</cell></row><row><cell>Location</cell><cell>14.1</cell><cell>Normandie</cell></row><row><cell>Date</cell><cell>7.3</cell><cell>1815</cell></row><row><cell>Verb</cell><cell>6.6</cell><cell>être dépoussiéré</cell></row><row><cell>Adjective</cell><cell>2.6</cell><cell>méprisant, distant et sec</cell></row><row><cell>Other</cell><cell>0.9</cell><cell>gimmick</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Answer type by frequency for the development set of FQuAD1.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>first demonstrates that the annotation process issued a wide range of question types, underlining the fact that What (que) represents almost half (47.8%) of the corpus. This important proportion may be explained by this formulation encompassing both the English What and Which, as well as a possible natural bias in the annotators way of asking questions. Our intuition is that this bias is the same during inference, as it originates from native French structure.</figDesc><table><row><cell>Question</cell><cell cols="2">Freq [%] Example</cell></row><row><cell>What (que)</cell><cell>47.8</cell><cell>Quel pays parvient à ...</cell></row><row><cell>Who</cell><cell>12.2</cell><cell>Qui va se marier bientôt ?</cell></row><row><cell>Where</cell><cell>9.6</cell><cell>Où est l'échantillon ...</cell></row><row><cell>When</cell><cell>7.6</cell><cell>Quand a eu lieu la ...</cell></row><row><cell>Why</cell><cell>5.3</cell><cell>Pourquoi l'assimile ...</cell></row><row><cell>How</cell><cell>6.8</cell><cell>Comment est le prix ...</cell></row><row><cell>How many</cell><cell>5.6</cell><cell>Combien d'albums ...</cell></row><row><cell>What (quoi)</cell><cell>4.1</cell><cell>De quoi est faite la ...</cell></row><row><cell>Other</cell><cell>1</cell><cell>Donner un avantage de ...</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Question type by frequency for the development set of FQuAD1.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Quel est le sujet principal du film ?Context: Le sujet majeur du film est le conflit de Rick Blaine entre l'amour et la vertu : il doit choisir entre... Quand John Gould a-t-il décrit la nouvelle espèce d'oiseau ?Context: E. c. albipennis décrite par John Gould en 1841, se rencontre dans le nord du Queensland, l'ouest du golfe de Carpentarie dans le Territoire du Nord et dans le nord de l'Australie-Occidentale. Combien d'auteurs ont parlé de la merveille du monde de Babylone ?Context: Dès les premières campagnes de fouilles, on chercha la « merveille du monde » de Babylone : les Jardins suspendus décrits par cinq auteurs... Qu'est ce qui rend la situation de menace des cobs précaire ?Context: En 1982, les chercheurs en concluent que le cob normand est victime de consanguinité, de dérive génétique et de la disparition de ses structures de coordination. L'âge avancé de ses éleveurs rend sa situation précaire.</figDesc><table><row><cell>Reasoning</cell><cell>Example</cell><cell>Frequency</cell></row><row><cell>Synonymy</cell><cell cols="2">Question: 35.2 %</cell></row><row><cell>World knowledge</cell><cell cols="2">Question: 11.1 %</cell></row><row><cell>Syntactic variation</cell><cell cols="2">Question: 57.4 %</cell></row><row><cell>Multiple sentence reasoning</cell><cell cols="2">Question: 17.6 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>table 7 reports the results obtained for FQuAD1.0 and FQuAD1.1. The human score on FQuAD1.0 reaches 92.1% F1 and 78.4% EM on the test set and 92.6% and 79.5% on the development set. On FQuAD1.1, it reaches 91.2% F1 and 75.9% EM on the test set and 92.1% and 78.3% on the development set. We observe that there is a noticeable gap between the human performance on FQuAD1.0 test dataset and the human performance on the new samples of FQuAD1.1 with 78.4% EM score on the 2189 questions of FQuAD1.0 test set and 74.1% EM score on the 3405 new questions of FQuAD1.1 test set.As explained in section 3 we insisted in our annotation guidelines of FQuAD1.1 that the questions should be more difficult. This gap in human performance constitutes for us a proof that answering to FQuAD1.1 new questions is globally more difficult than answering to FQuAD1.0 questions, hence making the final FQuAD1.1 dataset even more challenging.</figDesc><table><row><cell>Dataset</cell><cell cols="2">F1 [%] EM [%]</cell></row><row><cell>FQuAD1.0-test.</cell><cell>92.1</cell><cell>78.4</cell></row><row><cell>FQuAD1.1-test</cell><cell>91.2</cell><cell>75.9</cell></row><row><cell>"FQuAD1.1-test new samples"</cell><cell>90.5</cell><cell>74.1</cell></row><row><cell>FQuAD1.0-dev</cell><cell>92.6</cell><cell>79.5</cell></row><row><cell>FQuAD1.1-dev</cell><cell>92.1</cell><cell>78.3</cell></row><row><cell>"FQuAD1.1-dev new samples"</cell><cell>91.4</cell><cell>76.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :table 8 .</head><label>78</label><figDesc>Human Performance on FQuAD 5.3 Comparing FQuAD1.1 and SQuAD1.1The SQuAD1.1 dataset[Rajpurkar et al., 2016]   reports a human score for the test set equal to 91.2% F1 and 82.3% EM. Comparing the English score with the French ones, we notice that they are the same in terms of F1 score but differ by 6% on the Exact Match. This difference indicates a potential structural difference between FQuAD1.1 and SQuAD1.1. To better understand it we first compare the answer type distributions, then we compare the answer lengths for both datasets and finally we explore how the evaluation score varies with the answer length. For both datasets, the most represented answer type is Common Noun with FQuAD1.1 scoring 26.6% and SQuAD1.1 scoring 31.8%. The less represented ones are Adjective and Other which have a noticeable higher proportion for SQuAD1.1 than FQuAD1.1 Compared to SQuAD1.1, a significant difference exists on structured entities such as Person, Location, and Other Numeric where FQuAD1.1 consistently scores above SQuAD1.1 with the exception of the Date category where FQuAD scores less. Based on these observations, it is difficult to understand the difference in human score between the two datasets.</figDesc><table><row><cell>Answer type</cell><cell cols="2">FQuAD1.1 [%] SQuAD1.1 [%]</cell></row><row><cell>Common noun</cell><cell>26.6</cell><cell>31.8</cell></row><row><cell>Person</cell><cell>14.6</cell><cell>12.9</cell></row><row><cell>Other proper nouns</cell><cell>13.8</cell><cell>15.3</cell></row><row><cell>Location</cell><cell>14.1</cell><cell>4.4</cell></row><row><cell>Date</cell><cell>7.3</cell><cell>8.9</cell></row><row><cell>Other numeric</cell><cell>13.6</cell><cell>10.9</cell></row><row><cell>Verb</cell><cell>6.6</cell><cell>5.5</cell></row><row><cell>Adjective</cell><cell>2.6</cell><cell>3.9</cell></row><row><cell>Other</cell><cell>0.9</cell><cell>2.7</cell></row></table><note>Answer type distribution The comparison in answer type distribution between the FQuAD1.1 and SQuAD1.1 datasets are reported in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table /><note>Answer type comparison for the develop- ment sets of FQuAD1.1 and SQuAD1.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>For this purpose, we present a learning curve obtained on the FQuAD1.1 test set by training CamemBERT BASE on an increasing number of question and answer samples. Both the EM and F1 scores are reported on the learning curve.</figDesc><table><row><cell>PIAF The French Dataset PIAF has been re-</cell></row><row><cell>leased after the first release of the present work. In</cell></row><row><cell>order to assess the impact of the PIAF released sam-</cell></row><row><cell>ples (3885 training samples), we perform two exper-</cell></row><row><cell>iments using PIAF. First, we evaluate the Camem-</cell></row><row><cell>BERT models fine-tuned on FQuAD1.0 on the new</cell></row><row><cell>samples. Second, we concatenate FQuAD1.0 and</cell></row><row><cell>PIAF to train a new model and evaluate them on</cell></row><row><cell>the test set of FQuAD1.1 to understand if the new</cell></row><row><cell>samples bring additional score.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>table 9</head><label>9</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">, while training experiments</cell></row><row><cell cols="5">on FQuAD1.0-train are summed up in table 10.</cell></row><row><cell cols="5">The benchmark includes the training experiments</cell></row><row><cell cols="5">for CamemBERT, FlauBERT, Multilingual BERT</cell></row><row><cell cols="5">and XLM-R on training sets of FQuAD1.1 and</cell></row><row><cell cols="5">FQuAD1.0 All the models are evaluated on the</cell></row><row><cell cols="4">FQuAD1.1 test and development sets.</cell><cell></cell></row><row><cell></cell><cell cols="4">FQuAD1.1-test FQuAD1.1-dev</cell></row><row><cell>Model</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell></row><row><cell>Human Perf.</cell><cell>91.2</cell><cell>75.9</cell><cell>92.1</cell><cell>78.3</cell></row><row><cell>CamemBERTBASE</cell><cell>88.4</cell><cell>78.4</cell><cell>88.1</cell><cell>78.1</cell></row><row><cell cols="2">CamemBERTLARGE 92.2</cell><cell>82.1</cell><cell>91.8</cell><cell>82.4</cell></row><row><cell>FlauBERTBASE</cell><cell>77.6</cell><cell>66.5</cell><cell>76.3</cell><cell>65.5</cell></row><row><cell>FlauBERTLARGE</cell><cell>80.5</cell><cell>69.0</cell><cell>79.7</cell><cell>69.3</cell></row><row><cell>mBERT</cell><cell>86.0</cell><cell>75.4</cell><cell>86.2</cell><cell>75.5</cell></row><row><cell>XLM-RBASE</cell><cell>85.9</cell><cell>75.3</cell><cell>85.5</cell><cell>74.9</cell></row><row><cell>XLM-RLARGE</cell><cell>89.5</cell><cell>79.0</cell><cell>89.1</cell><cell>78.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Results of the experiments for various monolingual and multilingual models carried out on the training dataset of FQuAD1.1-train and evaluated on test and development sets of FQuAD1.1</figDesc><table><row><cell></cell><cell cols="4">FQuAD1.1-test FQuAD1.1-dev</cell></row><row><cell>Model</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell></row><row><cell>Human Perf.</cell><cell>91.2</cell><cell>75.9</cell><cell>92.1</cell><cell>78.3</cell></row><row><cell>CamemBERTBASE</cell><cell>86.0</cell><cell>75.8</cell><cell>85.5</cell><cell>74.1</cell></row><row><cell cols="2">CamemBERTLARGE 91.5</cell><cell>82.0</cell><cell>91.0</cell><cell>81.2</cell></row><row><cell>mBERT</cell><cell>83.9</cell><cell>72.3</cell><cell>83.1</cell><cell>71.8</cell></row><row><cell>XLM-RBASE</cell><cell>82.2</cell><cell>71.4</cell><cell>82.4</cell><cell>71.0</cell></row><row><cell>XLM-RLARGE</cell><cell>88.7</cell><cell>78.5</cell><cell>88.2</cell><cell>77.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Results of the experiments for various monolingual and multilingual models carried out on the training dataset of FQuAD1.0-train and evaluated on test and development sets of FQuAD1.1 Monolingual models The CamemBERT BASE trained on FQuAD1.1 reaches 88.4% F1 and 78.4% EM as reported on 9. Interestingly, the base version surpasses the Human Score in terms of Exact Match on the test set. The best model, CamemBERT LARGE trained on FQuAD1.1 reaches a performance of 92.2% F1 and 82.1% EM on the test set, which is the highest score across the experiments and surpasses already the Human Performance for both metrics on the test and development sets. By means of comparison, the best model of the SQuAD1.1 leader-board reaches 95.1% F1 and 89.9% EM on the SQuAD1.1 test set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Performance on question types. F 1 h and EM h refer to human scores</figDesc><table><row><cell>Answer Type</cell><cell>F 1</cell><cell>EM</cell><cell cols="2">F 1 h EM h</cell></row><row><cell>Date</cell><cell cols="3">95.8 82.1 92.6</cell><cell>78.1</cell></row><row><cell>Other</cell><cell>94.6</cell><cell cols="2">75.6 84.4</cell><cell>63.7</cell></row><row><cell>Location</cell><cell>92.8</cell><cell cols="2">80.7 92.0</cell><cell>78.5</cell></row><row><cell>Other numeric</cell><cell>92.8</cell><cell cols="2">79.1 91.7</cell><cell>76.7</cell></row><row><cell>Person</cell><cell>92.5</cell><cell cols="2">80.8 93.4</cell><cell>82.6</cell></row><row><cell cols="2">Other proper nouns 92.5</cell><cell cols="2">78.3 91.9</cell><cell>78.0</cell></row><row><cell>Common noun</cell><cell>91.3</cell><cell cols="2">74.4 89.8</cell><cell>73.1</cell></row><row><cell>Adjective</cell><cell>89.6</cell><cell cols="2">73.1 90.8</cell><cell>71.6</cell></row><row><cell>Verb</cell><cell>88.5</cell><cell cols="2">58.7 87.7</cell><cell>60.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Performance on answer types. F 1 h and EM h refer to human scores reach acceptable results on the reading comprehension task. However, to outperform the Human Score, i.e. 91.2% and 75.9 %, a larger number of samples is required. In the present case CamemBERT BASE outperforms the Human Exact Match after it us trained on 30k samples or more. Evolution of the F1 and EM scores for CamemBERT BASE depending on the number of samples in the training dataset PIAF Dataset The experiments carried out on PIAF are reported in table 13. To ease the comparison we also add the results from table 10. The results show that the F1 and EM performances reach a significantly lower level than on FQuAD1.1test. One of the reasons for such a gap is the fact that the PIAF dataset does not include several answers per question as it is the case in SQuAD1.1 or in the present work.</figDesc><table><row><cell>Learning curve The learning curve is obtained</cell></row><row><cell>by performing several experiments with an increas-</cell></row><row><cell>ing number of question and answer samples ran-</cell></row><row><cell>domly taken from the FQuAD1.1 dataset. For each</cell></row><row><cell>experiment, CamemBERT BASE is fine-tuned on the</cell></row><row><cell>training subset and is evaluated on the FQuAD1.1</cell></row><row><cell>test set. The F1 scores and Exact Match are re-</cell></row><row><cell>ported on the figure 5 with respect to the number of</cell></row><row><cell>samples involved in the training. The figure shows</cell></row><row><cell>that both the F1 and EM score follow the same</cell></row><row><cell>trend. First, the model is quickly improving upon</cell></row><row><cell>the first 10k samples. Then, F1 and EM are pro-</cell></row><row><cell>gressively flattening upon augmenting the number</cell></row><row><cell>of training samples. Finally, they reach a maximum</cell></row><row><cell>value of respectively 88.4% and 78.4%. The results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13</head><label>13</label><figDesc>SQuAD1.1 and FQuAD1.1 and then evaluated respectively on the development sets of FQuAD1.1 and SQuAD1.1 in order to evaluate the performance of zero-shot learning set-up.</figDesc><table><row><cell>: Results of the experiments for Camem-</cell></row><row><cell>BERT trained on FQuAD1.0-train and eval-</cell></row><row><cell>uated on PIAF. (1) has been trained with</cell></row><row><cell>CamemBERT BASE , (2) has been trained with</cell></row><row><cell>CamemBERT LARGE .</cell></row><row><cell>7.2 Cross-lingual Reading Comprehension</cell></row><row><cell>The results for the experiments on the cross-lingual</cell></row><row><cell>set-up are reported in table 14. On one hand, the</cell></row><row><cell>French monolingual models are fine-tuned on the</cell></row><row><cell>French translated version of SQuAD1.1 and evalu-</cell></row><row><cell>ated on the development set of FQuAD1.1. On the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14</head><label>14</label><figDesc>Compared to CamemBERT BASE fine-tuned on FQuAD, this result is about 6.3 points less effective in terms of F1 score and even more important in terms of EM score, i.e. 10.3. Second, the results for CamemBERT LARGE show an improved performance of 87.5% F1 and 73.9% EM. Compared to the native version, this result is lower by 4.3 points in terms of F1 Score and 8.5 points in terms of EM. These experiments show therefore that models finetuned on translated data do not perform as well as when they are fine-tuned on native dataset. This difference is probably explained by the fact that NMT produces translation inaccuracies that impact the EM score more than F1 score. When we merge the native and the translated dataset into what we call the Augmented dataset, we do not observe a significant performance improvement. Interestingly, the CamemBERT LARGE model performs slightly worse when fine-tuned on translated samples.Zero-shot learningTo evaluate how multilanguage models transfer on other languages similarly to and<ref type="bibr" target="#b0">Artetxe et al. [2019]</ref>, we report the results of our experiments with XLM-R BASE and XLM-R LARGE in 14. We find that XLM-R BASE trained on FQuAD1.1 reaches 83.0% F1 and 73.5 % EM on the SQuAD1.1 dev set. When trained on SQuAD1.1 it reaches 81.4% F1 and 68.4% EM on the FQuAD1.1 dev set. Next, we find that XLM-R LARGE reaches 88.8% F1 and 79.5% on the SQuAD1.1 dev set when trained on FQuAD1.1 and 86.1% F1 and 73.2% EM on the FQuAD1.1 dev set when trained on SQuAD1.1. The results show that the models perform very well compared to the results when trained on the native French and native English datasets. Indeed, XLM-R BASE shows a drop of only 4.1% and 6.5% in terms of F1 and EM score on the FQuAD1.1 dev set when compared to the model trained on the native french samples. And XLM-R LARGE show a drop on 3.0% and 5.7% in terms of F1 and EM score. Note that the same relationship can be observed for the model trained on FQuAD1.1 and evaluated on SQuAD1.1 although the drop in performance is slightly less important. Interestingly, the large models perform in general very well on the cross-lingual zero-shot set-up.</figDesc><table><row><cell>: Results for the zero-shot learning experi-</cell></row><row><cell>ments on the SQuAD1.1 and FQuAD1.1 develop-</cell></row><row><cell>ment sets</cell></row><row><cell>Translated Reading Comprehension First,</cell></row><row><cell>the results for CamemBERT BASE fine-tuned on</cell></row><row><cell>the French translated version of SQuAD1.1. show a</cell></row><row><cell>performance of 81.8% F1 and 67.8% EM as reported</cell></row><row><cell>in 14.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://juniorcs.fr/en/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgments</head><p>We would like to warmly thank Robert Vesoul, Co-Director of CentraleSupélec's Digital Innovation Chair and CEO of Illuin Technology, for his help and support in enabling and funding this project while leading it through.</p><p>We would also like to thank Enguerran Henniart, Lead Product Manager of Illuin annotation platform, for his assistance and technical support during the annotation campaign.</p><p>We more globally thank Illuin Technology for technical support, infrastructure and funding. We are also grateful to the Illuin Technology team for their reviewing and constructive feedbacks.</p><p>We share our warm thanks to Professor Céline Hudelot, professor at CentraleSupélec in charge of the Computer Science department, and head of the MICS laboratory (Mathematics and Informatics CentraleSupélec), for her guidance and support in this and other research works.</p><p>Finally, we would also also like to thank Sebastian Ruder for his constructive feed-back on suggesting further experiments on the cross-lingual learning approach.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the cross-lingual transferability of monolingual representations. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multilingual extractive reading comprehension by runtime machine translation. CoRR, abs/1809.03275</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1809.03275" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic spanish translation of the squad dataset for multilingual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">Ruiz</forename><surname>Casimiro Pio Carrino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">A R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonollosa</surname></persName>
		</author>
		<idno>abs/1912.05200</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Quac : Question answering in context. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1808.07036</idno>
		<ptr target="http://arxiv.org/abs/1808.07036" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A span-extraction dataset for Chinese machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1600</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1600" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="5883" to="5889" />
		</imprint>
	</monogr>
	<note>November 2019. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wietse De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Van Cranenburgh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Gertjan Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nissim</surname></persName>
		</author>
		<idno>abs/1912.09582</idno>
	</analytic>
	<monogr>
		<title level="m">Bertje: A dutch bert model</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sberquad -russian reading comprehension dataset: Description and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Efimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Braslavski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Crosslingual language model pretraining. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1901.07291" />
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Flaubert: Unsupervised language model pre-training for french</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Vial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jibril</forename><surname>Frej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Segonne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Crabbé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mlqa: Evaluating cross-lingual extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<idno>abs/1910.07475</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Korquad1.0: Korean qa dataset for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyoung</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoul</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Éric Villemonte de la Clergerie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Javier Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoann</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03894</idno>
	</analytic>
	<monogr>
		<title level="m">Djamé Seddah, and Benoît Sagot. CamemBERT: a Tasty French Language Model. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Javier Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-02148693" />
	</analytic>
	<monogr>
		<title level="m">7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)</title>
		<meeting><address><addrLine>Cardiff, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scaling neural machine translation. CoRR, abs/1806.00187</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1806.00187" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">How multilingual is multilingual bert? CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.01502" />
		<imprint>
			<date type="published" when="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Project piaf: Building a native french question-answering dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keraron</forename><surname>Rachel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lancrenon</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bras</forename><surname>Mathilde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allary</forename><surname>Frédéric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moyse</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scialom</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Soriano-Morales Edmundo-Pavel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Staiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference on Language Resources and Evaluation. The International Conference on Language Resources and Evaluation</title>
		<meeting>the 12th Conference on Language Resources and Evaluation. The International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<ptr target="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
	<note>November 2016. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>abs/1806.03822</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Coqa: A conversational question answering challenge. CoRR, abs/1808.07042</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1808.07042" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MCTest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D13-1020" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Nlp progress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<ptr target="https://nlpprogress.com" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multilingual question answering from formatted text applied to conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wissam</forename><surname>Siblini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Pasqual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Lavielle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Cauchois</surname></persName>
		</author>
		<idno>abs/1910.04659</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Newsqa: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno>abs/1611.09830</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Generalized autoregressive pretraining for language understanding. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<idno>abs/1809.10735</idno>
		<ptr target="http://arxiv.org/abs/1809.10735" />
		<imprint>
			<date type="published" when="1906" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Mark Yatskar. A qualitative comparison of coqa, squad 2.0 and quac</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Elle fut toutefois fréquemment évoquée dans la presse britannique et étrangère. « Un second référendum est la seule façon de clore le débat » du Brexit a affirmé au journal Le Monde Tony Blair. Le député britannique Dominic Grieve expulsé du Parti conservateur avec 21 autres collègues en septembre 2019 pour avoir voté contre Boris Johnson afin de bloquer une sortie sans accord, a affirmé dans un entretien à France 24 « que les Britanniques doivent connaître les conséquences d&apos;un « no deal » » et va plus loin en affirmant : « je ne suis pas optimiste sur le fait qu&apos;il soit possible de trouver un accord que le Parlement veuille. La seule solution est un second référendum. » Question: Quel évènement a été longuement mentionné dans la presse étrangère ? Answer: La possibilité d&apos;un second référendum Question: Combien de politiques ont été renvoyés du parti conservateur ? Answer: 21 Question: Sur quoi porte le second référendum ? Answer: projet de sortie du Royaume-Uni de l&apos;Union européenne avait Question: Quel journal a accordé une interview à Dominic Grieve ?</title>
		<imprint/>
		<respStmt>
			<orgName>A Example model predictions Article: Brexit Paragraph: La possibilité d&apos;un second référendum sur la question du projet de sortie du Royaume-Uni de l&apos;Union européenne avait peu de chance de se réaliser avec le Premier ministre Boris Johnson</orgName>
		</respStmt>
	</monogr>
	<note>Answer: à France 24 « Question: Quand Dominic Grieve a été renvoyé du parti conservateur ? Answer: septembre 2019</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Le RS15 (rapport spécial sur le réchauffement climatique de 1,5°C) résume, d&apos;une part, les recherches existantes sur l&apos;impact qu&apos;un réchauffement de 1,5°C aurait sur la planète et</title>
	</analytic>
	<monogr>
		<title level="m">Article: Rapport du GIEC Paragraph: Le réchauffement planétaire atteindra les 1,5°C entre 2030 et 2052 si la température continue d&apos;augmenter à ce rythme</title>
		<imprint/>
	</monogr>
	<note>d&apos;autre part, les mesures nécessaires pour limiter ce réchauffement planétaire</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accord de Paris, les émissions nettes augmenteraient par rapport à 2010, entraînant un réchauffement d&apos;environ 3°C d&apos;ici 2100, et davantage par la suite. En revanche, pour limiter le réchauffement au-dessous ou proche de 1,5°C, il faudrait diminuer les émissions nettes d&apos;environ 45 % d&apos;ici 2030 et atteindre 0 % en 2050. Même pour limiter le réchauffement climatique à moins de 2°C, les émissions de CO2 devraient diminuer de 25 %</title>
	</analytic>
	<monogr>
		<title level="m">Même en supposant la mise en oeuvre intégrale des mesures déterminées au niveau national soumises par les pays dans le cadre de l</title>
		<imprint/>
	</monogr>
	<note>ici 2030 et de 100 % d&apos;ici 2075</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dans ces filières, les énergies renouvelables devraient fournir 70 à 85 % de l&apos;électricité en 2050 et la part de l&apos;énergie nucléaire est modélisée pour augmenter. Il suppose également que d&apos;autres mesures soient prises simultanément : par exemple, les émissions autres que le CO2 (comme le méthane, le noir de carbone, le protoxyde d&apos;azote) doivent être réduites de manière similaire, la demande énergétique reste inchangée, voire réduite de 30 % ou compensée par des méthodes sans précédentes d&apos;élimination du dioxyde de carbone à mettre au point, tandis que de nouvelles politiques et recherches permettent d&apos;améliorer l</title>
	</analytic>
	<monogr>
		<title level="m">Les scénarios qui permettraient une telle réduction d&apos;ici 2050 ne permettraient de produire qu</title>
		<imprint/>
	</monogr>
	<note>environ 8 % de l&apos;électricité mondiale par le gaz et 0 à 2 % par le charbon (à compenser par le captage et le stockage du dioxyde de carbone). efficacité de l&apos;agriculture et de l&apos;industrie. Question: Quand risquons nous d&apos;atteindre un réchauffement à 1.5 degrés? Answer: entre 2030 et 2052</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Answer: les recherches existantes sur l&apos;impact qu&apos;un réchauffement de 1,5°C aurait sur la planète Question: Comment améliorer l&apos;efficacité de l&apos;industrie ? Answer: de nouvelles politiques et recherches Question: Quelles sont les conséquences d&apos;un scénario limitant le réchauffement à 1,5 degrés ? Answer: diminuer les émissions nettes d&apos;environ 45 % d&apos;ici 2030 et atteindre 0 % en 2050. Question: Quelle part d&apos;énergie doit être fournie par le renouvelable pour respecter l&apos;accord ? Answer: 70 à 85 % Question: Quelle source d&apos;énergie sera limitée à une production de 8</title>
	</analytic>
	<monogr>
		<title level="m">Question: Quels sont les gaz à effet de serre autres que le CO2? Answer: méthane, le noir de carbone, le protoxyde d&apos;azote) Question: Quelles recherches sont résumées dans ce rapport</title>
		<imprint/>
	</monogr>
	<note>% si les émissions maximales sont respectées ? Answer: gaz</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
