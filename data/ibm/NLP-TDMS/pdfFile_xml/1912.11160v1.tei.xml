<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RecVAE: a New Variational Autoencoder for Top-N Recommendations with Implicit Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-02">2020. Feb-ruary 3-7, 2020. February 3-7, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Alekseev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Tutubalina</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><forename type="middle">I</forename><surname>Nikolenko</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Samsung-PDMI Joint AI Center</orgName>
								<orgName type="institution" key="instit2">Steklov Institute of Mathematics at St. Petersburg</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Samsung-PDMI Joint AI Center, Steklov Institute of Mathematics at St. Petersburg</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Samsung-PDMI Joint AI Center, Steklov Institute of Mathematics at St. Petersburg</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Moscow Institute of Physics and Technology</orgName>
								<orgName type="laboratory">Neural Systems and Deep Learning Laboratory</orgName>
								<address>
									<country>Russia Sergey I. Nikolenko</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Samsung-PDMI Joint AI Center, Steklov Institute of Mathematics at St. Petersburg</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RecVAE: a New Variational Autoencoder for Top-N Recommendations with Implicit Feedback</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Reference Format: Ilya Shenbin</title>
						<meeting> <address><addrLine>Houston, TX, USA WSDM; Houston, TX, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">20</biblScope>
							<date type="published" when="2020-02">2020. Feb-ruary 3-7, 2020. February 3-7, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3336191.3371831</idno>
					<note>. ACM, New York, NY, USA, 9 pages. This is the author&apos;s version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in The Thirteenth ACM International Conference on Web Search and Data Mining (WSDM &apos;20), February 3-7, 2020, Houston, TX, USA, https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Latent variable models</term>
					<term>Learn- ing from implicit feedback</term>
					<term>Regularization</term>
					<term>Neural networks</term>
					<term>• In- formation systems → Learning to rank KEYWORDS deep learning, collaborative filtering, variational autoencoders</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent research has shown the advantages of using autoencoders based on deep neural networks for collaborative filtering. In particular, the recently proposed Mult-VAE model, which used the multinomial likelihood variational autoencoders, has shown excellent results for top-N recommendations. In this work, we propose the Recommender VAE (RecVAE) model that originates from our research on regularization techniques for variational autoencoders. RecVAE introduces several novel ideas to improve Mult-VAE, including a novel composite prior distribution for the latent codes, a new approach to setting the β hyperparameter for the β-VAE framework, and a new approach to training based on alternating updates. In experimental evaluation, we show that RecVAE significantly outperforms previously proposed autoencoder-based models, including Mult-VAE and RaCT, across classical collaborative filtering datasets, and present a detailed ablation study to assess our new developments. Code and models are available at https://github.com/ilya-shenbin/RecVAE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Matrix factorization (MF) has become the industry standard as the foundation of recommender systems based on collaborative filtering. However, there are certain general issues that arise with this family of models. First, the number of parameters in any matrix factorization model is huge: it linearly depends on the number of both users and items, which leads to slow model learning and overfitting. Second, to make a prediction for a new user/item based on their ratings, one has to run an optimization procedure in order to find the corresponding user/item embedding. Third, only a small amount of ratings are known for some (often for a majority of) users and items, which could also lead to overfitting. This makes it necessary to heavily regularize matrix factorization models, and standard L 1 or L 2 regularizers are hard to tune.</p><p>Recently proposed models such as the Collaborative Denoising Autoencoder (CDAE) <ref type="bibr" target="#b39">[39]</ref> partially solve these issues by using a parameterized function which maps user feedback to user embeddings. It performs regularization in an alternative way and makes it possible to predict item ratings for new users without additional iterative training. The Variational Autoencoder for Collaborative Filtering (Mult-VAE) <ref type="bibr" target="#b21">[22]</ref> is a subsequent improvement of CDAE that extends it to multinomial distributions in the likelihood, which are more suitable for recommendations.</p><p>In this work, we propose the Recommender VAE (RecVAE) model for collaborative filtering with implicit feedback based on the variational autoencoder (VAE) and specifically on the Mult-VAE approach. RecVAE presents a number of important novelties that together combine into significantly improved performance. First, we have designed a new architecture for the encoder network. Second, we have introduced a novel composite prior distribution for the latent code z in the variational autoencoder. The composite prior is a mixture of a standard Gaussian prior and the latent code distribution with parameters fixed from the previous iteration of the model (Section 3.3), an idea originating from reinforcement learning where it is used to stabilize training <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">30]</ref>. In the context of recommendations, we have also found that this prior improves training stability and performance. Third, we have developed a new approach to setting the hyperparameter β for the Kullback-Leibler term in the objective function (Section 3.4). We have found that β should be user-specific, β = β(x u ), and should depend on the amount of data (implicit feedback) available for a given user.</p><p>Finally, we introduce a novel approach for training the model. In RecVAE, training is done by alternating updates for the encoder and decoder (see <ref type="bibr">Section 3.5)</ref>. This approach has two important advantages. First, it allows to perform multiple updates of the encoder (a more complex network) for every update of the decoder (a very simple, single-layer network that contains item embeddings and biases). Second, it allows to use corrupted inputs (following the general idea of denoising autoencoders <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">31]</ref>) only for training the encoder while still training the decoder on clean input data. This is again beneficial for the final model training due to the differing complexities of the encoder and decoder.</p><p>As a result of the above novelties, our model significantly outperforms all autoencoder-based previous works and shows competitive or better results in comparison with other models across a variety of collaborative filtering datasets, including MovieLens-20M (ML-20M), Netflix Prize Dataset, and Million Songs Dataset (MSD).</p><p>The paper is organized as follows. In Section 2 we review the crucial components and approaches we are to employ in the proposed methods as well as other relevant prior art. In Section 3 we describe the basic Mult-VAE approach and our modifications. Section 4 contains the results of a comprehensive experimental study for our model, and Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK 2.1 Variational autoencoders and their extensions</head><p>The variational autoencoder (VAE) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">28]</ref> is a deep latent variable model able to learn complex distributions. We begin with a brief exposition of the basic assumptions behind VAE that have been extended for collaborative filtering in Mult-VAE and will be further extended in this work with RecVAE. First, under the assumption that the dataset belongs to a low-dimensional manifold embedded in a high-dimensional space, the marginal likelihood function can be expressed via a latent code z as p θ (x) = ∫ p θ (x |z)p(z)dz. Since the marginal likelihood function is intractable, it is usually approximated with the evidence lower bound (ELBO):</p><formula xml:id="formula_0">log p θ (x) ≥ L V AE = E q ϕ (z |x ) log p θ (x |z) − KL q ϕ (z|x) p(z) ,<label>(1)</label></formula><p>where KL is the Kullback-Leibler divergence, p(z) is the prior distribution, q ϕ (z|x) is a variational approximation of the posterior distribution defined as a parameterized function with ϕ, and θ are the parameters of p θ (x |z). This technique, known as amortized inference, provides additional regularization and allows to obtain variational parameters with a closed form function.</p><p>Variational autoencoders can be used not only as generative models but also for representation learning. β-VAE [10] is a modification of VAE designed to learn so-called disentangled representations by adding a regularization coefficient to the Kullback-Leibler term in the evidence lower bound. The objective function of β-VAE can still be considered as a valid ELBO with additional approximate posterior regularization that rescales the Kullback-Leibler divergence in formula (1) above by a factor of β <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">24]</ref>:</p><formula xml:id="formula_1">L β -V AE = E q ϕ (z |x ) log p θ (x |z) − βKL q ϕ (z|x) p(z) . (2)</formula><p>Denoising variational autoencoders (DVAE) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">31]</ref>, similar to denoising autoencoders, are trying to reconstruct the input from its corrupted version. The ELBO in this model is defined as</p><formula xml:id="formula_2">log p θ (x) ≥ L DVAE = E q ϕ (z |x ) E p(x |x ) log p θ (x |z) − KL q ϕ (z|x) p(z) . (3)</formula><p>It differs from VAE by the additional expectation E p(x |x ) , where p(x |x) is a noise distribution, usually Bernoulli or Gaussian. Similar to how denoising autoencoders are more robust and generalize better than regular autoencoders, DVAE improves over the performance of the basic VAE and makes it possible to learn more robust approximate posterior distributions.</p><p>The Conditional Variational Autoencoder (CVAE) <ref type="bibr" target="#b32">[32]</ref> is another VAE extension which is able to learn complex conditional distributions. Its ELBO is as follows:</p><formula xml:id="formula_3">log p θ (x |y) ≥ L CV AE = E q ϕ (z |x ,y) log p θ (x |z, y) − KL q ϕ (z|x, y) p θ (z, y) , (4)</formula><p>which is the same as the ELBO for regular VAE shown in (1) but with all distributions conditioned on y. VAE with Arbitrary Conditioning (VAEAC) <ref type="bibr" target="#b16">[17]</ref>, based on CVAE, solves the imputation problem for missing features, which is in many ways similar to collaborative filtering. It has an ELBO similar to <ref type="bibr" target="#b3">(4)</ref> where the variable are the unobserved features x b and the condition is x 1−b for some binary mask b:</p><formula xml:id="formula_4">log p θ,b (x b |x 1−b , b) ≥ L V AEAC = E q ϕ (z |x ,b) log p θ (x b |z, x 1−b , b) − −KL q ϕ (z|x, b) p θ (z | x 1−b , b) ; (5)</formula><p>an important point here is that the mask b can be different and even have different number of ones (unobserved features) for different inputs x. However, this approach cannot be directly applied to the implicit feedback case, which we consider in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Autoencoders and Regularization for Collaborative Filtering</head><p>Let U and I be the sets of users and items respectively in a collaborative filtering problem. Consider the implicit feedback matrix X ∈ {0, 1} |U |×|I | , where x ui = 1 if the uth user positively interacted with (liked, bought, watched etc.) the ith item (movie, good, article etc.) and x ui = 0 otherwise. We denote by x u the feedback vector of user u.</p><p>The basic idea of the Collaborative Denoising Autoencoder (CDAE) model <ref type="bibr" target="#b39">[39]</ref> is to reconstruct the user feedback vector x u from its corrupted versionx u . The corrupted vectorx u is obtained by randomly removing (setting to zero) some values in the vector x u . The encoder part of the model mapsx u to the hidden state. In CDAE, both encoder and decoder are single neural layers (so the model itself is basically a feedforward neural network with one hidden layer), with a user input node providing user-specific weights and the rest of the weights shared across all users; the latent representation z u and reconstructed feedbackx u are computed as</p><formula xml:id="formula_5">z u = σ W ⊤x u + V u + b ,<label>(6)</label></formula><formula xml:id="formula_6">x u = σ W ′ z u + b ′ ,<label>(7)</label></formula><p>where W and W ′ are input-to-hidden and hidden-to-output weight matrices respectively, and V u is the weight vector for the user input node). Note that the matrix W can be considered as a matrix of item embeddings, and the matrix V , as the matrix of user embeddings. Previous work also indicates that regularization plays a central role in collaborative filtering. Models based on matrix factorization (MF) with user/item embeddings almost invariably have an extremely large number of parameters that grows with dataset size; even a huge dataset with user ratings and other kinds of feedback cannot have more than a few dozen ratings per user (real users will not rate more items than that), so classical MF-based collaborative filtering models require heavy regularization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">26]</ref>. The standard solution is to use simple L 2 or L 1 regularizers for the embedding weights, although more flexible priors have also been used in literature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">29]</ref>. The works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">39]</ref> present an alternative way of regularization based on the amortization of user embeddings coupled with denoising <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b37">37]</ref>. Several more models are reviewed in Section 4.3.</p><p>The model which is nearest to our current work in prior art is Multinomial VAE (Mult-VAE) <ref type="bibr" target="#b21">[22]</ref>, an extension of variational autoencoders for collaborative filtering with implicit feedback. In the next section, we begin with a detailed description of this model and then proceed to presenting our novel contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH 3.1 Mult-VAE</head><p>We begin with a description of the Mult-VAE model proposed in <ref type="bibr" target="#b21">[22]</ref>. The basic idea of Mult-VAE is similar to VAE but with the multinomial distribution as the likelihood function instead of Gaussian and Bernoulli distributions commonly used in VAE. The generative model samples a k-dimensional latent representation z u for a user u, transforms it with a function f θ : R k → R |I | parameterized by θ , and then the feedback history x u of user u, which consists of n u interactions (clicks, purchases etc.), is assumed to be drawn from the multinomial distribution:</p><formula xml:id="formula_7">z u ∼ N (0, I), π (z u ) = softmax(f θ (z u )),<label>(8)</label></formula><p>x u ∼ Mult(n u , π (z u )).</p><p>Note that classical collaborative filtering models also follow this scheme with a linear f θ ; the additional flexibility of Mult-VAE comes from parameterizing f with a neural network with parameters θ .</p><p>To estimate θ one has to approximate the intractable posterior p(z u | x u ). This, similar to regular VAE, is done by constructing an evidence lower bound for the variational approximation where q(z u ) is assumed to be a fully factorized diagonal Gaussian distribution: q(z u ) = N µ u , diag(σ 2 u ) . The resulting ELBO is </p><formula xml:id="formula_9">L Mult-VAE = E q ϕ (z u |x u ) log p θ (x u |z u ) − βKL q ϕ (z u |x u ) p(z u ) ,<label>(10)</label></formula><p>which follows the general VAE structure with an additional hyperparameter β that allows to achieve a better balance between latent code independence and reconstruction accuracy, following the β-VAE framework <ref type="bibr" target="#b10">[11]</ref>. The likelihood p θ (x u |z u ) in the ELBO of Mult-VAE is multinomial distribution. The logarithm of multinomial likelihood for a single user u in Mult-VAE is now</p><formula xml:id="formula_10">log Mult(x u |n u , p u ) = i x ui log p ui + C u ,<label>(11)</label></formula><p>where C u is the logarithm of the normalizing constant which is ignored during training. We treat it as a sum of cross-entropy losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>Before introducing novel regularization techniques, we provide a general description of the proposed model. Our model is inherited from Mult-VAE, but we also suggest some architecture changes. The general architecture is shown on <ref type="figure" target="#fig_0">Figure 1</ref>; the figure reflects some of the novelties we will discuss below in this section. The first change is that we move to a denoising variational autoencoder, that is, move from the ELBO as shown in <ref type="formula" target="#formula_0">(10)</ref> to</p><formula xml:id="formula_11">L Mult-VAE = E q ϕ (z u |x u ) E p(x u |x u ) log p θ (x u |z u )− −βKL q ϕ (z u |x u ) p(z u ) . (12)</formula><p>Note that while the original paper <ref type="bibr" target="#b21">[22]</ref> compares Mult-VAE with Mult-DAE, a denoising autoencoder that applies Bernoulli-based noise to the input but does not have the VAE structure (Mult-DAE is a regular denoising autoencoder), in reality the authors used denoising for Mult-VAE as well. This is evidenced both by their code base and by our experiments, where we were able to match the results of <ref type="bibr" target="#b21">[22]</ref> when we used denoising and got nowhere even close without denoising (we will return to this discussion in Section 3.5). According to our intuition and experiments, the input noise for the denoising autoencoder and latent variable noise of Monte Carlo integration play different roles: the former forces the model not only to reconstruct the input vector but also to predict unobserved feedback, while the latter leads to more robust embedding learning.</p><p>Similar to Mult-VAE, we use the noise distribution p(x |x) defined as elementwise multiplication of the vector x by a vector of Bernoulli random variables parameterized by their mean µ noise .</p><p>We keep the structure of both likelihood and approximate posterior unchanged:</p><formula xml:id="formula_12">p θ (x u |z u ) = Mult(x |n u , π (z u )), (13) π (z u ) = softmax(f θ (z u )),<label>(14)</label></formula><formula xml:id="formula_13">f θ (z u ) = W z u + b,<label>(15)</label></formula><formula xml:id="formula_14">q ϕ (z u |x u ) = N (z u |ψ ϕ (x u )),<label>(16)</label></formula><p>where ψ ϕ (·) is the inference network, parameterized by ϕ, that predicts the mean vector and (diagonal) covariance matrix for the latent code z u . However, we change the underlying neural networks. Our proposed architecture for the inference network is shown on <ref type="figure">Figure 3</ref>; it uses the ideas of densely connected layers from dense CNNs <ref type="bibr" target="#b14">[15]</ref>, swish activation functions <ref type="bibr" target="#b27">[27]</ref>, and layer normalization <ref type="bibr" target="#b20">[21]</ref>. The decoder network is a simple linear layer with softmax activation, where θ = {W , b}. Here W and b can be considered as the item embeddings matrix and the item bias vector respectively. In a similar way, we can consider encoder ψ ϕ (·) as a function that maps user feedback to user embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Composite prior</head><p>Both input and output of Mult-VAE are high-dimensional sparse vectors. Besides, while shared amortized approximate posterior regularizes learning, posterior updates for some parts of the observed data may hurt variational parameters corresponding to other parts of the data. These features may lead to instability during training, an effect similar to a well-known "forgetting" effect in reinforcement learning. Previous work on policy-based reinforcement learning showed that it helps to regularize model parameters by bringing them closer to model parameters on the previous epoch <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">30]</ref>; in reinforcement learning, it helps to make the final score grow more smoothly, preventing the model from forgetting good behaviours. A direct counterpart of these ideas in our setting would be to use a standard Gaussian prior for the latent code z and add a separate regularization term in the form of the KL divergence between the new parameter distribution q ϕ (z|x) and the previous one q ϕ ol d (z|x), where ϕ old are the parameters from the previous epoch of the learning process. However, in our experiments it worked better to unite these two ideas (prior and additional regularizer) by using a composite prior</p><formula xml:id="formula_15">p(z|ϕ old , x) = αN (z|0, I) + (1 − α)q ϕ ol d (z|x),<label>(17)</label></formula><p>i.e., a convex combination (with 0 ≤ α ≤ 1) of a standard normal distribution and an approximate posterior q ϕ ol d (z|x) with fixed parameters carried over from the previous epoch. The second term regulates large steps during variational parameters optimization and can be interpreted as an auxiliary loss function, while the first term prevents overfitting. Note that this approach is not equivalent mathematically to a Gaussian prior and a separate KL regularizer that pulls current variational parameters to their previous values, and the fact that it works better makes this composite prior into a new meaningful contribution. We also note several works that argue for the benefits of trainable and/or complex prior distributions <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b40">40]</ref>, although in our experiments these approaches have not brought any improvements compared to the prior proposed above.</p><p>Conditioning the prior on variational parameters from the previous training epoch converts our model to a conditional variational autoencoder where we assume both approximate posterior and likelihood to be conditionally independent of variational parameters from the previous epoch. Comparing our model to VAEAC <ref type="bibr" target="#b16">[17]</ref>, the latter has a noised conditional prior while in our model we add noise to the approximate posterior during training. Also, unlike VAEAC, we do not train prior parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Rescaling KL divergence</head><p>We have already mentioned the β-VAE framework <ref type="bibr" target="#b10">[11]</ref> which is crucial for the performance of Mult-VAE and, by extension, for RecVAE. However, the question of how to choose or change β is still not solved conclusively. Some works (see, e.g., <ref type="bibr" target="#b4">[5]</ref>) advocate to increase the value of β from 0 to 1 during training, with 1 yielding the basic VAE model and the actual ELBO. For the training of Mult-VAE, the authors of <ref type="bibr" target="#b21">[22]</ref> proposed to increase β from 0 up to some constant. In our experiments, we have not found any improvements when β is set to increase over some schedule, so we propose to keep scale factor fixed; this also makes it easier to find the optimal value for this hyperparameter.</p><p>Moreover, we propose an alternative view on KL divergence rescaling. Assume that the user feedback data is partially observed. We denote by X o u the set of items which user u has positively interacted with (according to observed data); X f u similarly denotes the full set of items which user u has positively interacted with (together with unobserved items). Items in X f u and X o u are represented in one-hot encoding, so that x u = a ∈X o u 1 a , where x u is the feedback vector for user u and 1 a is a vector with one 1 in the position corresponding to item a. We denote</p><formula xml:id="formula_16">x u = a ∈X o u 1 a , x f u = a ∈X f u 1 a and abbreviate KL u = KL q ϕ (z u |x u ) p(z u ) and KL f u = KL q ϕ (z u |x f u ) p(z u ) .</formula><p>Consider the evidence lower bound of a variational autoencoder (1) with multinomial likelihood. It can be rewritten as follows:</p><formula xml:id="formula_17">L = E q ϕ (z u |x f u ) log Mult(x f u |π (z u )) − KL f u = E q ϕ (z u |x f u )        a ∈X f u log Cat(1 a |π (z u )) − KL f u        + C u = E q ϕ (z u |x f u ) a ∈X f u log Cat(1 a |π (z u )) − 1 |X f u | KL f u + C u ,<label>(18)</label></formula><p>where Cat(1 a |p u ) = p ua is the categorical distribution, and C ′ u a constant that depends on normalizing constant of the multinomial distribution Mult(x f u |π (z u )), which does not affect optimization. We approximate the ELBO obtained above by summing over observed feedback only and assuming that q ϕ (z u |x u ) ≈ q ϕ (z u |x f u ) and therefore KL u ≈ KL f u . In the sequence of equalities below, we first approximate the sum over X </p><formula xml:id="formula_18">L ≈ |X f u | |X o u | E q ϕ (z u |x f u ) a ∈X o u log Cat(1 a |π (z u )) − 1 |X f u | KL f u + C ′ u ≈ |X f u | |X o u | E q ϕ (z u |x u ) a ∈X o u log Cat(1 a |π (z u )) − 1 |X f u | KL u + C ′ u = |X f u | |X o u | E q ϕ (z u |x u )       a ∈X o u log Cat(1 a |π (z u )) − |X o u | |X f u | KL u       + C ′ u = |X f u | |X o u | E q ϕ (z u |x u ) log Mult(x u |π (z u )) − |X o u | |X f u | KL u + C ′′ u ,</formula><p>where C ′ u and C ′′ u are constants related to normalizing constants of the multinomial distribution.</p><p>Finally, we make the assumption that |X f u | is the same for every u and denote γ = 1/|X f u |. While this assumption might look strange, in effect |X f u | is not merely unknown but is actually under our control: it is the number of items u has feedback about plus the number of items for recommendation. Therefore, we reduce all |X f u | into a single hyperparameter γ :</p><formula xml:id="formula_19">L ≈ 1 γ |X o u | E q ϕ (z u |x u ) log Mult(x u |π (z u )) − γ |X o u |KL u .<label>(19)</label></formula><p>In practice, we drop the 1/γ |X o u | factor in front of the expectation: it does not change the relation between the log likelihood and KL regularizer but rather changes the learning rate individually for each user, which slightly degraded performance in our experiments. The resulting KL divergence scaling factor,</p><formula xml:id="formula_20">β ′ = β ′ (x u ) = γ |X o u | = γ i x ui ,<label>(20)</label></formula><p>where γ is a constant hyperparameter shared across all users and choosen with cross-validation, works better (see below). In total, we have proposed and motivated an approach where the β constant in β-VAE is proportional to the amount of feedback available for the current user, |X o u |; this is an important modification that has led to significant improvements in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Alternating Training and Regularization by Denoising</head><p>Alternating least squares (ALS) <ref type="bibr" target="#b1">[2]</ref> is a popular technique for matrix factorization. We train our model in a similar way, alternating between user and item embeddings. User embeddings are amortized by the inference network, while each item embedding is trained individually. This means that the two groups of parameters, ϕ in the Update θ based on L dec ; end end encoder network and θ in the item matrix and bias vector, are of a different nature and it might be best to train them in different ways. We propose to update ϕ and θ alternately with a different number of iterations: since the encoder is a much more complex network than the decoder, we make multiple updates of ϕ for each update of θ . This separation of training steps allows for another improvement. Both our experiments and prior art indicate that reconstruction of corrupted input data, i.e., using denoising autoencoders, is necessary in autoencoder-based collaborative filtering, forcing the model to learn to not only reconstruct previously observed feedback but also generalize to unobserved feedback during inference. However, we noticed that performance improves if we do not corrupt input data during the training of θ and leave the denoising purely for ϕ. Since other types of regularization (such as L 2 or moving to a Bayesian decoder) also lead to degraded performance, it appears that decoder parameters are overregularized. Thus, we propose to train the decoder as part of a basic vanilla VAE, with no denoising applied. As for the encoder, however, we train it as part of the denoising variational autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Summary</head><p>To summarize the proposed regularizers and changes, we first write down the ELBO for our model:</p><formula xml:id="formula_21">L = E q ϕ (z |x ) E q(x |x ) log p θ (x |z)− −β ′ (x)KL q ϕ (z|x) p(z|ϕ old , x) ,<label>(21)</label></formula><p>where the conditional prior distribution p(z|ϕ old , x) has been defined in <ref type="bibr" target="#b16">(17)</ref>, and the modified weight β ′ (x) of the KL term in <ref type="bibr" target="#b19">(20)</ref>.</p><p>To keep the input uncorrupted while training the decoder, we introduce a modified objective function for θ updates (we skip the KL term entirely because it does not depend on θ ):</p><formula xml:id="formula_22">L dec = E q ϕ (z |x ) log p θ (x |z).<label>(22)</label></formula><p>We train the model using alternating updates as shown in Algorithm 1; different parameters M enc and M dec reflect the asymmetry between encoder and decoder that we have discussed above. During training, we approximate both inner and outer expectation by single Monte-Carlo samples and use reparametrization similar to regular VAE. Now we can introduce the empirical lower bound L(x, θ, ϕ, ϕ old ) = log p θ (x |z ( * ) )− − β ′ (x) − log q ϕ z ( * ) |x + log p z ( * ) |ϕ old , x , <ref type="bibr" target="#b23">(23)</ref> where z ( * ) = д(ϵ, µ, σ ), for a Gaussian posterior д(ϵ, µ, σ ) = ϵ ·µ+σ , where [µ, log σ 2 ] = ψ ϕ (x), ϵ ∼ N (0, I), andx = x ⊙ m is the noised input, m ∼ Bernoulli(µ noise ). We use Monte-Carlo sampling for both log likelihood and KL divergence since the KL divergence between a Gaussian and a mixture of Gaussians cannot be calculated analytically. The dropout layer on <ref type="figure">Figure 3</ref> serves for noising; it is turned off during evaluation, decoder learning phase, and in the composite prior. Since we use the multinomial likelihood, the main component of the loss function is the classification cross-entropy as shown on <ref type="figure" target="#fig_0">Figure 1</ref>. The empirical lower bound for decoder training L dec is introduced in a similar way.</p><p>Similar to Mult-VAE, our model is also able to make predictions for users whose feedback was not observed during training. To do that, we predict the user embedding with the inference network z = ψ ϕ (x) based of the feedback x observed at test time and then predict top items using the trained decoder p θ (x | z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION 4.1 Metrics</head><p>Following prior art, we evaluate the models with information retrieval metrics for ranking quality: Recall@k and NDCG@k. Both metrics compares top-k predictions of a model with the test set X t u of user feedback for user u. To obtain recommendations for RecVAE and similar models, we sort the items in descending order of the likelihood predicted by the decoder, exclude items from the training set, and denote the item at the nth place in the resulting list as R (n) u . In this notation, evaluation metrics for a user u are defined as follows:</p><formula xml:id="formula_23">Recall@k(u) = 1 min(M, |X t u |) k n=1 1 R (n) u ∈ X t u ,<label>(24)</label></formula><p>where 1[·] denotes the indicator function,</p><formula xml:id="formula_24">DCG@k(u) = k n=1 2 1[R (n) u ∈X t u ] − 1 log(n + 1)</formula><p>,</p><formula xml:id="formula_25">NDCG@k(u) = |X t u | n=1 1 log(n + 1) −1 DCG@k(u),<label>(25)</label></formula><p>i.e., NDCG@k(u) is defined as DCG@k(u) divided by its highest theoretically possible value. Recall@k accounts for all top-k items equally, while NDCG@k assigns larger weights to top ranked items; thus, it is natural to choose a larger value of k for NDCG@k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>We have evaluated RecVAE on the MovieLens-20M dataset 1 <ref type="bibr" target="#b8">[9]</ref>, Netflix Prize Dataset 2 <ref type="bibr" target="#b2">[3]</ref>, and Million Songs Dataset 3 <ref type="bibr" target="#b3">[4]</ref>. We have preprocessed the datasets in accordance with the Mult-VAE approach <ref type="bibr" target="#b21">[22]</ref>. Dataset statistics after preprocessing are as follows: MovieLens-20M contains 9,990,682 ratings on 20,720 movies provided by 136,677 users, Netflix Prize Dataset contains 56,880,037 ratings on 17,769 movies provided by 463,435 users, and Million Songs Dataset contains 33,633,450 ratings on 41,140 songs provided by 571,355 users. In order to evaluate the model on users unavailable during the training, we have held out 10,000 users for validation and testing for MovieLens-20M, 40,000 users for the Netflix Prize, and 50,000 users for the Million Songs Dataset. We used 80% of the ratings in the test set in order to compute user embeddings and evaluated the model on the remaining 20% of the ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare the performance of the proposed model with several baselines, which we divide into three groups. The first group includes linear models from classical collaborative filtering. Weighted Matrix Factorization (WMF) <ref type="bibr" target="#b13">[14]</ref> binarizes implicit feedback r ua (number of times user u positively interacted with item a) as p ua = 1 [r ua &gt; 0] and decomposes the matrix of p ua similar to SVD but with confidence weights that increase with r ua :</p><formula xml:id="formula_26">min w u ,w a u,a (1 + αr ua ) p ua − w ⊤ u w a 2 + λ(∥W u ∥ 2 + ∥W a ∥ 2 ),<label>(26)</label></formula><p>where ∥ · ∥ 2 is the L 2 -norm. The Sparse LInear Method (SLIM) for top-N recommendation <ref type="bibr" target="#b25">[25]</ref> learns a sparse matrix of aggregation coefficients W that corresponds to the weights of rated items aggregated to produce recommendation scores, i.e., the prediction isr ia = w ⊤ i w a , or in matrix formR = RW , with the resulting optimization problem</p><formula xml:id="formula_27">min W 1 2 ∥R − RW ∥ 2 F + β 2 ∥W ∥ 2 F + λ∥W ∥ 1<label>(27)</label></formula><p>subject to W ≥ 0 and diag(W ) = 0, where ∥ · ∥ F is the Frobenius norm and ∥ · ∥ 1 is the L 1 -norm. The Embarrassingly Shallow Autoencoder (EASE) <ref type="bibr" target="#b33">[33]</ref> is a further improvement on SLIM with a closed-form solution, where the non-negativity constraint and L 1 regularization are dropped. Despite the name, we do not refer to this model as an autoencoder-based one. The second is learning to rank methods such as WARP <ref type="bibr" target="#b38">[38]</ref> and LambdaNet <ref type="bibr" target="#b5">[6]</ref>. LambdaNet is a learning to rank approach that allows to work with objective functions that are either flat or discontinuous; the main idea is to approximate the gradient of each item's score in the ranked list for the corresponding query. The result can be treated as a direction where the item is to "move" in the ranked list for the query when sorted with respect to the newly predicted scores. WARP considers every user-item pair (u, i) corresponding to a positive interaction when training to predict scores for recommendation ranking. For every user u, other random items i ′ are sampled until the first one with the predicted score lower than that of the i is found. The pair (u, i ′ ) is then treated as a negative sample, and (u, i) and (u, i ′ ) are employed as positive and a negative contributions respectively for the approximation of the indicator function for the ranking loss similar to those introduced in an earlier work <ref type="bibr" target="#b36">[36]</ref>. For more details about WARP and its performance we refer to the original work <ref type="bibr" target="#b38">[38]</ref>.</p><p>The third group includes autoencoder-based methods that we have already discussed in Sections 2 and 3.1: CDAE <ref type="bibr" target="#b39">[39]</ref>, Mult-DAE, and Mult-VAE <ref type="bibr" target="#b21">[22]</ref>. The proposed RecVAE model can also be considered as a member of this group. We also consider the very recently proposed Ranking-Critical Training (RaCT) <ref type="bibr" target="#b23">[23]</ref>. This model adopts an actor-critic framework for collaborative filtering on implicit data. A critic (represented by a neural network) learns to approximate the ranking scores, which in turn improves an MLE-based nonlinear latent variable model (VAE and possibly its variations) with the learned ranking-critical objectives. The critic neural network is feature-based and is using posterior sampling as exploration for better estimates. Both actor and critic are pretrained in this model. Scores for these models have been taken from <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b23">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation setup</head><p>RecVAE was trained with the Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with learning rate = 5 · 10 −4 , and batch size b = 500. M dec is selected so that each element in the dataset is selected once per epoch, i.e., M dec = |U | batch size , the Bernoulli noise parameter is µ noise = 0.5, and M enc = 3M dec . In addition to the standard normal distribution and the old posterior as parts of the composite prior, we also add a normal distribution with zero mean and log σ 2 = 10 to the mixture. Weights of these mixture components are 3/20, 3/4, and 1/10 respectively. Since the model is sensitive to changes of the parameter γ , we have picked it individually for each dataset: γ = 0.005 for MovieLens-20M, γ = 0.0035 for the Netflix Prize Dataset, and γ = 0.01 for the Million Songs Dataset. Each model was trained during N = 50 epochs (N = 100 for MSD), choosing the best model by the NDCG@100 score on a validation subset. Initially, we fit the model for MovieLens-20M and then fine-tuned it for the Netflix Prize Dataset and MSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>Performance scores for top-N recommendations in the three datasets are presented in <ref type="table" target="#tab_0">Table 1</ref>. The results clearly show that in terms of recommendation quality, RecVAE outperforms all previous autoencoderbased models across all datasets in the comparison, in particular, with a big improvement over Mult-VAE. Moreover, new features of RecVAE and RaCT are independent and can be used together for even higher performance. Nevertheless, the proposed model significantly outperforms EASE only on MovieLens-20M datasets, and shows competitive performance on the Netflix Prize Dataset.</p><p>For competing models in the comparison, we have used metrics reported in prior art; for RecVAE we have also indicated confidence intervals, showing that the difference in scores is significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation study and negative results</head><p>In order to demonstrate that each of the new features we introduced for RecVAE compared to Mult-VAE indeed helps to improve performance, we have performed a detailed ablation study, comparing various subsets of the features: (1) new encoder architecture, (2) composite prior for the latent codes, (3) β rescaling, (4) alternating training, and (5) removing denoising for the decoder.</p><p>Numerical results of the ablation study are presented in <ref type="table" target="#tab_1">Table 2</ref>. We see that each new feature indeed improves the results, with all proposed new features leading to the best NDCG@100 scores on We have also performed extended analysis of the composite prior, namely checked how the log p z|ϕ old , x regularizer that brings variational parameters closer to old ones affects model stability. This regularizer stabilizes training, as evidenced by the rate of change in the variational parameters. <ref type="figure" target="#fig_3">Figure 2</ref> illustrates how the composite prior fixes the "forgetting" problem. It shows how NDCG@100 changes for a randomly chosen user as training progresses: each  value is the difference in NDCG@100 for this user after each subsequent training update. Since each update changes the encoder network, it changes all user embeddings, and the changes can be detrimental for some users; note, however, that for the composite prior the changes remain positive almost everywhere while a simple Gaussian prior leads to much more volatile behaviour. In addition, we would like to report the negative results of our other experiments. First, autoencoder-based models replace the matrix of user embeddings with a parameterized function, so it was natural to try to do the same for item embeddings. We trained RecVAE with a symmetric autoencoder that predicts top users for a given item, training it alternately with regular RecVAE and regularizing the results of each encoder with embeddings from the other model. The resulting model trained much slower, required more memory, and could not reach the results of RecVAE.</p><p>Second, we have tried to use more complex prior distributions. Mixtures of Gaussians and the variational mixture VampPrior <ref type="bibr" target="#b35">[35]</ref> have (nearly) collapsed to a single node in our experiments, an effect previously noted in reinforcement learning <ref type="bibr" target="#b34">[34]</ref>. The RealNVP prior <ref type="bibr" target="#b7">[8]</ref> has yielded better performance compared to the standard Gaussian prior, but we have not been able to successfully integrate it into the proposed composite prior: the composite prior with a Gaussian term remained the best throughout our experiments. We note this as a potential direction for further research.</p><p>Third, instead of β-VAE-like weighing of KL divergence, we tried to re-weigh each of the terms in the decomposed KL divergence separately. It appears natural to assume that "more precise" regularization could be beneficial for both performance and understanding. However, neither a simple decomposition into entropy and crossentropy nor the more complex one proposed in <ref type="bibr" target="#b6">[7]</ref> has led the model to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we have presented a new model called RecVAE that combines several improvements for the basic Mult-VAE model, including a new encoder architecture, new composite prior distribution for the latent codes, new approach to setting the hyperparameter β, and a new approach to training RecVAE with alternating updates of the encoder and decoder. As a result, performance of RecVAE is comparable to EASE and significantly outperforms other models on classical collaborative filtering datasets such as MovieLens-20M, Netflix Prize Dataset, and Million Songs Dataset.</p><p>We note that while we have provided certain theoretical motivations for our modifications, these motivations are sometimes incomplete, and some of our ideas have been primarily motivated by practical improvements. We believe that a comprehensive theoretical analysis of these ideas might prove fruitful for further advances, and we highlight this as an important direction for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>RecVAE architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>fu</head><label></label><figDesc>from (18) with a sum over X o u with the corresponding rescaling coefficient |X f u |/|X o u |, then approximate KL f u with KL u :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ALGORITHM 1 :</head><label>1</label><figDesc>Proposed training procedure Data: D = {x 1 , . . . , x |U | } Result: ϕ, θ for n := 1, ..., N do for m := 1, ..., M enc do Sample batch {x 1 , . . . , x b } ∼ D; Update ϕ based on L; end ϕ old := ϕ; for m := 1, ..., M dec do Sample batch {x 1 , . . . , x b } ∼ D;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Differences in NDCG@100 for a random user as a function of the training iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation scores for RecVAE and baseline models on MovieLens-20M, Netflix Prize Dataset, and MSD. The best results are highlighted in bold. The second best ones are underlined. RecVAE (ours) 0.276±0.0010 0.374±0.0011 0.326±0.0010 all three datasets. Some new features are complementary: e.g., β rescaling and alternating training degrade the scores when applied individually, but together improve them; the new architecture does not bring much improvement by itself but facilitates other new features; β rescaling is dataset-sensitive, sometimes improving a lot and sometimes doing virtually nothing.</figDesc><table><row><cell></cell><cell cols="3">Recall@20 Recall@50 NDCG@100</cell></row><row><cell></cell><cell cols="2">MovieLens-20M Dataset</cell><cell></cell></row><row><cell>WARP [38]</cell><cell>0.314</cell><cell>0.466</cell><cell>0.341</cell></row><row><cell cols="2">LambdaNet[6] 0.395</cell><cell>0.534</cell><cell>0.427</cell></row><row><cell>WMF [14]</cell><cell>0.360</cell><cell>0.498</cell><cell>0.386</cell></row><row><cell>SLIM [25]</cell><cell>0.370</cell><cell>0.495</cell><cell>0.401</cell></row><row><cell>CDAE [39]</cell><cell>0.391</cell><cell>0.523</cell><cell>0.418</cell></row><row><cell cols="2">Mult-DAE [22] 0.387</cell><cell>0.524</cell><cell>0.419</cell></row><row><cell cols="2">Mult-VAE [22] 0.395</cell><cell>0.537</cell><cell>0.426</cell></row><row><cell>RaCT [23]</cell><cell>0.403</cell><cell>0.543</cell><cell>0.434</cell></row><row><cell>EASE [33]</cell><cell>0.391</cell><cell>0.521</cell><cell>0.420</cell></row><row><cell cols="4">RecVAE (ours) 0.414±0.0027 0.553±0.0028 0.442±0.0021</cell></row><row><cell></cell><cell cols="2">Netflix Prize Dataset</cell><cell></cell></row><row><cell>WARP [38]</cell><cell>0.270</cell><cell>0.365</cell><cell>0.306</cell></row><row><cell cols="2">LambdaNet[6] 0.352</cell><cell>0.441</cell><cell>0.386</cell></row><row><cell>WMF [14]</cell><cell>0.316</cell><cell>0.404</cell><cell>0.351</cell></row><row><cell>SLIM [25]</cell><cell>0.347</cell><cell>0.428</cell><cell>0.379</cell></row><row><cell>CDAE [39]</cell><cell>0.343</cell><cell>0.428</cell><cell>0.376</cell></row><row><cell cols="2">Mult-DAE [22] 0.344</cell><cell>0.438</cell><cell>0.380</cell></row><row><cell cols="2">Mult-VAE [22] 0.351</cell><cell>0.444</cell><cell>0.386</cell></row><row><cell>RaCT [23]</cell><cell>0.357</cell><cell>0.450</cell><cell>0.392</cell></row><row><cell>EASE [33]</cell><cell>0.362</cell><cell>0.445</cell><cell>0.393</cell></row><row><cell cols="4">RecVAE (ours) 0.361±0.0013 0.452±0.0013 0.394±0.0010</cell></row><row><cell></cell><cell cols="2">Million Songs Dataset</cell><cell></cell></row><row><cell>WARP [38]</cell><cell>0.206</cell><cell>0.302</cell><cell>0.249</cell></row><row><cell cols="2">LambdaNet[6] 0.259</cell><cell>0.355</cell><cell>0.308</cell></row><row><cell>WMF [14]</cell><cell>0.211</cell><cell>0.312</cell><cell>0.257</cell></row><row><cell>SLIM [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CDAE [39]</cell><cell>0.188</cell><cell>0.283</cell><cell>0.237</cell></row><row><cell cols="2">Mult-DAE [22] 0.266</cell><cell>0.363</cell><cell>0.313</cell></row><row><cell cols="2">Mult-VAE [22] 0.266</cell><cell>0.364</cell><cell>0.316</cell></row><row><cell>RaCT [23]</cell><cell>0.268</cell><cell>0.364</cell><cell>0.319</cell></row><row><cell>EASE [33]</cell><cell>0.333</cell><cell>0.428</cell><cell>0.389</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of RecVAE with different subsets of new features. The first row corresponds to Mult-VAE.</figDesc><table><row><cell>New architecture</cell><cell cols="2">Composite prior</cell><cell>β(x) rescaling</cell><cell>Alternating training</cell><cell>Decoder w/o denoising</cell><cell cols="2">NDCG@100 ML-20M Netflix</cell><cell>MSD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.426</cell><cell>0.386</cell><cell>0.319</cell></row><row><cell>✓</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.428</cell><cell>0.388</cell><cell>0.320</cell></row><row><cell cols="3">✓ ✓</cell><cell></cell><cell></cell><cell></cell><cell>0.435</cell><cell>0.392</cell><cell>0.325</cell></row><row><cell>✓</cell><cell></cell><cell></cell><cell>✓</cell><cell></cell><cell></cell><cell>0.435</cell><cell>0.390</cell><cell>0.321</cell></row><row><cell>✓</cell><cell></cell><cell></cell><cell></cell><cell cols="2">✓ ✓</cell><cell>0.427</cell><cell>0.387</cell><cell>0.319</cell></row><row><cell cols="4">✓ ✓ ✓</cell><cell></cell><cell></cell><cell>0.438</cell><cell>0.390</cell><cell>0.325</cell></row><row><cell></cell><cell cols="5">✓ ✓ ✓ ✓</cell><cell>0.420</cell><cell>0.380</cell><cell>0.308</cell></row><row><cell>✓</cell><cell></cell><cell></cell><cell cols="3">✓ ✓ ✓</cell><cell>0.434</cell><cell>0.383</cell><cell>0.321</cell></row><row><cell cols="3">✓ ✓</cell><cell></cell><cell cols="2">✓ ✓</cell><cell>0.437</cell><cell>0.392</cell><cell>0.323</cell></row><row><cell cols="5">✓ ✓ ✓ ✓</cell><cell></cell><cell>0.441</cell><cell>0.391</cell><cell>0.322</cell></row><row><cell cols="6">✓ ✓ ✓ ✓ ✓</cell><cell>0.442</cell><cell>0.394</cell><cell>0.326</cell></row><row><cell cols="2">0.01</cell><cell></cell><cell></cell><cell cols="3">Gaussian prior</cell><cell>Composite prior</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">−0.01</cell><cell>0</cell><cell></cell><cell>50</cell><cell></cell><cell>100</cell><cell>150</cell><cell>200</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://grouplens.org/datasets/movielens/20m/ 2 https://www.netflixprize.com/ 3 http://millionsongdataset.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was done at the Samsung-PDMI Joint AI Center at PDMI RAS and was supported by Samsung Research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A NETWORK ARCHITECTURE</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gediminas</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2005.99</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2005.99" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="734" to="749" />
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">icdm</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The netflix prize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Lanning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD cup and workshop</title>
		<meeting>KDD cup and workshop<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Million Song Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thierry Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Music Information Retrieval</title>
		<meeting>the 12th International Conference on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to rank with nonsmooth cost functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in VAEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2615" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Density estimation using Real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkpbnH9lx" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The MovieLens Datasets: History and Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2827872</idno>
		<ptr target="https://doi.org/10.1145/2827872" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interact. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Alexander Lerchner. 2017. bet a-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Beta VAE&apos;s Implicit Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://bayesiandeeplearning.org/2017/papers/66.pdf" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vime: Variational information maximizing exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><forename type="middle">De</forename><surname>Turck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1109" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<ptr target="http://arxiv.org/abs/1608.06993" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Denoising criterion for variational auto-encoding framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Im Jiwoong</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variational Autoencoder with Arbitrary Conditioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SyxtJh0qYm" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6114" />
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-linear matrix factorization with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<idno>arXiv:stat.ML/1607.06450</idno>
		<title level="m">Layer Normalization. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<imprint>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Lobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04281</idno>
		<title level="m">Towards Amortized Ranking-Critical Training for Collaborative Filtering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangling Disentanglement in Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/mathieu19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4402" to="4412" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Slim: Sparse linear methods for top-n recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 11th International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Literature Review and Classification of Recommender Systems Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Deuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyea Kyeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae Kyeong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2012.02.038</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2012.02.038" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="10059" to="10072" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Searching for Activation Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hkuq2EkPf" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/rezende14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using Markov chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Proximal Policy Optimization Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<ptr target="http://arxiv.org/abs/1707.06347" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Amortized inference regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mykel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kochenderfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Embarrassingly Shallow Autoencoders for Sparse Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3251" to="3257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Boosting Trust Region Policy Optimization by Normalizing Flows Policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipra</forename><surname>Agrawal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10326</idno>
		<ptr target="http://arxiv.org/abs/1809.10326" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">VAE with a VampPrior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ranking with ordered weighted pairwise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1057" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Second International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Collaborative denoising auto-encoders for top-n recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Ninth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">On the Necessity and Effectiveness of Learning the Prior of Variational Auto-Encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13452</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
