<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-16">16 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Xiaomi AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Minzu University of China</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Bo</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2020-07-16">16 Jul 2020</date>
						</imprint>
					</monogr>
					<note>Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Differentiable Neural Architecture Search · Image Classifi- cation · Failure of DARTS</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Differentiable Architecture Search (DARTS) is now a widely disseminated weight-sharing neural architecture search method. However, it suffers from well-known performance collapse due to an inevitable aggregation of skip connections. In this paper, we first disclose that its root cause lies in an unfair advantage in exclusive competition. Through experiments, we show that if either of two conditions is broken, the collapse disappears. Thereby, we present a novel approach called Fair DARTS where the exclusive competition is relaxed to be collaborative. Specifically, we let each operation's architectural weight be independent of others. Yet there is still an important issue of discretization discrepancy. We then propose a zero-one loss to push architectural weights towards zero or one, which approximates an expected multi-hot solution. Our experiments are performed on two mainstream search spaces, and we derive new state-of-the-art results on CIFAR-10 and ImageNet 3 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the wake of the DARTS's open-sourcing <ref type="bibr" target="#b22">[23]</ref>, a diverse number of its variants emerge in the neural architecture search community. Some of them extend its use in higher-level architecture search spaces with performance awareness in mind <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>, some learn a stochastic distribution instead of architectural parameters <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, and others offer remedies on discovering its lack of robustness <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>In spite of these endeavors, the aggregation of skip connections in DARTS that noticed by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b38">39]</ref> has not been solved with perfection. Observing that the aggregation leads to a dramatic performance collapse for the resulting architecture, P-DARTS <ref type="bibr" target="#b5">[6]</ref> utilizes dropout as a workaround to restrict the number of skip connections during optimization. DARTS+ <ref type="bibr" target="#b20">[21]</ref> directly puts a hard  limit of two skip-connections per cell. RobustDARTS <ref type="bibr" target="#b38">[39]</ref> finds out that these solutions coincide with high validation loss curvatures. To some extent, these approaches consider the poor-performing models as impurities from the solution set, for which they intervene in the training process to filter them out.</p><p>On the contrary, we extend the solution set and revise the optimization process so that aggregation of skip connections no longer causes the collapse. Moreover, there remains a discrepancy problem when discretizing continuous architecture encodings. DARTS <ref type="bibr" target="#b22">[23]</ref> leaves it as future work, but till now it has not been deeply studied. We reiterate the basic premise of DARTS is that the continuous solution approximates a one-hot encoding. Intuitively, the smaller discrepancies are, the more consistent it will be when we transform a continuous solution back to a discrete one. We summarize our contributions as follows:</p><p>Firstly, we disclose the root cause that leads to the collapse of DARTS, which we later define as an unfair advantage that drives skip connections into a monopoly state in exclusive competition. These two indispensable factors work together to induce a performance collapse. Moreover, if either of the two conditions is broken, the collapse disappears.</p><p>Secondly, we propose the first collaborative competition approach by offering each operation an independent architectural weight. The unfair advantage no longer prevails as we break the second factor. Furthermore, to address the discrepancy between the continuous architecture encoding and the derived discrete one in our method, we propose a novel auxiliary loss, called zero-one loss, to steer architectural weights towards their extremities, that is, either completely enabled or disabled. The discrepancy thus decreases to its minimum.</p><p>Thirdly, based on the root cause of the collapse, we provide a unified perspective to view current DARTS cures for skip connections' aggregation. The majority of these works either make use of dropout <ref type="bibr" target="#b30">[31]</ref> on skip connections <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39]</ref>, or play with the later termed boundary epoch by different early-stopping strategies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">39]</ref>. They can all be regarded as preventing the first factor from taking effect. Moreover, as a direct application, we can derive a hypothesis that adding Gaussian noise also disrupts the unfairness, which is later proved to be effective.</p><p>Lastly, we conduct thorough experiments in two widely used search spaces in both proxy and proxyless ways. Results show that our method can escape from performance collapse. We also achieve state-of-the-art networks on CIFAR-10 and ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Lately, neural architecture search <ref type="bibr" target="#b40">[41]</ref> has grown as a well-formed methodology to discover networks for various deep learning tasks. Endeavors have been made to reduce the enormous searching overhead with the weight-sharing mechanism <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref>. Especially in DARTS <ref type="bibr" target="#b22">[23]</ref>, a nested gradient-descent algorithm is exploited to search for the graphical representation of architectures, which is born from gradient-based hyperparameter optimization <ref type="bibr" target="#b23">[24]</ref>.</p><p>Due to the limit of the DARTS search space, ProxylessNAS <ref type="bibr" target="#b3">[4]</ref> and FBNet <ref type="bibr" target="#b35">[36]</ref> apply DARTS in much larger search spaces based on MobileNetV2 <ref type="bibr" target="#b29">[30]</ref>. ProxylessNAS also differs from DARTS in its supernet training process, where only two paths are activated, based on the assumption that one path is the best amongst all should be better than any single one. From a fairness point of view, as only two paths enhance their ability (get parameters updated) while others remain unchanged, it implicitly creates a bias. FBNet <ref type="bibr" target="#b35">[36]</ref>, SNAS <ref type="bibr" target="#b36">[37]</ref> and GDAS <ref type="bibr" target="#b11">[12]</ref> utilize the differentiable Gumbel Softmax <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17]</ref> to mimic one-hot encoding. However, the one-hot nature implies an exclusive competition, which risks being exploited by unfair advantages.</p><p>Superficially, the most relevant work to ours is RobustDARTS <ref type="bibr" target="#b38">[39]</ref>. Under several simplified search spaces, they state that the found solutions generalize poorly when they coincide with high validation loss curvature, where the supernet with an excessive number of skip connections happens to be such a solution. Based on this observation, they impose early-stop regularization by tracking the largest eigenvalue. Instead, our method doesn't need to perform early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Downside of DARTS</head><p>In this section, we aim to excavate the disadvantages of DARTS that possibly impede the searching performance. We first prepare a minimum background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary of Differentiable Architecture Search</head><p>For the case of convolutional neural networks, DARTS <ref type="bibr" target="#b22">[23]</ref> searches for a normal cell and a reduction cell to build up the final architecture. A cell is represented as a directed acyclic graph (DAG) of N nodes in sequential order. Each node stands for a feature map. The edge e i,j from node i to j operates on the input feature x i and its output is denoted as o i,j (x i ). The intermediate node j gathers all inputs from the incoming edges,</p><formula xml:id="formula_0">x j = i&lt;j o i,j (x i ).</formula><p>(1) </p><formula xml:id="formula_1">Let O = {o 1 i,j , o 2 i,j , ..., o M i,j }</formula><formula xml:id="formula_2">i,j (x) = o∈O exp(α oi,j ) o ∈O exp(α o i,j ) o(x),<label>(2)</label></formula><p>where each operation o i,j is associated with a continuous coefficient α oi,j . Regarding edge e i,j , this softmax is utilized to approximate one-hot encoding</p><formula xml:id="formula_3">β i,j = (β o 1 i,j , β o 2 i,j , ..., β o M i,j ). Formally, let α oi,j denote the architectural weights vector (α o 1 i,j , α o 2 i,j , ..., α o M i,j )</formula><p>. DARTS thus assumes the following as a valid approximation, sof tmax(α oi,j ) ≈ β i,j .</p><p>The architecture search problem is reduced to learning α * and network weights w * that minimize the validation loss L val (w * , α * ). DARTS resolves this problem with a bi-level optimization,</p><formula xml:id="formula_5">min α L val (w * (α), α) s.t. w * (α) = argmin w L train (w, α).<label>(4)</label></formula><p>We also adopt two common search spaces, the DARTS <ref type="bibr" target="#b22">[23]</ref> search space (S 1 ) and the ProxylessNAS <ref type="bibr" target="#b3">[4]</ref> search space (S 2 ) with minor modifications. More details are given in Section 2 (supplementary).</p><p>In S 2 , the output of the l-th layer is a softmax-weighted summation of N choices. Formally, it can be written as</p><formula xml:id="formula_6">x l = N k=1 exp(α k l−1,l ) N j=1 exp(α j l−1,l ) o k l−1,l (x l−1 ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Collapse Caused by Intractable Skip Connections</head><p>DARTS suffers from significant performance decay when skip connections become dominant <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>. It was described as a competition-and-cooperation issue in the bi-level optimization <ref type="bibr" target="#b20">[21]</ref>. Still, the reason behind this behavior is not clear, we hereby provide a different perspective.</p><p>First, to confirm this issue, we run DARTS k = 4 times with different random seeds. Following DARTS, we select 8 top-performing operations per cell (2 each for 4 intermediate nodes). Here we say one operation is dominant if it has top-2 sof tmax(α) among all incoming edges' candidates of a certain node. The results are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. In the beginning, all operations are given the same opportunity. As the over-parameterized network gradually converges, there is an evident aggregation of skip connections after 20 epochs (5 out of 8 in an extreme case).  When we utilize DARTS directly on ImageNet in S 2 , which is a single branch architecture, the same phenomenon rigorously reappears. The number of dominant skip-connections (highest sof tmax(α) among all operations in that layer) steadily increases and reaches 11 out of 19 layers in the end, which is shown on the left of <ref type="figure" target="#fig_8">Fig. 1</ref>.</p><p>But why is this happening? The underlying reasons are rarely discussed in depth. A brief and superficial analysis regarding information flow is given in <ref type="bibr" target="#b5">[6]</ref>. However, we claim that the reason for excessive skip connections is from exclusive competition among various operations. In Equation 2 and Equation 5, the skip connection is softmax-weighted and added to the output, which resembles a basic residual module as in ResNet <ref type="bibr" target="#b13">[14]</ref>. While this module greatly benefits the training, the architectural weight of a skip connection increases much faster than its competitors. Moreover, the softmax operation inherently provides an exclusive competition since increasing one is at the cost of suppressing others. As a result, skip connections become gradually dominant during optimization. We have to keep in mind that skip connection works well because it is in cooperation with convolutions <ref type="bibr" target="#b13">[14]</ref>. However, DARTS picks the top-performing one (skip connection here) and discards its collaborator (convolution), which results in a degenerate model. We further study this effect from the experiments on CIFAR-10 by recording the competition progress in  <ref type="figure" target="#fig_0">Fig. 3</ref>. The softmax evolution where skip connections gradually become dominant when running DARTS on CIFAR-10 (in S1). Last two subplots of edge (1,0) and (2,0) are from the normal cell, the rest are from the reduction cell. Black arrows point to boundary epochs where skip connections start to demonstrate its strength total 6 . ResNet <ref type="bibr" target="#b13">[14]</ref> discovers that skip connections begin to demonstrate power after a few epochs compared with models without them. Interestingly, a similar phenomenon is also observed in our experiments. We term this tipping point a boundary epoch. The boundary epochs may vary from edge to edge, but are generally at the early stage. From <ref type="figure" target="#fig_0">Fig. 3</ref>, we observe that skip connections colored in red-orange progressively obtain higher architectural weights after some certain boundary epochs. Meantime, other operations are suppressed and steadily decline. We consider this benefit from the residual module as an unfair advantage by Definition 1.</p><p>Definition 1. Unfair Advantage. Suppose that choosing one operation among others is a competition. This competition is deemed exclusive when only restricted operations can be selected. An operation in an exclusive competition is said to have an unfair advantage if this advantage contributes more to competition than to the performance of a resulted network.</p><p>From the above discussion, we can draw Insight 1: The root cause of excessive skip connections is the inherent unfair competition. The skip connection has an unfair advantage by forming a residual module which is convenient for the supernet training, but not equally beneficial for the performance of the outcome network where the residual module is broken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Non-negligible Discrepancy of Discretization</head><p>Apart from the above issue, DARTS reports that it suffers from discrepancies when discretizing continuous encodings <ref type="bibr" target="#b22">[23]</ref>. To verify the problem, we run DARTS in S 1 on CIFAR-10, and in S 2 on ImageNet. The values of sof tmax(α) of the last iteration are displayed in <ref type="figure">Fig. 4</ref> (S 1 ) and on the bottom left of <ref type="figure" target="#fig_8">Fig. 1</ref> (S 2 ). For S 1 , the largest value is about 0.3 while the smallest one is above 0.1 7 . This range is somewhat too narrow to differentiate 'good' operations from 'bad'. For instance on edge 2 of the reduction cell, the values are very close to each other, [0.174, 0.170, 0.176, 0.112, 0.116, 0.132, 0.118], it's hard to say that an operation weighted by 0.176 is better than the other by 0.174. For S 2 , the top-1 values are not so evidently particular from layer 2 to 7. Take the second layer for example, we have to use [0.235, 0.057, 0.17, 0.016, 0.187, 0.269, 0.066] to approximate [0, 0, 0, 0, 0, 1, 0]. This again confirms the existence of discrepancy.</p><p>In summary, DARTS is usually far from a good resemblance to a one-hot representation as required by its premise in Equation 3. We often have to make ambiguous choices without high confidence. Hence, we learn Insight 2: Relaxing from discrete categorical choices to continuous ones should make a close approximation. Based on Insight 1, we propose a cooperative mechanism to eliminate the existing unfair advantage. Not only should we exploit skip connection for smoother information flow, but we also have to provide equal opportunities for other operations. In a word, they need to avoid being trapped by unfair advantage from skip connections. On this regard, we apply a sigmoid activation (σ) for each α i,j , so that each operation can be switched on or off independently without being suppressed. Formally, we replace Equation 2 with the following,</p><formula xml:id="formula_7">o i,j (x) = o∈O σ(α oi,j )o(x).<label>(6)</label></formula><p>It's trivial to show that even if σ(α skip ) saturates to 1, other operations still can be optimized cooperatively. Promising operations continue to grow their architectural weights to reduce L val , which leads to a multi-hot approximation. Instead, DARTS attempts to derive a one-hot estimation. The difference is that we have extended the solution set. Consequently, it allows us to tackle the discretization discrepancy. We are left to find out how to drive σ(α) towards each extremity (0 or 1). Next, we discuss it in greater detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Resolve Discrepancy from Continuous Representation to Discrete Encoding</head><p>To abide by Insight 2, we explicitly coerce an extra loss called zero-one loss to push the sigmoid value of architectural weights towards 0 or 1. Let L 0−1 = f (z) denote this loss component, where z = σ(α). To achieve our goal, the loss design must meet three basic criteria, a) It needs to have a global maximum at z = 0.5 (a fair starting point) and a global minimum at 0 and 1. b) The gradient magnitude df dz | z≈0.5 has to be adequately small to allow architectural weights to fluctuate, but large enough to attract z towards 0 or 1 when they are a bit far from 0.5. c) It should be differentiable for backpropagation. According to the first requirement, we move σ(α) away from 0.5 towards 0 or 1 to minimize the discretization gap. The second one enacts explicit necessary constraints. Particularly, small gradients around the peak avoid stepping easily into two ends. Larger gradients around 0 and 1 instead help to quickly capture z nearby. Quite straightforward, we come up with a loss function to meet the above requirements, formally as,</p><formula xml:id="formula_8">L 0−1 = − 1 N N i (σ(α i ) − 0.5) 2<label>(7)</label></formula><p>In order to control its strength, we weight this loss by a coefficient w 0−1 , thus the total loss for α is formulated as,</p><formula xml:id="formula_9">L total = L val (w * (α), α) + w 0−1 L 0−1 .<label>(8)</label></formula><p>Like DARTS <ref type="bibr" target="#b22">[23]</ref>, the architectural weights can be optimized through backpropagation. From Equation 8, the search objective is to find an architecture of high accuracy with a good approximation from a continuous encoding to a discrete one.</p><p>Moreover, the second requirement is indispensable, otherwise the gradientbased approach may step into local minimum too early. Here we design another loss as a negative example. Let L 0−1 be the following,</p><formula xml:id="formula_10">L 0−1 = − 1 N N i |(σ(α i ) − 0.5)|.<label>(9)</label></formula><p>It's trivial to see that d|z−0.5| dz | z&gt;0.5 = 1 and d|z−0.5| dz | z&lt;0.5 = −1. Once z stays away from 0.5, it may receive the same gradient (1 or -1) in the later iterations, thus rapidly pushing the architectural weights towards two ends. This phenomenon is illustrated in <ref type="figure">Fig. 5</ref>.</p><p>To conclude, by combining Equation 4, 6 and 8, our method which we call Fair DARTS, can be now formally written as <ref type="figure">Fig. 5</ref>. Illustration about the auxiliary loss design: L0−1 (proposed) and L 0−1 (control)</p><formula xml:id="formula_11">min α L val (w * (α), α) + w 0−1 L 0−1 s.t. w * (α) = argmin w L train (w, α). o i,j (x) = o∈O σ(α oi,j )o(x). (10) 1 0 -0.25 -0.5 0.5 s(x) L 0-1 ' loss L 0-1 loss 0-1 dL = -1 0-1 dL = dL = 0 0-1 ' 0-1 dL = -0.5 0-1 dL = 0.5 0-1 dL = 1 0-1 dL = 1 ' 0-1 dL = -1 '</formula><p>It is also important to recognize that our zero-one loss is specially designed for Fair DARTS. Pushing σ(α) of one edge towards 0 or 1 is independent of others. It cannot be directly applied to DARTS given the exclusive competition by softmax. As the architectural weights converge to their extremities, it's natural to use a threshold value σ threshold in our approach to infer submodels instead of argmax .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Searching Architectures for CIFAR-10</head><p>At the search stage, we use similar hyperparameters and tricks as <ref type="bibr" target="#b22">[23]</ref>. We apply the first-order optimization and it takes 10 GPU hours. All experiments are done on a Tesla V100. We select our target models with σ threshold = 0.85 8 . We use the same data processing and training trick as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Our collaborative approach performs well with skip connections aggregation. To verify this, we repetitively search 7 times on different random seeds and report the number of skip connections in <ref type="figure" target="#fig_6">Fig. 7 (see supplementary)</ref>. Since the number of skip connections is more reasonable, we obtain an average top-1 accuracy 97.46%. Especially, the smallest FairDARTS-a reaches 97.46% accuracy on CIFAR-10 with reduced parameters and multiply-adds. A complete result of FairDARTS searched cells are shown in the supplementary ( <ref type="figure" target="#fig_5">Fig. 5, Fig. 6</ref> and <ref type="table">Table 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transferring to ImageNet</head><p>As a common practice, we transfer two searched cells (FairDARTS-a and b 9 ) to ImageNet. We keep the same configurations and use the identical training tricks as DARTS <ref type="bibr" target="#b22">[23]</ref>. Compared with SNAS <ref type="bibr" target="#b36">[37]</ref> and DARTS, FairDARTS-A only uses 3.6M number of parameters and 417M multiply-adds to obtain 73.7% top-1 accuracy on ImageNet validation set. FairDARTS-B also achieves stateof-the-art 75.1% in S 1 with a smaller number of parameters than comparable counterparts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Searching Proxylessly on ImageNet</head><p>Relaxing exclusive competition to collaboration greatly extends the size of the search space. In ProxylessNAS <ref type="bibr" target="#b3">[4]</ref>, there are 19 searchable layers and each layer contains 7 choices, consisting of 7 19 possible models. In our approach, every choice can be activated independently, thus, S 2 contains (2 7 ) 19 = 128 19 possible models. To our knowledge, this is a most gigantic search space ever proposed, about 18 <ref type="bibr" target="#b18">19</ref> times that of <ref type="bibr" target="#b3">[4]</ref>.</p><p>For this search phase, we train for 30 epochs with a batch size of 1024, which takes about 3 GPU days. The final architectural weight matrix (after sigmoid activation) on the bottom right of <ref type="figure" target="#fig_8">Fig. 1</ref> is used to derive target models. Under this cooperative setting, the skip connections and other inverted bottleneck blocks can be both chosen to work together, where the former facilitates the training and the latter learn the residual information <ref type="bibr" target="#b19">[20]</ref>. In contrast, under the competitive setting of DARTS, it's impossible to achieve this, as shown in the bottom left of <ref type="figure" target="#fig_8">Fig. 1</ref>. Within 19 layers have 11 skip connection operation is preferred, which cuts down the overall depth of searchable layers to 8.</p><p>To be fair, we select at most two choices per layer if there are more than two above σ threshold (0.75) and use the same training tricks as <ref type="bibr" target="#b32">[33]</ref>. We exclude squeeze and excitation <ref type="bibr" target="#b15">[16]</ref> and refrain from using AutoAugment <ref type="bibr" target="#b9">[10]</ref> tricks though they can boost the classification accuracy further. The searched model FairDARTS-D is shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, which places the summation of two inverted bottleneck blocks nearby the down-sampling stage to keep more information. It also utilizes large kernels and big expansion blocks at the tail end. Further, We raise the σ threshold as 0.8 to get a more lightweight model FairDARTS-C. FairDARTS-C achieves 75.1% top-1 accuracy using only 4.2 M number of parameters. To make comparisons with EfficientNetB0 <ref type="bibr" target="#b33">[34]</ref>, MobileNetV3 <ref type="bibr" target="#b14">[15]</ref> and MixNet <ref type="bibr" target="#b34">[35]</ref>, FairDARTS-C obtains 77.2% top-1 accuracy with the same tricks such as squeeze-and-excitation <ref type="bibr" target="#b15">[16]</ref>, AutoAugment <ref type="bibr" target="#b9">[10]</ref> and Swish <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skip_Connect</head><p>Global_Pooling <ref type="table" target="#tab_2">+ FC  CONV_K1  CONV_K3  MBE3_K3  IBE6_K3  MBE6_K5  MBE6_K3  MBE6_K3  IBE6_K3  IBE3_K3  MBE3_K5  MBE6_K3  MBE3_K3  IBE6_K5  IBE6_K7  BOTTLE_K3  IBE6_K3   IBE6_K5   IBE6_K7  IBE3_K5   IBE6_K3  IBE6_K7  MBE6_K3   3x224x224  32x112x112  16x112x112  32x56x56  32x56x56  40x28x28  40x28x28  40x28x28  40x28x28  80x14x14  80x14x14  80x14x14  80x14x14  80x14x14  96x14x14  96x14x14  96x14x14  192x7x7  192x7x7  192x7x7  192x7x7  320x7x7  1280x7x7   IBE3_K3   IBE3_K5   IBE6_K3  IBE6_K7  Skip_Connect  Global_Pooling + FC  CONV_K1  CONV_K3  MBE3_K3  IBE6_K3  MBE6_K5  MBE6_K3  MBE6_K3  MBE3_K5  IBE3_K5  MBE6_K3  MBE3_K3  IBE6_K5  IBE6_K7   3x224x224  32x112x112  16x112x112  32x56x56  32x56x56  40x28x28  40x28x28  40x28x28  40x28x28  80x14x14  80x14x14  80x14x14  80x14x14  80x14x14  96x14x14  96x14x14  96x14x14  192x7x7  192x7x7  192x7x7  192x7x7  320x7x7  1280x7x7   IBE6_K7  IBE6_K3  MBE6_K3  IBE3_K3  BOTTLE_K3</ref> IBE3_K3   Removing Zero-One Loss. We design two comparison groups for Fair DARTS with and without zero-one loss. Other settings are kept the same. We count the distribution of the sigmoid outputs from architectural weights and plot it on the left of <ref type="figure" target="#fig_7">Fig. 8</ref>. The one without zero-one loss covers a wide range between 0 and 0.6. So we have to make ambiguous choices again. Whereas the proposed loss has narrowed the distribution into two ends around 0 and 1. To further evaluate the influences of removing L 0−1 , we repeat the searching for 7 times using different random seeds. The averaged top-1 accuracy for these models is 97.33±0.15 (532M FLOPS on average, 74 M more than FairDARTS with L 0−1 ). Therefore, although the unfair advantage is balanced, making ambiguous choices still bring noises to the final search result, which is better solved by L 0−1 . Discrepancy elimination seems to helps find more light-weight and accurate models. Zero-One Loss Design. We run two experiments on CIFAR-10, one with Loss Sensitivity. As the weight w 0−1 of this auxiliary loss goes higher, it should squeeze more operations towards two ends, but it must not overshadow the main entropy loss. We perform several experiments where an integer w 0−1 varies within [0, 16]. The right of <ref type="figure" target="#fig_7">Fig. 8</ref> shows the final number of dominant operations for each. We select a reasonable w 0−1 = 10 for the best trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussions From Fairness Perspective</head><p>We review the existing methods that seek to avoid the discussed weaknesses. In general, adding dropouts <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39]</ref> to operations is similar to blending them with a simple additive Gaussian noise, both reduce the performance gain from unfair advantages. Early-stopping <ref type="bibr" target="#b20">[21]</ref> avoids the case before unfairness prevails.</p><p>Adding dropout to skip connections reduces unfairness. The operationlevel dropout <ref type="bibr" target="#b30">[31]</ref> inserted after skip connections by P-DARTS <ref type="bibr" target="#b5">[6]</ref> can be viewed as an alleviation of unfair advantage. However, it comes with two obvious drawbacks. First, this dropout rate is hard to tune. Second, it is not so effective that they must involve another prior: setting the number of skip connections in the final cell to M . This is a very strong prior for searching good architectures <ref type="bibr" target="#b20">[21]</ref>.</p><p>Adding dropout to all operations also helps. Dropout troubles the training of skip connections and thus weakens the unfair advantage. Reasonably, higher dropout rates are more effective, especially for parameter-free operations. Therefore, RobustDARTS <ref type="bibr" target="#b38">[39]</ref> adds dropout to all operations and obtains promising results.</p><p>Early stopping matters. DARTS+ <ref type="bibr" target="#b20">[21]</ref> explicitly limits the maximum number of skip connections, which can be viewed as an early-stopping strategy nearby the previously mentioned boundary epoch, right before too many skip connections rise into power. RobustDARTS <ref type="bibr" target="#b38">[39]</ref> also exploits early-stopping when the maximal Hessian eigenvalues change too fast. Limiting the number of skip connections is a strong prior. In the regularized search space of P-DARTS <ref type="bibr" target="#b5">[6]</ref> and DARTS+ <ref type="bibr" target="#b20">[21]</ref>, we find that simply by restricting M = 2, it is possible to generate competitive models even without searching. We randomly sample models from their search space and report the results in <ref type="table" target="#tab_4">Table 3</ref>. In Experiment 1, the second group restricts the multiply-adds to be above 500M, to further leverage the average performance. Surprisingly, both groups outperform DARTS <ref type="bibr" target="#b22">[23]</ref>.</p><p>Random noise can break unfair advantage. Based on our theory, we can boldly postulate that adding a random noise also disrupts the unfair advantage. Therefore, on top of DARTS <ref type="bibr" target="#b22">[23]</ref>, we mix the skip connections' architectural weights with a standard Gaussian noise N (0, 1), which has a cosine decay on 50 epochs. The results strongly confirm our hypothesis, as shown in <ref type="table" target="#tab_4">Table 3</ref>. We repeat it 4 times to have similar results.</p><p>Remove unfair advantages or destroy the exclusive competition? In principle, we can break either one of the indispensable factors to avoid collapse. However, FairDARTS breaks the latter which is simple and effective. Besides, it paves the way to eliminate the discrepancy by scheming an auxiliary loss L 0−1 . Otherwise, the discrepancy issue remains hard to solve. However, to tackle the discrepancy issue, it's promising that the existing approaches might benefit from tricks like L 0−1 . This remains to be our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We unveil two indispensable factors of the DARTS's aggregation of excessive skip connections: unfair advantages and exclusive competition. We prove that breaking any one of them can improve the robustness. First, by allowing collaborative competition, each operation develops its architectural weight independently. Meanwhile, the non-negligible discrepancy of discretization is reduced at maximum by coercing a novel auxiliary loss which polarizes the architectural weights. In this regard, we achieve state-of-the-art performance both on CIFAR-10 and ImageNet. Second, disturbing the differentiable process with a Gaussian noise removes unfair advantage which leads to competitive results.</p><p>One of our future work is to make it more memory-friendly. As Gumbel softmax is used to replace categorical distribution <ref type="bibr" target="#b35">[36]</ref>, is there a similar way to our approach? More methods remain to be explored on our basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary of "Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture</head><p>Search" Weight-sharing in neural architecture search is now prominent because of its efficiency <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b12">13]</ref>. They can roughly be divided into two categories.</p><p>One-stage approaches. Specifically, ENAS <ref type="bibr" target="#b26">[27]</ref> adopts a reinforced approach to train a controller to sample subnetworks from the supernet. 'Good' subnetworks yield high rewards so that the final policy of the controller is able to find competitive ones. Notice the controller and subnetworks are trained alternatively. DARTS <ref type="bibr" target="#b22">[23]</ref> and many of its variants <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6]</ref> are also a nested optimization based on weight-sharing but in a differentiable way. Besides, DARTS creates an exclusive competition by selecting only one operation on an edge, opposed to ENAS where more operations can be activated at the same time.</p><p>Two-stage approaches. There are some other weight-sharing methods who use the trained supernet as an evaluator <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b12">13]</ref>. We need to make a distinction here as DARTS is a one-stage approach. The supernet of DARTS is meant to learn towards a single solution, where other paths are less valued (weighted). Instead like in <ref type="bibr" target="#b12">[13]</ref>, all paths are uniformly sampled, so to give an equal importance on selected paths. As the supernet is used for different purposes, two-stage approaches should be singled out for discussion in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Search Spaces</head><p>There are two search spaces extensively adopted in recent NAS approaches. The one DARTS proposed, we denote as S 1 , is cell-based with flexible inner connections <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6]</ref>, and the other S 2 is at the block level of the entire network <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref>. We use the term proxy and proxyless to differentiate whether it directly represents the backbone architecture. Our experiments cover both categories, if otherwise explicitly written, the first is searched on CIFAR-10 and the second on ImageNet.</p><p>Search Space S 1 . Our first search space S 1 (show in <ref type="figure" target="#fig_8">Fig. 1</ref>) follows DARTS <ref type="bibr" target="#b22">[23]</ref> with an exception of excluding the zero operation, as done in <ref type="bibr" target="#b38">[39]</ref>. Namely, max pool 3x3, avg pool 3x3, skip connect, -sep conv 3x3, sep conv 5x5, -dil conv 3x3, dil conv 5x5.</p><p>Search Space S 2 . The second search space S 2 is similar to that of Prox-ylessNAS <ref type="bibr" target="#b3">[4]</ref> which uses MobileNetV2 <ref type="bibr" target="#b29">[30]</ref> as its backbone. We make several essential modifications. Specifically, there are L = 19 layers and each contains N = 7 following choices, -Inverted bottlenecks with an expansion rate x in (3,6), a kernel size y in <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7)</ref>, later referred to as ExKy, -skip connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment Details</head><p>The list of all experiments we perform for this paper are summarized in <ref type="table">Table 1</ref>.</p><p>To summarize, we run DARTS <ref type="bibr" target="#b22">[23]</ref> in S 1 and S 2 , confirming the aggregation of skip connections in both search spaces. We also show the large discretization gap in S 2 .</p><p>In comparison, we run Fair DARTS in S 1 and S 2 to show their differences. First, due to our collaborative approach, we can allow a substantial number of skip connections in S 1 , see <ref type="figure" target="#fig_6">Fig. 7 (supplementary)</ref>. Second, <ref type="figure">Fig. 4</ref> (supplementary) exhibit the final heatmap of architectural weights σ where skip connections coexist with other operations, meantime, the values of σ are also close to 0 or 1, which can minimize the discretization gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Search on CIFAR-10 in Search Space S 1</head><p>We initialize all architectural weights to 0 3 and set w 0−1 = 10 . We comply with the same hyperparameters with some exceptions: a batch size 128, learning rate 0.005, and Adam optimizer with a decay of 3e-3 and beta (0.9, 0.999). Moreover, we comply with the same strategy of grouping training and validation data as <ref type="bibr" target="#b22">[23]</ref>. The edge correspondence in search space S 1 is given in <ref type="table">Table 2</ref>. We also use the first-order optimization to save time.</p><p>We illustrate some of the searched cells in <ref type="figure">Fig. 5</ref> and <ref type="figure" target="#fig_5">Fig. 6</ref>. The normal cell of FairDARTS-a is rather simple and only 3 nodes are activated, which is previously rarely seen. In the reduction cell, more edges are activated to compensate for the information loss due to down-sampling. As reduction cells are of small proportion, FairDARTS-a thus retains to be lightweight. Other searching results are listed in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search on ImageNet in Search Space S 2</head><p>We use the SGD optimizer with an initial learning rate 0.02 (cosine decayed by epochs) for the network weights and Adam optimizer with 5e-4 for architectural weights. Besides, we set σ threshold = 0.75 and w 0−1 = 1.0.</p><p>There is a minor issue need to concern. The innate skip connections are removed from inverted bottlenecks since skip connections have already been made a parallel choice in our search space. <ref type="table">Table 1</ref>. All experiments conducted for this paper. By default, S1 is for CIFAR-10, S2 for ImageNet. Note L val refers to L val (w * (α), α). † : Main text. ‡ : In the supplementary.</p><p>: N (0, 1), run k = 4 times, * :w0−1 ∈ range(16)</p><formula xml:id="formula_12">Idx Method Search Space Loss Optimization Fig † Fig ‡ Table ‡ 1 DARTS S1 Lval Bi-level 2,3,4 9,13 2 DARTS S2 Lval Bi-level 1,4 3</formula><p>Fair DARTS S1 Lval Bi-level <ref type="bibr">8 4</ref> Fair DARTS * S1 Lval + w0−1L0−1 Bi-level 8 5</p><p>Fair DARTS S1 Lval + 10 * L0−1 Bi-level <ref type="bibr">8 3,4,5,6,12,14 5,7 6</ref> Fair DARTS S1 Lval + 10 * L 0−1 Bi-level 2,10 7</p><p>Fair DARTS S1 Lval + 10 * L0−1 Single-level 3, <ref type="bibr">11 8</ref> Fair DARTS S2 Lval + 10 * L0−1 Bi-level 1,6 9</p><p>DARTS S1 \ {skips} Lval Bi-level 7 <ref type="bibr">15 10</ref> Random S1 ∩ {priors} n/a n/a 7 11</p><p>Random S1 ∩ {priors} ∩ {FLOPS ≥ 500M } n/a n/a 8 12 Noise S1 Lval Bi-level 6 <ref type="bibr" target="#b12">13</ref> DARTS S1 Lval + 10 * L0−1 Bi-level 8,16 <ref type="figure" target="#fig_2">Fig. 2</ref> compares results on two different loss designs. With the proposed loss function L 0−1 so that Fair DARTS is less subjective to initialization. The sigmoid values reach to their optima more slowly than that of L 0−1 . It also gives a fair opportunity near 0.5, the 3×3 dilation convolution on Edge <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref> first increases and then decreases, which matches the second criteria of loss design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Zero-one Loss Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Single-level vs. Bi-level Optimization</head><p>The 5 × 5 separable convolution on edge (2, 2) under bi-level setting weighs higher at the early stage but much decreased in the end, which can be viewed as robustness to a local optimum. See <ref type="figure" target="#fig_0">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results on COCO Object Detection</head><p>We use our models as drop-in replacements of the backbone of RetinaNet <ref type="bibr" target="#b21">[22]</ref>. Here we only consider comparable mobile backbones. Particularly, we use the MMDetection <ref type="bibr" target="#b4">[5]</ref> toolbox since it provides good implementations for various <ref type="table">Table 2</ref>. Edge correspondence in S1. Edge i is represented as a pair (j, k) where j is an intermediate node, and k is the incoming node. Note j is numbered only on intermediate nodes. And k counts previous cell outputs as well. See <ref type="figure" target="#fig_8">Fig. 1</ref> for correspondence Edge (j, k) 0 (0, 0) 1 (0, 1) 2</p><p>(1, 0) 3</p><p>(1, 1) 4</p><p>(1, 2) 5</p><p>(2, 0) 6 (2, 1) 7</p><p>(2, 2) 8</p><p>(2, 3) 9</p><p>(3, 0) 10 (3, 1) 11 (3, 2) 12 (3, 3) 13 (3, 4) <ref type="table" target="#tab_4">Table 3</ref>. Dropout strategies of DARTS variants</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mothods</head><p>Skip Connection Other Ops DARTS <ref type="bibr" target="#b22">[23]</ref> No Drop No Drop PDARTS <ref type="bibr" target="#b5">[6]</ref> Drop No Drop RobustDARTS <ref type="bibr" target="#b38">[39]</ref> Drop Drop  <ref type="table" target="#tab_6">Table 4</ref> shows that our model achieves the best average precision 31.9%.   <ref type="figure" target="#fig_0">Fig. 9 and 13</ref> gives the complete softmax evolution when running DARTS on CIFAR-10 in S 1 . <ref type="figure" target="#fig_8">Fig. 15</ref> is a similar case except the skip connection is removed. <ref type="figure" target="#fig_8">Fig. 10</ref> gives the complete sigmoid evolution when running Fair DARTS on CIFAR-10 in S 1 with L 0−1 . <ref type="figure" target="#fig_2">Fig. 11, 12</ref> gives the complete sigmoid evolution when running Fair DARTS on CIFAR-10 in S 1 with single-level and bi-level optimization respectively. <ref type="figure" target="#fig_8">Fig. 14</ref> is a stacked-bar version of <ref type="figure" target="#fig_2">Fig. 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Special Study: DARTS with L 0−1</head><p>Although our auxiliary loss L 0−1 is particularly designed for Fair DARTS, it is interesting to see how DARTS behave under this new condition. <ref type="figure" target="#fig_5">Fig. 8 and  16</ref> give us such an illustration, where L 0−1 has the same weight w = 10 as in Fair DARTS. Not surprisingly, under the exclusive competition by softmax, skip connections exploit even more from unfair advantages as we drive the weakperforming operations towards zero <ref type="figure" target="#fig_5">(Fig. 16</ref>). Noticeably, there is a domino effect, as the weakest operation decrease its weight, the second weakest follows, so on and so forth. This effect speeds up the aggregation and as a result, more skip connections stand out <ref type="figure" target="#fig_7">(Fig. 8</ref>). As the rest better-performing operations are contending with each other, it still finds difficulty to determine which one is the best. Besides, training its inferred best models reaches 96.77 ± 0.29% on CIFAR-10 (run 7 times each with different seeds), which is not too different from the original DARTS. Therefore, we conclude that applying L 0−1 alone is not enough to solve the existing problems in DARTS. In fact, L 0−1 cannot handle the softmax case inherently.  Model Architecture Genotype 0 Genotype(normal=[('sep conv 3x3', 1), ('sep conv 3x3', 0), ('dil conv 5x5', 1), ('dil conv 3x3', 2), ('sep conv 3x3', 1), ('dil conv 5x5', 3), ('dil conv 5x5', 4), ('dil conv 3x3', 3)], normal concat=range <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b5">6)</ref>, reduce=[('max pool 3x3', 0), ('max pool 3x3', 1), ('avg pool 3x3', 0), ('dil conv 5x5', 2), ('skip connect', 1), ('max pool 3x3', 0), ('skip connect', 3), ('avg pool 3x3', 1)], reduce concat=range(2, 6)) 1</p><p>Genotype(normal=[('dil conv 3x3', 1), ('sep conv 3x3', 0), ('dil conv 3x3', 2), ('dil conv 3x3', 1), ('sep conv 3x3', 1), ('dil conv 5x5', 3), ('dil conv 3x3', 3), ('dil conv 5x5', 4)], normal concat=range <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b5">6)</ref>, reduce=[('dil conv 5x5', 1), ('skip connect', 0), ('max pool 3x3', 0), ('dil conv 3x3', 2), ('skip connect', 3), ('skip connect', 2), ('skip connect', 2), ('skip connect', 3)], reduce concat=range(2, 6)) 2</p><p>Genotype(normal=[('dil conv 3x3', 1), ('sep conv 3x3', 0), ('dil conv 3x3', 1), ('dil conv 3x3', 2), ('sep conv 3x3', 1), ('dil conv 3x3', 3), ('dil conv 3x3', 4), ('dil conv 3x3', 1)], normal concat=range <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b5">6)</ref>, reduce=[('max pool 3x3', 0), ('dil conv 3x3', 1), ('avg pool 3x3', 0), ('dil conv 5x5', 2), ('max pool 3x3', 0), ('skip connect', 2), ('skip connect', 2), ('avg pool 3x3', 0)], reduce concat=range(2, 6)) 3</p><p>Genotype(normal=[('dil conv 3x3', 1), ('skip connect', 0), ('dil conv 3x3', 2), ('dil conv 3x3', 1), ('sep conv 3x3', 1), ('dil conv 3x3', 3), ('dil conv 3x3', 4), ('dil conv 5x5', 2)], normal concat=range <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b5">6)</ref>, reduce=[('avg pool 3x3', 0), ('dil conv 5x5', 1), ('avg pool 3x3', 0), ('dil conv 5x5', 2), ('avg pool 3x3', 0), ('dil conv 5x5', 3), ('dil conv 5x5', 4), ('avg pool 3x3', 0)], reduce concat=range(2, 6)) <ref type="table">Table 7</ref>. Randomly sampled architecture genotypes in S1 setting M = 2. Discussed in Section 6.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Equal Contribution. 3</head><label>3</label><figDesc>Code is available here: https://github.com/xiaomi-automl/FairDARTS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The number of dominant skip connections continues to grow when searching with DARTS (run k = 4 times) on CIFAR-10 (in S1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The derived model has 8 skip connections in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 . 4 . 1</head><label>441</label><figDesc>Heatmap of softmax values in the normal cell and the reduction cell at the last searching epoch when running DARTS on CIFAR-10 (in search space S1) Stepping out the Pitfalls of Skip Connections</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The Architecture of Fair DARTS-D (top) and C (bottom</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Stacked area plot of the softmax evolution in S1 \ {skip} when running DARTS on CIFAR-10. With unfair advantages removed, all operations enjoy a fair treatment 6.2 How Does Zero-One Loss Matter?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>L 0−1 (proposed) and the other L 0−1 (control). To some extent, L 0Left: Histogram of sigmoid values in the last searching epoch without (left) and with L0−1 (right). On the right, this auxiliary loss has pushed the values towards 0 or 1. Right: Number of dominant operations in the last searching epoch running Fair DARTS on CIFAR-10 w.r.t the sensitivity weight w0−1 of auxiliary loss L0−1 stepping out of the local minimum while L 0−1 selects operations only at an early stage which depends greatly on the initialization. This matches our analysis in Section 4.2. The detailed results under both loss functions are shown in Fig. 2 (supplementary).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>S 1</head><label>1</label><figDesc>works in the previously mentioned DAG of N = 7 nodes, first two nodes in cell c k−1 are the outputs of last two cells c k−1 and c k−2 , four intermediate nodes with each has incoming edges from the former nodes. The output node of DAG is a concatenation of all intermediate nodes. Each edge contains 7 candidate operations:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 c k− 1 Fig. 1 .</head><label>211</label><figDesc>The DARTS search space at cell-level. Red labels indicate intermediate nodes. The outputs of all intermediate nodes are concatenated to form the output c k objective methods. All the models are trained and evaluated on MS COCO dataset (train2017 and val2017 respectively) for 12 epochs with a batch size of 16. The initial learning rate is 0.01 and divided by 10 at epochs 8 and 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 2 .Fig. 3 . 4 Figures 4 . 1</head><label>23441</label><figDesc>Comparison of sigmoid evolution when running Fair DARTS with L 0−1 (top) and L0−1 (bottom) in S1. With the proposed loss at the bottom, sigmoid values manage to step out of local optima Comparison of sigmoid evolution when running Fair DARTS with single and bi-level optimization. Bi-level has better robustness to local minimum Evolution of Architectural Weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 4 . 1 CFig. 5 .Table 5 .</head><label>4155</label><figDesc>Heatmap of σ(α) in the normal cell and the reduction cell at the last epoch when searching with Fair DARTS on CIFAR-10 (in S1). As a result of the sigmoid feature and the auxiliary loss L0−1, the values are mainly around 0 and FairDARTS-a cells searched on CIFAR-10 in S1 Fair DARTS architecture genotypes. See FairDARTS-a inFig. 5 Model Architecture Genotype FairDARTS-b Genotype(normal=[('sep conv 3x3', 2, 0), ('sep conv 3x3', 2, 1), ('sep conv 3x3', 3, 1), ('dil conv 3x3', 4, 0), ('sep conv 5x5', 4, 1), ('dil conv 5x5', 5, 1)], normal concat=range(2, 6), re-duce=[('skip connect', 2, 0), ('dil conv 3x3', 2, 1), ('skip connect', 3, 0), ('dil conv 3x3', 3, 1), ('max pool 3x3', 4, 0), ('sep conv 3x3', 4, 1), ('skip connect', 5, 2), ('max pool 3x3', 5, 0)], reduce concat=range(2, 6)) FairDARTS-c Genotype(normal=[('max pool 3x3', 2, 0), ('sep conv 5x5', 2, 1), ('dil conv 3x3', 4, 0), ('dil conv 5x5', 4, 2), ('skip connect', 5, 3), ('sep conv 3x3', 5, 0)], normal concat=range(2, 6), re-duce=[('dil conv 3x3', 2, 1), ('dil conv 5x5', 2, 0), ('dil conv 3x3', 3, 0), ('sep conv 3x3', 3, 1), ('sep conv 5x5', 4, 0), ('sep conv 5x5', 4, 3), ('sep conv 5x5', 5, 0), ('skip connect', 5, 1)], reduce concat=range(2, 6)) FairDARTS-d Genotype(normal=[('sep conv 3x3', 2, 0), ('sep conv 5x5', 2, 1), ('dil conv 3x3', 3, 1), ('max pool 3x3', 3, 0), ('dil conv 3x3', 4, 0), ('dil conv 3x3', 4, 1), ('sep conv 3x3', 5, 0), ('dil conv 5x5', 5, 1)], normal concat=range(2, 6), reduce=[('max pool 3x3', 2, 0), ('sep conv 5x5', 2, 1), ('avg pool 3x3', 3, 0), ('dil conv 5x5', 3, 2), ('dil conv 3x3', 4, 3), ('avg pool 3x3', 4, 0), ('avg pool 3x3', 5, 0), ('skip connect', 5, 3)], reduce concat=range(2, 6)) FairDARTS-e Genotype(normal=[('sep conv 3x3', 2, 0), ('sep conv 3x3', 2, 1), ('dil conv 3x3', 4, 1), ('dil conv 3x3', 4, 2), ('dil conv 3x3', 5, 0), ('dil conv 5x5', 5, 1)], normal concat=range(2, 6), re-duce=[('max pool 3x3', 2, 1), ('max pool 3x3', 2, 0), ('max pool 3x3', 3, 1), ('max pool 3x3', 3, 0), ('sep conv 5x5', 4, 1), ('max pool 3x3', 4, 0), ('avg pool 3x3', 5, 0), ('dil conv 5x5', 5, 1)], reduce concat=range(2, 6)) FairDARTS-f Genotype(normal=[('max pool 3x3', 2, 0), ('sep conv 3x3', 2, 1), ('dil conv 3x3', 3, 1), ('sep conv 5x5', 4, 1), ('sep conv 3x3', 5, 0), ('sep conv 3x3', 5, 1)], normal concat=range(2, 6), re-duce=[('max pool 3x3', 2, 0), ('max pool 3x3', 2, 1), ('max pool 3x3', 3, 0), ('dil conv 3x3', 3, 1), ('dil conv 3x3', 4, 2), ('max pool 3x3', 4, 0), ('max pool 3x3', 5, 0), ('sep conv 3x3', 5, 1)], reduce concat=range(2, 6)) FairDARTS-g Genotype(normal=[('sep conv 3x3', 2, 0), ('sep conv 3x3', 2, 1), ('skip connect', 4, 3), ('sep conv 5x5', 4, 1), ('dil conv 3x3', 5, 0), ('sep conv 3x3', 5, 1)], normal concat=range(2, 6), re-duce=[('avg pool 3x3', 2, 1), ('skip connect', 2, 0), ('skip connect', 3, 2), ('max pool 3x3', 3, 1), ('sep conv 5x5', 4, 3), ('max pool 3x3', 4, 0), ('dil conv 3x3', 5, 1), ('dil conv 3x3', 5, 4)], reduce concat=range(2, 6))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Table 8 .Fig. 6 .Fig. 7 .</head><label>867</label><figDesc>3 (main text)Model Architecture Genotype 0 Genotype(normal=[('skip connect', 1), ('sep conv 5x5', 0), ('skip connect', 1), ('dil conv 5x5', 0), ('avg pool 3x3', 1), ('dil conv 3x3', 0), ('max pool 3x3', 0), ('sep conv 3x3', 3)], normal concat=range<ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b5">6)</ref>, reduce=[('dil conv 3x3', 1), ('dil conv 5x5', 0), ('max pool 3x3', 1), ('dil conv 5x5', 2), ('skip connect', 0), ('dil conv 3x3', 1), ('avg pool 3x3', 4), ('sep conv 5x5', 1)], reduce concat=range(2, 6))1 Genotype(normal=[('skip connect', 1), ('skip connect', 0), ('dil conv 3x3', 0), ('sep conv 5x5', 2), ('dil conv 5x5', 1), ('sep conv 5x5', 3), ('dil conv 3x3', 3), ('max pool 3x3', 4)], normal concat=range(2, 6), reduce=[('sep conv 3x3', 1), ('sep conv 3x3', 0), ('dil conv 3x3', 2), ('max pool 3x3', 1), ('sep conv 3x3', 0), ('sep conv 3x3', 1), ('max pool 3x3', 2), ('skip connect', 3)], reduce concat=range(2, 6))2 Genotype(normal=[('avg pool 3x3', 0), ('skip connect', 1), ('dil conv 5x5', 0), ('dil conv 3x3', 2), ('sep conv 5x5', 2), ('skip connect', 3), ('avg pool 3x3', 1), ('avg pool 3x3', 4)], normal concat=range(2, 6), reduce=[('avg pool 3x3', 0), ('avg pool 3x3', 1), ('avg pool 3x3', 0), ('sep conv 5x5', 1), ('sep conv 3x3', 2), ('avg pool 3x3', 1), ('sep conv 3x3', 2), ('max pool 3x3', 1)], reduce concat=range(2, 6)) 3 Genotype(normal=[('skip connect', 1), ('dil conv 3x3', 0), ('sep conv 3x3', 0), ('avg pool 3x3', 2), ('dil conv 5x5', 0), ('dil conv 3x3', 3), ('max pool 3x3', 1), ('skip connect', 4)], normal concat=range(2, 6), reduce=[('dil conv 3x3', 0), ('sep conv 5x5', 1), ('sep conv 5x5', 1), ('sep conv 5x5', 0), ('avg pool 3x3', 0), ('sep conv 5x5', 2), ('sep conv 3x3', 0), ('max pool 3x3', 1)], reduce concat=range(2, 6)) 4 Genotype(normal=[('dil conv 5x5', 1), ('skip connect', 0), ('sep conv 5x5', 0), ('skip connect', 2), ('sep conv 5x5', 3), ('dil conv 5x5', 2), ('avg pool 3x3', 0), ('max pool 3x3', 3)], normal concat=range(2, 6), reduce=[('dil conv 3x3', 0), ('dil conv 3x3', 1), ('max pool 3x3', 0), ('avg pool 3x3', 2), ('max pool 3x3', 0), ('avg pool 3x3', 2), ('dil conv 3x3', 0), ('dil conv 3x3', 2)], reduce concat=range(2, 6)) 5 Genotype(normal=[('sep conv 5x5', 0), ('skip connect', 1), ('sep conv 3x3', 2), ('sep conv 3x3', 0), ('avg pool 3x3', 2), ('skip connect', 0), ('sep conv 3x3', 0), ('dil conv 5x5', 3)], normal concat=range(2, 6), reduce=[('avg pool 3x3', 1), ('sep conv 5x5', 0), ('dil conv 3x3', 2), ('skip connect', 1), ('avg pool 3x3', 2), ('skip connect', 1), ('sep conv 3x3', 4), ('dil conv 3x3', 1)], reduce concat=range(2, 6)) 6 Genotype(normal=[('skip connect', 1), ('sep conv 5x5', 0), ('max pool 3x3', 0), ('dil conv 3x3', 1), ('sep conv 5x5', 1), ('avg pool 3x3', 0), ('skip connect', 4), ('sep conv 5x5', 0)], normal concat=range(2, 6), reduce=[('dil conv 5x5', 1), ('sep conv 3x3', 0), ('sep conv 5x5', 0), ('dil conv 5x5', 1), ('max pool 3x3', 1), ('max pool 3x3', 3), ('dil conv 5x5', 2), ('max pool 3x3', 1)], reduce concat=range(2, 6)) Randomly sampled architecture genotypes in S1 setting M = 2 and multiplyadds &gt; 500M. Discussed in Section 6.3 (main text) Model Architecture Genotype 0 Genotype(normal=[('skip connect', 1), ('sep conv 3x3', 0), ('skip connect', 0), ('dil conv 3x3', 2), ('sep conv 3x3', 1), ('dil conv 5x5', 2), ('sep conv 3x3', 2), ('dil conv 5x5', 1)], normal concat=range(2, 6), reduce=[('skip connect', 0), ('sep conv 5x5', 1), ('avg pool 3x3', 2), ('dil conv 5x5', 0), ('sep conv 3x3', 0), ('dil conv 5x5', 2), ('sep conv 5x5', 1), ('dil conv 5x5', 2)], reduce concat=range(2, 6)) 1 Genotype(normal=[('dil conv 3x3', 0), ('sep conv 3x3', 1), ('skip connect', 2), ('skip connect', 1), ('sep conv 5x5', 0), ('dil conv 3x3', 1), ('sep conv 5x5', 1), ('dil conv 5x5', 4)], normal concat=range(2, 6), reduce=[('dil conv 3x3', 0), ('dil conv 3x3', 1), ('avg pool 3x3', 1), ('skip connect', 0), ('dil conv 5x5', 1), ('skip connect', 3), ('skip connect', 0), ('max pool 3x3', 3)], reduce concat=range(2, 6)) 2 Genotype(normal=[('sep conv 5x5', 0), ('max pool 3x3', 1), ('sep conv 5x5', 1), ('skip connect', 0), ('skip connect', 2), ('max pool 3x3', 1), ('sep conv 5x5', 3), ('sep conv 3x3', 2)], normal concat=range(2, 6), reduce=[('sep conv 5x5', 0), ('skip connect', 1), ('max pool 3x3', 1), ('sep conv 5x5', 2), ('dil conv 5x5', 3), ('max pool 3x3', 0), ('dil conv 3x3', 1), ('max pool 3x3', 2)], reduce concat=range(2, 6)) 3 Genotype(normal=[('sep conv 3x3', 0), ('max pool 3x3', 1), ('dil conv 5x5', 2), ('dil conv 5x5', 0), ('sep conv 5x5', 3), ('sep conv 5x5', 0), ('skip connect', 3), ('skip connect', 0)], normal concat=range(2, 6), reduce=[('avg pool 3x3', 0), ('sep conv 5x5', 1), ('avg pool 3x3', 1), ('dil conv 3x3', 0), ('dil conv 5x5', 3), ('sep conv 5x5', 2), ('avg pool 3x3', 1), ('dil conv 5x5', 4)], reduce concat=range(2, 6)) 4 Genotype(normal=[('dil conv 5x5', 0), ('sep conv 5x5', 1), ('sep conv 3x3', 2), ('skip connect', 0), ('sep conv 5x5', 3), ('sep conv 5x5', 0), ('avg pool 3x3', 1), ('skip connect', 0)], normal concat=range(2, 6), reduce=[('max pool 3x3', 1), ('skip connect', 0), ('dil conv 5x5', 1), ('dil conv 5x5', 2), ('sep conv 5x5', 0), ('sep conv 3x3', 1), ('avg pool 3x3', 1), ('skip connect', 0)], reduce concat=range(2, 6)) 5 Genotype(normal=[('sep conv 5x5', 1), ('sep conv 5x5', 0), ('skip connect', 2), ('sep conv 3x3', 0), ('dil conv 5x5', 2), ('dil conv 3x3', 3), ('max pool 3x3', 0), ('skip connect', 2)], normal concat=range(2, 6), reduce=[('max pool 3x3', 0), ('dil conv 5x5', 1), ('max pool 3x3', 2), ('skip connect', 0), ('dil conv 5x5', 0), ('sep conv 5x5', 3), ('sep conv 3x3', 1), ('dil conv 5x5', 3)], reduce concat=range(2, 6)) 6 Genotype(normal=[('avg pool 3x3', 0), ('sep conv 5x5', 1), ('sep conv 5x5', 0), ('skip connect', 1), ('dil conv 5x5', 0), ('sep conv 5x5', 1), ('skip connect', 4), ('sep conv 5x5', 0)], normal concat=range(2, 6), reduce=[('dil conv 5x5', 1), ('avg pool 3x3', 0), ('avg pool 3x3', 2), ('sep conv 5x5', 0), ('max pool 3x3', 0), ('dil conv 5x5', 2), ('sep conv 5x5', 3), ('skip connect', 0)], reduce concat=range(2,6)) FairDARTS-b cells searched on CIFAR-10 in S1 The number of dominant skip connections when searching with Fair DARTS on CIFAR-10. Here we select top-2 operations as dominants as in DARTS (our proposed way selects dominants according to σ threshold ), our method can still escape from too many skip connections. Note that we choose the first 4 experiments to show</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 8 .</head><label>8</label><figDesc>The number of dominant skip connections when searching with DARTS equipped with L0−1 on CIFAR-10. Notice the aggregation of skip connections gets even worse</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 9 .</head><label>9</label><figDesc>The softmax evolution when running DARTS on CIFAR-10 in S1 (k = 3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 10 .Fig. 11 .Fig. 12 .Fig. 13 .Fig. 14 .Fig. 15 .Fig. 16 .</head><label>10111213141516</label><figDesc>The sigmoid evolution when running Fair DARTS with L 0−1 loss in S1 The sigmoid evolution when running Fair DARTS with single-level optimization in S1 The sigmoid evolution when running Fair DARTS with bi-level optimization in S1 The evolution of sof tmax(α) when running DARTS on CIFAR-10 in S1. Skip connections on edge 0,2,4,5,11 in the normal cell and edge<ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12</ref> in the reduction cell gradually suppress others caused by unfair advantage The evolution of σ(α) when running Fair DARTS on CIFAR-10 in S1. Skip connections enjoy an equal opportunity under collaborative competition. See edge<ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12</ref> in normal cell and almost all edges in reduction cell The evolution of σ(α) when running DARTS on CIFAR-10 in S1 without skip connections. With unfair advantages removed, all operations are encouraged to demonstrate each real strength The evolution of sof tmax(α) when running DARTS with the auxiliary loss L0−1 enabled on CIFAR-10 in S1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>No. of Dominant OpsFig. 1. Top: Stacked area plot of the number of dominant operations 5 of DARTS and Fair DARTS when searching on ImageNet in search space S2 (19 searchable layers). Bottom: Heatmaps of softmax (DARTS) and sigmoid (Fair DARTS) values in the last searching epoch. DARTS finally chooses a shallow model (11 layers removed by activating skip connections only) which obtains 66.4% top-1 accuracy. While in Fair DARTS, all operations develop independently that an excessive number of skip connections no longer leads to poor performance. Here it infers a deeper model (only one layer is removed) with 75.6% top-1 accuracy</figDesc><table><row><cell></cell><cell></cell><cell>Fair DARTS</cell><cell></cell></row><row><cell></cell><cell></cell><cell>E3K3</cell><cell></cell></row><row><cell></cell><cell></cell><cell>E3K5</cell><cell></cell></row><row><cell></cell><cell></cell><cell>E3K7</cell><cell></cell></row><row><cell></cell><cell></cell><cell>E6K3</cell><cell></cell></row><row><cell></cell><cell></cell><cell>E6K5</cell><cell></cell></row><row><cell></cell><cell></cell><cell>E6K7</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Skip</cell><cell></cell></row><row><cell>DARTS sof tmax(α)</cell><cell></cell><cell>Fair DARTS sigmoid(α)</cell><cell></cell></row><row><cell>E3K3 E3K5</cell><cell>0.8</cell><cell>E3K3 E3K5</cell><cell>0.8</cell></row><row><cell>E3K7 E6K3 E6K5</cell><cell>0.4 0.6</cell><cell>E6K5 E6K3 E3K7</cell><cell>0.4 0.6</cell></row><row><cell>E6K7</cell><cell>0.2</cell><cell>E6K7</cell><cell>0.2</cell></row><row><cell>Skip</cell><cell></cell><cell>Skip</cell><cell></cell></row><row><cell>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19</cell><cell></cell><cell>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19</cell><cell></cell></row><row><cell>Layer</cell><cell></cell><cell>Layer</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2</head><label>12</label><figDesc>Comparison of architectures on CIFAR-10. † : MultAdds computed using the genotypes provided by the authors. : Averaged on training the best model for several times . ‡ : Averaged on models from 7 runs of FairDARTS (Search + Full Train)</figDesc><table><row><cell>Models</cell><cell></cell><cell cols="4">Params (M) ×+ (M) Top-1 (%) Type</cell></row><row><cell>NASNet-A [41]</cell><cell></cell><cell>3.3</cell><cell>608  †</cell><cell>97.35</cell><cell>RL</cell></row><row><cell>ENAS [27]</cell><cell></cell><cell>4.6</cell><cell>626  †</cell><cell>97.11</cell><cell>RL</cell></row><row><cell>MdeNAS[40]</cell><cell></cell><cell>3.6</cell><cell>599  †</cell><cell>97.45</cell><cell>MDL</cell></row><row><cell cols="3">DARTS(second order) [23] 3.3</cell><cell>528  †</cell><cell cols="2">97.24±0.09 GD</cell></row><row><cell>SNAS [37]</cell><cell></cell><cell>2.8</cell><cell>422  †</cell><cell cols="2">97.15±0.02 GD</cell></row><row><cell>GDAS [12]</cell><cell></cell><cell>3.37</cell><cell>519  †</cell><cell>97.07</cell><cell>GD</cell></row><row><cell cols="2">SGAS (Cri.2 avg.) [19]</cell><cell>3.9±0.22  †</cell><cell cols="3">640±39  † 97.33±0.21 GD</cell></row><row><cell>P-DARTS [6]</cell><cell></cell><cell>3.4</cell><cell>532  †</cell><cell>97.5</cell><cell>GD</cell></row><row><cell>PC-DARTS [38]</cell><cell></cell><cell>3.6</cell><cell>558  †</cell><cell>97.43</cell><cell>GD</cell></row><row><cell>RDARTS [39]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>97.05</cell><cell>GD</cell></row><row><cell>FairDARTS-a</cell><cell></cell><cell>2.8</cell><cell>373</cell><cell>97.46</cell><cell>GD</cell></row><row><cell>FairDARTS  ‡</cell><cell></cell><cell>3.32±0.46</cell><cell cols="3">458±61 97.46±0.05 GD</cell></row><row><cell cols="2">MobileNetV2(1.4) [30] 585</cell><cell>6.9</cell><cell>74.7</cell><cell>92.2</cell><cell>-</cell></row><row><cell>NASNet-A [41]</cell><cell>564</cell><cell>5.3</cell><cell>74.0</cell><cell>91.6</cell><cell>2000</cell></row><row><cell>AmoebaNet-A[29]</cell><cell>555</cell><cell>5.1</cell><cell>74.5</cell><cell>92.0</cell><cell>3150</cell></row><row><cell>MnasNet-92 [33]</cell><cell>388</cell><cell>3.9</cell><cell>74.79</cell><cell>92.1</cell><cell>1667</cell></row><row><cell>DARTS [23]</cell><cell>574</cell><cell>4.7</cell><cell>73.3</cell><cell>91.3</cell><cell>4</cell></row><row><cell>FBNet-C [36]</cell><cell>375</cell><cell>5.5</cell><cell>74.9</cell><cell>92.3</cell><cell>9</cell></row><row><cell>Proxyless GPU  ‡ [4]</cell><cell>465  *</cell><cell>7.1</cell><cell>75.1</cell><cell>92.4</cell><cell>8.3</cell></row><row><cell>FairNAS-C  ‡ [9]</cell><cell>321</cell><cell>4.4</cell><cell>74.7</cell><cell>92.1</cell><cell>10</cell></row><row><cell>SNAS [37]</cell><cell>522</cell><cell>4.3</cell><cell>72.7</cell><cell>90.8</cell><cell>1.5</cell></row><row><cell>GDAS [12]</cell><cell>581</cell><cell>5.3</cell><cell>74.0</cell><cell>91.5</cell><cell>0.2</cell></row><row><cell>P-DARTS  † † [6]</cell><cell>577</cell><cell>5.1</cell><cell>74.9  *</cell><cell>92.3  *</cell><cell>0.3</cell></row><row><cell>PC-DARTS  † [38]</cell><cell>586</cell><cell>5.3</cell><cell>74.9</cell><cell>92.2</cell><cell>3.8</cell></row><row><cell>FairDARTS-A  †</cell><cell>417</cell><cell>3.6</cell><cell>73.7</cell><cell>91.7</cell><cell>0.4</cell></row><row><cell>FairDARTS-B  †</cell><cell>541</cell><cell>4.8</cell><cell>75.1</cell><cell>92.5</cell><cell>0.4</cell></row><row><cell>FairDARTS-C  ‡</cell><cell>380</cell><cell>4.2</cell><cell>75.1</cell><cell>92.4</cell><cell>3</cell></row><row><cell>FairDARTS-D  ‡</cell><cell>440</cell><cell>4.3</cell><cell>75.6</cell><cell>92.6</cell><cell>3</cell></row><row><cell>MobileNetV3 [15]</cell><cell>219</cell><cell>5.4</cell><cell>75.2</cell><cell>92.2</cell><cell>-</cell></row><row><cell>MoGA-A [8]</cell><cell>304</cell><cell>5.1</cell><cell>75.9</cell><cell>92.8</cell><cell>12</cell></row><row><cell>MixNet-M [35]</cell><cell>360</cell><cell>5.0</cell><cell>77.0</cell><cell>93.3</cell><cell>-</cell></row><row><cell>EfficientNet B0 [34]</cell><cell>390</cell><cell>5.3</cell><cell>76.3</cell><cell>93.2</cell><cell>-</cell></row><row><cell>SCARLET-A [7]</cell><cell>365</cell><cell>6.7</cell><cell>76.9</cell><cell>93.4</cell><cell>10</cell></row><row><cell>FairDARTS-C</cell><cell>386</cell><cell>5.3</cell><cell>77.2</cell><cell>93.5</cell><cell>3</cell></row></table><note>. Comparison of architectures on ImageNet. : Based on its published code.† : Searched on CIFAR-10. † † : Searched on CIFAR-100.‡ : Searched on ImageNet (cost more than those transferred).• : in GPU days. : w/ SE and Swish Models ×+ (M) Params (M) Top-1 (%) Top-5 (%) Cost •</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). IBEx Ky refers to an inverted bottleneck without an inset skip connection, while MBEx Ky is the one with it. BOTTLE K3 is the inverted bottleneck without expansion 6 Ablation Study and Analysis 6.1 Removing Skip Connections from S 1As unfair advantages are mainly from skip connections, if we remove them from S 1 and get the reduced search space S 1 \ {skip}, we should expect a fair play even in an exclusive competition. Several runs of this experiment also show that there is indeed no more prevailing operations that suppress others, including other parameter-less ones like max-pooling and average pooling(Fig. 7). For S 1 \{skip}, we run all the experiments with 7 different random seeds and we train the searched models from scratch. The best models (96.88 ± 0.18%) are slightly higher than DARTS (96.76 ± 0.32%) 10 , but lower than FairDARTS (97.41 ± 0.14%) in S 1 . The lowered accuracy indicates that adequate skip connections are indeed beneficial for accuracy.</figDesc><table><row><cell></cell><cell></cell><cell>(1, 2)</cell><cell></cell><cell></cell><cell>(2, 2)</cell><cell></cell><cell></cell><cell>(2, 3)</cell><cell></cell><cell></cell><cell>(3, 2)</cell><cell></cell><cell></cell><cell>(3, 3)</cell><cell></cell><cell></cell><cell>(0, 0)</cell><cell></cell><cell></cell><cell>(1, 0)</cell><cell></cell><cell></cell><cell>(2, 0)</cell></row><row><cell>sof tmax(α)</cell><cell>0.5 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>Epochs</cell><cell>50</cell><cell>0</cell><cell>Epochs</cell><cell>50</cell><cell>0</cell><cell>Epochs</cell><cell>50</cell><cell>0</cell><cell>Epochs</cell><cell>50</cell><cell>0</cell><cell>Epochs</cell><cell>50</cell><cell>0</cell><cell>Epochs</cell><cell>50</cell><cell>0</cell><cell>Epochs</cell><cell>50</cell><cell>0</cell><cell>Epochs</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">max pool 3x3</cell><cell></cell><cell cols="3">avg pool 3x3</cell><cell></cell><cell cols="2">sep conv 3x3</cell><cell></cell><cell cols="3">sep conv 5x5</cell><cell></cell><cell cols="2">dil conv 3x3</cell><cell></cell><cell cols="3">dil conv 5x5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Experiment 1: Random sampling (7 models each, averaged) from regularized search space (M = 2). Experiment 2: Adding Gaussian noise to DARTS (repeated 4 times, averaged)</figDesc><table><row><cell>Methods</cell><cell>CIFAR-10 Top-1 Acc (%)</cell></row><row><cell>Random (M=2)</cell><cell>97.01 ±0.24</cell></row><row><cell>Random (M=2, MultAdds ≥ 500M)</cell><cell>97.14 ±0.28</cell></row><row><cell>DARTS + Gaussian (cosine decay)</cell><cell>97.12 ±0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Object detection of various drop-in backbones.</figDesc><table><row><cell>Backbones</cell><cell cols="5">×+ Params Acc AP AP50 AP75 APS APM APL</cell></row><row><cell></cell><cell>(M) (M)</cell><cell>(%) (%) (%)</cell><cell>(%)</cell><cell>(%) (%)</cell><cell>(%)</cell></row><row><cell cols="2">MobileNetV2 [30] 300 3.4</cell><cell cols="4">72.0 28.3 46.7 29.3 14.8 30.7 38.1</cell></row><row><cell cols="2">SingPath NAS [32] 365 4.3</cell><cell cols="4">75.0 30.7 49.8 32.2 15.4 33.9 41.6</cell></row><row><cell cols="2">MobileNetV3 [15] 219 5.4</cell><cell cols="4">75.2 29.9 49.3 30.8 14.9 33.3 41.1</cell></row><row><cell>MnasNet-A2 [33]</cell><cell>340 4.8</cell><cell cols="4">75.6 30.5 50.2 32.0 16.6 34.1 41.1</cell></row><row><cell>MixNet-M [35]</cell><cell>360 5.0</cell><cell cols="4">77.0 31.3 51.7 32.4 17.0 35.0 41.9</cell></row><row><cell>FairDARTSC  †</cell><cell>386 5.3</cell><cell cols="4">77.2 31.9 51.9 33.0 17.4 35.3 43.0</cell></row></table><note>† : w/ SE and Swish</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Architecture genotypes when adding Gaussian noise to DARTS. Discussed in Section 6.3 (main text)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In DARTS, it refers to the one with the highest architectural weight. In FairDARTS, it means the one whose σ &gt; σ threshold . Here we use σ threshold = 0.75.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">corresponding to the experiment (k = 3 ) inFig. 2.<ref type="bibr" target="#b6">7</ref> We run DARTS 4 times and it holds every time.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The maximum number of edges for a node is also limited to 2 as in DARTS.<ref type="bibr" target="#b8">9</ref> Their architectures are given inFig. 5 and 6 (supplementary).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">This differs from DARTS' reported values as it trains one model for several times.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Its sigmoid output is 0.5 (fair starting point).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and Simplifying One-Shot Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11831</idno>
		<title level="m">Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SMASH: One-Shot Model Architecture Search through HyperNetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<title level="m">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<title level="m">Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06022</idno>
		<title level="m">SCARLET-NAS: Bridging the gap between scalability and fairness in neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MoGA: Searching Beyond MobileNetV3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1908.01314.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<title level="m">FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning Augmentation Policies from Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-Shot Neural Architecture Search via Self-Evaluated Template Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3681" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for a Robust Neural Architecture in Four GPU Hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single Path One-Shot Neural Architecture Search with Uniform Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Searching for MobileNetV3</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11926</idno>
		<title level="m">StacNAS: Towards stable and consistent optimization for differentiable Neural Architecture Search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convergence Analysis of Two-layer Neural Networks with Relu Activation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">DARTS+: Improved Differentiable Architecture Search with Early Stopping</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>DARTS: Differentiable Architecture Search</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gradient-based hyperparameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">XNAS: Neural Architecture Search with Expert Advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Regularized Evolution for Image Classifier Architecture Search. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>MobileNetV2: Inverted Residuals and Linear Bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MnasNet: Platform-Aware Neural Architecture Search for Mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">MixConv: Mixed Depthwise Convolutional Kernels. The British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">SNAS: Stochastic Neural Architecture Search. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<title level="m">PC-DARTS: Partial Channel Connections for Memory-efficient Differentiable Architecture Search</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding and robustifying differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1gDNyrKDS" />
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<title level="m">Multinomial Distribution Learning for Effective Neural Architecture Search. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Transferable Architectures for Scalable Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
