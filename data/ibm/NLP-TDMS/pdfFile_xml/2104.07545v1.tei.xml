<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Learning for Generation with Long Source Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Rohde</surname></persName>
							<email>tobiasr@birch.ai</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxia</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
							<email>yinhan@birch.ai</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Seattle</roleName><forename type="first">Birch</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Learning for Generation with Long Source Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the challenges for current sequence to sequence (seq2seq) models is processing long sequences, such as those in summarization and document level machine translation tasks. These tasks require the model to reason at the token level as well as the sentence and paragraph level. We design and study a new Hierarchical Attention Transformer-based architecture (HAT) that outperforms standard Transformers on several sequence to sequence tasks. In particular, our model achieves stateof-the-art results on four summarization tasks, including ArXiv, CNN/DM, SAMSum, and AMI, and we push PubMed R1 &amp; R2 SOTA further. Our model significantly outperforms our document-level machine translation baseline by 28 BLEU on the WMT19 EN-DE document translation task. We also investigate what the hierarchical layers learn by visualizing the hierarchical encoder-decoder attention. Finally, we study hierarchical learning on encoder-only pre-training and analyze its performance on classification downstream tasks. 1   . 2016. Discourse parsing with attention-based hierarchical neural networks. In EMNLP, pages 362-371. . 2020. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726-742.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence to sequence (seq2seq) models have been successfully used for a variety of natural language processing (NLP) tasks, including text summarization, machine translation and question answering. Often sequence to sequence models consist of an encoder that processes some source sequence and a decoder that generates the target sequence. <ref type="bibr">Originally, Sutskever et al. (2014)</ref> used recurrent neural networks as encoder and decoder for machine translation on the WMT-14 dataset. <ref type="bibr" target="#b1">Bahdanau et al. (2016)</ref> introduced the attention mechanism, where the decoder computes a distribution over the hidden states of the encoder and uses it to weigh the * Work performed while Xiaoxia Wu interned at Birch AI. She is currently a postdoc fellow at The University of Chicago and Toyota Technological Institute at Chicago. 1 https://github.com/birch-research/ hierarchical-learning hidden states of the input tokens differently at each decoding step. <ref type="bibr">Vaswani et al. (2017)</ref> then introduced a new architecture for sequence to sequence modeling -the Transformer, which is based on the attention mechanism but not recurrent allowing for more efficient training.</p><p>While successful, both recurrent neural networks and Transformer-based models have limits on the input sequence length. When the input is long, the learning degrades particularly for tasks which require a comprehensive understanding of the entire paragraph or document. One of the main learning challenges for seq2seq models is that the decoder needs to attend to token level representations from the encoder to predict the next token, while at the same time it must learn from a large context.</p><p>A commonly used method for attempting to solve the long-sequence problem is hierarchical attention <ref type="bibr">(Yang et al., 2016)</ref>. This method was studied primarily on long sequence classification tasks, where the model learns a document representation which is used as the input to a classifier. Since then, many successful papers proposed methods using hierarchical attention (see Section 2 for full details). While hierarchical attention has been successfully applied to classification tasks, its potential for being applied to large document sequence to sequence tasks remains an interesting and open question.</p><p>In this paper, we present a hierarchical attention model based on the standard Transformer <ref type="bibr">(Vaswani et al., 2017)</ref> that produces sentence level representations, and combine them with token level representations to improve performance on long document sequence to sequence tasks. Our main contributions include 1) We design a hierarchical seq2seq attention network architecture named HAT (Hierachical Attention Transformer) (See <ref type="figure" target="#fig_0">Figure 1)</ref>. We conduct extensive experiments on various generation tasks for our proposed model in Sectios 4 and 5 and achieve new state-of-the-art  results on several datasets.</p><p>2) In Sections 6 and 7, we study the generated output of our architecture to further understand the benefits of hierarchical attention and compare it with those generated by plain seq2seq Transformer models. Furthermore, we analyze how the decoder makes use of the sentence level representations of the encoder by visualizing the hierarchical encoderdecoder attention.</p><p>3) Finally, we apply hierarchical attention to an encoder-only architecture and pre-train it on a Books and Wiki corpus similar to the one used in <ref type="bibr">RoBERTa (Liu et al., 2019)</ref>. We finetune our pre-trained model on several downstream tasks and analyze the performance in Section 7.</p><p>2 Background Transformer models. The attention-based Transformer architecture <ref type="bibr">(Vaswani et al., 2017)</ref> currently dominates the state-of-the-art performance in many NLP tasks. This is largely due to the success of language model pre-training prior to fine-tuning the Transformer on the actual downstream task of interest. Pre-training has been applied in different ways to different variations of the Transformer architecture, including encoder-only pre-training <ref type="bibr">(BERT (Devlin et al., 2019)</ref>, <ref type="bibr">RoBERTa (Liu et al., 2019)</ref>, <ref type="bibr">XLNet (Yang et al., 2019)</ref>), decoder-only pre-training <ref type="bibr">(GPT (Radford et al., 2019)</ref>), encoderdecoder pre-training (T5 <ref type="bibr">(Raffel et al., 2020)</ref>, <ref type="bibr">BART (Lewis et al., 2020)</ref>) and multilingual pretraining <ref type="bibr">(MBART (Liu et al., 2020)</ref>, <ref type="bibr">Roberta-XLM (Conneau et al., 2020)</ref>). Both classification and generation downstream task performance improves significantly by initializing parameters from pre-trained models. To use a pre-trained model, the downstream task model architecture needs to be the same or similar to the one used in pre-training. Transformers have become the most popular architectures in NLP. However, one disadvantage of Transformers is that when the input sequence is long, the performance of the attention mechanism can become worse.</p><p>Long document modeling. Understanding and modeling large documents has been a longstanding challenge that has become increasingly demanded in practice <ref type="bibr">(Nakao, 2000;</ref><ref type="bibr">Mihalcea and Ceylan, 2007;</ref><ref type="bibr">Iyer et al., 2016;</ref><ref type="bibr">Zhang et al., 2017;</ref><ref type="bibr">Ç elikyilmaz et al., 2018;</ref><ref type="bibr">Paulus et al., 2018;</ref><ref type="bibr">Tasnim et al., 2019;</ref><ref type="bibr">Zhao et al., 2019;</ref><ref type="bibr">Gunel et al., 2019;</ref><ref type="bibr">Ye et al., 2019;</ref><ref type="bibr" target="#b2">Beltagy et al., 2020;</ref><ref type="bibr">Zhang et al., 2020</ref> In this work, we apply hierarchical learning to Transformer models for improving performance on generation tasks with long documents, including summarization and document-level machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We modify the standard sequence to sequence transformer architecture <ref type="bibr">(Vaswani et al., 2017)</ref> by adding hierarchical attention for improved processing of long documents <ref type="figure" target="#fig_0">(Figure 1</ref>).</p><p>We use 12 encoder and decoder layers, a hidden size of 1024, 4096 for the dimension of the fullyconnected feed-forward networks and 16 attention heads in both the encoder and the decoder. Unlike the original Transformer, we use GELU activation instead of ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data pre-processing</head><p>During pre-processing, we insert BOS tokens at the start of every sentence in each source document as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We simply determine sentence boundaries by punctuation or by using prior sentence segmentation present in the documents. By using BOS tokens as hierarchical tokens, the hierarchical attention can benefit from the representations learned for the BOS token during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder</head><p>We use the same encoder as in <ref type="bibr">Transformer Vaswani et al. (2017)</ref>. This produces an embedding for each input token. After those, we add an additional encoder layer (pink block in <ref type="figure" target="#fig_0">Figure 1</ref>) which only attends to the embeddings of the BOS tokens that we inserted during data-preprocessing. We refer to this layer as the hierarchical encoder layer, which produces another level of contextual representations for each of the BOS tokens, which can be interpreted as sentence level representations. We find that a single hierarchical layer works the best, although multiple hierarchical layers may be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder</head><p>As in <ref type="bibr">Vaswani et al. (2017)</ref>, each layer first performs self attention over the previously generated tokens and then attends over the outputs of the final token level encoder layer, similarly to the vanilla Transformer. We add an attention module that attends over the BOS token embeddings from the hierarchical encoder layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our architecture is specifically designed to better handle long sequences, thus we evaluate it on summarization and document level translation tasks, which tend to have long source sequences as the input. We also run experiments with non-generation tasks with an encoder-only hierarchical attention architecture (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Summarization Tasks</head><p>We characterize all the summarization tasks into several categories, test our architectures with different weight initializations and compare them with their non-hierarchical counterparts using the same weight initializations.</p><p>Long sequence datasets The PubMed and arXiv datasets <ref type="bibr">(Cohan et al., 2018)</ref> contain scientific articles from PubMed and arXiv respectively, and use the abstract of the articles as the target summary. The sequences in each of these datasets are long and need to be truncated to be processed by most transformers. Statistics on the PubMed and arXiv datasets are given in <ref type="table">Table 1</ref>.</p><p>We add a BOS token at the beginning of each sentence in the source sequences during data preprocessing. We use 3072 as the maximum source length, 512 as the maximum target length. Longer sequences are truncated. We followed <ref type="bibr">BART (Lewis et al., 2020)</ref> and use GPT2 (Radford et al., 2019) byte-pair encoding (BPE). We random initialize the hierarchical encoder layer, the hierarchical attention modules in the decoder layers and the additional positional embedding from 512 to 3072. We initialize all the remaining weights with pre-trained weights from BART. We initialize also initialize a plain seq2seq model with BART pre-trained weights for direct comparison.</p><p>For training, we use a batch-size of 128. We set weight decay to 0.01 and use the Adam optimizer with (β 1 , β 2 ) = (0.9, 0.999) and ε = 10 −8 <ref type="bibr">(Kingma and Ba, 2015)</ref>. We train for 30000 steps and warm up the learning rate for 900 steps to 3 × 10 −5 and decay it linearly afterwards. We use a dropout rate of 0.1 for attention and all layers. We also use label smoothing with smoothing constant 0.1. We use mixed precision for training. We complete 30000 steps in approximately 115 hours (without validation and checkpoint saving) on 2 A100 GPUs.</p><p>For generation, we use a beam width of 2, length penalty of 1 and minimum and maximum generation lengths of 72 and 966 respectively. <ref type="bibr">(Hermann et al., 2015)</ref> and <ref type="bibr">XSum (Narayan et al., 2018)</ref> are commonly used news summarization datasets. Both datasets are sourced from news articles, which frequently include a short summary in the article. Statistics for CNN/DM and Xsum are in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>News datasets CNN/DailyMail</head><p>We use 1024 as maximum source length for article and 256 as maximum target length for the summary. Longer sequences are truncated. We apply the same data-processing and model initialization for these two datasets as we did for the long sequence datasets. We train on CNN/DM for 20000 steps with a batch-size of 64 and a peak learning rate of 3 × 10 −5 with linear decay afterwards. During generation, we use beam size 1, no length penalty and minimum and maximum generation lengths of 40 and 120, respectively. We use the same training and generation parameters for both the hierarchical seq2seq model and the plain seq2seq model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversational datasets</head><p>Since conversational summarization datasets are rare, we only consider the SAMSum (Gliwa et al., 2019) corpus. It consists of staged chat conversations between two people and corresponding abstractive summaries written by linguists. Statistics for SAMSum datset are presented in <ref type="table">Table 1</ref>.</p><p>During data-processing, we concatenate the role string with its utterance and add a BOS token at the beginning of each speaker turn. Then we concatenate all the turns together and use this as the source sequence. We add segmentation embeddings to our model, where we map the same role along with its utterance to the same segment embedding. The input to the encoder layers is the sum of the  token embeddings, position embeddings and segment embeddings. We randomly initialize segment embedding parameters, and initialize the remaining parameters with the hierarchical seq2seq model trained on CNN-DM. For comparison, we also use train a plain seq2seq model initialized with weights from a plain seq2seq trained on CNN-DM.</p><p>Meeting datasets The AMI <ref type="bibr">(Carletta et al., 2006)</ref> and ISCI (Janin et al., 2003) corpora consist of staged meetings which are transcribed and annotated with abstractive summaries. The meeting transcripts are extremely long and the turn-based structure of the meetings makes these datasets particularly suitable for the hierarchical architecture, since speaker turns can be marked by hierarchical tokens. Statistics for AMI and ISCI datasets are illustrated in <ref type="table">Table 1</ref>. We followed <ref type="bibr">(Shang et al., 2018)</ref> for splitting the data.</p><p>Since the meeting transcripts are transcribed from a speech to text model, we first built a small lexicon that filters out meaningless words. Then we add BOS tokens to each turn and concatenate all the turns together, which we use as input to the model. Following the conversational dataset approach, we add segment embeddings for each role. We follow the same weight initialization procedure as with the conversational datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Document level Machine Translation.</head><p>Historically, sentence level translation approaches have outperformed document level approaches for translating entire documents. This indicates that document level approaches are not able to incorporate the larger context of the document during translation. We test our hierarchical model on a translation task and see significant improvements over the non-hierarchical counterpart.</p><p>Dataset We evaluate our architecture on the WMT-19 English to German document translation task. For the Europarl corpus, we use a newer version from WMT-20, since it contains metadata useful for finding document boundaries. Dataset stats for EN-DE document pairs are shown in Table 2. We only process 512 tokens at once due to memory constraints. Thus we split documents into chunks of at most 512 tokens. We only split at sentence boundaries to maintain alignment between the source and target languages. We translate each chunk separately and afterwards concatenate the translated chunks to compute the BLEU score for the entire document. We use moses for preprocessing the data and the fastBPE implementation of Sennrich et al. <ref type="formula">(2016)</ref> for byte-pair encoding with 40000 bpe codes. We build a joined dictionary between English and German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model and Optimization</head><p>We use the Transformer architecture <ref type="bibr">(Vaswani et al., 2017)</ref> with 6 encoder/decoder layers, a hidden size of 1024, 16 attention heads and 4096 for the dimension of the feedforward networks. For training, we use an effective batch size of approximately 400 (32 gradient accumulation steps on 4 V100 GPUs with at most 1024 tokens per batch). We use the Adam op-timizer with (β 1 , β 2 ) = (0.9, 0.98) and ε = 10 −6 . We minimize the label smoothed cross entropy loss with smoothing constant 0.1. We train with mixed precision for a total of 50000 steps and warm up the learning rate for 1250 steps to 10 −4 and decay it linearly afterwards. We use one hierarchical layer. During generation, we use a beam width of 4, no length penalty and we generate until we encounter an EOS token. Note that the above parameters were not extensively tuned and we do not use a monolingual pre-train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>As shown in <ref type="table" target="#tab_7">Table 3</ref>, we achieve state-of-the-art results on the PubMed and arXiv datasets. As shown in <ref type="table" target="#tab_8">Table 4</ref>, our hierarchical seq2seq architecture outperforms its plain seq2seq peer initialized with the same pretrained weights on news datasets. We also achieve state-of-the-art results on the CNN/DM dataset. We outperform the previous baseline by 7 Rouge scores on SAMSum as shown in <ref type="table" target="#tab_9">Table 5</ref>. We also achieve state-of-theart on ROUGE R2 on the AMI dataset, shown in <ref type="table" target="#tab_10">Table 6</ref>. Our hierarchical seq2seq architecture outperforms the plain seq2seq baseline by 28 BLEU score on EN-DE document level machine translation as shown in table 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>The addition of hierarchical learning improves rouge scores over prior state-of-the-art methods for several summarization datasets. Here we compare the generated output of our hierarchical model with the non-hierarchical counterpart for three instances from the arXiv test data. We also include the introduction of each article. These can be found in Appendix A. Since there is often overlap between the abstract and the introduction, the models have learned to extract sentences from the introduction. We highlight the sentences extracted from the introduction by the hierarchical model in blue and the sentences extracted by the standard model in boldface. We observe that the hierarchical model extracts sentences throughout multiple paragraphs of the introduction, while the plain model generally extracts only from the first paragraph and sometimes does not extract entire sentences. In addition, we include our document level machine translation results in Appendix B. The quality of generated German text matches sentence level translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ablation</head><p>Encoder-only transformer hierarchical attention. We evaluate our hierarchical attention model on several classification tasks. Instead of using our seq2seq architecture, we design a similar encoder-only architecture with hierarchical attention. This also allows us to easily pre-train the hierarchical layers.</p><p>Our architecture is based on the encoder of <ref type="bibr">(Vaswani et al., 2017)</ref>. We add a hierarchical attention module after the self attention module in each of the encoder layers. Similarly to how we preprocessed the summarization and translation datasets, we insert BOS tokens at the beginning of all sentences.</p><p>We follow RoBERTa (Liu et al., 2019) for pretraining our model by using the same dataset, preprocessing steps and pre-training objective. We evaluate the pre-trained model on three downstream tasks: SQuAD 2.0 <ref type="bibr">(Rajpurkar et al., 2018)</ref>, <ref type="bibr">MNLIm (Williams et al., 2018)</ref> and <ref type="bibr">RACE (Lai et al., 2017)</ref>. We observe that the pre-training converges faster to a better optimum with lower complexity than RoBERTa with the same hyperparameters. However, downstream task performance does not improve.  The results are given in <ref type="table" target="#tab_5">Table 8</ref>. We observe that for SQuAD 2.0 and MNLI-m our hierarchical model does not perform better than the nonhierarchical model. However, the performance for RACE is significantly better, which suggests that there are some benefits to using hierarchical attention for classification tasks with long source sequences. Note that when fine-tuning on RACE, we had to disable dropout for the first epoch and then set it to 0.1, otherwise the model did not converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What has the hierarchical attention learned?</head><p>In order to better understand what the hierarchical model has learned, we plot a heatmap of the hierarchical attention between the decoder and the encoder. We use the best performing hierarchical model for this experiment <ref type="table" target="#tab_7">(Table 3)</ref>. We gener-      ate summaries for each of the sample articles and record the hierarchical attention between the decoder and the BOS embeddings from the encoder at each step of generating the summary. For each of the 12 decoder layers and each of the 16 attention heads we get a distribution over the BOS tokens for each of the generated summary tokens. To visualize the attention more concisely, we aggregate across the attentions heads by choosing only the 16 BOS tokens with the highest weight for each generated token. We normalize the distribution such that the weights sum to 1. The hierarchical attention heatmaps for each layer are shown in <ref type="figure">Figures 2, 3</ref> and 4. We see that across different layers the model attends to different BOS tokens across the entire document. For each layer there are several horizontal lines, which indicates that some BOS tokens are assigned large weights at each step during generation. However, we also observe many individual points and discontinuous horizontal lines on the heatmaps, indicating that the model selectively attends to different BOS tokens across the entire document. We note that in the first few layers, the model seems to attend to the layers more uniformly while in the last few layers the model attends more heavily to certain BOS tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>We designed a transformer based architecture with hierarchical attention and obtained improvements on several sequence to sequence generation tasks, especially summarization datasets with long source documents. We showed significant improvements on document-level machine translation on the WMT-19 EN-DE translation task, as compared to our baseline. We did not see significant gains when applying hierarchical attention to encoderonly classification tasks.</p><p>Future work might include further investigating the benefits of hierarchical attention for documentlevel machine translation, since we only experimented with a single dataset and language pair; we did not tune the hyperparameters extensively. We believe that hierarchical attention document-level translation could outperform sentence-level translation. In addition, we initialize the hierarchical components randomly since pre-training similar to BART where permuted sentences might be difficult. However, as the hierarchical attention operates on the rich pre-trained BOS embeddings, we believe that would not be a major issue, and pre-training on the hierarchical components could further increase the performance on sequence to sequence tasks.</p><p>Finally, although we did not see significant gains when using a hierarchical encoder-only model for classification, we believe that some modifications on hierarchical attention might improve over current non-hierarchical models. Particularly, the impact of dropout on hierarchical layers should be investigated more closely. A Appendix arXiv test introduction -1 deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision @xcite , to speech recognition @xcite , natural language processing @xcite , and domain adaptation @xcite . as the sizes of data mount up , people usually have to increase the number of parameters in dnns so as to absorb the vast volume of supervision . high performance computing techniques are investigated to speed up dnn training , concerning optimization algorithms , parallel synchronisations on clusters w / o gpus , and stochastic binarization / ternarization , etc @xcite . on the other hand the memory and energy consumption is usually , if not always , constrained in industrial applications @xcite . for instance , for commercial search engines ( e.g. , google and baidu ) and recommendation systems ( e.g. , netflix and youtube ) , the ratio between the increased model size and the improved performance should be considered given limited online resources . compressing the model size becomes more important for applications on mobile and embedded devices @xcite . having dnns running on mobile apps owns many great features such as better privacy , less network bandwidth and real time processing . however , the energy consumption of battery -constrained mobile devices is usually dominated by memory access , which would be greatly saved if a dnn model can fit in on -chip storage rather than dram storage ( c.f . @xcite for details ) . a recent trend of studies are thus motivated to focus on compressing the size of dnns while mostly keeping their predictive performance @xcite . with different intuitions , there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings . the first type tries to revise the training target into more informative supervision using dark knowledge . in specific , hinton @xcite suggested to train a large network ahead , and distill a much smaller model on a combination of the original labels and the soft -output by the large net . the second type observes the redundancy existence in network weights @xcite , and exploits techniques to constrain or reduce the number of free -parameters in dnns during learning . this paper focuses on the latter type . to constrain the network redundancy , efforts @xcite formulated an original weight matrix into either lowrank or fast -food decompositions . moreover @xcite proposed a simple -yet -effective pruning -retraining iteration during training , followed by quantization and fine -tuning . @xcite proposed hashednets to efficiently implement parameter sharing prior to learning , and showed notable compression with much less loss of accuracy than low -rank decomposition . more precisely , prior to training , a hash function is used to randomly group ( virtual ) weights into a small number of buckets , so that all weights mapped into one hash bucket directly share a same value . hashednets was further deliberated in frequency domain for compressing convolutional neural networks in @xcite . in applications , we observe hashednets compresses model sizes greatly at marginal loss of accuracy for some situations , whereas also significantly loses accuracy for others . after revisiting its mechanism , we conjecture this instability comes from at least three factors . first , hashing and training are disjoint in a two -phase manner , i.e. , once inappropriate collisions exist , there may be no much optimization room left for training . second , one single hash function is used to fetch a single value in the compression space , whose collision risk is larger than multiple hashes @xcite . third , parameter sharing within a buckets implicitly uses identity mapping from the hashed value to the virtual entry . this paper proposes an approach to relieve this instability , still in a two -phase style for preserving efficiency . specifically , we use multiple hash functions @xcite to map per virtual entry into multiple values in compression space . then an additional network plays in a mapping function role from these hashed values to the virtual entry before hashing , which can be also regarded as " reconstructing " the virtual entry from its multiple hashed values . plugged into and jointly trained within the original network , the reconstruction network is of a comparably ignorable size , i.e. , at low memory cost . this functional hashing structure includes hashednets as a degenerated special case , and facilitates less value collisions and better value reconstruction . shortly denoted as funhashnn , our approach could be further extended with dual space hashing and multi -hops . since it imposes no restriction on other network design choices ( e.g. dropout and weight sparsification ) , funhashnn can be considered as a standard tool for dnn compression . experiments on several datasets demonstrate promisingly larger reduction of model sizes and/or less loss on prediction accuracy , compared with hashednets . <ref type="table">Table 9</ref>: Introduction of sample article from arXiv test data (1). arXiv prediction and target -1 HAT deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision , to speech recognition , natural language processing , and domain adaptation . as the sizes of data mount up , people usually have to increase the number of parameters in dnn so as to absorb the vast volume of supervision . however , the ratio between the increased model size and the improved performance should be considered given limited online resources . compressing the model size becomes more important for applications on mobile and embedded devices . with different intuitions , there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings . the first type tries to revise the training target into more informative supervision using dark knowledge . in specific , hinton suggested to train a large network ahead , and distill a much smaller model on a combination of the original labels and the soft -output by the large net . the second type observes the redundancy existence in network weights , and exploits techniques to constrain or reduce the free -parameters during learning . hashednets is one of the two types , and has been studied as a dimension reduction method for reducing model storage size without maintaining the mapping matrices like random projection . in applications , we observe hashednet compresses model sizes greatly at marginal loss of accuracy for some situations , whereas also significantly loses accuracy for others . after revisiting its mechanism , we conjecture this instability comes from at least three factors . first , hashing and training are disjoint in a two -phase manner , i.e. , once inappropriate collisions exist , there may be no much optimization room left for training . second , one single hash function is used to fetch a single value in the compression space , whose collision risk is larger than multiple hashes . third , parameter sharing within a buckets implicitly uses identity mapping from the hashed value to the virtual entry . this paper proposes an approach to relieve this instability . specifically , we use multiple hash functions to map per virtual entry into multiple values in compression space . then an additional network plays in a mapping function role from these hashed values to thevirtual entry before hashing , which can be also regarded as " reconstructing " the virtual entries from its multiplehashed values . plugged into and jointly trained within the original network , the reconstruction network is of a comparably ignorable size , and requires little additional memory . this functional hashing structure includes hashed nets as a degenerated special case , and facilitates less value collisions and better value reconstruction . shortly denoted as funhashnn , our approach could be further extended with dual space hashing and multi -hops . experiments on several datasets demonstrate promisingly larger reduction of model sizes and/or less loss on prediction accuracy , compared with hashed net .</p><p>Transformer deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision , to speech recognition , natural language processing , and domain adaptation . as the sizes of data mount up , people usually have to increase the number of parameters in dnns so as to absorb the vast volume of supervision . high performance computing techniques are investigated to speed up dnn training , concerning optimization algorithms , parallel synchronisations on clusters w / o gpus , and stochastic binarization / ternarization . on the other hand , the memory and energy consumption is usually , if not always , constrained in industrial applications . for instance , for commercial search engines ( e.g. , google and baidu ) and recommendation systems ( e.g. , netflix and youtube ) , the ratio between the increased model size and the improved performance should be considered given limited online resources . compressing the model size becomes more important for applications on mobile and embedded devices . with different intuitions Target as the complexity of deep neural networks ( dnns ) trend to grow to absorb the increasing sizes of data , memory and energy consumption has been receiving more and more attentions for industrial applications , especially on mobile devices . this paper presents a novel structure based on functional hashing to compress dnns , namely funhashnn . for each entry in a deep net , funhashnn uses multiple low -cost hash functions to fetch values in the compression space , and then employs a small reconstruction network to recover that entry . the reconstruction network is plugged into the whole network and trained jointly . funhashnn includes the recently proposed hashednets @xcite as a degenerated case , and benefits from larger value capacity and less reconstruction loss . we further discuss extensions with dual space hashing and multi -hops . on several benchmark datasets , funhashnn demonstrates high compression ratios with little loss on prediction accuracy . convolutional neural networks typically consist of an input layer , a number of hidden layers , followed by a softmax classification layer . the input layer , and each of the hidden layers , is represented by a three -dimensional array with size , say , @xmath0 . the second and third dimensions are spatial . the first dimension is simply a list of features available in each spatial location . for example , with rgb color images @xmath1 is the image size and @xmath2 is the number of color channels . the input array is processed using a mixture of convolution and pooling operations . as you move forward through the network , @xmath3 decreases while @xmath4 is increased to compensate . when the input array is spatially sparse , it makes sense to take advantage of the sparsity to speed up the computation . more importantly , knowing you can efficiently process sparse images gives you greater freedom when it comes to preparing the input data . consider the problem of online isolated character recognition ; online means that the character is captured as a path using a touchscreen or electronic stylus , rather than being stored as a picture . recognition of isolated characters can be used as a building block for reading cursive handwriting , and is a challenging problem in its own right for languages with large character sets . each handwritten character is represented as a sequence of strokes ; each stroke is stored as a list of @xmath5-and @xmath6-coordinates . we can draw the characters as @xmath1 binary images : zero for background , one for the pen color . the number of pixels is @xmath7 , while the typical number of non -zero pixels is only @xmath8 , so the first hidden layer can be calculated much more quickly by taking advantage of sparsity . another advantage of sparsity is related to the issue of spatial padding for convolutional networks . convolutional networks conventionally apply their convolutional filters in valid mode they are only applied where they fit completely inside the input layer . this is generally suboptimal as makes it much harder to detect interesting features on the boundary of the input image . there are a number of ways of dealing with this . padding the input image @xcite with zero pixels . this has a second advantage : training data augmentation can be carried out in the form of adding translations , rotations , or elastic distortions to the input images . adding small amounts of padding to each of the convolutional layers of the network ; depending on the amount of padding added this may be equivalent to applying the convolutions in full mode . this has a similar effect to adding lots of padding to the input image , but it allows less flexibility when it comes to augmenting the training data . applying the convolutional network to a number of overlapping subsets of the image @xcite ; this is useful if the input images are not square . this can be done relatively computationally efficiently as there is redundancy in the calculation of the lower level convolutional filters . however , the ( often large ) fully connected classification layers of the network must be evaluated several times . sparsity has the potential to combine the best features of the above . the whole object can be evaluated in one go , with a substantial amount of padding added at no extra cost .  arXiv prediction and target -2 HAT spatial sparsity is an important feature of convolutional neural networks ( cnns ) . when the input array is sparse , it makes sense to take advantage of the sparsity to speed up the computation . more importantly , knowing you can efficiently process sparse images gives you greater freedom when it comes to preparing the input data . we describe a family of cnn architectures with many layers of max -pooling , and show how sparsity can be used to improve the performance of online character recognition and image recognition .</p><p>Transformer convolutional neural networks ( cnns ) typically consist of an input layer , a number of hidden layers , followed by a softmax classification layer . the input layer is represented by a three -dimensional array with size , say , @xmath0 . the second and third dimensions are spatial . the first dimension is simply a list of features available in each spatial location . for example , with rgb color images @xmath1 is the image size and @xmath2 is the number of color channels . the input array is processed using a mixture of convolution and pooling operations . as you move forward through the network , @xmath3 decreases while @xmath4 is increased to compensate . when the input array is spatially sparse , it makes sense to take advantage of the sparsity to speed up the computation . more importantly , knowing you can efficiently process sparse images , at the top of the network is an output layer . if the input layer has spatial size @xmath1 with Target convolutional neural networks ( cnns ) perform well on problems such as handwriting recognition and image classification . however , the performance of the networks is often limited by budget and time constraints , particularly when trying to train deep networks . motivated by the problem of online handwriting recognition , we developed a cnn for processing spatially -sparse inputs ; a character drawn with a one -pixel wide pen on a high resolution grid looks like a sparse matrix . taking advantage of the sparsity allowed us more efficiently to train and test large , deep cnns . on the casia -olhwdb1.1 dataset containing 3755 character classes we get a test error of 3.82% . although pictures are not sparse , they can be thought of as sparse by adding padding . applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the cifar small picture datasets : 6.28% on cifar-10 and 24.30% for cifar-100 . * keywords : * online character recognition , convolutional neural network , sparsity , computer vision question answering ( qa ) aims to automatically understand natural language questions and to respond with actual answers . the state -of -the -art qa systems usually work relatively well for factoid , list and definition questions , but they might not necessarily work well for real world questions , where more comprehensive answers are required . frequently asked questions ( faq ) based qa is an economical and practical solution for general qa @xcite . instead of answering questions from scratch , faq -based qa tries to search the faq archives and check if a similar question was previously asked . if a similar question is found , the corresponding answer is returned to the user . the faq archives are usually created by experts , so the returned answers are usually of higher -quality . the core of faq -based qa is to calculate semantic similarities between questions . this is a very challenging task , because two questions , which share the same meaning , may be quite different at the word or syntactic level . for example , how do i add a vehicle to this policy ? " and what should i do to extend this policy for my new car ? " have few words in common , but they share the same answer . in the past two decades , many efforts have been made to tackle this lexical gap problem . one type of methods tried to bridge the lexical gap by utilizing semantic lexicons , like wordnet @xcite . another method treated this task as a statistical machine translation problem , and employed a parallel question set to learn word -to -word or phrase -to -phrase translation probabilities @xcite . both of these methods have drawbacks . the first method is hard to adapt to many other languages , because the semantic lexicon is unavailable . for the second method , a large parallel question set is required to learn the translation probabilities , which is usually hard or expensive to acquire . to overcome these drawbacks , we utilize distributed word representations to calculate the similarity between words , which can be easily trained by only using amount of monolingual data . in this paper , we propose a novel word -alignment -based method to solve the faq -based qa tasks . the characteristics of our method include : ( 1 ) a neural network model for calculating question similarity with word alignment features . for an input question and a candidate question , the similarities of each word pairs ( between the two questions ) are calculated first , and then the best word alignment for the two questions is computed . we extract a vector of dense features from the word alignment , then import the feature vector into a neural network and calculate the question similarity in the network s output layer .</p><p>( 2 ) a bootstrap -based feature extraction method . the faq archives usually contain less than a few hundred questions , and in order to avoid overfitting , we are unable to use too many sparse features . therefore , we come up with this method to extract a small set of effective sparse features according to our system s ranking results .</p><p>( 3 ) a learning -to -rank algorithm for training . the faq -based qa task is essentially a ranking task , our model not only needs to calculate a proper similarity for each question pair , but also needs to rank the most relevant one on top of the other candidates . so we propose a learning -to -rank method to train parameters more suitable for ranking . experimental results , conducted on faq archives from three languages , demonstrate that our method is very effective . we also evaluate our method on the answer sentence selection task . experimental results on the standard trec data set show that our method outperforms all previous state -of -the -art systems . arXiv prediction and target -3 HAT in this paper , we propose a novel word -alignment -based method to solve the frequently asked questions ( faq ) -based question answering ( qa ) tasks . the characteristics of our method include : ( 1 ) a neural network model for calculating question similarity with word alignment features . for an input question and a candidate question , the similarities of each word pairs ( between the two questions ) are calculated first , and then the best word alignment for the two question is computed . we extract a vector of dense features from the word alignment , then import the feature vector into a neural networks and calculate the question similarity in the network s output layer .</p><p>( 2 ) a bootstrap -based feature extraction method . the faq archives usually contain less than a few hundred questions , and in order to avoid overfitting , we are unable to use too many sparse features . therefore , we come up with this method to extract a small set of effective sparse features according to our system s ranking results .</p><p>( 3 ) a learning -to -rank algorithm for training . since faq -based qa task is essentially a ranking task , our model not only needs to calculate a proper similarity for each question pair , but also needs to rank the most relevant candidate on top of the other candidates . so we propose an explicit learning -to -rank method to train parameters more suitable for ranking . experimental results , conducted on faq archive from three languages , demonstrate that our method is very effective . we also evaluate our method on the answer sentence selection task , and experimental results on the standard trec data set show that our system outperforms all previous stateof -the -art systems .</p><p>Transformer faq -based question answering ( qa ) aims to automatically understand natural language questions and to respond with actual answers . the state -of -the -art systems usually work relatively well for factoid , list and definition questions , but they might not necessarily work well for real world questions , where more comprehensive answers are required . frequently asked questions ( faq ) based qa is an economical and practical solution for general qa . instead of answering questions from scratch , faq -based qa tries to search the faq archives and check if a similar question was previously asked . if a similar question is found , the corresponding answer is returned to the user . the core of faq -based qa is to calculate semantic similarities between questions . this is a very challenging task , because two questions , which share the same meaning , may be quite different at the word or syntactic frequency level . in this paper , we propose a novel word -alignment -based method to solve the faq -based Target in this paper , we propose a novel word -alignment -based method to solve the faq -based question answering task . first , we employ a neural network model to calculate question similarity , where the word alignment between two questions is used for extracting features . second , we design a bootstrap -based feature extraction method to extract a small set of effective lexical features . third , we propose a learning -to -rank algorithm to train parameters more suitable for the ranking tasks . experimental results , conducted on three languages ( english , spanish and japanese ) , demonstrate that the question similarity model is more effective than baseline systems , the sparse features bring 5% improvements on top-1 accuracy , and the learning -to -rank algorithm works significantly better than the traditional method . we further evaluate our method on the answer sentence selection task . our method outperforms all the previous systems on the standard trec data set . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of HAT: Hierarchical Attention Transformer. The blocks shaded in pink are the additional hierarchical layers added to the existing Transformer (Vaswani et al., 2017) architecture. The extra BOS tokens used for hierarchical learning are highlighted in purple. The figure is based on Figure 1 in Vaswani et al. (2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2104.07545v1 [cs.CL] 15 Apr 2021</figDesc><table><row><cell cols="2">1X</cell><cell cols="3">Hierarchical</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Learning</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[bos 1] +</cell><cell cols="2">sentence 1</cell><cell>+</cell><cell>[bos 2] +</cell><cell>sentence 2</cell><cell>+…</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Statistics for WMT-19 EN-DE document translation task. Both average number of words and number of BPE tokens are presented.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Hierarchical learning for encoder only.</figDesc><table /><note>1 The results are taken from Table 1 (DOC-SENTENCES) in Liu et al. (2019)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>19.56 27.42 44.67 17.18 25.73 BigBird 46.32 20.65 42.33 46.63 19.02 41.77 LSH 48.12 21.06 42.72 ---Transformer-BART 45.54 19.1 34.65 43.92 16.36 39.16 HAT-BART 48.25 21.35 36.69 46.74 19.19 42.20</figDesc><table><row><cell></cell><cell></cell><cell>PubMed</cell><cell></cell><cell></cell><cell>arXiv</cell></row><row><cell></cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell></row><row><cell>PEGASUS</cell><cell>45.09</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Results on summarization tasks with long source sequences. PEGASUS (Zhang et al., 2019) results are from BigBird (Zaheer et al., 2020) paper. BigBird uses source sequence length of 4096, LSH (Huang et al., 2021) uses 7168, while Transformer-BART and HAT-BART use 3072 due to memory constraints. Transformer-BART and HAT-BART were trained using the same parameter settings. PEGASUS 44.17 21.47 41.11 47.21 24.56 39.25 BigBird 43.84 21.11 40.74 47.12 24.05 38.80 BART 44.16 21.28 40.9 45.14 22.27 37.25 Transformer-BART 44.45 21.27 41.51 45.26 22.19 37.04 HAT-BART 44.48 21.31 41.52 45.92 22.79 37.84</figDesc><table><row><cell cols="3">CNN/DailyMail</cell><cell></cell><cell>XSum</cell><cell></cell></row><row><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell></row><row><cell>seq2seq-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Results on standard news summarization tasks. PEGASUS results are from Zaheer et al. (2020). Big-Bird (Zaheer et al., 2020), Transformer-BART and HAT-BART use use a source sequence length of 1024. BART and HAT-BART were trained on the same parameters settings.</figDesc><table><row><cell></cell><cell></cell><cell>SAMSum</cell></row><row><cell></cell><cell>R1</cell><cell>R2</cell><cell>RL</cell></row><row><cell cols="4">DynamicConv + GPT2 45.41 20.65 41.45</cell></row><row><cell cols="4">Transformer-CNNDM 53.00 28.03 48.59</cell></row><row><cell>HAT-CNNDM</cell><cell cols="3">53.01 28.27 48.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results on conversational summarization tasks. The plain Transformer model (Transformer-CNNDM) was initialized by the BART CNN/DM model. The hierarchical model (HAT-CNNDM) was initialized by the hierarchical seq2seq model trained on CNN/DM. CNNDM 52.06 19.27 50.02 43.77 11.65 41.64 HAT-CNNDM 52.27 20.15 50.57 43.98 10.83 41.36</figDesc><table><row><cell></cell><cell></cell><cell>AMI</cell><cell></cell><cell></cell><cell>ISCI</cell></row><row><cell></cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell></row><row><cell>HMNet</cell><cell cols="2">53.02 18.57</cell><cell>-</cell><cell cols="2">46.28 10.60</cell><cell>-</cell></row><row><cell>Transformer-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Results on meeting summarization tasks. The plain Transformer model (Transformer-CNNDM) was initialized by the BART CNN/DM model. The hierarchical model (HAT-CNNDM) was initialized by the hierarchical Transformer model trained on CNN/DM.</figDesc><table><row><cell></cell><cell>WMT-19 EN-DE</cell></row><row><cell></cell><cell>d-BLEU</cell></row><row><cell>Transformer (no-pretrain)</cell><cell>7.7</cell></row><row><cell>HAT (no-pretrain)</cell><cell>35.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Results on WMT-19 EN-DE document translation task. The plain Transformer result was obtained from MBART (Liu et al., 2020). Both models are initialized randomly.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807, Brussels, Belgium. Association for Computational Linguistics. Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. 2019. Bp-transformer: Modelling long-range context via binary partitioning. arXiv preprint arXiv:1911.04070.</figDesc><table><row><cell>Zihao Ye, Manzil Zaheer, Guru Guruganesh, Kumar Avinava</cell><cell>on Neural Information Processing Systems -Vol-ume 2, NIPS'14, page 3104-3112, Cambridge, MA, USA. MIT Press. Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017. Abstractive document summarization with a graph-</cell></row><row><cell>Dubey, Joshua Ainslie, Chris Alberti, Santiago On-</cell><cell>based attentional neural model. In Proceedings</cell></row><row><cell>tanon, Philip Pham, Anirudh Ravula, Qifan Wang,</cell><cell>of the 55th Annual Meeting of the Association for</cell></row><row><cell>Romain Paulus, Caiming Xiong, and Richard Socher. Li Yang, et al. 2020. Big bird: Transformers for</cell><cell>Computational Linguistics (Volume 1: Long Papers),</cell></row><row><cell>2018. A deep reinforced model for abstractive sum-longer sequences. Advances in Neural Information</cell><cell>pages 1171-1181.</cell></row><row><cell>marization. In International Conference on Learn-Processing Systems, 33.</cell><cell></cell></row><row><cell>ing Representations. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-ter J. Liu. 2019. Pegasus: Pre-training with ex-tracted gap-sentences for abstractive summarization. models are unsupervised multitask learners. OpenAI Ningyu Zhang, Shumin Deng, Juan Li, Xi Chen, blog, 1(8):9. Wei Zhang, and Huajun Chen. 2020. Summariz-ing chinese medical answer with graph convolution networks and question-focused dual attention. In EMNLP (Findings), pages 15-24.</cell><cell>Mayesha Tasnim, Diego Collarana, Damien Graux, Fabrizio Orlandi, and Maria-Esther Vidal. 2019. Summarizing entity temporal evolution in knowl-edge graphs. In WWW (Companion Volume), pages 961-965. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International</cell></row><row><cell>Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan, Ricardo Henao, and Lawrence Carin. 2017. De-</cell><cell>Conference on Neural Information Processing Sys-tems, pages 6000-6010.</cell></row><row><cell>convolutional paragraph representation learning. In NIPS, pages 4172-4182.</cell><cell>Adina Williams, Nikita Nangia, and Samuel R. Bow-man. 2018. A broad-coverage challenge corpus</cell></row><row><cell>Yue Zhao, Xiaolong Jin, Yuanzhuo Wang, and Xueqi</cell><cell>for sentence understanding through inference. In</cell></row><row><cell>Cheng. 2018. Document embedding enhanced event</cell><cell>NAACL-HLT, pages 1112-1122.</cell></row><row><cell>detection with hierarchical and supervised attention.</cell><cell></cell></row><row><cell>In ACL (2), pages 414-419.</cell><cell>Shuzhe Wu, Meina Kan, Shiguang Shan, and Xilin</cell></row><row><cell></cell><cell>Chen. 2019. Hierarchical attention for part-aware</cell></row><row><cell>Zhou Zhao, Haojie Pan, Changjie Fan, Yan Liu, Lin-</cell><cell>face detection. International Journal of Computer</cell></row><row><cell>lin Li, and Min Yang. 2019. Abstractive meeting</cell><cell>Vision, 127(6-7):560-578.</cell></row><row><cell>summarization via hierarchical adaptive segmental</cell><cell></cell></row><row><cell>network learning. In WWW, pages 3455-3461.</cell><cell>Chen Xing, Yu Wu, Wei Wu, Yalou Huang, and Ming</cell></row><row><cell></cell><cell>Zhou. 2018. Hierarchical recurrent attention net-</cell></row><row><cell>Chenguang Zhu, Ruochen Xu, Michael Zeng, and Xue-</cell><cell>work for response generation. In Proceedings of</cell></row><row><cell>dong Huang. 2020. A hierarchical network for ab-</cell><cell>the AAAI Conference on Artificial Intelligence, vol-</cell></row><row><cell>stractive meeting summarization with cross-domain pretraining. In Proceedings of the 2020 Conference</cell><cell>ume 32.</cell></row><row><cell>on Empirical Methods in Natural Language Process-</cell><cell>Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu.</cell></row><row><cell>ing: Findings, pages 194-203.</cell><cell>2020a. Discourse-aware neural extractive text sum-</cell></row><row><cell>Asli Ç elikyilmaz, Antoine Bosselut, Xiaodong He, and</cell><cell>marization. In ACL, pages 5021-5031.</cell></row><row><cell>Guokan Shang, Wensi Ding, Zekun Zhang, An-toine Tixier, Polykarpos Meladianos, Michalis Vazir-sentence compression and budgeted submodular vised abstractive meeting summarization with multi-giannis, and Jean-Pierre Lorré. 2018. Unsuper-Yejin Choi. 2018. Deep communicating agents for abstractive summarization. In NAACL-HLT, pages 1662-1675.</cell><cell>Information Processing Systems, 33. tention for text-based games. Advances in Neural Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. 2020b. Deep reinforcement learning with stacked hierarchical at-</cell></row><row><cell>maximization. In Proceedings of the 56th An-nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 664-674, Melbourne, Australia. Association for Compu-tational Linguistics.</cell><cell>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-bonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural In-formation Processing Systems, volume 32. Curran</cell></row><row><cell>Jun Song, Jun Xiao, Fei Wu, Haishan Wu, Tong Zhang,</cell><cell>Associates, Inc.</cell></row><row><cell>Zhongfei Mark Zhang, and Wenwu Zhu. 2017. Hi-</cell><cell></cell></row><row><cell>erarchical contextual attention recurrent neural net-</cell><cell>Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,</cell></row><row><cell>work for map query suggestion. IEEE Trans. Knowl.</cell><cell>Alex Smola, and Eduard Hovy. 2016. Hierarchi-</cell></row><row><cell>Data Eng., 29(9):1888-1901.</cell><cell>cal attention networks for document classification.</cell></row><row><cell></cell><cell>In Proceedings of the 2016 conference of the North</cell></row><row><cell>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.</cell><cell>American chapter of the association for computa-</cell></row><row><cell>Sequence to sequence learning with neural networks.</cell><cell>tional linguistics: human language technologies,</cell></row><row><cell>In Proceedings of the 27th International Conference</cell><cell>pages 1480-1489.</cell></row></table><note>Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1-67. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable ques- tions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 2: Short Papers), pages 784-789. Heechang Ryu, Hayong Shin, and Jinkyoo Park. 2020. Multi-agent actor-critic with hierarchical graph at- tention network. In Proceedings of the AAAI Con- ference on Artificial Intelligence, volume 34, pages 7236-7243. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715- 1725, Berlin, Germany. Association for Computa- tional Linguistics.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: ROUGE(HAT): 36.91/13.47/34.90; ROUGE(Transformer): 37.11/10.38/34.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>in section [ deepcnet]-[deepcnin ] we describe a family of convolutional networks with many layers of max -pooling . in section [ sparsity][nn ] we describe how sparsity applies to character recognition and image recognition . in section [ results ] we give our results . in section [ sec : conclusion ] we discuss other possible uses of sparse cnns .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Introduction of sample article from arXiv test data (2).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>: ROUGE(HAT): 37.25/12.25/34.01; ROUGE(Transformer): 33.54/7.01/32.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>Introduction of sample article from arXiv test data (3).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14 :</head><label>14</label><figDesc>ROUGE(HAT): 58.71/37.41/55.37; ROUGE(Transformer): 38.16/13.91/35.53</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Kevin Terrell for reading the first version of our draft. We acknowledge the support from the Google Cloud team and the PyTorch/XLA team, particularly Taylan Bilal.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interpretable online banking fraud detection based on hierarchical attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarit</forename><surname>Idan Achituve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/MLSP.2019.8918896</idno>
	</analytic>
	<monogr>
		<title level="m">MLSP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150.BAppendixWMT19EN-DEsource-1</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lindsey Graham of South Carolina, and Mike Lee and Orrin Hatch, both of Utah. The information appeared online Thursday, posted by an unknown person on Capitol Hill during a Senate panel&apos;s hearing on the sexual misconduct allegations against Supreme Court nominee Brett Kavanaugh. The leak came sometime after the three senators had questioned Kavanaugh. Conservative sites such as Gateway Pundit and RedState reported that the IP address that identifies the source of the posts was associated with Waters&quot; office and released the information of a member of Waters&apos; staff, the Hill reported</title>
	</analytic>
	<monogr>
		<title level="m">Maxine Waters denies staffer leaked GOP senators&apos; data, blasts &apos;dangerous lies&apos; and &apos;conspiracy theories&apos; U.S. Rep. Maxine Waters on Saturday denounced allegations that a member of her staff had posted the personal information of three Republican U.S. senators onto the lawmakers&apos; Wikipedia pages. The Los Angeles Democrat asserted that the claims were being pedaled by &quot;ultra-right wing&quot; pundits and websites</title>
		<meeting><address><addrLine>U.S. Sens</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Big if true,&quot; he tweeted. In her statement, Waters said her office had alerted &quot;the appropriate authorities and law enforcement entities of these fraudulent claims. We will ensure that the perpetrators will be revealed,&quot; she continued, &quot;and that they will be held legally liable for all of their actions that are destructible and dangerous to any and all members of my staff.&quot; Table 15: English source text of sample document from WMT19 EN-DE test data</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Das Mitglied meines Personals -dessen Identität, persönliche Informationen und Sicherheit als Folge dieser betrügerischen und falschen Behauptungen beeinträchtigt wurden -war in keiner Weise verantwortlich für das Blatt dieser Informationen. Diese unbegründete Behauptung ist völlig falsch und eine absolute Lüge</title>
	</analytic>
	<monogr>
		<title level="m">HAT Maxine Waters bestreitet, dass ein Mitglied ihres Personals die persönlichen Informationen von drei republikanischen Senatoren auf die Wikipedia-Seiten der Gesetzgeber gepostet habe. Die Demokraten in Los Angeles behaupteten, dass die Behauptungen von &quot;ultra-right wing</title>
		<imprint/>
		<respStmt>
			<orgName>sagte Waters in einer Erklärung auf Twitter. Die veröffentlichten Informationen enthielten angeblich die Home-Adressen und Telefonnummern von U.S. Sens. Lindsey Graham von South Carolina, und Mike Lee und Orrin Hatch, beide von Utah</orgName>
		</respStmt>
	</monogr>
	<note>Fleischer erschien, um Waters. Reaktion auf die Kritik der Demokraten an Richter Kavanaugh zu vergleichen. der von Kritikern beschuldigt wurde. während der Anhörung am Donnerstag zu wütend zu sein</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zu den veröffentlichten Informationen gehörten Berichten zufolge die Privatadressen und Telefonnummern der US-Senatorin Lindsey Graham aus South Carolina sowie der US-Senatoren Mike Lee und Orrin Hatch, die beide aus Utah stammen. Die Informationen erschienen Donnerstag online, gepostet von einer unbekannten Person auf dem Kapitolshügel während einer Anhörung des Senatsüber die Vorwürfe des sexuellen Fehlverhaltens gegenüber des Kandidaten für den Obersten Gerichtshofs Brett Kavanaugh. Die Daten der drei Senatoren wurden kurz nach der Befragung von Kavanaugh veröffentlicht</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ein Republikanischer Kandidat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Der Bei Den Zwischenwahlen Die Waters Verlassen Wollte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gedanken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Twitter</surname></persName>
		</author>
		<idno>34.3</idno>
	</analytic>
	<monogr>
		<title level="m">Fleischer schien Waters Reaktion mit der Kritik der Demokraten an Richter Kavanaugh zu vergleichen, der von Kritikern beschuldigt wurde, während der Anhörung am Donnerstag zu wütend zu erscheinen</title>
		<imprint/>
	</monogr>
	<note>ihrem Statement sagte Waters, dass ihr Büro &quot; die zuständigen Behörden und Strafverfolgungsbehördenüber die betrügerischen Ansprüche in Kenntnis gesetzt hat. Wir werden dafür sorgen, dass die Täter gefasst und für ihre Taten, die sich destruktiv und gefährlich auf meine Mitarbeiter auswirken, zur Rechenschaft gezogen werden. fuhr sie fort. Table 16: BLEU(HAT</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wmt19</forename><surname>En-De</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">I yelled at everyone to get out of the water: &apos;There&apos;s a shark in the water!&apos;&quot; Hammel added. The boy was airlifted to Rady Children&apos;s Hospital in San Diego where he is listed in critical condition. The species of shark responsible for the attack was unknown. Lifeguard Capt. Larry Giles said at a media briefing that a shark had been spotted in the area a few weeks earlier, but it was determined not to be a dangerous species of shark. Giles added the victim sustained traumatic injuries to his upper torso area. Officials shut down beach access from Ponto Beach in Casablad to Swami&apos;s in Ecinitas for 48 hours for investigation and safety purposes. Giles noted that there are more than 135 shark species in the area</title>
	</analytic>
	<monogr>
		<title level="m">Shark injures 13-year-old on lobster dive in California A shark attacked and injured a 13-year-old boy Saturday while he was diving for lobster in California on the opening day of lobster season</title>
		<imprint/>
	</monogr>
	<note>Chad Hammel told KSWB-TV in San Diego he had been diving with friends for about half an hour Saturday morning when he heard the boy screaming for help and then paddled over with a group to help pull him out of the water. Hammel said at first he thought it was just excitement of catching a lobster, but then he &quot;realized that he was yelling, &apos;I got bit! I got bit!&apos; His whole clavicle was ripped open. but most are not considered dangerous. Table 17: English source text of sample document from WMT19 EN-DE test data</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">There&apos; s a shark in the water! &quot;Hammel fügte hinzu. Der Junge wurde in das Rady Children &apos;s Hospital in San Diego gebracht, wo er in kritischem Zustand aufgeführt wird. Die Art von Hai, die für den Angriff verantwortlich ist, war unbekannt. Lifeguard Capt. Larry Giles sagte bei einer Medienmitteilung, dass ein Hai in der Gegend ein paar Wochen zuvor entdeckt worden sei, aber er sei nicht eine gefährliche Art von Hai. Giles fügte dem Opfer hinzu, dass er in seinem oberen Torso-Gebiet traumatische Verletzungen erlitten habe</title>
	</analytic>
	<monogr>
		<title level="m">WMT-19 EN-DE prediction and target -2 HAT Shark injures 13-jähriger auf Hummertauchen in Kalifornien Ein Hai attackierte und verletzte einen 13-jährigen Jungen Samstag, während er am Eröffnungstag der Hummersaison in Kalifornien für Hummer tauchte</title>
		<imprint/>
	</monogr>
	<note>Tschad Hammel erzählte KSWB-TV in San Diego, dass er am Samstagmorgen etwa eine halbe Stunde lang mit Freunden tauche, als er hörte, dass der Junge um Hilfe schreibe und dann mit einer Gruppeüberhäuft wurde, um ihn aus dem Wasser zu ziehen. Offiziere, die den Zugang zum Strand von Ponto Beach in Casablad für 48 Stunden in Swami&apos; s in Ecinitas sperren, um Sicherheitsvorkehrungen zu treffen</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rettungsschwimmer Kapitän Larry Giles sagte bei einer Medienbesprechung, dass ein Hai einige Wochen zuvor in der Gegend gesichtet worden war, aber es wurde festgestellt, dass es sich nicht um eine gefährliche Haiart handelt. Giles fügte im Oberkörperbereich seines Opfers traumatische Verletzungen hinzu. Beamte schlossen den Zugang zum Strand von Ponto Beach in Casablad zu Swami&apos;s in Ecinitas für 48 Stunden aus Sicherheitsgründen. Giles stellte fest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammel</surname></persName>
		</author>
		<idno>21.8</idno>
	</analytic>
	<monogr>
		<title level="m">Target Hai verletzt 13-jährigen Jungen beim Hummertauchen in Kalifornien Am Samstag griff ein Hai einen 13-jährigen Jungen an und verletzte ihn, während er in Kalifornien am Eröffnungstag der Hummersaison nach Hummern tauchte, sagten Beamte</title>
		<imprint/>
	</monogr>
	<note>Chad Hammel sagte KSWB-TV in San Diego, er habe mit Freunden für eine halbe Stunde am Samstagmorgen getaucht, als er den Jungen um Hilfe schreien hörte. Er sei dann mit den anderen rübergepaddelt, um ihn aus dem Wasser zu retten. dass es mehr als 135 Haiarten in der Gegend gibt, aber die meisten gelten nicht als gefährlich. Table 18: BLEU(HAT</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">But despite his colleagues fleeing for their lives, 21-year-old Anthonius Gunawan Agung refused to leave his post in the wildly swaying control tower at Mutiara Sis Al Jufri Airport Palu airport. He stayed put to make sure that the Batik Air Flight 6321, which was on the runway at the time, was able to take off safely. He then jumped off the traffic control tower when he thought it was collapsing. He died later in hospital. Spokesman for Air Navigation Indonesia, Yohannes Sirait, said the decision may have saved hundreds of lives, Australia&apos;s ABC News reported</title>
	</analytic>
	<monogr>
		<title level="m">Air Traffic Controller Dies To Ensure Hundreds On Plane Can Escape Earthquake An air traffic controller in Indonesia is being hailed as a hero after he died ensuring that a plane carrying hundreds of people made it safely off the ground</title>
		<imprint/>
	</monogr>
	<note>Indonesia&apos;s President Joko Widodo arrived in Palu on Sunday and told the country&apos;s military: &quot;I am asking all of you to work day and night to complete every tasks related to the evacuation. Are you ready?&quot; CNN reported. Indonesia was hit earlier this year by earthquakes in Lombok in which more than 550 people died. Table 19: English source text of sample document from WMT19 EN-DE test data</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Die Zahl der Todesopfer wird immer noch steigen, da viele Körper noch unter dem Trümmerstand waren, während viele nicht in der Lage sind, erreicht zu werden,&quot; sagte der Sprecher der Agentur Sutopo Purwo Nugroho. Wellen, die bis zu sechs Meter erreicht haben, haben Palu verwüstet, die am Sonntag eine Massenattacke halten wird. Militärische und kommerzielle Flugzeuge bringen Hilfe und Lieferungen. Risa Kusuma, eine 35-jährige Mutter, sagte Sky News</title>
	</analytic>
	<monogr>
		<title level="m">HAT Air Traffic Controller Dies Um Hunderte Auf Plane Can Escape Erdbeben sicher zu machen Ein Air Traffic Controller in Indonesien wird als Held gefeiert, nachdem er starb, um sicherzustellen, dass ein Flugzeug mit Hunderten von Menschen es sicher vor dem Boden gemacht hat. Mehr als 800 Menschen sind gestorben und viele sind vermisst, nachdem ein großes Erdbeben die Insel Sulawesi am Freitag getroffen hat, was einen Tsunami ausgelöst hat. Starke Nachbeben plagen weiterhin das Gebiet und viele sind in Schutt in der Stadt Palu gefangen</title>
		<imprint/>
	</monogr>
	<note>Indonesiens Präsident Joko Widodo kam am Sonntag in Palu an und erzählte dem Militär des Landes. Ich bitte Sie alle, Tag und Nacht zu arbeiten, um alle Aufgaben im Zusammenhang mit der Evakuierung abzuschließen. Sind Sie bereit? &quot;CNN berichtete. Indonesien wurde Anfang dieses Jahres von Erdbeben in Lombok getroffen, in dem mehr als 550 Menschen starben</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Seine Kollegen liefen um ihr Leben, aber der 21-jährige Anthonius Gunawan Agung weigerte sich, seinen Posten im extrem schwankenden Kontrollturm am Flughafen Mutiara Sis Al Jufri Airport Palu zu verlassen. Er blieb an Ort und Stelle, um sich zu vergewissern, dass der Batik-Air-Flug 6321, der sich zum Zeitpunkt auf der Startbahn befand, sicher starten konnte. Er sprang dann vom Verkehrskontrollturm, als er dachte, dieser würde einstürzen. Er erlag später im Krankenhaus seinen Verletzungen. Der Sprecher der Air Navigation Indonesia, Yohannes Sirait, sagte, die Entscheidung habe wohlmöglich Hunderte von Leben gerettet, berichtete ABC-News in Australien</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Das Indonesische Rote Kreuz arbeitet unermüdlich, umÜberlebenden zu helfen, aber wir wissen nicht, was sie dort finden werden. Es ist bereits eine Tragödie, aber es könnte noch viel schlimmer werden</title>
		<imprint/>
		<respStmt>
			<orgName>weil er dafür gesorgt hat, dass ein Flugzeug mit Hunderten von Menschen sicher auf dem Boden landen konnte. Mehr als 800 Menschen sind gestorben und viele werden vermisst, nachdem am Freitag ein schweres Erdbeben die Insel Sulawesi heimgesucht und einen Tsunami ausgelöst hat. Starke Nachbeben plagen das Gebiet weiterhin und viele sind in der Stadt Palu in Trümmern gefangen</orgName>
		</respStmt>
	</monogr>
	<note>Indonesiens Präsident Joko Widodo ist am Sonntag in Palu eingetroffen und sagte dem Militär. Ich bitte Sie alle, Tag und Nacht zu arbeiten, um alle Aufgaben im Zusammenhang mit der Evakuierung zu erfüllen. Sind Sie bereit?&quot; berichtete CNN. Indonesien wurde in diesem Jahr von Erdbeben in Lombok heimgesucht</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
