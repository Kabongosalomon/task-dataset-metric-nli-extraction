<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Video Interpolation Using Cycle Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Dundar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>NVIDIA</roleName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Video Interpolation Using Cycle Consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to synthesize high frame rate videos via interpolation requires large quantities of high frame rate training videos, which, however, are scarce, especially at high resolutions. Here, we propose unsupervised techniques to synthesize high frame rate videos directly from low frame rate videos using cycle consistency. For a triplet of consecutive frames, we optimize models to minimize the discrepancy between the center frame and its cycle reconstruction, obtained by interpolating back from interpolated intermediate frames. This simple unsupervised constraint alone achieves results comparable with supervision using the ground truth intermediate frames. We further introduce a pseudo supervised loss term that enforces the interpolated frames to be consistent with predictions of a pre-trained interpolation model. The pseudo supervised loss term, used together with cycle consistency, can effectively adapt a pre-trained model to a new target domain. With no additional data and in a completely unsupervised fashion, our techniques significantly improve pretrained models on new target domains, increasing PSNR values from 32.84dB to 33.05dB on the Slowflow and from 31.82dB to 32.53dB on the Sintel evaluation datasets. Code is available at https://github.com/NVIDIA/unsupervisedvideo-interpolation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the advancement of modern technology, consumergrade smartphones and digital cameras can now record videos at high frame rates (e.g. 240 frames-per-second). However, achieving this comes at the cost of high power consumption, larger storage requirements, and reduced video resolution. Given these limitations, regular events are not typically recorded at high frame rates. Yet, important life events happen unexpectedly and hence tend to be recorded at standard frame rates. It is thus greatly desirable to have the ability to produce arbitrarily high FPS videos * Currently affiliated with Google. Video frame interpolation addresses this need by generating one or more intermediate frames from two consecutive frames. Increasing the number of frames in videos essentially allows one to visualize events in slow motion and appreciate content better. Often, video interpolation techniques are employed to increase the frame rate of already recorded videos, or in streaming applications to provide a high refresh rate or a smooth viewing experience.</p><p>Video interpolation is a classical vision and graphics problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref> and has recently received renewed research attention <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. Particularly, supervised learning with convolutional neural networks (CNNs) has been widely employed to learn video interpolation from paired input and ground truth frames, often collected from raw video data. For instance, recent CNN-based approaches such as <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b15">[16]</ref>, trained with large quantities of public high FPS videos, obtain high quality interpolation results when the test videos are similar to the training ones.</p><p>However, these methods may fail if the training data dif-fer from the target domain. For instance, the target domain might be to slow down videos of fish taken underwater, but available training data only contains regular outdoor scenes, thus leading to a content gap. Additionally, there might be more subtle domain gaps due to differences such as camera parameters, encoding codecs, and lighting, leading to the well-known co-variate shift problem <ref type="bibr" target="#b17">[18]</ref>. It is impractical to address the issue by collecting high FPS videos covering all possible scenarios, because it is expensive to capture and store very high FPS videos, e.g., videos with more than 1000-fps at high spatial resolutions.</p><p>In this work, we propose a set of unsupervised learning techniques to alleviate the need for high FPS training videos and to shrink domain gaps in video interpolation. Specifically, we propose to learn video frame interpolation, without paired training data, by enforcing models to satisfy a cycle consistency constraint <ref type="bibr" target="#b1">[2]</ref> in the time. That is, for a given triplet of consecutive frames, if we generate the two intermediate frames between the two consecutive frames, and generate back their intermediate frame, the resulting frame must match the original input middle frame (shown schematically in <ref type="figure">Fig. 3</ref>). We show such simple constraint alone is effective to learn video interpolation, and achieve results that compete with supervised approaches.</p><p>In domain adaptation applications, where we have access to models pre-trained on out-of-domain datasets, but lack ground truth frames in target domains, we also propose unsupervised fine-tuning techniques that leverage such pretrained models (See <ref type="figure">Fig. 2</ref>). We fine-tune models on target videos, with no additional data, by optimizing to jointly satisfy cycle consistency and minimize the discrepancy between generated intermediate frames and corresponding predictions from the known pre-trained model. We demonstrate our joint optimization strategy leads to significantly superior accuracy in upscaling frame rate of target videos than fine-tuning with cycle consistency alone or directly applying the pre-trained models on target videos (see <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>Cycle consistency has been utilized for image matching <ref type="bibr" target="#b24">[25]</ref>, establishing dense 3D correspondence over object instances <ref type="bibr" target="#b22">[23]</ref>, or in learning unpaired image-to-image translation in conjunction with Generative Adversarial Networks (GANs) <ref type="bibr" target="#b25">[26]</ref>. To the best of our knowledge, this is the first attempt to use a cycle consistency constraint to learn video interpolation in a completely unsupervised way.</p><p>To summarize, the contributions of our work include: </p><formula xml:id="formula_0">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Video Interpolation: The task is to interpolate intermediate frames between a pair of input frames. Classical approaches such as <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b11">[12]</ref> estimate optical flow and interpolate intermediate frames at intermediate positions along the estimated trajectory of pixels, and further make use of forward and backward optical flow consistency to reason about occlusions. Given the recent rise in popularity of deep learning methods, several end-to-end trainable methods have been proposed for video interpolation. Specifically, these methods can be trained to interpolate frames using just input and target frames and no additional supervision. Liu et al. <ref type="bibr" target="#b8">[9]</ref> and Jiang et al. <ref type="bibr" target="#b5">[6]</ref> both indirectly learn to predict optical flow using frame interpolation. Works such as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> are similarly end-to-end trainable, but instead of learning optical flow vectors to warp pixels, they predict adaptive convolutional kernels to apply at each location of the two input frames. Our work presents unsupervised techniques to train or fine-tune any video interpolation model, for instance the Super SloMo <ref type="bibr" target="#b5">[6]</ref>, which predicts multiple intermediate frames, or the Deep Voxel Flow <ref type="bibr" target="#b8">[9]</ref>, which predicts one intermediate frame.</p><p>Cycle Consistency: One of the key elements of our proposed method is the use of a cycle consistency constraint. This constraint encourages the transformations predicted by a model to be invertible, and is often used to regularize the model behavior when direct supervision is unavailable. Cycle consistency has been used in a variety of applications, including determining the quality of language translations <ref type="bibr" target="#b1">[2]</ref>, semi-supervised training for image-description generation <ref type="bibr" target="#b12">[13]</ref>, dense image correspondences <ref type="bibr" target="#b23">[24]</ref>, identifying false visual relations in structure from motion <ref type="bibr" target="#b21">[22]</ref>, and image-to-image translation <ref type="bibr" target="#b25">[26]</ref>, to name a few.</p><p>A cycle consistency constraint, in the context of video interpolation, means that we should be able to reconstruct the original input frames by interpolating between predicted intermediate frames at the appropriate time stamps. Most related to our work is <ref type="bibr" target="#b7">[8]</ref>, which uses such a constraint to regularize a fully supervised video interpolation model. Our work differs in several critical aspects. First, our method is based on the Super SloMo <ref type="bibr" target="#b5">[6]</ref> architecture, and is thus capable of predicting intermediate frames at arbitrary timestamps, whereas <ref type="bibr" target="#b7">[8]</ref> is specifically trained to predict the middle timestamp. Next, and most critically, our proposed method is fully unsupervised. This means that the target intermediate frame is never used for supervision, and that it can learn to produce high frame rate interpolated sequences from any lower frame rate sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this work, we propose to learn to interpolate arbitrarily many intermediate frames from a pair of input frames, in an unsupervised fashion, with no paired intermediate ground truth frames. Specifically, given a pair of input frames I 0 and I 1 , we generate intermediate frameÎ t , aŝ</p><formula xml:id="formula_1">I t = M I 0 , I 1 , t ,<label>(1)</label></formula><p>where t ∈ (0, 1) is time, and M is a video frame interpolation model we want to learn without supervision. We realize M using deep convolutional neural networks (CNN). We chose CNNs as they are able to model highly non-linear mappings, are easy to implement, and have been proven to be robust for various vision tasks, including image classification, segmentation, and video interpolation. Inspired by the recent success in learning unpaired image-to-image translation using Generative Adversarial Networks (GAN) <ref type="bibr" target="#b25">[26]</ref>, we propose to optimize M to maintain cycle consistency in time. Let I 0 , I 1 and I 2 are a triplet of consecutive input frames. We define the time-domain cycle consistency constraint such that for generated intermediate frames at time t between (I 0 , I 1 ) and between (I 1 , I 2 ), a subsequently generated intermediate frame at time (1 − t) between the interpolated results (Î t ,Î t+1 ) must match the original middle input frame I 1 . Mathematically, a cycle reconstructed frame using M is given by,</p><formula xml:id="formula_2">I 1 = M M I 0 , I 1 , t , M I 1 , I 2 , t , 1 − t .<label>(2)</label></formula><p>We then optimize M to minimize the reconstruction error betweenÎ 1 and I 1 , as arg min</p><formula xml:id="formula_3">θ (M) Î 1 − I 1 1 .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3 schematically presents our cycle consistency based approach.</head><p>A degenerate solution to optimizing equation 3 might be to copy the input frames as the intermediate predictions (i.e. outputs). However, in practice this does not occur. In order for M to learn to do copy frames in this way, it would have to learn to identify the input's time information within a single forward operation (eq. 2), as I 1 is a t = 1 input in the መ = ℳ( 0 , 1 , ) <ref type="figure">Figure 3</ref>. An overview of time-domain cycle consistency constrain. I0, I1 and I2, shown as green circles, are a triplet of consecutive input frames. If we generate intermediate frames at time t between (I0, I1) and between (I1, I2), and subsequently generate back an intermediate frame at time (1 − t) between (Ît,Ît+1), the resulting frame must match the original middle input frame, I1.</p><formula xml:id="formula_4">T = 2 መ +1 = ℳ( 1 , 2 , ) መ 1 = ℳ( መ , መ +1 , 1 − ) T = + 1 T = 1 T = T = 0</formula><p>first pass, and I 1 is a t = 0 input in the second pass. This is difficult, since the same weights of M are used in both passes. We support this claim in all of our experiments, where we compared our learned approach using equation 3 with the trivial case of using inputs as intermediate prediction.</p><p>It is true that triplets of input frames could be exploited directly. For example, the reconstruction error between M(I 0 , I 2 , t = 0.5) and I 1 could be used without cycle consistency. However, our experiments in Section 4.4.2 suggest that larger time-step lead to significantly worse accuracy if used without cycle consistency.</p><p>Optimizing M to the satisfy cycle consistency (CC) constraint in time, as will show in our experiments in Sections 4.2 and 4.3, is effective and is able to generate arbitrarily many intermediate frames that are realistic and temporally smooth. It also produces results that are competitive with supervised approaches, including the same model M, but trained with supervision.</p><p>In this work, we also propose techniques that can make unsupervised fine-tuning processes robust. It is quite common to have access to out-of-domain training videos in abundance or access to already pre-trained interpolation models. On the other hand, target domain videos are often limited in quantity, and most critically, lack ground truth intermediate frames. We aim to optimize M in target videos to jointly satisfy cycle consistency as defined in equation 3 and also learn to approximate a known pre-trained interpolation model, denoted as F. Mathematically, our modified objective is given as, arg min</p><formula xml:id="formula_5">θ (M) Î 1 − I 1 1 + Î t − F I 0 , I 1 , t 1 + Î t+1 − F I 1 , I 2 , t 1 ,<label>(4)</label></formula><p>whereÎ 1 is the cycle reconstructed frame given by equation 2,Î t andÎ t+1 are given by equation 1, and θ (M) are the parameters of M that our optimization processes update.</p><p>The added objective function to approximate F, help regularize M to generate realistic hidden intermediate framesÎ t anÎ t+1 by constraining them to resemble predictions of a known frame interpolation model, F. As optimization progresses and M learns to pick-up interpolation concepts, one can limit the contribution of the regularizing "pseudo" supervised (PS) loss and let optimizations be guided more by the cycle consistency. Such a surrogate loss term, derived from estimated intermediate frames, can make our training processes converge faster or make our optimization processes robust by exposing it to many variations of F.</p><p>In this work, for the sake of simplicity, we chose F to be the same as our M, but pre-trained with supervision on a disjoint dataset that has ground-truth high frame rate video, and denote it as M pre . Our final objective can be given by,</p><formula xml:id="formula_6">arg min θ (M) λ rc Î 1 − I 1 1 + λ rp Î t − M pre I 0 , I 1 , t 1 + λ rp Î t+1 − M pre I 1 , I 2 , t 1 ,<label>(5)</label></formula><p>where λ rc and λ rp are weights of CC and PS losses.</p><p>As we will show in the experiments, optimizing equation 5 by relying only on the PS loss, without cycle consistency, will teach M to perform at best as good as M pre , i.e., the model used in the same PS loss. However, as we show in Section 4.4.1, by weighting cycle consistency and PS losses appropriately, we achieve frame interpolation results that are superior to those obtained by learning using either CC or PS losses alone.</p><p>Finally, we implement our M using the Super SloMo video interpolation model <ref type="bibr" target="#b5">[6]</ref>. Super SloMo is a state of the art flow-based CNN for video interpolation, capable of synthesizing an arbitrary number of high quality and temporally stable intermediate frames. Our technique is not limited to this particular interpolation model, but could be adopted with others as well.</p><p>In the subsequent subsections we provide a short summary of the Super SloMo model, our loss functions, and techniques we employed to make our unsupervised training processes stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video Interpolation Model</head><p>To generate one or more intermediate framesÎ t from a pair of input frames (I 0 , I 1 ), first the Super SloMo model estimates an approximate bi-directional optical flow from any arbitrary time t to 0, F t→0 , and from t to 1, F t→1 . Then, it generates a frame by linearly blending the input frames after they are warped by the respective estimated op-tical flows, aŝ</p><formula xml:id="formula_7">I t = αT (I 0 , F t→0 ) + (1 − α)T (I 1 , F t→1 ),<label>(6)</label></formula><p>where T is an operation that bilinearly samples input frames using the optical flows, and α weighs the contribution of each term. The blending weight α models both global property of temporal consistency as well as local or pixel-wise occlusion or dis-occlusion reasoning. For instance, to maintain temporal consistency, I 0 must contribute more toÎ t when t is close to 0. Similarly, I 1 contributes more toÎ t , when t is close to 1.</p><p>To cleanly blend the two images, an important property of video frame interpolation is exploited, i.e. not all pixels at time t are visible in both input frames. Equation 6 can thus be defined by decomposing α to model both temporal consistency and occlusion or de-occlusions, aŝ</p><formula xml:id="formula_8">I t = 1 Z 1 − t V t←0 T I 0 , F t→0 + tV t←1 T I 1 , F t→1 ,<label>(7)</label></formula><p>where V t←0 and V t←0 are visibility maps, and Z = (1 − t)V t←0 + tV t←1 is a normalisation factor. For a pixel p, V t←0 (p) ∈ [0, 1] denotes visibility of p at time t (0 means fully occluded or is invisible at t).</p><p>The remaining challenge is estimating the intermediate bi-direction optical flows (F t→0 , F t→1 ) and the corresponding visibility maps (V t←0 , V t←1 ). For more information, we refer the reader to <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training and Loss Functions</head><p>We train M to generate arbitrarily many intermediate frames {Î ti } N i=1 without using the corresponding groundtruth intermediate frames {I ti } N i=1 , with N and t i ∈ (0, 1) being frame count and time, respectively. Specifically, as described in Section 3, we optimize M to (a) minimize the errors between the cycle reconstructed frameÎ 1 and I 1 and (b) to minimize the errors between the intermediately predicted framesÎ t andÎ t+1 and the corresponding estimated or pseudo ground-truth frames M pre (I 0 , I 1 , t) and M pre (I 1 , I 2 , t).</p><p>Note that, during optimization a cycle reconstructed frameÎ 1 can be obtained via arbitrarily many intermediately generated frames {Î ti ,Î ti+1 } N i=1 . Thus, many reconstruction errors can be computed from a single triplets of training frames {I 0 , I 1 , I 2 }. However, we found doing so makes optimizations unstable and often unable to converge to acceptable solutions. Instead, we found establishing very few reconstruction errors per triplet to make our training stable and generate realistic intermediate frames. In our experiments, we calculate one reconstruction error per triplet, at random time t i ∈ (0, 1).</p><p>Our training loss functions are given by,</p><formula xml:id="formula_9">L = λ rc L rc + λ rp L rp + λ p L p + λ w L w + λ s L s ,<label>(8)</label></formula><p>where L rc , defined as,</p><formula xml:id="formula_10">L rc = Î 1 − I 1 1 ,<label>(9)</label></formula><p>models how good the cycle reconstructed frame is, and L rp , defined as,</p><formula xml:id="formula_11">L rp = Î ti − M pre (I 0 , I 1 , t i ) 1 + Î ti+1 − M pre (I 1 , I 2 , t i ) 1 ,<label>(10)</label></formula><p>models how close the hidden intermediate frames are to our pseudo intermediate frames. L p models a perceptual loss defined as the L 2 norm on the high-level features of VGG-16 model, pre-trained on ImageNet, and is given as,</p><formula xml:id="formula_12">L p = Ψ(Î 1 ) − Ψ(I 1 ) 2 ,<label>(11)</label></formula><p>with Ψ representing the conv4 3 feature of the VGG-16 model. Our third loss, L w is a warping loss that make optical flow predictions realistic, and is given by,</p><formula xml:id="formula_13">L w = T (I 0 , F 1→0 ) − I 1 1 + T (I 1 , F 0→1 ) − I 0 1 + T (I 1 , F 2→1 ) − I 2 1 + T (I 2 , F 1→2 ) − I 1 1 + T (Î t , F t+1→t ) −Î t+1 1 + T (Î t+1 , F t→t+1 ) −Î t 1 .<label>(12)</label></formula><p>In a similar way as the Super SloMo framework, we also enforce a smoothness constraint to encourage neighbouring optical flows to have similar optical flow values, and it is given as,</p><formula xml:id="formula_14">L s = ∆F t→t+1 1 + ∆F t+1→t 1 + ∆F 0→1 1 + ∆F 1→0 1 + ∆F 1→2 1 + ∆F 2→1 1 ,<label>(13)</label></formula><p>where F t→t+1 and F t+1→t are the forward and backward optical flows between the the intermediately predictedÎ t andÎ t+1 frames. Finally, we linearly combine our losses using experimentally selected weights: λ rc = 0.8, λ rp = 0.8, λ p = 0.05, λ w = 0.4, and λ s = 1, see Section 4.4.1 for details on weight selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>We use Adam solver <ref type="bibr" target="#b6">[7]</ref> for optimization with β 1 = 0.9, β 2 = 0.999, and no weight decay. We train our models for a total of 500 epochs, with a total batch size of 32 on 16 V100 GPUs, using distributed training over two nodes. Initial learning rate is set to 1e −4 , and then scaled-down by 10 after 250, and again after 450 epochs. <ref type="table">Table 1</ref> summarizes datasets we used for training and evaluation. We used Adobe-240fps <ref type="bibr" target="#b19">[20]</ref> (76.7K frames) and YouTube-240fps <ref type="bibr" target="#b5">[6]</ref> (296.4K frames) for supervised training to establish baselines. For unsupervised training, we considered low FPS Battlefield1-30fps videos <ref type="bibr" target="#b16">[17]</ref> (320K frames), and Adobe-30fps (9.5K frames), obtained by temporally sub-sampling Adobe-240fps videos, by keeping only every other 8th frame. We chose game frames because they contain a large range of motion that could make learning processes robust. We used UCF101 <ref type="bibr" target="#b18">[19]</ref> datasets for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>To study our unsupervised fine-tuning techniques in bridging domain gaps, we considered two particularly distinct, high FPS and high resolution, target video datasets: Slowflow-240fps and Sintel-1008fps <ref type="bibr" target="#b4">[5]</ref>. Slowflow is captured from real life using professional high speed cameras, whereas Sintel is a game content. We split Slowflow dataset into disjoint low FPS train (3.4K frames) and a high FPS test (414 frames) subsets, see <ref type="table">Table 1</ref>. We create the test set by selecting nine frames in each of the 46 clips. We then create our low FPS train subset by temporally sub-sampling the remaining frames from 240-fps to 30-fps. During evaluation, our models take as input the first and ninth frame in each test clip and interpolate seven intermediate frames. We follow a similar procedure for Sintel-1008fps <ref type="bibr" target="#b4">[5]</ref>, but interpolate 41 intermediate frames, i.e., conversion of frame rate from 24-to 1008-fps.</p><p>To quantitatively evaluate interpolations we considered Peak-Signal-To-Noise (PSNR), the Structural-Similarity-Image-Metric (SSIM), and the Interpolation-Error (IE) <ref type="bibr" target="#b0">[1]</ref>, which is calculated as the root mean-squared-error between generated and ground truth frames. High PSNR and SSIM scores indicate better quality, whereas for IE score it is the opposite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Large-Scale Unsupervised Training</head><p>In this section, we consider the scenario where we do not have any high frame rate videos to train a base model, but we have abundant low frame rate videos. We test our models on UCF101 dataset; for every triplet of frames, the first and third ones are used as input to predict the second frame.</p><p>Results are presented in random initialization, for video frame interpolation. We further study the impact of frame count in unsupervised training. For this study, we used the low FPS Battlefield-1 sequences. The higher the frame count of low FPS frames, the better our unsupervised model performs, when evaluated on UCF101. Using Battlefield-30fps, at frame count four times larger than Adobe-240fps, we achieve results on par with supervised techniques, achieving IE of 5.38 and 5.48, respectively. <ref type="table">Table 2</ref> also presents results of trivial copy, which is the simple case of using inputs as predictions. Compared to cycle consistency, trivial prediction leads to significantly worse interpolation, further indicating our approach does in fact allow us to synthesize intermediate frames from unpaired raw video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Unsupervised Fine-tuning for Domain Transfer</head><p>One particularly common situation in video frame interpolation is that we have access to pre-trained models or access to high FPS out-of-domain videos in abundance, but lack ground truth frames in target videos, which are also commonly limited in quantity. Our unsupervised techniques allow us to fine-tune pre-trained models directly on target videos without additional data, and demonstrate significant gain in accuracy in upscaling frame rates of target videos.</p><p>First, we consider the scenario where train and test videos are collected with different camera set-ups. We assume we have access to high fps videos collected by handheld cameras, which is the Adobe-240fps, YouTube-240fps, UCF101 datasets, and consider the Slowflow dataset as our target, a particularly high resolution and high FPS video captured by high speed professional cameras in real life.</p><p>Our baseline is a frame interpolation model trained with supervision. Specifically, we consider SuperSloMo and Deep Voxel Flow (DVF) <ref type="bibr" target="#b8">[9]</ref>. DVF is another widely-used singleframe interpolation method. We apply our unsupervised fine-tuning directly on the low FPS train split of Slowflow, and evaluate on its test split.  <ref type="table">Table 4</ref>. Multi-frame interpolation results on Sintel for frame rate conversion from 24 to 1008 FPS, and domain transfer experiments using baselines obtained by pre-training with supervision on Adobe-or Adobe+YouTube-240fps.</p><p>Adobe→Slowflow: Our unsupervised training with cycle consistency alone performs quite closely to the baseline (Super SloMo pre-trained with supervision), achieving PSNR of 32.35 and 32.84, respectively. While a total of 76.7K Adobe-240fps frames are used in supervised pretraining, our unsupervised training is performed with only 3K frames of Slowflow, which indicates the efficiency and robustness of our proposed unsupervised training technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth Intermediate</head><p>Baseline Supervised (SuperSloMo)</p><p>Proposed Unsupervised Fine-tuning (SuperSloMo) <ref type="figure">Figure 4</ref>. Visual results of a sample from Slowflow dataset. Baseline supervised model is trained with Adobe dataset and proposed unsupervised model is fine-tuned with Slowflow. Improvements seen as the person's back squeezed in supervised (middle) but preserved in unsupervised (right). On bottom row, although both techniques blur the regions surrounding the helmet, the shape of helmet is preserved in our proposed technique. Furthermore, fine-tuning the pre-trained model by jointly optimizing to satisfy cycle consistency and to minimize our proposed pseudo supervised loss (CC + PS), we outperform the pre-trained baseline by a large margin, with PSNR of 33.05 vs. 32.84. The PS loss relies on the same pre-trained baseline model, as discussed in Section 3, and regularizes our training process. If used alone, i.e without cycle consistency, it performs at best as good as the baseline pre-trained model, see Section 4.4.1 for more details. Adobe+YouTube→Slowflow: Here, our baseline model is pre-trained on a larger dataset Adobe+YouTube, total of 372.7K frames, and achieves better accuracy than pretraining on Adobe alone, achieving PSNR 33.13 vs. 32.84, when directly applied on Slowflow test videos. Even with improved pre-trained baseline, we observe consistent benefit with our proposed unsupervised fine-tuning, improving PSNR from 33.13 to 33.20.</p><p>Another interesting observation from this study is that it takes an extra 296.K frames from YouTube-240fps to improve PSNR from 32.84 to 33.13 on Slowflow, via pretraining with supervision. We achieve a comparable improvement of PSNR from 32.84 to 33.05 by simply finetuning on the target low FPS frames in a completely unsupervised way. Sample interpolation results from this study can be found at <ref type="figure" target="#fig_0">Figure 1</ref>, where improvements on the bicy-cle tire and the shoe are highlighted, and at <ref type="figure">Figure 4</ref>, where improvements on the persons' back and helmet regions are highlighted. <ref type="table">Table 4</ref> present results of unsupervised finetuning for domain transfer from Adobe→Sintel and Adobe+YouTube→Sintel for the task of upscaling frame rate from 24-to 1008-fps. Similarly to the Slowflow experiments, in <ref type="table">Table 3</ref>, the results indicate the utility of our unsupervised techniques in shrinking domain gaps or achieving results that compete with supervised techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Slowflow</head><p>Sintel  <ref type="table">Table 5</ref>. Comparison of supervised training at quarter resolution (baseline) and unsupervised fine-tuning at full resolution (proposed) for frame rate upscaling from 30 to 240 FPS (Slowflow) and 24 to 1008 FPS (Sintel).</p><formula xml:id="formula_15">PSNR(↑) SSIM(↑) IE(↓) PSNR(↑) SSIM(↑) IE(↓)</formula><p>UCF101→Slowflow: <ref type="table" target="#tab_4">Table 6</ref> and <ref type="figure" target="#fig_1">Figure 5</ref> present results of fine-tuning DVF on Slowflow. We use an off-the-shelf implementation of DVF, pre-trained on UCF101 1 . Our unsupervised techniques improve the PSNR from 24.64dB to 28.38dB, demonstrating that our method generalizes well to different interpolation techniques, and is not limited to SuperSloMo.</p><p>UCF101→Slowflow using DVF <ref type="bibr" target="#b8">[9]</ref> Loss  In our second domain transfer setting, we consider the scenario where target and test datasets share similarities in content and style but they are in different resolution. This is a very practical scenario given the scarcity of the highframe high-resolution videos. Therefore, it is highly desirable to learn from low resolution videos, and be able to interpolate higher resolutions. We establish a low resolution Proposed Unsupervised Fine-tuning (SuperSloMo) <ref type="figure">Figure 6</ref>. Visual results of a sample from Slowflow dataset. Baseline supervised model is trained with Slowflow dataset in quarter resolution and proposed unsupervised model is fine-tuned with full resolution Slowflow. The tree in the background is deformed, and also deformed in the supervised (middle), while it is predicted well in proposed (right). Bottom row, supervised shows blurriness in the grass, while it is crisper in the proposed.</p><p>baseline by training with supervision on 240 fps Slowflowtrain dataset, after down-sampling its frames by 4 in each dimension. Our test video is Slowflow-test split at its original resolution. We repeat similar setting for Sintel. <ref type="table">Table 5</ref> shows results where our fine-tuning technique on the test domain improves PSNR from 30.79 to 31.42 for Slowflow, and from 29.14 to 29.71 for Sintel. Visual samples from this experiment can be found in <ref type="figure">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>We conduct ablation studies to analyze various design choices of the proposed method. <ref type="figure">Figure 7</ref> presents interpolation results in PSNR for our models trained with a range of PS weight, λ rp , values. We fix CC's weight, λ rc to 0.8, and vary λ rp in small increments from 0 to 64. When λ rp = 0, optimization is guided entirely by CC, it achieves PSNR of 32.35 for unsupervised Adobe+YouTube→Slowflow domain transfer. Interpolation accuracy gradually increases, and plateaus approximately after 0.8. Based on this, we select λ rp = 0.8, and fix its value for all our experiments. At large values of λ rp , the optimization is mostly guided by PS loss, and as such, trained models perform very similarly to the pretrained model that the PS loss depends on. losses on the other hand leads to results that are superior to those obtained using either loss alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Optimal CC and PS Weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Large Time-step Supervision</head><p>We study the effect of using loss terms, such as M(I 0 , I 2 , t = 0.5) − I 1 or its variations, defined over a longer time. <ref type="table">Table 7</ref> presents Adobe+YouTube→Slowflow fine-tuning with cycle consistency, the loss derived from two step interpolation alone or together with cycle consistency. Optimizing using losses derived from long step interpolation result in worse accuracy than optimizing with cycle consistency. When used with cycle consistency, we also did not find it to lead to notable improvement. We attribute this because the model's capacity might be spent to solve the harder problem of interpolating large steps, and provide little benefit to the task of synthesizing intermediate frames between consecutive frames.  <ref type="table">Table 7</ref>. Comparison of cycle consistency with objectives derived from longer time step interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented unsupervised learning techniques to synthesize high frame rate videos directly from low frame rate videos by teaching models to satisfy cycle consistency in time. Models trained with our unsupervised techniques are able to synthesize arbitrarily many, high quality and temporally smooth intermediate frames that compete with supervised approaches. We further apply our techniques to reduce domain gaps in video interpolation by fine-tuning pre-trained models on target videos using a pseudo supervised loss term and demonstrate significant gain in accuracy. Our work shows the potential of learning to interpolate B. Interpolation Result vs Intermediate Time For all models, interpolation accuracy decreases as time-points move away from t = 0 or t = 1. Compared to the baseline, our CC-based fine-tuning performs better at the end points (close to t = 0 or t = 1), and worse at midway points. On the other hand, our CC+PS-based unsupervised fine-tuning achieves the best of both CC and Baseline, performing better than both CC and Baseline at all time points. Proposed CC 32.33±0.028 0.886±0.000 6.78±0.021 CC + PS 33.20±0.006 0.891±0.001 6.57±0.010 <ref type="table">Table 8</ref>. Mean and Standard deviation of PSNR, SSIM, and IE for domain adaptation of upscaling frame rate from 30-to 240-fps for Adobe→or Adobe+YouTube→Slowflow. CC refers to cycle consistency, and PS pseudo supervised loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adobe→Slowflow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Visual results of a sample from Slowflow dataset. Baseline supervised model is trained with Adobe and YouTube datasets and proposed unsupervised model is fine-tuned with Slowflow. See project website for video comparisons. from low FPS videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparison of unsupervised fine-tuning of DVF with supervised DVF pre-trained on UCF-101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Figure 7 .</head><label>17</label><figDesc>Figure 7shows this trend. Optimizing with optimally combined CC and PS λ rp Interpolation accuracy in PSNR versus λrp (PS weight) used in our proposed joint CC and PS optimization techniques, when applied for Adobe+YouTube→Slowflow unsupervised domain transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8</head><label>8</label><figDesc>presents mean PSNR score at each of the 41 time-points for Adobe→Sintel domain adaptation using (a) Super SloMo<ref type="bibr" target="#b5">[6]</ref> pre-trained with supervision (Baseline), (b) unsupervised fine-tuning with cycle consistency loss alone (CC), and (c) unsupervised fine-tuning with cycle consistency and pseudo supervised losses (CC+PS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Mean PSNR score at each of the 41 time points for Adobe→Sintel domain adaptation using (a) Super SloMo<ref type="bibr" target="#b5">[6]</ref> pre-trained with supervision (Baseline), (b) unsupervised fine-tuning with cycle consistency loss alone (CC), and (c) unsupervised fine-tuning with cycle consistency and pseudo supervised losses (CC+PS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 1 .</head><label>21</label><figDesc>Our unsupervised technique trained on Adobe-30fps performs competitively with results obtained with supervision on Adobe-240fps, achieving PSNR of 34.47, and 34.63 respectively. Compared to the supervised training, our unsupervised training uses 1/8th of the frame count, and performs comparably to techniques trained with supervision. This shows the effectiveness of cycle consistency constraint alone in training models, from Statistics of video datasets used in training or evaluation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">FPS Frame count Clip count</cell><cell>Resolution</cell><cell>Train Test</cell></row><row><cell></cell><cell cols="2">UCF101 [19]</cell><cell>25</cell><cell>1,137</cell><cell></cell><cell>379</cell><cell>256 × 256</cell><cell>x</cell></row><row><cell></cell><cell cols="2">YouTube [6]</cell><cell>240</cell><cell cols="2">296,352</cell><cell>1,014</cell><cell>720 × 1280</cell><cell>x</cell></row><row><cell></cell><cell cols="2">Battlefield-1 [17]</cell><cell>30</cell><cell cols="2">329,222</cell><cell>363</cell><cell>1080 × 1920</cell><cell>x</cell></row><row><cell></cell><cell cols="2">Adobe [20]</cell><cell>30 240</cell><cell>9,551 76,768</cell><cell></cell><cell>112</cell><cell>720 × 1280</cell><cell>x x</cell></row><row><cell></cell><cell cols="2">Slowflow [5]</cell><cell>30 240</cell><cell>3,470 414</cell><cell></cell><cell>46</cell><cell>2048 × 2560</cell><cell>x</cell><cell>x</cell></row><row><cell></cell><cell>Sintel [5]</cell><cell cols="2">24 1008</cell><cell>551 559</cell><cell></cell><cell>13</cell><cell>872 × 2048</cell><cell>x</cell><cell>x</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UCF101</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Training data</cell><cell cols="4">PSNR(↑) SSIM(↑) IE(↓)</cell><cell></cell></row><row><cell>Trivial Copy</cell><cell>N/A</cell><cell>31.27</cell><cell></cell><cell>0.895</cell><cell>8.35</cell><cell></cell></row><row><cell>Baseline</cell><cell>Adobe-240fps</cell><cell>34.63</cell><cell></cell><cell>0.946</cell><cell>5.48</cell><cell></cell></row><row><cell>Proposed</cell><cell>Adobe-30fps BattleField-30fps</cell><cell>34.47 34.55</cell><cell></cell><cell>0.946 0.947</cell><cell>5.50 5.38</cell><cell></cell></row><row><cell cols="6">Table 2. Interpolation results for single intermediate frame inter-</cell><cell></cell></row><row><cell cols="2">polation on UCF101.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Slowflow 30-to 60-FPS conversion. The baseline DVF is pre-trained on UCF-101 with supervision.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">PSNR(↑) SSIM(↑) IE(↓)</cell></row><row><cell>Trivial Copy</cell><cell>N/A</cell><cell>24.26</cell><cell>0.698</cell><cell>15.60</cell></row><row><cell>Baseline</cell><cell>PairedGT</cell><cell>25.64</cell><cell>0.778</cell><cell>12.77</cell></row><row><cell>Proposed</cell><cell>CC + PS</cell><cell>28.38</cell><cell>0.820</cell><cell>9.79</cell></row><row><cell>Ground Truth</cell><cell cols="2">Baseline Supervised</cell><cell cols="2">Proposed Unsupervised</cell></row><row><cell>Intermediate</cell><cell></cell><cell>(DVF)</cell><cell cols="2">Fine-tuning (DVF)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>33±0.028 0.886±0.000 6.78±0.021 PS 32.88±0.006 0.887±0.000 6.74±0.006 CC + PS 33.05±0.006 0.890±0.000 6.62±0.000 Adobe+YouTube→Slowflow</figDesc><table><row><cell></cell><cell cols="2">PSNR</cell><cell>SSIM</cell><cell>IE</cell></row><row><cell>Baseline PairedGT</cell><cell></cell><cell>32.84</cell><cell>0.887</cell><cell>6.67</cell></row><row><cell cols="2">Proposed 32.Baseline PairedGT CC</cell><cell>33.13</cell><cell>0.889</cell><cell>6.63</cell></row><row><cell>PS</cell><cell cols="4">33.14±0.006 0.889±0.000 6.63±0.006</cell></row><row><cell>Proposed</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/lxx1991/pytorch-voxel-flow Ground Truth Intermediate Baseline Supervised (SuperSloMo)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>high frame rate videos using only low frame rate videos and opens new avenues to leverage large amounts of low frame rate videos in unsupervised training.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Insensitivity to Randomness</head><p>We train our models three times to account for random weight initialisation or random data augmentation. <ref type="table">Table 8</ref> presents the mean and standard deviation of domain adaptation results for Adobe→Slowflow and Adobe+YouTube→Slowflow. Results indicate our models' insensitivity to randomness, as shown by the margins of improvements in PSNR from 32.84 to 33.05 for Adobe, or from 33.13 to 33.20 for Adobe+YouTube. <ref type="table">Table 8</ref> also shows fine-tuning with PS loss alone leads to results that are similar to the Baseline.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Back-translation for cross-cultural research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard W Brislin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cross-cultural psychology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="216" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Occlusion reasoning for temporal interpolation using optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Occlusion reasoning for temporal interpolation using optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<idno>UW-CSE-09-08-01</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Engineering, University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Slow flow: Exploiting high-speed cameras for accurate and diverse optical flow reference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep video frame interpolation using cyclic frame generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Tung</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gucan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gucan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Moving gradients: a path-based method for plausible image interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Chung</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sdc-net: Video prediction using spatially-displaced convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>Center for Research in Computer Vision</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prediction error as a quality metric for motion and stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disambiguating visual relations using loop constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Klopschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1426" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flowweb: Joint image set alignment by weaving consistent, pixel-wise correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyosha A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1191" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning dense correspondence via 3d-guided cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiimage matching via fast alternating minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4032" to="4040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-quality video view interpolation using a layered representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>C Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="600" to="608" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
