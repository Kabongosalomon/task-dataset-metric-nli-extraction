<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning Face Representation by Joint Identification-Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning Face Representation by Joint Identification-Verification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset <ref type="bibr" target="#b10">[11]</ref>, 99.15% face verification accuracy is achieved. Compared with the best deep learning result [21] on LFW, the error rate has been significantly reduced by 67%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Faces of the same identity could look much different when presented in different poses, illuminations, expressions, ages, and occlusions. Such variations within the same identity could overwhelm the variations due to identity differences and make face recognition challenging, especially in unconstrained conditions. Therefore, reducing the intra-personal variations while enlarging the inter-personal differences is an eternal topic in face recognition. It can be traced back to early subspace face recognition methods such as LDA <ref type="bibr" target="#b0">[1]</ref>, Bayesian face <ref type="bibr" target="#b16">[17]</ref>, and unified subspace <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. For example, LDA approximates inter-and intra-personal face variations by using two linear subspaces and finds the projection directions to maximize the ratio between them. More recent studies have also targeted the same goal, either explicitly or implicitly. For example, metric learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15]</ref> maps faces to some feature representation such that faces of the same identity are close to each other while those of different identities stay apart. However, these models are much limited by their linear nature or shallow structures, while inter-and intra-personal variations are complex, highly nonlinear, and observed in high-dimensional image space.</p><p>In this work, we show that deep learning provides much more powerful tools to handle the two types of variations. Thanks to its deep architecture and large learning capacity, effective features for face recognition can be learned through hierarchical nonlinear mappings. We argue that it is essential to learn such features by using two supervisory signals simultaneously, i.e. the face identification and verification signals, and the learned features are referred to as Deep IDentification-verification features (DeepID2). Identification is to classify an input image into a large number of identity classes, while verification is to classify a pair of images as belonging to the same identity or not (i.e. binary classification). In the training stage, given an input face image with the identification signal, its DeepID2 features are extracted in the top hidden layer of the learned hierarchical nonlinear feature representation, and then mapped to one of a large number of identities through another function g(DeepID2). In the testing stage, the learned DeepID2 features can be generalized to other tasks (such as face verification) and new identities unseen in the training data. The identification supervisory signal tend to pull apart DeepID2 of different identities since they have to be classified into different classes. Therefore, the learned features would have rich identity-related or interpersonal variations. However, the identification signal has a relatively weak constraint on DeepID2 extracted from the same identity, since dissimilar DeepID2 could be mapped to the same identity through function g(Â·). This leads to problems when DeepID2 features are generalized to new tasks and new identities in test where g is not applicable anymore. We solve this by using an additional face verification signal, which requires that every two DeepID2 vectors extracted from the same identity are close to each other while those extracted from different identities are kept away. The strong per-element constraint on DeepID2 can effectively reduce the intra-personal variations. On the other hand, using the verification signal alone (i.e. only distinguishing a pair of DeepID2 at a time) is not as effective in extracting identity-related features as using the identification signal (i.e. distinguishing thousands of identities at a time). Therefore, the two supervisory signals emphasize different aspects in feature learning and should be employed together.</p><p>To characterize faces from different aspects, complementary DeepID2 features are extracted from various face regions and resolutions, and are concatenated to form the final feature representation after PCA dimension reduction. Since the learned DeepID2 features are diverse among different identities while consistent within the same identity, it makes the following face recognition easier. Using the learned feature representation and a recently proposed face verification model <ref type="bibr" target="#b2">[3]</ref>, we achieved the highest 99.15% face verification accuracy on the challenging and extensively studied LFW dataset <ref type="bibr" target="#b10">[11]</ref>. This is the first time that a machine provided with only the face region achieves an accuracy on par with the 99.20% accuracy of human to whom the entire LFW face image including the face region and large background area are presented to verify.</p><p>In recent years, a great deal of efforts have been made for face recognition with deep learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref>. Among the deep learning works, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8]</ref> learned features or deep metrics with the verification signal, while <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> learned features with the identification signal and achieved accuracies around 97.45% on LFW. Our approach significantly improves the state-ofthe-art. The idea of jointly solving the classification and verification tasks was applied to general object recognition <ref type="bibr" target="#b15">[16]</ref>, with the focus on improving classification accuracy on fixed object classes instead of hidden feature representations. Our work targets on learning features which can be well generalized to new classes (identities) and the verification task, while the classification accuracy on identities in the training set is not crucial for us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Identification-verification guided deep feature learning</head><p>We learn features with variations of deep convolutional neural networks (deep ConvNets) <ref type="bibr" target="#b12">[13]</ref>. The convolution and pooling operations in deep ConvNets are specially designed to extract visual features hierarchically, from local low-level features to global high-level ones. Our deep ConvNets take similar structures as in <ref type="bibr" target="#b20">[21]</ref>. It contains four convolutional layers, the first three of which are followed by max-pooling. To learn a diverse number of high-level features, we do not require weight-sharing on the entire feature map in higher convolutional layers <ref type="bibr" target="#b9">[10]</ref>. Specifically, in the third convolutional layer of our deep ConvNets, neuron weights are locally shared in every 2 Ã 2 local regions. In the fourth convolutional layer, which is more appropriately called a locally-connected layer, weights are totally unshared between neurons. The ConvNet extracts a 160-dimensional DeepID2 vector at its last layer of the feature extraction cascade. The DeepID2 layer to be learned are fully-connected to both the third and fourth convolutional layers. Since the fourth convolutional layer extracts more global features than the third one, the DeepID2 layer takes multi-scale features as input, forming the so called multi-scale ConvNets <ref type="bibr" target="#b18">[19]</ref>. We use rectified linear units (ReLU) <ref type="bibr" target="#b17">[18]</ref> for neurons in the convolutional layers and the DeepID2 layer. ReLU has better fitting abilities than the sigmoid units for large training datasets <ref type="bibr" target="#b11">[12]</ref>. An illustration of the ConvNet structure used to extract DeepID2 is shown in <ref type="figure" target="#fig_0">Fig. 1</ref> given an RGB input of size 55 Ã 47. When the size of the input region changes, the map sizes in the following layers will change accordingly. The DeepID2 extraction process is denoted as f = Conv(x, Î¸ c ), where Conv(Â·) is the feature extraction function defined by the ConvNet, x is the input face patch, f is the extracted DeepID2 vector, and Î¸ c denotes ConvNet parameters to be learned.</p><p>DeepID2 features are learned under two supervisory signals. The first is face identification signal, which classifies each face image into one of n (e.g., n = 8192) different identities. Identification is achieved by following the DeepID2 layer with an n-way softmax layer, which outputs a probability distribution over the n classes. The network is trained to minimize the cross-entropy loss, which we call the identification loss. It is denoted as</p><formula xml:id="formula_0">Ident(f, t, Î¸ id ) = â n i=1 âp i logp i = â logp t ,<label>(1)</label></formula><p>where f is the DeepID2 vector, t is the target class, and Î¸ id denotes the softmax layer parameters. p i is the target probability distribution, where p i = 0 for all i except p t = 1 for the target class t.p i is the predicted probability distribution. To correctly classify all the classes simultaneously, the DeepID2 layer must form discriminative identity-related features (i.e. features with large interpersonal variations). The second is face verification signal, which encourages DeepID2 extracted from faces of the same identity to be similar. The verification signal directly regularize DeepID2 and can effectively reduce the intra-personal variations. Commonly used constraints include the L1/L2 norm and cosine similarity. We adopt the following loss function based on the L2 norm, which was originally proposed by Hadsell et al. <ref type="bibr" target="#b6">[7]</ref> for dimensionality reduction,</p><formula xml:id="formula_1">Verif(f i , f j , y ij , Î¸ ve ) = 1 2 f i â f j 2 2 if y ij = 1 1 2 max 0, m â f i â f j 2 2 if y ij = â1 ,<label>(2)</label></formula><p>where f i and f j are DeepID2 vectors extracted from the two face images in comparison. y ij = 1 means that f i and f j are from the same identity. In this case, it minimizes the L2 distance between the two DeepID2 vectors. y ij = â1 means different identities, and Eq. (2) requires the distance larger than a margin m. Î¸ ve = {m} is the parameter to be learned in the verification loss function. Loss functions based on the L1 norm could have similar formulations <ref type="bibr" target="#b15">[16]</ref>. The cosine similarity was used in <ref type="bibr" target="#b17">[18]</ref> as</p><formula xml:id="formula_2">Verif(f i , f j , y ij , Î¸ ve ) = 1 2 (y ij â Ï(wd + b)) 2 ,<label>(3)</label></formula><p>where d = fiÂ·fj fi 2 fj 2 is the cosine similarity between DeepID2 vectors, Î¸ ve = {w, b} are learnable scaling and shifting parameters, Ï is the sigmoid function, and y ij is the binary target of whether the two compared face images belong to the same identity. All the three loss functions are evaluated and compared in our experiments.</p><p>Our goal is to learn the parameters Î¸ c in the feature extraction function Conv(Â·), while Î¸ id and Î¸ ve are only parameters introduced to propagate the identification and verification signals during training. In the testing stage, only Î¸ c is used for feature extraction. The parameters are updated by stochastic gradient descent. The identification and verification gradients are weighted by a hyperparameter Î». The margin m in Eq. (2) is a special case, which cannot be updated by gradient descent since its gradient would always be nonnegative. Instead, we adaptively update m during training such that </p><formula xml:id="formula_3">input: training set Ï = {(x i , l i )}, initialized parameters Î¸ c , Î¸ id , and Î¸ ve , hyperparame- ter Î», learning rate Î·(t), t â 0 while not converge do t â t + 1 sample two training samples (x i , l i ) and (x j , l j ) from Ï f i = Conv(x i , Î¸ c ) and f j = Conv(x j , Î¸ c ) âÎ¸ id = âIdent(fi,li,Î¸ id ) âÎ¸ id + âIdent(fj ,lj ,Î¸ id ) âÎ¸ id âÎ¸ ve = Î» Â· âVerif(fi,fj ,yij ,Î¸ve) âÎ¸ve</formula><p>, where y ij = 1 if l i = l j , and y ij = â1 otherwise. it is the threshold that gives the lowest verification error on recent training samples. Our learning algorithm is summarized in Tab. 1.</p><formula xml:id="formula_4">âf i = âIdent(fi,li,Î¸ id ) âfi + Î» Â· âVerif(fi,fj ,yij ,Î¸ve) âfi âf j = âIdent(fj ,lj ,Î¸ id ) âfj + Î» Â· âVerif(fi,fj ,yij ,Î¸ve) âfj âÎ¸ c = âf i Â· âConv(xi,Î¸c) âÎ¸c + âf j Â· âConv(xj ,Î¸c) âÎ¸c update Î¸ id = Î¸ id â Î·(t) Â· Î¸ id , Î¸ ve = Î¸ ve â Î·(t) Â· Î¸ ve , and Î¸ c = Î¸ c â Î·(t) Â· Î¸ c . end while output Î¸ c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Face Verification</head><p>To evaluate the feature learning algorithm described in Sec. 2, DeepID2 features are embedded into the conventional face verification pipeline of face alignment, feature extraction, and face verification. We first use the recently proposed SDM algorithm <ref type="bibr" target="#b24">[25]</ref> to detect 21 facial landmarks. Then the face images are globally aligned by similarity transformation according to the detected landmarks. We cropped 400 face patches, which vary in positions, scales, color channels, and horizontal flipping, according to the globally aligned faces and the position of the facial landmarks. Accordingly, 400 DeepID2 vectors are extracted by a total of 200 deep ConvNets, each of which is trained to extract two 160-dimensional DeepID2 vectors on one particular face patch and its horizontally flipped counterpart, respectively, of each face.</p><p>To reduce the redundancy among the large number of DeepID2 features and make our system practical, we use the forward-backward greedy algorithm <ref type="bibr" target="#b25">[26]</ref> to select a small number of effective and complementary DeepID2 vectors (25 in our experiment), which saves most of the feature extraction time during test. <ref type="figure" target="#fig_1">Fig. 2</ref> shows all the selected 25 patches, from which 25 160-dimensional DeepID2 vectors are extracted and are concatenated to a 4000-dimensional DeepID2 vector. The 4000-dimensional vector is further compressed by PCA for face verification.</p><p>We learned the Joint Bayesian model <ref type="bibr" target="#b2">[3]</ref> for face verification based on the extracted DeepID2. Joint Bayesian has been successfully used to model the joint probability of two faces being the same or different persons <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. It models the feature representation f of a face as the sum of inter-and intra-personal variations, or f = Âµ + , where both Âµ and are modeled as Gaussian distributions and are estimated from the training data. Face verification is achieved through log-likelihood ratio test, log P (f1,f2|Hinter) P (f1,f2|Hintra) , where the numerator and denominator are joint probabilities of two faces given the inter-or intra-personal variation hypothesis, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We report face verification results on the LFW dataset <ref type="bibr" target="#b10">[11]</ref>, which is the de facto standard test set for face verification in unconstrained conditions. It contains 13, 233 face images of 5749 identities collected from the Internet. For comparison purposes, algorithms typically report the mean face verification accuracy and the ROC curve on 6000 given face pairs in LFW. Though being sound as a test set, it is inadequate for training, since the majority of identities in LFW have only one face image. Therefore, we rely on a larger outside dataset for training, as did by all recent highperformance face verification algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14]</ref>. In particular, we use the CelebFaces+ dataset <ref type="bibr" target="#b20">[21]</ref> for training, which contains 202, 599 face images of 10, 177 identities (celebrities) collected from the Internet. People in CelebFaces+ and LFW are mutually exclusive. DeepID2 features are learned from the face images of 8192 identities randomly sampled from CelebFaces+ (referred to as CelebFaces+A), while the remaining face images of 1985 identities (referred to as CelebFaces+B) are used for the following feature selection and learning the face verification models (Joint Bayesian). When learning DeepID2 on CelebFaces+A, CelebFaces+B is used as a validation set to decide the learning rate, training epochs, and hyperparameter Î». After that, CelebFaces+B is separated into a training set of 1485 identities and a validation set of 500 identities for feature selection. Finally, we train the Joint Bayesian model on the entire CelebFaces+B data and test on LFW using the selected DeepID2. We first evaluate various aspect of feature learning from Sec. 4.1 to Sec. 4.3 by using a single deep ConvNet to extract DeepID2 from the entire face region. Then the final system is constructed and compared with existing best performing methods in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Balancing the identification and verification signals</head><p>We investigates the interactions of identification and verification signals on feature learning, by varying Î» from 0 to +â. At Î» = 0, the verification signal vanishes and only the identification signal takes effect. When Î» increases, the verification signal gradually dominates the training process. At the other extreme of Î» â +â, only the verification signal remains. The L2 norm verification loss in Eq. (2) is used for training. <ref type="figure" target="#fig_3">Figure 3</ref> shows the face verification accuracy on the test set by comparing the learned DeepID2 with L2 norm and the Joint Bayesian model, respectively. It clearly shows that neither the identification nor the verification signal is the optimal one to learn features. Instead, effective features come from the appropriate combination of the two. This phenomenon can be explained from the view of inter-and intra-personal variations, which could be approximated by LDA. According to LDA, the inter-personal scatter matrix is</p><formula xml:id="formula_5">S inter = c i=1 n i Â· (x i âx) (x i âx)</formula><p>, wherex i is the mean feature of the i-th identity,x is the mean of the entire dataset, and n i is the number of face images of the i-th identity. The intra-personal scatter When only the identification signal is used (Î» = 0), the learned features contain both diverse interand intra-personal variations, as shown by the long tails of the red curves in both figures. While diverse inter-personal variations help to distinguish different identities, large and diverse intrapersonal variations are noises and makes face verification difficult. When both the identification and verification signals are used with appropriate weighting (Î» = 0.05), the diversity of the interpersonal variations keeps unchanged while the variations in a few main directions become even larger, as shown by the green curve in the left compared to the red one. At the same time, the intra-personal variations decrease in both the diversity and magnitude, as shown by the green curve in the right. Therefore, both the inter-and intra-personal variations changes in a direction that makes face verification easier. When Î» further increases towards infinity, both the inter-and intra-personal variations collapse to the variations in only a few main directions, since without the identification signal, diverse features cannot be formed. With low diversity on inter-personal   variations, distinguishing different identities becomes difficult. Therefore the performance degrades significantly. <ref type="figure" target="#fig_6">Figure 6</ref> shows the first two PCA dimensions of features learned with Î» = 0, 0.05, and +â, respectively. These features come from the six identities with the largest numbers of face images in LFW, and are marked by different colors. The figure further verifies our observations. When Î» = 0 (left), different clusters are mixed together due to the large intra-personal variations, although the cluster centers are actually different. When Î» increases to 0.05 (middle), intra-personal variations are significantly reduced and the clusters become distinguishable. When Î» further increases towards infinity (right), although the intra-personal variations further decrease, the cluster centers also begin to collapse and some clusters become significantly overlapped (as the red, blue, and cyan clusters in <ref type="figure" target="#fig_6">Fig. 6 right)</ref>, making it hard to distinguish again.</p><formula xml:id="formula_6">matrix is S intra = c i=1 xâDi (x âx i ) (x âx i ) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rich identity information improves feature learning</head><p>We investigate how would the identity information contained in the identification supervisory signal influence the learned features. In particular, we experiment with an exponentially increasing number of identities used for identification during training from 32 to 8192, while the verification signal is generated from all the 8192 training identities all the time. <ref type="figure" target="#fig_4">Fig. 4</ref> shows how the verification accuracies of the learned DeepID2 (derived from the L2 norm and Joint Bayesian) vary on the test set with the number of identities used in the identification signal. It shows that identifying a  large number (e.g., 8192) of identities is key to learning effective DeepID2 representation. This observation is consistent with those in Sec. 4.1. The increasing number of identities provides richer identity information and helps to form DeepID2 with diverse inter-personal variations, making the class centers of different identities more distinguishable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Investigating the verification signals</head><p>As shown in Sec. 4.1, the verification signal with moderate intensity mainly takes the effect of reducing the intra-personal variations. To further verify this, we compare our L2 norm verification signal on all the sample pairs with those only constrain either the positive or negative sample pairs, denoted as L2+ and L2-, respectively. That is, the L2+ only decreases the distances between DeepID2 of the same identity, while L2-only increases the distances between DeepID2 of different identities if they are smaller than the margin. The face verification accuracies of the learned DeepID2 on the test set, measured by the L2 norm and Joint Bayesian respectively, are shown in <ref type="table" target="#tab_1">Table 2</ref>. It also compares with the L1 norm and cosine verification signals, as well as no verification signal (none). The identification signal is the same (classifying the 8192 identities) for all the comparisons.</p><p>DeepID2 features learned with the L2+ verification signal are only slightly worse than those learned with L2. In contrast, the L2-verification signal helps little in feature learning and gives almost the same result as no verification signal is used. This is a strong evidence that the effect of the verification signal is mainly reducing the intra-personal variations. Another observation is that the face verification accuracy improves in general whenever the verification signal is added in addition to the identification signal. However, the L2 norm is better than the other compared verification metrics. This may be due to that all the other constraints are weaker than L2 and less effective in reducing the intra-personal variations. For example, the cosine similarity only constrains the angle, but not the magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Final system and comparison with other methods</head><p>Before learning Joint Bayesian, DeepID2 features are first projected to a low dimensional feature space by PCA. After PCA, the Joint Bayesian model is trained on the entire CelebFaces+B data and tested on the 6000 given face pairs in LFW, where the log-likelihood ratio given by Joint Bayesian is compared to a threshold optimized on the training data for face verification. Tab. 3 shows the face verification accuracy with an increasing number of face patches to extract DeepID2, as well as the time used to extract those DeepID2 features from each face with a single Titan GPU. We achieve 98.97% accuracy with all the 25 selected face patches and 180-dimensional DeepID2 features after PCA 1 . The feature extraction process is also efficient and takes only 35 ms for each face image.  high-dim LBP <ref type="bibr" target="#b3">[4]</ref> 95.17 Â± 1.13 TL Joint Bayesian <ref type="bibr" target="#b1">[2]</ref> 96.33 Â± 1.08 DeepFace <ref type="bibr" target="#b21">[22]</ref> 97.35 Â± 0.25 DeepID <ref type="bibr" target="#b20">[21]</ref> 97.45 Â± 0.26 GaussianFace <ref type="bibr" target="#b13">[14]</ref> 98.52 Â± 0.66 DeepID2 99.15 Â± 0.13 To further exploit the rich pool of DeepID2 features extracted from the large number of patches. We repeat the feature selection algorithm for another six times, each time choosing DeepID2 from the patches that have not been selected by previous feature selection steps. Then we learn the Joint Bayesian model on each of the seven groups of selected features, respectively. We fuse the seven Joint Bayesian scores on each pair of compared faces by further learning an SVM. In this way, we achieve an even higher 99.15% face verification accuracy. The accuracy and ROC comparison with previous state-of-the-art methods on LFW are shown in Tab. 4 and <ref type="figure" target="#fig_7">Fig. 7</ref>, respectively. We achieve the best results and improve previous results with a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper have shown that the effect of the face identification and verification supervisory signals on deep feature representation coincide with the two aspects of constructing ideal features for face recognition, i.e., increasing inter-personal variations and reducing intra-personal variations, and the combination of the two supervisory signals lead to significantly better features than either one of them. When embedding the learned features to the traditional face verification pipeline, we achieved an extremely effective system with 99.15% face verification accuracy on LFW.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The ConvNet structure for DeepID2 extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Patches selected for feature extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where D i is the set of features of the i-th identity,x i is the corresponding mean, and c is the number of different identities. The inter-and intra-personal variances are the eigenvalues of the corresponding scatter matrices, and are shown inFig. 5. The corresponding eigenvectors represent different variation patterns. Both the magnitude and diversity of feature variances matter in recognition. If all the feature variances concentrate on a small number of eigenvectors, it indicates the diversity of intra-or inter-personal variations is low. The features are learned with Î» = 0, 0.05, and +â, respectively. The feature variances of each given Î» are normalized by the corresponding mean feature variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Face verification accuracy by varying the weighting parameter Î». Î» is plotted in log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Face verification accuracy of DeepID2 learned by both the the face identification and verification signals, where the number of training identities (shown in log scale) used for face identification varies. The result may be further improved with more than 8192 identities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Spectrum of eigenvalues of the inter-and intra-personal scatter matrices. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>The first two PCA dimensions of DeepID2 extracted from six identities in LFW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>ROC comparison with the previous best results on LFW. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The DeepID2 learning algorithm.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different verification signals.</figDesc><table><row><cell>verification signal</cell><cell>L2</cell><cell>L2+</cell><cell>L2-</cell><cell>L1</cell><cell>cosine none</cell></row><row><cell>L2 norm (%)</cell><cell cols="5">94.95 94.43 86.23 92.92 87.07 86.43</cell></row><row><cell cols="6">Joint Bayesian (%) 95.12 94.87 92.98 94.13 93.38 92.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Face verification accuracy with DeepID2 extracted from an increasing number of face patches.</figDesc><table><row><cell># patches</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>25</cell></row><row><cell cols="7">accuracy (%) 95.43 97.28 97.75 98.55 98.93 98.97</cell></row><row><cell>time (ms)</cell><cell>1.7</cell><cell>3.4</cell><cell>6.1</cell><cell>11</cell><cell>23</cell><cell>35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy comparison with the previous best results on LFW.</figDesc><table><row><cell>method</cell><cell>accuracy (%)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The best PCA dimension is found by cross-validation on CelebFaces+B. The performance keeps almost the same from 150 to 250 dimensions.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A practical transfer learning algorithm for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Is that you? Metric learning approaches for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Large scale strongly supervised ensemble metric learning, with applications to face verification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno>TR115</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">NEC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning hierarchical representations for face verification with convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Labeled Faces in the Wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07- 49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Surpassing human-level face verification performance on LFW with GaussianFace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3840</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PCCA: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bayesian face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>PR</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1771" to="1782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multi-scale convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hybrid deep learning for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unified subspace analysis for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A unified framework for subspace face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1222" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre Frade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive forward-backward greedy algorithm for learning sparse representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theor</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="4689" to="4708" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning identity-preserving face space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
