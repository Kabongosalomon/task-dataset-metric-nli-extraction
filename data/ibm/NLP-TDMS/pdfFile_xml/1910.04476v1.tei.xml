<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Super-Resolution via Attention based Back Projection Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University Hung Hom</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University Hung Hom</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Tak</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University Hung Hom</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University Hung Hom</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yui-Lam</forename><surname>Chan</surname></persName>
							<email>enylchan@polyu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong Polytechnic University Hung Hom</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Super-Resolution via Attention based Back Projection Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning based image Super-Resolution (SR) has shown rapid development due to its ability of big data digestion. Generally, deeper and wider networks can extract richer feature maps and generate SR images with remarkable quality. However, the more complex network we have, the more time consumption is required for practical applications. It is important to have a simplified network for efficient image SR. In this paper, we propose an Attention based Back Projection Network (ABPN) for image superresolution. Similar to some recent works, we believe that the back projection mechanism can be further developed for SR. Enhanced back projection blocks are suggested to iteratively update low-and high-resolution feature residues. Inspired by recent studies on attention models, we propose a Spatial Attention Block (SAB) to learn the cross-correlation across features at different layers. Based on the assumption that a good SR image should be close to the original LR image after down-sampling. We propose a Refined Back Projection Block (RBPB) for final reconstruction. Extensive experiments on some public and AIM2019 Image Super-Resolution Challenge [4] datasets show that the proposed ABPN can provide state-of-the-art or even better performance in both quantitative and qualitative measurements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As a fundamental low-level vision problem, image super-resolution (SR) attracts much attention in the past few years. The objective of image SR is to super-resolve lowresolution (LR) images to the desired dimension as the same high-resolution (HR) images with pleasing visual quality. For α× image SR, we need to approximate α × α times pixels for up-sampling. Thanks to the architectural innovations and computation advances, it is possible to utilize larger datasets and more complex models for image SR. Various deep learning based approaches with different network architectures have achieved image SR with good qual-  <ref type="figure">Figure 1</ref>. SR results on image HinagikuKenzan with SR factor 16. We applied 2 times of 4× SR ity. Most SR works are based on the residual mapping modified from ResNet <ref type="bibr" target="#b12">[12]</ref>. In order to deliver good superresolution quality, we need to build a very deep network to cover receptive fields of the image as large as possible to learn different levels of feature abstrction. The advent of 4K/8K UHD (Ultra High Definition) displays demand for more accurate image SR with less computation at different up-sampling factors. It is essential to have a deep neural network with the ability to capture long-term dependencies to efficiently learn the reconstruction mapping for SR. Attention or non-local modeling is one of the choices to globally capture the feature response across the whole image. A lot of related works <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b4">5]</ref> have been proposed for computing vision successfully. There are several advantages of using attention operations: 1) It can directly compute the correlation between patterns across the image regardless of their distances; 2) It can efficiently reduce the number of kernels and depth of the network to achieve comparable or even better performance and 3) Finally, it is also easy to be embedded into any structure for operations. As shown in <ref type="figure">Figure 1</ref>, we tested the state-ofthe-art SR approaches on 16× enlargement by applying two times of 4× SR using pre-trained models. ESRGAN <ref type="bibr" target="#b29">[28]</ref> and RCAN <ref type="bibr" target="#b32">[31]</ref> tend to generate fake edges which do not exist in the HR images while the proposed ABPN can still predict correct patterns.</p><p>Inspired by Non-local neural networks <ref type="bibr" target="#b28">[27]</ref> and Back Projection based image SR <ref type="bibr" target="#b21">[20]</ref>, we propose an Attention based Back Projection Network (ABPN) for efficient image SR. Our method focuses on studying the global feature correlation to make full use of non-local mean operation. Specifically, instead of using plain concatenation or addition operations, we propose the Spatial Attention Block (SAB) to compute the auto-and cross-correlation of the feature maps extracted at different levels. That is, we use proposed SAB to measure the similarity between two feature maps to obtain the global correlation maps. By further investigating the SR methods, we find that back projection based network is a better choice for the backbone of feature extraction because it can iteratively up-and downsample the feature maps to update the residues of LR and HR features. To make a step forward, we propose a Refined Back Projection Block (RBPB) as the final stage to directly minimize the residues between the original LR images and down-sampled predicted SR images.</p><p>We summarize our contributions as follows: 1) By making use of the proposed Spatial Attention Block, we modified the back projection network to Attention based Back Projection Network (ABPN) for efficient single image super-resolution. <ref type="bibr" target="#b1">(2)</ref> We propose a Refined Back Projection Block (RBPB) to replace the common post back projection process in image SR. (3) We tested our proposed SR method on various datasets and real images. Extensive experiments show that the ABPN can achieve the state-of-theart SR or even better performance both quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Non-local Image Processing. Non-local mean is a conventional algorithm for image processing. The idea is that it searches not only the local areas but also the non-local areas for repeated patterns. It allows distant pixels or patches to contribute to the filtered region. The idea is generalized as a non-local convolution operation which maps the neighborhood region to the whole region of images or videos. It is commonly used in image denoising <ref type="bibr" target="#b5">[6]</ref>, inpainting <ref type="bibr" target="#b1">[2]</ref> and super-resolution <ref type="bibr" target="#b9">[10]</ref>.</p><p>Nowadays, non-local processing is also explicitly or implicitly embedded into deep neural networks to capture the long-term dependencies. In most deep learning algorithms, stacking more and more convolution operations with small kernels (e.g. 3×3) can cover a larger receptive field for global modeling. This repeated local operation has the limitations of 1) inefficient computation for practical applications, 2) difficulty in optimizing networks and 3) a feedforward operation without feedback. Recurrent Neural Net-works (RNN) <ref type="bibr" target="#b30">[29]</ref> are the dominant approaches for sequential data by forming a close loop to progressively process the data. However, it still works on a local neighborhood and its performance is not optimal. Recently, there is a trend of using self-attention <ref type="bibr" target="#b27">[26]</ref> or non-local neural network <ref type="bibr" target="#b28">[27]</ref> for modeling the sequential data in language and images. Note that in this paper, we use the term "attention" to describe the non-local modeling process in deep feature extraction. There are several great works on making use of attention mechanism in computing vision. <ref type="bibr" target="#b27">[26]</ref> first proposed selfattention for machine translation. The idea is to decompose each word as a weighted combination of all positions in the sequence. That is, the model looks into onward and backward words to ensure the consistency of the translation. Similar self-attention based works were proposed in various computing fields. For example, <ref type="bibr" target="#b28">[27]</ref> proposed non-local neural network to investigate the possible solution to spatial attention for video classification. <ref type="bibr" target="#b15">[15]</ref> proposed an efficient attention computation mechanism called Criss-Cross Network for semantic segmentation. <ref type="bibr" target="#b4">[5]</ref> used the idea of bilateral filter to learn robust weighting model for object recognition. Besides, "attention" has also been proposed for image super-resolution and shown its great potential. For example, inspired by the squeeze and excitation network <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b32">[31]</ref> proposed to model the channel correlation by residual channel attention network. <ref type="bibr" target="#b6">[7]</ref> further modified the idea of channel attention to second-order attention enhancement. However, these approaches still do not fully explore the non-local property in the spatial domain. Hence, there is a great potential for further study.</p><p>Super-Resolution Deep Neural Networks. In the past few years, deep neural networks have shown remarkable ability on image SR. From the beginning of the pioneer work <ref type="bibr" target="#b7">[8]</ref>, CNN has outperformed conventional learning approaches significantly. The capabilities of resolving complex nonlinear mapping models and digestion on huge datasets encourage researchers to design deeper networks for better performance. Most of the state-of-the-art SR approaches adopt the residual architecture, like SRGAN <ref type="bibr" target="#b18">[18]</ref>, EDSR <ref type="bibr" target="#b20">[19]</ref>, DenseSR <ref type="bibr" target="#b33">[32]</ref> and ESRGAN <ref type="bibr" target="#b29">[28]</ref>. There are also some SR approaches that have different architectures for reconstruction. For example, <ref type="bibr" target="#b26">[25]</ref> proposed the Pix-elCNN for image reconstruction. <ref type="bibr" target="#b23">[22]</ref> proposed to use recursive neural network to iteratively predict the SR image. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">20]</ref> proposed to embed the back projection into the super-resolution to update the LR and HR feature residual. This can be considered as a generalized residual model.</p><p>Recently, using generative adversarial networks (GAN) for perceptual image SR attracts a lot of attention. The idea is to add one discriminator as an indicator for SR estimation. The backbones for generator and discriminator are more or less the same as aforementioned SR algorithms. A better architecture can further improve the perceptual quality. Once the training is finished, we only need the generator for testing. It is important to make sure the model complexity of the generator to be as small as possible for real-time applications. In this paper, we have not investigated our proposed SR method on perceptual quality but it can be modified as the generator for efficient recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Let us formally define the image SR. Mathematically, given a LR image X ∈ R m×n×3 down-sampled from the corresponding HR image Y ∈ R αm×αn×3 , where (m, n) is the dimension the image and α is the up-sampling factor. They are related by th following degradation model,</p><formula xml:id="formula_0">X = DY + µ<label>(1)</label></formula><p>where µ is the additive white Gaussian noise and D is the down-sampling operator. The goal of image SR is to resolve Equation 1 as Maximum A Posterior (MAP) problem as follows,</p><formula xml:id="formula_1">Ŷ = arg max Y logp(X|Y) + logp(Y)<label>(2)</label></formula><p>whereŶ is the predicted SR image. logp(X|Y) represents the log-likelihood of LR images given HR images and logp(Y) is the prior of HR images that is used for model optimization. Formally, we resolve the image SR problem as follows,</p><formula xml:id="formula_2">min θ Y −Ŷ r s.t.Ŷ = arg min Y 1 2 X − DY 2 + λΩ(Y) (3)</formula><p>where * r represents the r -th order estimation of pixel based distortion. The regularization term Ω(Y) controls the complexity of the model. Using external or internal images, we can form LR-HR image pairs to train the proposed Attention based Back Projection Network (ABPN) model to approximate the ideal mapping model. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the complete structure of ABPN contains three basic modules: Feature extraction, Enhanced Back Projection Blocks and Refined Back Projection Block. Feature extraction includes two convolution layers and followed by a selfattention block as a global weighting process. Enhanced Back Projection Blocks are modified from <ref type="bibr" target="#b21">[20]</ref> and the difference are twofold: 1) the concatenation layer is replaced by the proposed Spatial Attention Block and 2), the LR feature maps are combined with HR feature map together to form the final feature maps. Finally, the Refined Back Projection Block updates the feature residues between the estimated and original LR images to refine the final SR image. The detailed structure is discussed in the following parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Back Projection Blocks for image SR</head><p>The Back Projection block was first proposed in DBPN <ref type="bibr" target="#b10">[11]</ref> and the further modified version is formed in HBPN <ref type="bibr" target="#b21">[20]</ref>. Let us see <ref type="figure" target="#fig_1">Figure 3</ref>, the idea of back projection is based on the assumption that a good SR image should have an estimated LR image that is as close as possible to the original LR image. We follow the same idea to build our basic module entitled as Enhanced Down-sampling Back Projection blocks (EDBP) for down-sampling and Enhanced Up-sampling Back Projection block (EUBP) for upsampling. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, We stack multiple back projection blocks in up-down order to extract deep feature representation. For the final reconstruction, the intermediate feature maps are concatenated together to learn the SR images. The only structural difference between <ref type="bibr" target="#b21">[20]</ref> and ours is that we also concatenate the LR feature maps together (yellow lines shown in <ref type="figure" target="#fig_0">Figure 2</ref>) with HR feature maps for final reconstruction. Note that since the LR feature maps are α× smaller than HR, we use one deconvolution layer to up-sample them to the same size as the HR feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Spatial Attention Blocks (SAB)</head><p>Spatial Attention Blocks are the major contribution of this work. The idea is to learn cross-correlation between features at different levels. In the proposed ABPN network, we have two types of attention blocks: self-attention blocks and spatial attention blocks. The self-attention block is exactly the same as the one in <ref type="bibr" target="#b27">[26]</ref> that is situated at the end of the feature extraction (the pink block in <ref type="figure" target="#fig_0">Figure 2</ref>(a)). And the spatial attention block is located at each EDBP block (pink blocks in <ref type="figure" target="#fig_0">Figure 2</ref> with words "SAB") to extract the attention maps for following up-sampling. Their detailed differences are described in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>Inside self-attention and spatial attention blocks, there are three convolution layers that decompose the input data into three components: θ, φ and g. Then two dot product operations are done using two of the three components. There is a short connection between input to the output so the attention models need to learn the residual mapping rela- tionship. The difference is that the self-attention takes only the input X for calculation while the spatial attention block takes both X and Y for calculation. The attention model can be understood as a non-local convolution process. For input X, we can define the nonlocal operation as follows,</p><formula xml:id="formula_3">: n11 : n11 g: n11 3211 X Y softmax : n11 : n11 g: n11 3211 X softmax (b) Spatial Attention Block (a) Self-Attention Block</formula><formula xml:id="formula_4">Z = f (X, X T )g(X)<label>(4)</label></formula><p>where f represents the relationship of each pixel to another on the input image X. Following the description of selfattention, we can further rewrite Equation 4 as,</p><formula xml:id="formula_5">Z = sof tmax (θ(X) φ(X T ))g(X)<label>(5)</label></formula><p>Similarly, for spatial attention block, we can write it as,</p><formula xml:id="formula_6">Z = sof tmax (θ(X)) φ(X T ))g(Y)<label>(6)</label></formula><p>The non-local operation in both self-attention and spatial attention consider all positions on the feature maps. The dot product of θ(X)φ(X T ) can be regarded as the covariance of the input data. It measures the degree of tendency between two feature maps at different channels. A convolution operation or channel attention model <ref type="bibr" target="#b32">[31]</ref> can only sum up the weighted input in a local region while the attention model can compute the whole data, It can be also related to the Principal Component Analysis (PCA). As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, input X is decomposed into θ(X) and φ(X T ). Then we vectorize the feature maps along the channel dimension so that i-th vector represents the feature map at i-th channel. Their dot products calculate the autocorrelation of the input data. Using Softmax operation can normalize each of the vectors to become a unit vector. Once this is done, each of the unit vector can be interpreted as an axis of the input data. Multiplying g(X) to the normalized vectors can be considered as projecting data to a new coordinate system. The output of Softmax can be called the global weighting matrix that measures the importance of each feature map. Note that the goal of PCA is to reduce the dimension of data so it calculates the statistical correlation of a group of data and find the eigenvectors to project all the data with maximum variance. However, the self-attention and spatial attention focus on finding the principal features across the whole spatial domain so that they calculate the feature correlation across the channel domain and find the basis for projection. Generally, most deep learning based SR approaches concatenate feature maps from different layers to form a large feature map for next operation. In order to reduce the computation, a 1 × 1 convolution is used to globally weight all feature maps to output one compressed result. The disadvantage is that when the model goes deep, the more feature maps we concatenate and the heavier computation we need to cost on the 1 × 1 convolution. It is difficult to train global weighting to obtain optimal results. On the contrary, using spatial attention blocks can enhance the correlation of feature maps from different layers because the feature maps are not equally important, we only need an attention map to assign the confidence scores to the feature maps for estimation. Importantly, symbols θ, φ and g represent 1×1 convolution operation without using any activation functions because 1) the correlation or covariance is a measure of linear dependence among data. Nonlinear data is more computationally demanding and 2), the input data X are the activated feature maps so there is no need to add another activation operation to increase the training difficulty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Refined Back Projection Block (RBPB)</head><p>Finally, we have modified the Enhanced Back Projection Block to the proposed Refined Back Projection Block (RBPB) for final reconstruction. The detailed structure is shown in <ref type="figure" target="#fig_0">Figure 2d</ref>. The reason is that the EDBP and EUBP blocks are stacked in order to update LR and HR feature residues but they never feedback to the original LR images to simulate the iterative back projection process. To form the close loop the same as <ref type="figure" target="#fig_1">Figure 3</ref>, we use RBPB to connect the input LR image to the final SR image. In most of the SR approaches, researchers assume that the LR image is downsampled by the Bicubic operator so we also use Bicubic to down-sample the estimated SR image to obtain the estimated LR. Then we estimate the LR residues between estimated LR and input LR images by using another feature extraction block (the purple box at the top of <ref type="figure" target="#fig_0">Figure 2</ref>). Finally, we up-sample the LR residues by Bicubic and add to the estimate SR to obtain the final SR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Preparation and Network Implementation</head><p>We synthesized the training image pairs based on the settings of AIM2019 SR challenge <ref type="bibr" target="#b3">[4]</ref>. The training images include 800 2K images from DIV2K <ref type="bibr" target="#b25">[24]</ref> and 2650 2K images from Flickr <ref type="bibr" target="#b20">[19]</ref>. Each image was rotated and flipped for augmentation to increase the number of images eight times. The LR images were obtained by using Bicubic function in MATLAB according to down-sampling factors α. We extracted LR-HR patch pairs from images of size 32α×32α and 32×32, respectively. The testing images include Set5 <ref type="bibr" target="#b2">[3]</ref>, Set14 <ref type="bibr" target="#b31">[30]</ref>, BSD100 <ref type="bibr" target="#b0">[1]</ref>, Urban100 <ref type="bibr" target="#b14">[14]</ref>, Manga109 <ref type="bibr" target="#b22">[21]</ref>, DIV2K <ref type="bibr" target="#b25">[24]</ref> and DIV8K <ref type="bibr" target="#b3">[4]</ref> with 4×, 8× and 16× SR enlargement.</p><p>To efficiently super-resolve images, we designed the proposed ABPN network using 32 kernels for all convolution and deconvolution layers. For short connections and atten-tion models, we used 1×1 kernels with stride 1 and pad 1. For the convolution and deconvolution in EDBP and EUBP, we used 6×6 kernels with stride 4 and pad 1 for 4× SR and 10×10 kernels with stride 8 and pad 1 for 8× SR. Note that most SR approaches use 64 kernels for convolution or deconvolution, we only use half of convolution kernels to build the network. With the help of the proposed attention blocks, in the following experiments, we will demonstrate that the proposed ABPN can achieve comparable or even better SR performance with much less convolutional parameters.</p><p>We conducted our experiments using Pytorch 1.1, MAT-LAB R2016b on two NVIDIA GTX1080Ti GPUs. During the training, we set the learning rate to 0.0001 for all layer. The batch size is 8 for 1×10 6 iterations. For optimization, we used Adam with the momentum to 0.9 and the weight decay of 0.0001. The executive codes and experimental results can be found in the following link: https: //github.com/Holmes-Alan/ABPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model analysis</head><p>Attention Back Projection Block. For our proposed ABPN, the attention back projection block replaces the concatenation layer to combine feature maps from different layers. The self-attention is used in the feature extraction and the spatial attention is used after the enhanced downsampling back projection blocks. To demonstrate the capability of the attention models, we design the same ABPN network using concatenation layers as Model-C and the ABPN network using attention layers as Model-A. Depending on the up-sampling factors, we conducted multiple experiments for 2×, 4× and 8× enlargement on Set5 and Set14 to make comparison.</p><p>The results are shown in <ref type="table" target="#tab_1">Table 1</ref>. We compare Model-C and Model-A on SR with different up-sampling factors. Model-A outperforms Model-C about 0.4 dB in PSNR and 0.01 in SSIM. It indicates the effectiveness of using attention over concatenation. Furthermore, to understand the physical meaning of attention models, we visualize the feature maps obtained from EDBP and SAB blocks. The feature maps on the first row of <ref type="figure" target="#fig_3">Figure 5</ref> were used to compute the basis for projection (same as input X in <ref type="figure" target="#fig_2">Figure 4</ref>) and  the feature maps on the second row of <ref type="figure" target="#fig_3">Figure 5</ref> are projected to the basis to obtain the SAB outputs (the third row of <ref type="figure" target="#fig_3">Figure 5</ref>). EDBP n represents the n-th down-sampling back projection blocks. NOte the red boxes on the visualization and we can find that the output of SAB blocks are the weighted results of two EDBP blocks. For example, the red boxes in EDBP 1 are located at the feature maps that estimate the complete image so that the basis can be across the whole frequency band which shows no focus on specific features. However, the feature maps on EDBP 3 only have responses to the edges in the neighborhood area. After the projection, the feature map on the SAB block enhanced the edge information across the whole image which is the purpose of using attention model to find the non-local property for reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Refined Back Projection Block</head><p>For the final reconstruction, we used the proposed Refined Back Projection Block (RBPB) to further improve the SR performance. There are some related deep learning based SR works <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b35">33,</ref><ref type="bibr" target="#b29">28]</ref> that first super-resolve the LR image via the deep networks and then use back projection as the post processing to the obtained SR image for refinement. It can improve the PSNR by about 0.01∼0.1 dB but the problem is the back projection is not connected to the network to form an end-to-end architecture. We directly attached the post back projection at the end of network to jointly train the model for better SR. To make a comparison, we tested ABPN without final back projection (A), ABPN with post back projection (B) and ABPN with RBPB (C) on Set5 and Set14 for 2×, 4× and 8× enlargement.</p><p>The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. We can find that com-pared to model (A), using back projection as a post processing for (B) can help to boost up the PSNR performance. And when we add the Refined Back Projection Block in the network, model (C) can further improve the PSNR about 0.1 dB. Note that the effect of back projection is limited when we super-resolve LR with larger up-sampling factors. For example, in 4× image SR, using RBPB can outperform the model without back projection by about 0.2 dB but the improvement decreases to about 0.1 dB in 8× superresolution. The reason is that the residual information is getting smaller when the down-sampling factor is larger. Using Bicubic as the assumed down-sampling operator may not be sufficient to estimate the ground truth distribution of the LR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the state-of-the-art SR approaches</head><p>To prove the effectiveness of the proposed ABPN network, we conducted experiments by comparing most of (if not all) the state-of-the-art SR algorithms: Bicubic, A+ <ref type="bibr" target="#b24">[23]</ref>, CRFSR <ref type="bibr" target="#b35">[33]</ref>, SRCNN <ref type="bibr" target="#b7">[8]</ref>, LapSRN <ref type="bibr" target="#b17">[17]</ref>, EDSR <ref type="bibr" target="#b20">[19]</ref>, HBPN <ref type="bibr" target="#b21">[20]</ref>, RCAN <ref type="bibr" target="#b32">[31]</ref> and ESRGAN <ref type="bibr" target="#b29">[28]</ref>. PSNR and SSIM were used to evaluate the proposed method and others. Generally, PSNR and SSIM were calculated by converting RGB image to YUV and only the Y-channel image was taken for consideration. During the testing, we flipped and rotated LR images for augmentation to generate several augmented inputs and then applied inverse augmentation and average all the outputs to form the final SR images. For different scaling factors s, we excluded s pixels at boundaries to avoid boundary effect. For these SR results, A+ and CRFSR were provided by the corresponding authors, SRCNN was reimplemented and provided by the authors of <ref type="bibr" target="#b17">[17]</ref>, EDSR, HBPN, RCAN and ESRGAN were reimplemented using the codes that are provided by the corresponding authors. Note that, our proposed approach also participated in the AIM2019 Image Super-resolution Challenge <ref type="bibr" target="#b3">[4]</ref>. <ref type="table" target="#tab_3">Table 3</ref> shows the comparison of all SR approaches at 4×, 8× and 16×. We did not conduct image SR with up-sampling factor smaller than 4 because all state-of-the-art SR approaches have achieved great perfor-  mance in that scenario and the differences are too small to be compared. Instead, we show the extreme case with 16× enlargement. We chose the SR approaches that achieve the best performance in 4× and 8× for extreme comparison. The 16× results for EDSR, RCAN and ESRGAN were obtained by applying 2 times of the 4× SR using the provided pre-trained models. For a fair comparison, we also tried to use our proposed 4× ABPN SR model twice for enlargement. We can find that the proposed ABPN can achieve 0.1∼0.2 dB improvement in PSNR and 0.01∼0.2 in SSIM. It indicates that the proposed ABPN is more robust than others that can handle image SR even without further training. Note that we did not test Set5 and Set14 for two reasons: 1) the images in these two dataset are too small for evaluation and 2), the released codes for EDSR, RCAN and ESRGAN cannot be reimplemented in these two datasets so we tested on using DIV2K validation dataset, BSD100, Urban100 and Manga109 datasets. Furthermore, AIM2019 Image Superresolution Challenge provided another 8K dataset for 16× SR and we show the results of using our proposed ABPN on the validation dataset. In conclusion, from the comparison on PSNR and SSIM across different up-sampling factors, we can find that using proposed ABPN can achieve comparable or even better performance compared with other stateof-the-art SR approaches. It demonstrates that the proposed ABPN is robust and accurate to handle image SR with different up-sampling factors, even in extreme conditions.</p><p>More importantly, we are also interested in the computa-tion complexity of different models. Hence, we selected some of the state-of-the-art SR approaches for comparison, including SRCNN, VDSR, LapSRN, DBPN, HBPN, ESRGAN, RCAN. Note that we used the models and network setting that the authors claimed the best in their papers. We calculated the number of parameters by using the source code provided by <ref type="bibr" target="#b8">[9]</ref>, and used it as one indicator to show the model complexity. We also list the size of the pre-trained model file as another indicator. Since different models can be implemented with different computers and with different platforms. We did not test the running time to complicate the comparison. In <ref type="figure">Figure 6</ref>, we show the number of parameters and PSNR for 4× SR for Urban100 dataset.</p><p>In <ref type="figure">Figure 6</ref>, orange dots indicate the model size and green dots indicate the number of parameters. The right bottom corner means good with higher PSNR and less model complexity. We can see that using proposed ABPN can achieve better PSNR than ESRGAN and RCAN with much less number of parameters. Note that the size of the model is consistent with the number of parameters (for some SR approaches, the orange and green dots overlap together) because the SR approaches used for comparison were all conducted using Pytorch and saved in the files with the same format. With the help of attention models, ABPN can reduce at least 2∼3 times of parameters to outperform about 0.1 dB in PSNR.</p><p>Finally, we show some typical images from the testing datasets for visual comparison. <ref type="figure">Figure 7</ref> gives the visualization of 4 × image SR. We can see that the proposed ABPN can generate SR images with comparable quality similar to other state-of-the-art SR approaches. For example, the pattern in <ref type="figure">Figure 7</ref> B is supposed to approximately horizontal. Affected by the vertical lines on the original image, other SR approaches tend to reconstruct diagonal patterns while the proposed ABPN can correctly reconstruct the pattern. In <ref type="figure">Figure 7</ref> C, EDSR and HBPN can generate sharp edges around the balcony but with some distortion. Our proposed ABPN can generate the pattern with better quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this paper, we explore the attention mechanism in image super-resolution, and then propose the Attention based Back Projection Network (ABPN) for image SR. There are three contributions in this network: modified enhanced back projection blocks, Spatial Attention Block (SAB) and Refined Back Projection Block (RBPB). The key modification is the Spatial Attention Block that can be used to replace the concatenation layer so that the correlation relationship between the intermediate feature maps can be extracted as a non-local weighting model. Without increasing the complexity of the CNN network, SAB can substantially improve the quality of super-resolution. The final Refined Back Projection Block works as a residual feedback that can form a close loop between the input LR and output SR images to further boost up the performance. Results on quantitative and qualitative evaluation show its advantages over other approaches. The exciting results of attention models for image SR indicate its great potential for further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Proposed ABPN structure. It can iteratively up-and down-sample the feature maps to update feature residues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Back Projection procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Comparison between self-attention and spatial attention blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of the proposed spatial attention blocks. The SAB is obtained by computing the correlation between EDBP features on the first and second rows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Comparison between model complexity and image quality. Left vertical axis is the number of parameters and right vertical axes is the size of the model file. Visual comparison of different SR approaches on Urban100 for 4× enlargement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the network using plain concatenation block or attention block, including PSNR and SSIM for scale 2×, 4× and 8× SR on Set5 and Set14. Red indicates the best results.</figDesc><table><row><cell cols="2">Algorithm Scale</cell><cell cols="3">Set5 PSNR SSIM PSNR SSIM Set14</cell></row><row><cell>Model-C</cell><cell>2</cell><cell>37.78</cell><cell>0.955 33.77</cell><cell>0.913</cell></row><row><cell>Model-A</cell><cell>2</cell><cell>38.29</cell><cell>0.961 34.18</cell><cell>0.922</cell></row><row><cell>Model-C</cell><cell>4</cell><cell>32.48</cell><cell>0.894 28.78</cell><cell>0.774</cell></row><row><cell>Model-A</cell><cell>4</cell><cell>32.69</cell><cell>0.900 28.94</cell><cell>0.789</cell></row><row><cell>Model-C</cell><cell>8</cell><cell>26.84</cell><cell>0.774 24.65</cell><cell>0.618</cell></row><row><cell>Model-A</cell><cell>8</cell><cell>27.25</cell><cell>0.786 25.08</cell><cell>0.638</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the network using with or without back projection or RBPB, including PSNR and SSIM for scale 2×, 4× and 8× SR on Set5 and Set14. Red indicates the best results.</figDesc><table><row><cell cols="3">Algorithm Scale Back Projection</cell><cell cols="3">Set5 PSNR SSIM PSNR SSIM Set14</cell></row><row><cell>A</cell><cell>2</cell><cell>none</cell><cell>38.05</cell><cell>0.960</cell><cell>33.89 0.919</cell></row><row><cell>B</cell><cell>2</cell><cell>post BP</cell><cell>38.20</cell><cell>0.961</cell><cell>34.07 0.921</cell></row><row><cell>C</cell><cell>2</cell><cell>RBPB</cell><cell>38.29</cell><cell>0.961</cell><cell>34.18 0.922</cell></row><row><cell>A</cell><cell>4</cell><cell>none</cell><cell>32.48</cell><cell>0.899</cell><cell>28.74 0.788</cell></row><row><cell>B</cell><cell>4</cell><cell>post BP</cell><cell>32.58</cell><cell>0.899</cell><cell>28.83 0.788</cell></row><row><cell>C</cell><cell>4</cell><cell>RBPB</cell><cell>32.69</cell><cell>0.900</cell><cell>28.94 0.789</cell></row><row><cell>A</cell><cell>8</cell><cell>none</cell><cell>27.16</cell><cell>0.786</cell><cell>24.97 0.638</cell></row><row><cell>B</cell><cell>8</cell><cell>post BP</cell><cell>27.20</cell><cell>0.786</cell><cell>25.01 0.638</cell></row><row><cell>C</cell><cell>8</cell><cell>RBPB</cell><cell>27.25</cell><cell>0.786</cell><cell>25.08 0.638</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative evaluation of state-of-the-art SR approaches, including PSNR and SSIM for scale 4×, 8× and 16×. Red indicates the best and blue indicates the second best results.</figDesc><table><row><cell>Algorithm</cell><cell>Scale</cell><cell cols="10">Set5 PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM Set14 BSD100 Urban100 Manga109</cell></row><row><cell>Bicubic</cell><cell></cell><cell>28.42</cell><cell>0.810</cell><cell>26.10</cell><cell>0.704</cell><cell>25.96</cell><cell>0.669</cell><cell>23.64</cell><cell>0.659</cell><cell>25.15</cell><cell>0.789</cell></row><row><cell>A+ [23]</cell><cell></cell><cell>30.30</cell><cell>0.859</cell><cell>27.43</cell><cell>0.752</cell><cell>26.82</cell><cell>0.710</cell><cell>24.34</cell><cell>0.720</cell><cell cols="2">27.02 0.850</cell></row><row><cell>CRFSR [33]</cell><cell></cell><cell cols="2">31.10 0.871</cell><cell>27.87</cell><cell>0.765</cell><cell>27.05</cell><cell cols="2">0.719 24.89</cell><cell>0.744</cell><cell>28.12</cell><cell>0.872</cell></row><row><cell>SRCNN [8]</cell><cell></cell><cell>30.49</cell><cell>0.862</cell><cell>27.61</cell><cell cols="2">0.754 26.91</cell><cell>0.712</cell><cell>24.53</cell><cell>0.724</cell><cell cols="2">27.66 0.858</cell></row><row><cell>LapSRN [17]</cell><cell>4×</cell><cell>31.54</cell><cell>0.885</cell><cell>28.19</cell><cell cols="2">0.772 27.32</cell><cell>0.728</cell><cell>25.21</cell><cell>0.756</cell><cell cols="2">29.09 0.890</cell></row><row><cell>EDSR [19]</cell><cell></cell><cell>32.46</cell><cell>0.897</cell><cell>28.80</cell><cell cols="2">0.788 27.71</cell><cell>0.742</cell><cell>26.64</cell><cell>0.803</cell><cell cols="2">31.02 0.915</cell></row><row><cell>RCAN [31]</cell><cell></cell><cell>32.63</cell><cell>0.900</cell><cell>28.87</cell><cell cols="2">0.789 27.77</cell><cell>0.744</cell><cell>26.82</cell><cell>0.809</cell><cell cols="2">31.22 0.917</cell></row><row><cell>ESRGAN [28]</cell><cell></cell><cell>32.73</cell><cell>0.901</cell><cell>28.99</cell><cell>0.792</cell><cell>27.85</cell><cell>0.745</cell><cell>27.03</cell><cell cols="2">0.815 31.66</cell><cell>0.920</cell></row><row><cell>ABPN(Ours)</cell><cell></cell><cell cols="2">32.69 0.900</cell><cell>28.94</cell><cell>0.789</cell><cell>27.82</cell><cell>0.743</cell><cell>27.06</cell><cell>0.811</cell><cell>31.79</cell><cell>0.921</cell></row><row><cell>Bicubic</cell><cell></cell><cell>24.39</cell><cell>0.657</cell><cell>23.19</cell><cell>0.568</cell><cell>23.67</cell><cell>0.547</cell><cell>21.24</cell><cell>0.516</cell><cell>21.68</cell><cell>0.647</cell></row><row><cell>A+ [23]</cell><cell></cell><cell>25.52</cell><cell>0.692</cell><cell>23.98</cell><cell>0.597</cell><cell>24.20</cell><cell>0.568</cell><cell>21.37</cell><cell>0.545</cell><cell cols="2">22.39 0.680</cell></row><row><cell>CRFSR [33]</cell><cell></cell><cell cols="2">26.07 0.732</cell><cell>23.97</cell><cell>0.600</cell><cell>24.20</cell><cell cols="2">0.569 21.36</cell><cell>0.550</cell><cell>22.59</cell><cell>0.688</cell></row><row><cell>SRCNN [8]</cell><cell></cell><cell>25.33</cell><cell>0.689</cell><cell>23.85</cell><cell cols="2">0.593 24.13</cell><cell>0.565</cell><cell>21.29</cell><cell>0.543</cell><cell cols="2">22.37 0.682</cell></row><row><cell>LapSRN [17]</cell><cell>8×</cell><cell>26.15</cell><cell>0.738</cell><cell>24.42</cell><cell cols="2">0.622 24.59</cell><cell>0.587</cell><cell>21.88</cell><cell>0.583</cell><cell cols="2">23.60 0.742</cell></row><row><cell>EDSR [19]</cell><cell></cell><cell>26.97</cell><cell>0.775</cell><cell>24.94</cell><cell cols="2">0.640 24.80</cell><cell>0.596</cell><cell>22.47</cell><cell>0.620</cell><cell cols="2">24.58 0.778</cell></row><row><cell>RCAN [31]</cell><cell></cell><cell>27.47</cell><cell>0.791</cell><cell>25.40</cell><cell cols="2">0.655 25.05</cell><cell>0.608</cell><cell>23.22</cell><cell>0.652</cell><cell cols="2">25.58 0.809</cell></row><row><cell>HBPN [20]</cell><cell></cell><cell cols="2">27.17 0.785</cell><cell>24.96</cell><cell>0.642</cell><cell>24.93</cell><cell>0.602</cell><cell>23.04</cell><cell>0.647</cell><cell>25.24</cell><cell>0.802</cell></row><row><cell>ABPN(Ours)</cell><cell></cell><cell cols="2">27.25 0.786</cell><cell>25.08</cell><cell>0.638</cell><cell>24.99</cell><cell>0.604</cell><cell>23.04</cell><cell>0.641</cell><cell>25.29</cell><cell>0.802</cell></row><row><cell></cell><cell></cell><cell cols="2">DIV8K val</cell><cell cols="2">DIV2K val</cell><cell cols="2">BSD100</cell><cell cols="2">Urban100</cell><cell cols="2">Manga109</cell></row><row><cell>Bicubic</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">22.867 0.598</cell><cell>21.73</cell><cell>0.477</cell><cell>18.92</cell><cell>0.434</cell><cell>19.10</cell><cell>0.568</cell></row><row><cell>EDSR [19]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>24.13</cell><cell>0.631</cell><cell>22.62</cell><cell>0.506</cell><cell>19.96</cell><cell>0.481</cell><cell cols="2">20.62 0.635</cell></row><row><cell>RCAN [31]</cell><cell>16×</cell><cell>-</cell><cell>-</cell><cell>24.30</cell><cell cols="2">0.639 22.69</cell><cell>0.511</cell><cell>20.20</cell><cell>0.496</cell><cell cols="2">20.88 0.656</cell></row><row><cell>ESRGAN [28]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>19.09</cell><cell>0.421</cell><cell>18.01</cell><cell>0.281</cell><cell>15.42</cell><cell cols="2">0.262 17.41</cell><cell>0.428</cell></row><row><cell>ABPN(Ours)</cell><cell></cell><cell>26.71</cell><cell>0.65</cell><cell>24.38</cell><cell>0.641</cell><cell>22.72</cell><cell cols="2">0.512 20.39</cell><cell>0.515</cell><cell>21.25</cell><cell>0.673</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberi</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="135" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="http://aiweb.techfak.uni-bielefeld.de/content/bworld-robot-control-software/" />
		<title level="m">AIM 2019 Image SR Challenge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<idno>abs/1811.12814</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<ptr target="http://aiweb.techfak.uni-bielefeld.de/content/bworld-robot-control-software/" />
		<title level="m">AIM 2019 Image SR evaluation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1803.02735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1811.11721</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1511.04587</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1704.03915</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<imprint>
			<publisher>Zehan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<idno>abs/1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1707.02921</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hierarchical back projection network for image superresolution. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Tak</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<idno>abs/1510.04389</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2790" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A+: Adjusted anchored neighborhood regression for fast superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Desmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="volume">9006</biblScope>
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
	<note>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1606.05328</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. CoRR, abs/1706.03762</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1711.07971</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">ESRGAN: enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1809.00219</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization. CoRR, abs/1409.2329</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Protter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="volume">6920</biblScope>
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno>abs/1807.02758</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1802.08797</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cascaded random forests for fast image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhi-Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="2531" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
