<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain and View-point Agnostic Hand Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sabater</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Alonso</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
						</author>
						<title level="a" type="main">Domain and View-point Agnostic Hand Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hand action recognition is a special case of human action recognition with applications in human robot interaction, virtual reality or life-logging systems. Building action classifiers that are useful to recognize such heterogeneous set of activities is very challenging. There are very subtle changes across different actions from a given application but also large variations across domains (e.g. virtual reality vs life-logging). This work introduces a novel skeleton-based hand motion representation model that tackles this problem. The framework we propose is agnostic to the application domain or camera recording viewpoint. We demonstrate the performance of our proposed motion representation model both working for a single specific domain (intra-domain action classification) and working for different unseen domains (cross-domain action classification). For the intra-domain case, our approach gets better or similar performance than current state-of-the-art methods on well-known hand action recognition benchmarks. And when performing cross-domain hand action recognition (i.e., training our motion representation model in frontal-view recordings and testing it both for egocentric and third-person views), our approach achieves comparable results to the state-of-the-art methods that are trained intra-domain. All our code, learned models and data splits will be released upon acceptance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and gestures related to computer interaction, life-logging and sign language domains respectively. Our intra-domain classification results show that our framework gets better or similar performance than current state-of-the-art intradomain classifiers in well-known benchmarks. More importantly, our cross-domain classification approach obtains comparable accuracy to intra-domain methods by being trained just with the SHREC-17 dataset, and then evaluated on the F-PHAB and MSRA datasets. This demonstrates that our motion representation model generalizes well for different action domains and camera view-points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In the following, we summarize relevant works on the core topics of this work: pose modeling, skeleton-based action recognition models and generalization to unseen action categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pose modeling modeling for action recognition</head><p>Action recognition was first tackled by directly analyzing RGB videos <ref type="bibr" target="#b12">[13]</ref> or depth maps <ref type="bibr" target="#b13">[14]</ref>. Current approaches have settled the standard of extracting the intermediate representation of skeleton poses <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b7">[8]</ref>. This representation has shown great performance since it encodes human poses regardless their appearance and surrounding and presents strong robustness to occlusions.</p><p>Certain works directly use the Cartesian coordinates of the skeleton joints, using them as input for full-body action recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref> and for hand action recognition <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. In order to achieve a standardized and generic skeleton pose description, several full-body action recognition approaches propose different strategies, such as learning the most suitable view-point for each action <ref type="bibr" target="#b5">[6]</ref> or transforming all coordinates to a common coordinate system <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. However, this kind of transformations cannot be directly applied to hand action recognition, where orientation plays a key role.</p><p>In order to get more informative pose representations than raw Cartesian coordinates, many approaches propose to compute additional geometric features. Chen et al. <ref type="bibr" target="#b20">[21]</ref> use joint orientations, velocity and acceleration, and their distance to other joints, lines and planes. Zhang et al. <ref type="bibr" target="#b21">[22]</ref> calculate distances between joints and planes, and Yang et al. <ref type="bibr" target="#b7">[8]</ref> use joint distances and their motion speeds at different scales.</p><p>Our approach proposes a simplification of the skeleton representation reducing coordinate redundancy by using just a set of key joints. Then, relative pose coordinates are computed to describe the hand pose, along with specific geometric features that describe its motion and orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action recognition models</head><p>As in many other fields, deep learning have become the state-of-the-art in action recognition. Of particular relevance for this work, Recurrent Neural Networks (RNN) have been widely used to model temporal dependencies in hand action recognition. Ma et al. <ref type="bibr" target="#b16">[17]</ref> use a LSTM-based Memory Augmented Neural Network to model dynamic hand gestures. Chen et al. <ref type="bibr" target="#b22">[23]</ref> use a LSTM Network to combine skeleton coordinates, global motions and finger motion features. Li et al. <ref type="bibr" target="#b17">[18]</ref> combine a bidirectional Independently Recurrent Neural Network with a self-attention based graph convolutional network.</p><p>Another common approach is to make use of Convolutional Networks. Liu et al. <ref type="bibr" target="#b23">[24]</ref> recognize posture and action by using a 3D convolutional neural network. Yang et al. <ref type="bibr" target="#b7">[8]</ref> use 1D convolutions to process and fuse different hand motion features. Hou <ref type="bibr" target="#b15">[16]</ref> propose to focus on the most informative hand gesture features by using a ResNet-like 1D convolutional network with attention.</p><p>Our method uses a Temporal Convolutional Network (TCN) <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> that implements 1D dilated convolutions to learn long-term temporal dependencies from variable-length input sequences, achieving comparable or better results than RNNs <ref type="bibr" target="#b24">[25]</ref>. TCNs have already demonstrated good performance on hand motion unsupervised learning <ref type="bibr" target="#b26">[27]</ref> and general action recognition <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalization to unseen action categories</head><p>Learning a model able to classify unseen categories is a challenging task. It is commonly tackled by encoding every new data sample into a descriptor and using a K-Nearest Neighbors classifier (KNN) to evaluate and assign labels according to the similarity between a few new category reference samples and the target samples <ref type="bibr" target="#b29">[29]</ref>.</p><p>Several works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref> address this problem for action recognition by extracting intermediate feature maps from a supervised action recognition model. Koneripalli et al. <ref type="bibr" target="#b26">[27]</ref> train an autoencoder to learn these descriptors in an unsupervised fashion. Ma et al. <ref type="bibr" target="#b16">[17]</ref> learn these descriptors directly in a semi-supervised manner by training an encoder with metric-learning techniques. Other recent works use wor2vec <ref type="bibr" target="#b30">[30]</ref> and sent2vec <ref type="bibr" target="#b31">[31]</ref> approaches for this descriptor learning.</p><p>Previous works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref> are aimed to recognize unseen full-body action categories where no drastic camera view-points are found, but up to our knowledge, generalization to unseen hand view-points and domains is still to be studied. The present work uses metric-learning along with specific data augmentation to learn meaningful hand sequence descriptors. Our framework is able to perform accurate action recognition of sequences from unseen categories and recording view-points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HAND ACTION RECOGNITION FRAMEWORK</head><p>The core of the proposed framework is the motion representation model summarized in <ref type="figure">Fig. 1</ref>. First, our approach calculates specific hand motion features for each skeleton. These features are fed to a Temporal Convolutional Network to generate a set of motion descriptors. Additionally, a motion summarization module combine them, according to their relevance, into the final action representation. In the following, we describe these steps, as well as, how to train </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hand pose modeling</head><p>Human hand actions sequences are defined by sets of T hand skeleton poses X = {X 1 , ..., X T }, extracted from video frames. Each hand pose X t is composed by a set of J joint coordinates, X t = {x 1 , ..., x J }, x j R 3 , which are logically connected by a set of B bones (see <ref type="figure" target="#fig_0">Fig. 2</ref>).</p><p>1) Skeleton standardization: Hand joints belonging to the same bones (fingers) are highly coupled and can be represented with a smaller number of degrees of freedom. Based on this assumption, we propose to use just a subset of 7 joints to define a hand pose (see <ref type="figure" target="#fig_0">Fig. 2</ref>), corresponding to the wrist, the top of the palm, and the tips of the 5 fingers; which we connect with a total of 6 hand bones, one for the palm and one more for each one of the fingers. This simpler skeleton representation makes the learning process easier and less prone to overfitting.</p><p>Since action sequences can be performed by different people with heterogeneous hand sizes, we standardize each hand pose in order to achieve scale-invariant skeleton representations. Given a skeleton pose X t , its standardized versionX t is obtained by applying to its coordinates the transformation that makes the palm of size equal to 1:</p><formula xml:id="formula_0">X n = X n |P | ,</formula><p>where |P | is the euclidean distance between the bone defined by the wrist and the top of the palm. Since we also want hand actions to be independent of the position where they are executed, we compute locationinvariant coordinates by translating the top of the palm to the origin of the reference coordinate system.</p><p>2) Hand pose description: These relative hand coordinates describe properly the intra-relation of their joints, but they do not have information about the hand translation. Different from full body action sequences (e.g. walking) where this movement can be inferred from the relative coordinates of its bones (e.g. legs), hands can be translated trough any direction without any change of their relative coordinates. Since the translation information is essential Then, they are grouped with a single perceptron layer to calculate their summarization weights. Finally, the action sequence is summarized into a single descriptor by performing a weighted average over the initial motion embeddings. in certain actions (e.g. pointing to specific directions), we generate extra translation and orientation-aware features from the original hand skeletons:</p><p>• Difference of coordinates, defined as the difference of each joint coordinate with itself in the previous timestep. These features describe the translation direction and speed of each coordinate for each of the 3 axes:</p><formula xml:id="formula_1">d coord (t, j) = x j,t − x j,t−1 , ∀j J, t T<label>(1)</label></formula><p>• Difference of bone angles, defined as the difference of the elevation ϕ and azimuth θ of a bone b B with itself in the previous time-step. These features describe the rotation direction and rotation speed of each bone:</p><formula xml:id="formula_2">d ϕ (t, b ϕ ) = b ϕ,t − b ϕ,t−1 , ∀b B, t T (2) d θ (t, b θ ) = b θ,t − b θ,t−1 , ∀b B, t T<label>(3)</label></formula><p>Our final hand representation consists then of a vector of size 54, which stands for 7 × 3 relative hand coordinates, 7 × 3 coordinate difference features, and 6 × 2 bone angle differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motion representation model</head><p>The core of our action recognition framework is a model that encodes the skeleton features from each frame, described in the previous section, into single descriptors with a Temporal Convolutional Network (TCN) <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The TCN processes sequences of skeleton features, generating a descriptor at each time-step that represents the motion performed up to that frame (see <ref type="figure">Fig. 1</ref>), i.e. with no information from the future.</p><p>For a given sequence, the last descriptor of the TCN encodes all the information up to that point and it is used to represent the sequence. In our case, however, the last part of action sequences is not always the most informative one to distinguish it from others. Consequently, the last descriptor is not the optimal one either.</p><p>To alleviate this issue, we learn the relevance of the temporal patterns of the actions. More precisely, we add a motion summarization module after the TCN, which combines all the descriptors generated for the input hand motion, up the TCN memory length, by performing a weighted average over them (see <ref type="figure" target="#fig_1">Fig. 3</ref>). These weights represent how important each descriptor is for the final action representation. They are learned with a simple Neural Network trained end-to-end along with the TCN. This network consists of a single 1D Convolutional layer with kernel 1 that reduces the per-frame descriptors dimensionality, and a single Fully Connected layer with a sigmoid activation layer, that takes as input all the simplified descriptors and outputs a vector of categorical probabilities (i.e. descriptor weights). These final weights are L1 normalized before performing the final descriptor summarization.</p><p>This summarization module efficiently describes hand action sequences and helps the TCN to focus just on the meaningful data during training. However, there are real use cases where actions, at test time, present a longer length than our motion representation module can handle. In these cases, although the summarization module has been trained along with the TCN, it is more interesting to discard it and classify individually all the descriptors generated by the TCN, which still contains meaningful motion representations.</p><p>So far, we have shown how to encode an action sequence X into a robust simple descriptor z = f (X), where the function f () represents our motion representation module. In the next two sections, we describe how to optimize these action representations to perform intra-domain classification and cross-domain classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Intra-domain classification</head><p>Intra-domain action classification aims to recognize the same actions categories seen during the learning phase, with no drastic variation on the camera view-point. For this classification, intra-domain class probabilities P = g(z) are predicted by a linear classifier g trained end-to-end along with our motion representation model f . Intra-domain classification is learnt by the optimization of the categorical cross-entropy loss</p><formula xml:id="formula_3">CCE = −Σ C c=1 y i,c log (p i,c )<label>(4)</label></formula><p>which evaluates the predicted probabilities p i,c that belongs to a class c C, given their true label y i . For each iteration during training, the mini-batch is composed by two sequences sampled randomly for each one of the training classes C. To ensure the data variability that might not be contained in small hand datasets, each motion sequence within the mini-batch is included three times more with different data augmentations. This data augmentation is applied to the per-frame skeletons X t , before the feature computation from Section III-A, as follows:</p><p>• Movement speed variation. Joint coordinates are randomly re-sampled by interpolation over the temporal dimension. This simulates that the action is performed at a different speed, and thus, having a different length. • Frame skipping. Contiguous video frames contain similar joint information so, using one out of every three frames reduces the data redundancy and makes the learning process easier. Our approach uses one out of three frames and augments each action sequence by initializing it randomly between the three first frames. • Random cropping. When the sampled action is longer than a defined maximum length (i.e. TCN memory lenght), it is randomly cropped. • Random noise. Gaussian noise is added to the skeleton coordinates to simulate inaccurate joint estimations. • Random rotation noise. The whole action sequence is rotated randomly over the 3D axes. This rotation is limited to low angles, to simulate just subtle variations in the recording view-point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross-domain classification</head><p>Cross-domain hand action classification aims to recognize action sequences whose category and recording camera viewpoint were not present in the training data. To obtain viewpoint agnostic motion representation, our motion representation model f is trained to learn an embedding space where descriptors belonging to the same action category must be close to each other, and far away from other category descriptors. This is achieved optimizing the normalized temperaturescaled cross-entropy loss (NT-Xent) <ref type="bibr" target="#b32">[32]</ref>:</p><formula xml:id="formula_4">l i,j = − log exp (sim (z i , z j ) /τ ) 2N k=1 1 [k =i] exp (sim (z i , z k ) /τ ) ,<label>(5)</label></formula><p>which is computed in each training iteration for each pair of actions i and j that belong to the same action category. NT-Xent maximizes the cosine similarity of both embedded actions z i and z j and minimizes their similarity to the descriptors related to different action classes. τ is a temperature parameter.</p><p>The training of our motion representation model is performed with the same batch construction and data augmentation techniques described in Section III-C. We added an extra data augmentation step that rotates randomly all the action sequences of the mini-batch. This batch augmentation is crucial to boost the performance achieved with the NT-Xent loss in different domains and camera view-points.</p><p>Once this generic motion representation model has been trained, on a given source domain, we use a N-shot approach <ref type="bibr" target="#b29">[29]</ref> and generate motion sequence descriptors for a small set of N reference actions from a different target domain, with no specific training on the latter. To perform action classification in this new domain, we use a simple K-Nearest Neighbors classifier (KNN) to assign a label to new sequences depending on their descriptor distance to the descriptors of the reference examples of the target domain. To improve the performance of the KNN, we extend our reference action set by applying the same data augmentation strategies described in Section III-C, and we compute descriptors for all the new augmented sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>This section details the different datasets used to evaluate our framework and its implementation details. Then, we expose the main framework design choices and evaluate its performance in different scenarios. We evaluate our crossdomain classification, both for action sequences (using summarization module) and for long motion sequences (without summarization module, i.e., per-frame classification). Finally, we evaluate our intra-domain classification of action sequences from well-known benchmarks.</p><p>A. Experimental setup 1) Datasets: The validation of the presented approach has been run using different datasets (see frame samples in <ref type="figure" target="#fig_2">Fig. 4)</ref>, covering a variety of application domains as well as different camera view-points:</p><p>SHREC-17 <ref type="bibr" target="#b9">[10]</ref>: it contains action sequences (22-joint hand skeletons) related to human-machine interaction domains recorded from a frontal third-person view. The data is categorized with two levels of granularity, presenting 14 and 28 actions respectively. The dataset contains 1960 action sequences for training and 840 sequences for validation. Actions are performed by 28 different users.</p><p>F-PHAB <ref type="bibr" target="#b10">[11]</ref>: it contains action sequences (21-joint hand skeletons) recorded from an egocentric view related to kitchen, office and social scenarios, which involve the interaction with different objects. Actions have been performed by 6 different users and labeled with 45 action categories. The dataset consists of 1175 sequences and proposes the following data splits for train and test:</p><p>• 1:3, 1:1, 3:1. Stand for splitting the dataset according to different training/testing ratios at a sequence level. • cross-person. Stand for a 6-fold leave-one-out crossvalidation at a user level. Only the original cross-subject and 1:1 splits are available, for the other two data partitions we create 3 random data folds to perform 3-fold cross-validation.</p><p>MSRA <ref type="bibr" target="#b11">[12]</ref>: it contains recordings (17-joint hand skeletons) of 17 different American Sign Language gestures performed by 9 different users. Each gesture sequence has a length of 500 frames recorded from a third-person view. For the classification of this data, we use the action samples from the two first subjects as reference, leaving the remaining 7 ones as the target samples, as suggested in <ref type="bibr" target="#b23">[24]</ref>.</p><p>2) Implementation and training details: Hand skeleton: Since each dataset used provides different skeleton joints format, we use the 20 joints that SHREC-17 and F-PHAB have in common, and our proposed 7-joint skeletons representation, described in Section III-A, suitable for the three datasets considered.</p><p>Motion representation architecture: the backbone of our motion representation model is a TCN consisting on two stacks of residual blocks with dilations of 1, 2 and 4 for the layers within each block, and convolutional filters of size 4, making a memory length of 32 frames long. Note that, since the feature pre-processing filters out 2 out of 3 consecutive frames, this memory length is extended to 96 real frames. Our backbone uses 256 filters in each convolutional layer, which generates action sequence embeddings with a size of 256. The summarization module reduces their dimensionality to 64 with a single 1D convolutional layer and then a single perceptron layer of size 32 generate the final descriptor weights. When the sequence summarization module is not used, the descriptor generated by the TCN at the last action time-step is the one used for the action representation (Last TCN descriptor). This model has been optimized using Eq. 5 with τ = 0.07.</p><p>KNN classifier: Our KNN classifier weights pairs of target-reference descriptors according to the inverse of their distance. We validate the use of different number of neighbors, i.e. 1, 3, 5, 7, 9, 11, and we report the results of the neighbor that optimizes the final classification accuracy. Additionally, the reference augmentation step increases the reference descriptors set randomly up to 40 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. General purpose framework design evaluation</head><p>This subsection analyzes and validates the main components of our framework using the cross-domain approach of Section III-D since this setup is more demanding in terms of generalization capabilities. We train our base motion representation model on the front view SHREC-17 dataset and then we validate it on the egocentric F-PHAB target. To analyze the effect of different design choices, we start using the descriptors generated at the last time-step of the input sequences for the final motion representation (no summarization). These are the descriptors that are later evaluated by the KNN classifier.  Additionally, as seen in <ref type="table" target="#tab_0">Table II</ref>, using the 28 SHREC classes to discriminate the action sequences while training the motion representation model provides more fine-grained information than the 14-labels format. Results show that by using more specific labels we are able to get a better representation of the action, improving the final performance.   We now analyze the effect of the motion summarization module (see <ref type="table" target="#tab_0">Table III</ref>). Results show how the learned summarization module improves the classification accuracy with respect to the last descriptor of our base TCN backbone. Moreover, extending the reference action set with random data augmentation increases the accuracy in all the data splits by a noticeable margin. These results also indicate that our framework performs good even when not many reference actions are available (splits 1:3). However, we still find an accuracy drop when generalizing to actions of users not available on the action reference set (cross-person splits). This is due to the high inter-subject action variability of the F-PHAB dataset, and the fact that no data from the target dataset has been involved to train our representation model.  TCN descriptor refers to the descriptor generated by the TCN at the last time-step. Summarization refers to descriptor generated by our summarization module for the final action representation. <ref type="figure">Figure 5</ref> shows the weights learned by our summarization module on the F-PHAB validation split (1:1). Initial timesteps are the less relevant for the final action summarization, while weights tend to increase with time. However, they do not exhibit a continuous growth along time, probably due to the fact that contiguous time descriptors contain similar information. Although final descriptors may encode information about the whole action, they may also encode motion not related with the action itself. Therefore, they are not always very relevant for the final action representation. <ref type="figure">Fig. 5</ref>: Weights generated by our summarization module for each sample of the F-PHAB validation split (1:1). The red line in front shows the average of all the generated weights. Sampled frames belong to the action pour liquid soap. Initial descriptors lack of relevance for not containing previous motion information. Ending frames gain relevance by encoding all the previous information. Often the last frames encode motion out of the action scope, reducing their relevance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross-domain action classification</head><p>This experiment evaluates the cross-domain generalization of our framework by classifying action sequences from action categories and camera view-points not seen in the training data. For this experiment, we train our motion representation model as defined in the Section III-D only on the front view SHREC-17 dataset (28 labels), and we measure its classification accuracy on the egocentric F-PHAB dataset. Results from our framework correspond to the processing of 7-joint skeletons and the use of our proposed action summarization module.   <ref type="table" target="#tab_0">Table IV</ref> summarizes the accuracy of the best performing methods on the F-PHAB dataset which have been trained as an intra-domain problem (upper block), and the results of our cross-domain approach (bottom block). The later include the evaluation of DD-Net <ref type="bibr" target="#b7">[8]</ref>, one of the best performing methods on the SHREC-17 classification benchmark. We used the available public code to train the DD-Net with the SHREC-17 dataset (20-joint skeletons) as the authors state, extracting F-PHAB descriptors from its backbone and classifying them with our N-shot approach. Results from the DD-Net cross-domain classification show a lack of domain adaptation, showing that our method clearly outperforms other methods in this scenario.</p><p>The results show that our approach clearly outperforms the RGB <ref type="bibr" target="#b12">[13]</ref> and depth-based <ref type="bibr" target="#b13">[14]</ref> models trained on the test domain. We also get better or comparable results than a regular LSTM network <ref type="bibr" target="#b33">[33]</ref>, specially when not many reference actions are available (1:3 split) or when not all the subjects have been present in the reference split (cross-person splits). Although our cross-domain performance is behind the best intra-domain classification model <ref type="bibr" target="#b8">[9]</ref>, we show later in Section IV-E that we outperform them when training in the same domain. Remember that no specific training with the F-PHAB data splits has been performed in these evaluations of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross-domain classification of frames from long video sequences</head><p>In this experiment we use the MSRA dataset, whose hand motion sequences are much longer than the memory of our representation model. This will allow us to illustrate two characteristics of our method. First, that the features learned by the TCN are robust across domains thanks to our motion summarization module (see <ref type="table" target="#tab_0">Table III</ref>). Second, that they can be used without summarization for long sequences that exceed the memory of the model.</p><p>To do so, we used the same model trained in section IV-C and evaluate its cross-domain performance in the MSRA dataset. Since sequences are too long for our summarization, we perform the KNN classification of all the motion descriptors generated by our TCN at each time-step (denoted as online action classification). We also report the average of the class probabilities of the frames within a video sequence for comparison with previous works (denoted as video classification). For computational reasons, we randomly select just 8000 reference descriptors for the KNN evaluation. <ref type="table" target="#tab_9">Table V</ref> shows that, even though MSRA motion sequences do not correspond to the kind of actions seen in the training data, our online KNN classification is able to properly classify 85.8% of the validation frames. Moreover, a simple average of the predicted frame probabilities results in a 97.1% accuracy, similar to current state-of-the-art results specifically trained on the MSRA dataset. In this case, reference action data augmentation does not provide an edge, probably because the MSRA motion sequences already contain enough variations of hand poses.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Intra-domain classification and reference actions study</head><p>Our final experiment evaluates the performance of our method for intra-domain action classification using the linear classifier described in Section III-C and training and evaluating actions in the same dataset.</p><p>1) SHREC-17 evaluation: <ref type="table" target="#tab_0">Table VI</ref> shows the classification accuracy of our framework trained and evaluated on the SHREC-17 dataset. Note that we are using just 7 out of the 22 original skeleton joints, that helps generalization to other datasets but it might lose domain-specific information. Still, our approach is able to get comparable results to the best performing methods for the benchmark.  2) F-PHAB evaluation: <ref type="table" target="#tab_0">Table VII</ref> shows the classification accuracy of our framework trained and evaluated on the F-PHAB dataset. For the DD-net results, we used the available original code and follow the paper to train on the F-PHAB dataset. Our intra-classification results outperform the current state-of-the-art in all the splits. Note that, even when less training data is used (1:3 split), we achieve comparable accuracy to the data splits with larger training sets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>The present work introduces a complete solution for hand action recognition, that has been designed to work for different action domains and recording view-points. Our framework processes skeleton hand action sequences by first simplifying the skeleton representation, calculating specific hand pose features, and then, our motion representation model encodes them into single descriptors. This motion representation model is based on a Temporal Convolutional Network that generates sets of descriptors to describe the input motion up to each time-step. Then, a simple motion summarization module weights the descriptors, according to their relevance, ending up with a final action representation. Finally, we demonstrate the performance of our motion representation model for different classification purposes. First, we evaluate its classification accuracy when being trained for specific action domains, obtaining better or similar results than different state-of-the-art methods in wellknown benchmarks. Second, we demonstrate how our motion representation model generalizes to different unseen target domains and camera view-points (cross-domain). Here, with no specific training on the target domains, we get comparable results to best the methods that do train for those specific domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Hand skeleton simplification. Left diagram refers to a detailed hand skeleton of 20 joints (dots) connected by 19 bones (lines). Right diagram refers to our proposed hand skeleton simplification of 7 joints (dots) and 5 bones (lines). our motion representation model, both for specific intradomain and cross-domain classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Action summarization module. Per-frame motion embeddings are simplified with a 1D Convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>(a) SHREC-17 depth sample frames (b) F-PHAB RGB sample frames (c) MSRA depth sample frames with skeleton joints Sample frames from the different evaluated data domains. (a) SHREC-17 dataset. Examples of actions grab, expand and rotation clockwise. (b) F-PHAB dataset. Examples of actions clean glasses, handshaking and pour juice. (c) MSRA dataset. Examples of the signs IP, RP and three.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>SHREC</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I</head><label>I</label><figDesc>shows the benefits of using our proposed hand skeleton simplification. Using this 7-joint skeleton format we improve the generalization for all the different data splits.</figDesc><table><row><cell>Skeleton size</cell><cell>1:3</cell><cell>1:1</cell><cell>3:1</cell><cell>cross-person</cell></row><row><cell>20 joints</cell><cell>63.8</cell><cell>69.9</cell><cell>69.8</cell><cell>51.4</cell></row><row><cell>7 joints</cell><cell>66.3</cell><cell>71.0</cell><cell>73.8</cell><cell>53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Influence of the number of skeleton joints in the hand representation. Motion representation model trained on SHREC. Action recognition accuracy validated on F-PHAB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Influence of the training categories.</figDesc><table><row><cell>Motion</cell></row><row><cell>representation model trained on SHREC. Action recognition</cell></row><row><cell>accuracy evaluated on F-PHAB.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Influence of the action sequence summarization technique. Motion representation model trained on SHREC. Action recognition accuracy validated on F-PHAB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell>: F-PHAB accuracy comparison with state-of-the-</cell></row><row><cell>art. Upper block shows methods trained on the F-PHAB</cell></row><row><cell>dataset (intra-domain classification). Bottom block shows</cell></row><row><cell>the methods trained on SHREC-17 dataset (cross-domain</cell></row><row><cell>classification).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>MSRA accuracy comparison. Batched vs. online predictions. Our results correspond to our motion representation model trained on SHREC-17 data (cross-domain).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VI :</head><label>VI</label><figDesc>Intra-domain classification on SHREC-17 data splits.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VII :</head><label>VII</label><figDesc>Intra-domain classification on F-PHAB data splits. only best methods fromTable IV are shown.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparison of multimodal heading and pointing gestures for co-located mixed reality human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krupke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Steinicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lubos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jonetzko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Görner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A generative model for intention recognition and manipulation assistance in teleoperation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Tanwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On-line simultaneous learning and recognition of everyday activities from virtual reality performances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramirez-Amaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Inamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Interaction relational network for mutual action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04963</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">View adaptive neural networks for high performance skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multimodal human-robot interaction manager for assistive robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Monaikul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rysbek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Eugenio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zefran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Make skeleton-based action recognition model smaller, faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Asia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient temporal sequence comparison and classification using gram matrix embeddings on a riemannian manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shrec&apos;17 track: 3d hand gesture recognition using a depth and skeletal dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vandeborre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DOR-10th Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">First-person hand action benchmark with rgb-d videos and 3d hand pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatialtemporal attention res-tcn for skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skeleton-based dynamic hand gesture recognition using an enhanced network with one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A two-stream neural network for pose-based hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08926</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">One-shot action recognition towards novel assistive therapies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predict &amp; cluster: Unsupervised skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a 3d human pose distance metric from geometric pose descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On geometric features for skeletonbased action recognition using multilayer lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mfanet: Motion feature augmented network for dynamic hand gesture recognition from skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d posturenet: A unified framework for skeleton-based posture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rate-invariant autoencoding of time-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koneripalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Action2vec: A crossmodal embedding approach to action learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00484</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Skeleton based zero shot action recognition in joint pose-language semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mazagonwalla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11344</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cooccurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
