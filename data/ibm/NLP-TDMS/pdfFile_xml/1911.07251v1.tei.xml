<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoze</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of ASEE</orgName>
								<orgName type="laboratory">Intelligent Computing and Machine Learning Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
							<email>zcqin@buaa.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of ASEE</orgName>
								<orgName type="laboratory">Intelligent Computing and Machine Learning Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of ASEE</orgName>
								<orgName type="laboratory">Intelligent Computing and Machine Learning Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
							<email>xizhang@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
							<email>huyue@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Different from Visual Question Answering task that requires to answer only one question about an image, Visual Dialogue involves multiple questions which cover a broad range of visual content that could be related to any objects, relationships or semantics. The key challenge in Visual Dialogue task is thus to learn a more comprehensive and semantic-rich image representation which may have adaptive attentions on the image for variant questions. In this research, we propose a novel model to depict an image from both visual and semantic perspectives. Specifically, the visual view helps capture the appearance-level information, including objects and their relationships, while the semantic view enables the agent to understand high-level visual semantics from the whole image to the local regions. Futhermore, on top of such multiview image features, we propose a feature selection framework which is able to adaptively capture question-relevant information hierarchically in fine-grained level. The proposed method achieved state-of-the-art results on benchmark Visual Dialogue datasets. More importantly, we can tell which modality (visual or semantic) has more contribution in answering the current question by visualizing the gate values. It gives us insights in understanding of human cognition in Visual Dialogue.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>To understand the real world by analyzing vision and language together is a priority for AI to achieve human-like abilities, which enables the development of diverse applications, such as Visual Question Answering (VQA) <ref type="bibr" target="#b0">(Agrawal et al. 2017</ref><ref type="bibr">), Referring Expressions (Wang et al. 2019</ref>), Image Captioning <ref type="bibr" target="#b7">(Johnson, Karpathy, and Fei-Fei 2016)</ref>, etc.</p><p>To move a step further, this work focuses on the Visual Dialogue <ref type="bibr" target="#b2">(Das et al. 2017)</ref> problem, which requires the agent to answer a series of questions in natural language regarding an image. It is more challenging because it demands the agent to adaptively focus on diverse visual content with respect to the current question, while other vision-language problems mostly attend to some specific objects or regions. Considering the dialogue in <ref type="figure">Figure 1</ref>: Given " Q1: Is the man on the v Q6: Are there other people around?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogue History</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Current Question</head><p>Objects v Answer C: A man doing a grind on a skateboard. Q1: Is the man on the skateboard? A1: Yes, he is. … Q4: Is he young or older? A4: He is in the middle-aged. Q5: Is there sky in the picture? A5: Yes, the sky is deep blue with some clouds.  <ref type="figure">Figure 1</ref>: An illustration of DualVD. Left: the input of the dialogue system. Right: visual and semantic modules designed to adaptively understand the visual content like humans. The answer is inferred depending on multi-modal evidence.</p><p>skateboard?", the agent should be aware of the foreground visual content, i.e. the man, the skateboard, while " Q5: Is there sky in the picture?" changes the attention of the agent to the background of sky. Besides appearance-level questions like Q1 and Q5, " Q4: Is he young or older?" requires the agent to reason about the visual content for higher-level semantics. How to adaptively capture the desired visual content through dialogue becomes one of the most critical challenges in visual dialogue.</p><p>The typical solution for visual dialogue is to firstly fuse visual (i.e. image) features and textual (i.e. dialogue history, current question) features together and then to infer the correct answer. Most approaches focus on enhancing the textural representations by recovering the dialogue relational structure <ref type="bibr" target="#b21">(Zheng et al. 2019)</ref>, imperfect dialogue history <ref type="bibr" target="#b21">(Yang, Zha, and Zhang 2019)</ref>, and dialogue consistency <ref type="bibr" target="#b15">(Qi et al. 2018</ref>). However, the role of visual information is at present less studied. Existing models simply use CNN (Simonyan and Zisserman 2014) or R-CNN <ref type="bibr" target="#b16">(Ren et al. 2017)</ref> to extract visual features and focus on the question-relevant content. Such visual features have limited expressive ability due to the monolithic representations ). On one hand, questions in a visual dialogue refer to a wide range of visual content, including objects, relationships and high-level semantics, which can not be covered by monolithic features. On the other hand, the referred visual content may change remarkably from visual appearance to highlevel semantics through the dialogue, which is difficult for monolithic features to capture.</p><p>Our work is inspired by the Dual-coding theory <ref type="bibr" target="#b14">(Paivio 1971)</ref> of human cognition process. Dual-coding theory postulates that our brain encodes information in two ways: visual imagery and textual associations. When asked to act upon a concept, our brain retrieves either images or words, or both simultaneously. The ability to encode a concept by two different ways strengthens the capacity of memory and understanding. Inspired by the cognitive process, we first propose a novel scheme to comprehensively depict an image from both visual and semantic perspectives, where the major objects and their relationships are kept in the visual view while the higher-level abstraction is provided in the semantic view. We propose a model called Dual Encoding Visual Dialogue (DualVD) to adaptively select question-relevant information from the image in a hierarchical mode: intra-modal selection first captures the visual and semantic information individually from the object-relational visual features and global-local semantic features; then inter-modal selection obtains the joint visual-semantic knowledge by correlating vision and semantics. This hierarchical framework imitates human cognition process to capture targeted visual clues from multiple perceptual views and semantic levels.</p><p>The main contributions are summarized as follows: (1) We exploit the possibility of cognition in visual dialogue by depicting an image from both visual and semantic views, which covers a broad range of visual content referred by most of questions in the visual dialogue task; (2) We propose a hierarchical visual information selection model, which is able to progressively select question-adaptive clues from intra-modal and inter-modal information for answering diverse questions. It supports explicit visualization in visualsemantic knowledge selection and reveals which modality has more contribution to answer the question; (3) The proposed model outperforms state-of-the-art approaches on benchmark visual dialogue datasets, which demonstrates the feasibility and effectiveness of the proposed model. The code is available at https://github.com/JXZe/DualVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Visual Question Answering (VQA) focuses on answering arbitrary natural language questions conditioned on an image. The typical solutions in VQA build multi-modal representations upon CNN-RNN architecture <ref type="bibr" target="#b17">(Ren, Kiros, and Zemel 2015;</ref><ref type="bibr">Qi et al. 2017)</ref>. Existing approaches incorporate context-aware visual features. For example, <ref type="bibr" target="#b17">(Ren, Kiros, and Zemel 2015)</ref> applies CNN features of the whole image as global context, <ref type="bibr" target="#b19">(Xu and Saenko 2016;</ref><ref type="bibr" target="#b1">Anderson et al. 2018</ref>) adopt patches and salient objects learned by attention mechanism as the region context, and <ref type="bibr" target="#b3">(Gao et al. 2018;</ref><ref type="bibr" target="#b10">Li et al. 2019b</ref>) exploits inter-object relationships via graph attention networks or convolutional networks to model the relational context. However, how to leverage the external visual-semantic knowledge to learn more informative relational representations for better semantic understanding has not been well exploited yet. Another emerging line of work represents visual content explicitly by natural language and solves VQA as a reading comprehension problem. In <ref type="bibr" target="#b9">(Li et al. 2019a</ref>), the image is wholly converted into descriptive captions, which preserves information at semantic-level in textual domain. However, this kind of approaches use the generated captions, which could not be correct as we desired, and that they fully abandon the informative and subtle visual features. Besides the specific tasks, our model has notable progress compared to the above approaches. We adopt dual encoding mechanism to provide both appearance-level and semantic-level visual information, so that it incorporates the strong points of the above two kinds of approaches.</p><p>Visual Dialogue aims to answer a current question conditioned on an image and dialogue history. Most existing works are based on late fusion framework and focused on modeling the dialogue history. Sequential co-attention mechanism <ref type="bibr" target="#b15">(Qi et al. 2018)</ref> enables the model to identify question-relevant image regions and dialogue history to keep the dialogue consistency. <ref type="bibr" target="#b21">(Yang, Zha, and Zhang 2019)</ref> introduces false response in dialogue history for an adverse critic on the historic error. <ref type="bibr" target="#b21">(Zheng et al. 2019)</ref> introduces an Expectation Maximization algorithm to infer the dialogue structure and the answers via graph neural networks. By contrast to extensive study on modeling dialogue history, the image content has been less studied. Although some works devise attention mechanism to focus on the essential visual features most relevant to the question and dialogue history, such monolithic visual representations still have limited expressive abilities. In this work, we exploit the role of visual information in visual dialogue. Different from existing works merely modeling the appearance, our model is able to adaptively capture visual and semantic information in a hierarchical mode inspired by the Dual-coding theory of human cognition process to provide adequate visual clues for diverse questions in visual dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>The visual dialogue task can be described as follows: given an image I and its caption C, a dialogue history till round t-1, H t = {C, (Q 1 , A 1 ), ..., (Q t−1 , A t−1 )}, and the current question Q t , the task is to rank a list of 100 candidate answers A = {A 1 , A 2 , ..., A 100 } and return the best answer A t to Q t . In this section, we first introduce the idea of depicting an image from both visual and semantic perspectives. It covers a broad range of visual content like objects, relationships, global semantics and local semantics. Then we introduce a hierarchical feature selection approach to adaptively capture question-relevant visual-semantic information. Our model is based on the late fusion (LF) framework <ref type="bibr" target="#b2">(Das et al. 2017</ref>), which will be described at the end of this section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Late Fusion</head><p>Image I …</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question-Guided Relation Attention</head><p>Question Embedding </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question-Guided Graph Convolution</head><formula xml:id="formula_0">! # $ "# $ #" ! " ! # $ "# $ #" % ! # % ! " $ "# $ #"</formula><p>A man doing a trick on his skateboard on a wall.</p><p>• The man is wearing black pants.</p><p>• Skateboard in the air. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual-Semantic Dual Encoding</head><p>In visual dialogue, two types of information play the primary role to depict an image and answer the diverse questions: visual information and semantic information ( <ref type="figure" target="#fig_2">Figure  2</ref>). For visual information, the major objects and relationships should be kept. In semantic information, higher-level abstraction of the image content should be provided, which involves prior knowledge and complex cognition. In this section, we introduce a dual encoding scheme to generate both visual and semantic representations to depict an image. A scene graph is proposed to represent the visual information while multi-level captions in natural language are leveraged to represent the semantic information. These representations are served as the input of our DualVD model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Graph Construction</head><p>Each image is represented as a scene graph. Let V = {v i } N denotes its nodes, which represents objects detected by a pre-trained object detector and let E = {e ij } N ×N denotes its edges, which represents the semantic visual relationships embedded by our visual relationship encoder. We use a pre-trained Faster-RCNN <ref type="bibr" target="#b16">(Ren et al. 2017)</ref> to detect N objects in an image and describe the object v i as a 2, 048-dimensional vector, denoted by h i . The visual relationship encoder , which is pretrained on a visual relationship benchmark, i.e. GQA (Hudson and Manning 2019), encodes relationships between the subject v i and object v j as a 512-dimensional relation embedding, denoted as r ij . We assume that certain relationship exists between any pair of objects by considering "unknownrelationship" as a special kind of relationship. Therefore, the scene graph we constructed is fully-connected. The visual relationship encoder embeds the relationships between objects into a semantic space which is aligned with their corresponding descriptions in natural language. Such continuous representations instead of discrete labels can preserve the discriminative capability and contextual awareness. Inspired by recent work (Zhang et al. 2019), our encoder consists of a visual part and a textual part. The visual part takes three CNN feature maps corresponding to the visual regions of subject, object and their union region as input and outputs the three encoded embeddings x s , x o and x r . The textual part uses a shared GRU to encode the annotations and yield textual embeddings. The loss function is designed to minimize the cosine similarity between the embeddings of positive visual-textual pairs and alienate negative pairs. The union embedding x r is served as the visual relationship representation r ij between v i and v j .</p><p>Multi-level Image Captions The advantages of captions compared to visual features lie in that captions are represented by natural language with high-level semantics, which can provide straightforward clues for the questions without "heterogeneous gap". Global image caption C (provided by the dataset) is beneficial to response to questions exploring the scene. Meanwhile, dense captions <ref type="bibr" target="#b7">(Johnson, Karpathy, and Fei-Fei 2016)</ref>, denoted as Z = {z 1 , z 2 , ..., z k } (k is the number of dense captions), provide a set of locallevel semantics, including the object properties (position, color, shape, etc.), the prior knowledge related to the objects (weather, species, emotion, etc.), and the relationships between objects (interactions, spatial positions, comparison, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive Visual-Semantic Knowledge Selection</head><p>On top of the visual and semantic image representations, we propose a novel feature selection framework to adaptively select question-relevant information from the image. Under the guidance of the current question, the feature selection process is devised in a hierarchical mode: intra-modal selection first captures the visual and semantic information respectively from the visual module and semantic module; then inter-modal selection obtains the desired visual knowledge from both the visual module and semantic module via selective visual-semantic fusion. The advantages of such hierarchical framework is that it can explicitly reveal the progressive feature selection mode and preserve fine-grained information as much as possible.</p><p>Visual Module This module is presented on the top of <ref type="figure" target="#fig_2">Figure 2</ref>. Based on the constructed scene graph introduced in Scene Graph Construction, we aim to select questionrelevant relation information and object information. For relation information, we propose a relation-based graph attention network to enrich the object representations with question-aware relationships. It mainly consists of two units: Question-Guided Relation Attention highlights the critical relationships and Question-Guided Graph Convolution enriches the object features by its relation-critical neighbors. For object information, we highlight the most informative objects to answer the question. Finally, the clues of objects and relationships are further fused in Object-Relation Information Fusion to obtain the question-relevant visual content. Question-Guided Relation Attention: The questionguided relation attention examines all the relationships to highlight the ones most relevant to the question. First, we select question-relevant information from the dialogue history to merge into the question representation via a gate operation, which is defined as:</p><formula xml:id="formula_1">gate q t = σ(W q [ H t , Q t ] + b q )<label>(1)</label></formula><formula xml:id="formula_2">Q g t = W 1 (gate q t • [ H t , Q t ]) + b 1</formula><p>(2) where "[·, ·]" denotes concatenation, "•" denotes the element-wise product. Each word is represented by concatenating the hidden states extracted from pre-trained GloVe and ELMo models. Then dialogue history H t and the current question Q t are separately encoded with two different LSTMs, denoted as H t and Q t , respectively. gate q t is a vector of gate values over H t and Q t , W 1 (as well as W 2 , ..., W 7 mentioned below) is the linear transformation layer and Q g t is the encoded history-aware question features. The attention weights α ij of all the visual relationships are calculated under the guidance of the question Q g t :</p><formula xml:id="formula_3">α ij = sof tmax(W ρ (W 2 Q g t • W 3 r ij ) + b r )<label>(3)</label></formula><p>Each relation embedding is updated based on the attention importance. Formally defined as:</p><formula xml:id="formula_4">r ij = α ij r ij<label>(4)</label></formula><p>where r ij is the question-guided relation embedding. Question-Guided Graph Convolution: This module further updates each object's representation under the guidance of questions by aggregating information from its neighborhood and the corresponding relationships. Given the feature h j of object v j and its relation embedding r ij , the attention value of v j w.r.t. v i is calculated as:</p><formula xml:id="formula_5">β ij = sof tmax(W g ( Q g t • (W 4 [h j , r ij ])) + b g ) (5)</formula><p>The obtained attention values for all the neighbors of v i are used to compute a linear combination of their features, which serves as the updated representation h i for v i :</p><formula xml:id="formula_6">h i = N j=1 β ij h j<label>(6)</label></formula><p>Since the scene graph is a fully connected graph, the number of neighbors N for each object is equal to the number of objects detected in each image.</p><p>Object-Relation Information Fusion: In visual dialogue, the object appearance and the visual relationships will contribute to infer the answer, but with different contributions. In this module, we adaptively fuse question-relevant object features from both original object feature h i and relationaware object feature h i again by a gate, which is defined by:</p><formula xml:id="formula_7">gate v i = σ(W v [h i , h i ] + b v ) (7) h g i = W 5 (gate v i • [h i , h i ]) + b 5<label>(8)</label></formula><p>where h g i is the updated representation of object v i . The whole image representation I is obtained as the weighted sum of the object representations. In order to strengthen the influence of the current question Q t and the original object features on the retrieved visual clues, we calculate the attention value γ v i for h i under the guidance of Q t :</p><formula xml:id="formula_8">γ v i = sof tmax(W s (Q t • (W 6 h i )) + b s )<label>(9)</label></formula><p>Then the the whole representation of the image I can be updated by:</p><formula xml:id="formula_9">I = N i=1 γ v i h g i<label>(10)</label></formula><p>Semantic Module This module aims to select and merge question-relevant semantic information from global and local captions with a Question-Guided Semantic Attention module and a Global-Local Information Fusion module. The semantic module is located in the middle of <ref type="figure" target="#fig_2">Figure 2</ref>. Question-Guided Semantic Attention: The semantic attention mechanism highlights relevant captions at both globallevel and local-level. This type of attention is guided by the current question which is enhanced with corresponding information from the dialogue history (as introduced above). According to the attention distribution, we enrich the caption representations in order to better adapt to the question. The attention value for each caption in m i ∈ { C, z 1 , z 2 , ..., z k } is calculated as follows:</p><formula xml:id="formula_10">δ q i = sof tmax((W z1 Q g t + b z1 ) T (W z2 m i + b z2 )) (11)</formula><p>The caption representation for C and Z will be updated to C q and Z q :</p><formula xml:id="formula_11">C q = δ q 1 C (12) Z q = k+1 i=2 δ q i z i−1 (13)</formula><p>Global-Local Information Fusion: Some questions are global-related while others are local-related. This step adaptively selects the information from the global caption C q and local caption Z q via a gate as described above:</p><formula xml:id="formula_12">gate c = σ(W c [ C q , Z q ] + b c )<label>(14)</label></formula><formula xml:id="formula_13">T = W 7 (gate c • [ C q , Z q ]) + b 7<label>(15)</label></formula><p>where T is the textural representations for the abstract visual semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective Visual-Semantic Fusion</head><p>When asked to answer a question, the agent will retrieve either the visual information or the semantic information individually, or both simultaneously. In this module, we design a gate operation to decide the contributions of the two modalities on the answer prediction. The gate operation and the final visual knowledge representation S are calculated as:</p><formula xml:id="formula_14">gate s = σ(W s [ I, T ] + b s ) (16) S = gate s • [ I, T ]<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Late Fusion and Discriminative Decoder</head><p>The full model consists of late fusion encoder and discriminative (softmax) decoder. The encoder first embeds each part in a dialogue tuple D = {I, H t , Q t }. Then we concatenate H t and Q t with the visual knowledge representation S into a joint input embedding for answer prediction. The decoder ranks all the answers from a set of 100 candidates A. It first encodes each candidate via a common LSTM. Then a dot product followed by softmax operation is calculated between the joint input embedding and candidates to get the posterior probability over each candidate. We obtain the correct answer by ranking the candidates based on their posterior probabilities. Our model can also be applied to more complex decoders and fusion strategies, such as memory network, co-attention, adversarial network, etc. In this paper, we utilize the simple late fusion and discriminative decoder to highlight the advantages of our visual encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Datasets: We conduct extensive experiments on datasets <ref type="bibr" target="#b2">(Das et al. 2017</ref>): VisDial v0.9 and VisDial v1.0. For both datasets, the examples are split into "train", "val" and "test" and each dialogue contains 10 rounds of question-answer pairs. VisDial v1.0 is an upgraded version of VisDial v0.9. For VisDial v0.9, all the splits are built on MSCOCO images. For VisDial v1.0, all the splits of VisDial v0.9 serve as "train" (120k), while "val" (2k) and "test" (8k) consist of dialogues on extra 10k COCO-like images from Flickr. Evaluation Metrics: We follow the metrics in <ref type="bibr" target="#b2">(Das et al. 2017)</ref> to evaluate the response performance. In the test stage, the model is asked to rank 100 candidate answer options and evaluated by Mean Reciprocal Rank (MRR), Re-call@ k(k = 1, 5, 10) and Mean Rank of human response (Mean) on both datasets. For VisDial v1.0, Normalized Discounted Cumulative Gain (NDCG) is added as an extra metric for more comprehensive analysis. Lower value for Mean and higher value for other metrics are desired.</p><p>Implementation Details: For the textual part, the maximum sentence length of the dialogue history, dense captions and the current question is all set to 20. The hidden state size of all the LSTM blocks is set to 512. We use Faster-RCNN with the ResNet-101 to detect object regions and extract the 2048-dimensional region features. Since some captions with low confidence are likely to introduce unexpected noise and too many captions will decrease the computation efficiency, we select the top 6 (the mean value of the caption distribution) dense captions in our model. We train all of our models by Adam optimizer with 16 epochs, where the mini-batch size is 15 and the dropout ratio is 0.5. For the strategy of learning rate, we first apply warm up strategy for 2 epoches with initial learning rate 1 × 10 −3 and warm-up factor 0.2. Then we adopt cosine annealing learning strategy with initial learning rate η max =1 × 10 −3 and termination learning rate η min =3.4 × 10 −4 for the rest epoches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Results</head><p>In <ref type="table" target="#tab_2">Table 1 and Table 2,</ref>   <ref type="bibr" target="#b13">(Niu et al. 2019</ref>) and DL-61 <ref type="bibr" target="#b4">(Guo, Xu, and Tao 2019)</ref>. Our model consistently outperforms all the approaches on most metrics, which highlights the importance of visual understanding from visual and semantic modules in visual dialogue. CoAtt and HeiCoAtt-QI are relevant to our model in the sense that they leverage attention mechanism to identify question-relevant visual features. However, they ignore the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Ablation study on VisDial v1.0 validation set exploits the influence of the essential components of DualVD. We use the same discriminative decoder for all the following variations: Visual Module without Relationships (VisNoRel): this is our full visual module except that the relation embeddings are replaced by unlabeled edges and the convolution is conducted via the intra-modal attention .</p><p>Visual Module (VisMod): this is our full visual module, which fuses objects and relation features.</p><p>Global Caption (GlCap): this model uses LSTM to encode the global caption to represent the image.</p><p>Local Caption (LoCap): this model uses LSTM to encode the local captions to represent the image.</p><p>Semantic Module (SemMod): this is our full semantic module, which fuses global and local features. DualVD (full model): this is our full model, which incorporates both the visual module and semantic module.</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, models in the first block are designed to evaluate the influence of key components in the visual module.</p><p>ObjRep only considers isolated objects and ignores the relational information, which achieves worse performance compared with VisMod. RelRep considers the relationships by introducing relation embedding. However, empirical study indicates that enhancing visual relationships while weakening object appearance is still not sufficient for better performance. VisNoRel fuses the information from both object appearance and neighborhoods without relational semantics, which achieves slight improvement compared to ObjRep. On top of VisNoRel, VisMod moves a step further by aggregating all the neighborhood features with relational information, which achieves the best performance compared to above three models. Orthogonal to visual part, models in the second block evaluate the influence of key components in the semantic part. The overall performance of either GlCap or LoCap decreases by 1% and 0.15% respectively, compared to their integrated version SemMod, which adaptively selects and fuses the task-specific descriptive clues from both globallevel and local-level captions.</p><p>DualVD results in a great boost compared to SemMod and a relatively slight boost compared to VisMod. This unbalanced boost indicates that visual module provides comparatively richer clues than semantic module. Combining the two modules together gains an extra boost because of their complementary information. The performance of DualVD without ELMo embedding decrease slightly, which proves that the improvement of DualVD mainly comes from the contribution of the novel visual representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretability</head><p>A critical advantage of DualVD lies in its interpretability: DualVD is capable to predict the attention weights in the visual module, semantic module and the gate values in visualsemantic fusion. It supports explicit visualization and can reveal DualVD's mode in information selection. <ref type="figure">Figure 3</ref> shows three examples with variant dependence on visual and semantic modules. The third example (third and fourth rows in <ref type="figure">Figure 3</ref>) shows three round of dialogues about an image. In each round of dialogue, DualVD is capable to capture the most relevant visual and semantic information regarding the current question. In the first question, the visual module highlights the face of a boy and the relationships to his body and the other boy, while the semantic module puts more attention on the captions describing the two boys, which all provide useful clues to infer the correct answer. In the second and third round of dialogues, DualVD respectively attends to the whole grass and the discs. In this example, the attended information is adaptively changed through the dialogue and this explains why the correct answer is selected. Woman sitting on bench.</p><p>Woman with long hair.</p><p>A woman sitting on a bench.</p><p>Woman with long hair.</p><p>A green grassy field.</p><p>Green grass on the ground.</p><p>A woman sits on a bench holding a guitar in her lap.</p><p>Girl with blue hair.</p><p>The girl has long hair.</p><p>A dog with a collar.</p><p>A white and black dog.</p><p>A dog with a black and white collar.</p><p>A girl and dog on a motorcycle dressed in costumes.</p><p>White shorts on a man.</p><p>A man with a white shirt.</p><p>A white hat on a man.</p><p>Boy playing tennis.</p><p>Two women holding tennis rackets.</p><p>Two men playing tennis.</p><p>A man is instructing children in the sport of tennis.</p><p>alues: 52.09% Ratio of total gate values: 47.91%</p><p>Ratio of total gate values: 39.07%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question top</head><p>Yes, a ton of trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Are the boys teenagers?</head><p>They are young boys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do you see a lot of trees?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question1</head><p>Question2 Question3 Answer1 Answer2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer3</head><p>They are both holding discs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dose 1 of the boys holding the disc?</head><p>A man wearing blue shorts.</p><p>Boy wearing blue shirt.</p><p>A blue shirt on a man.</p><p>Two people playing with a frisbee.</p><p>Boy holding blue frisbee.</p><p>Blue shorts on the man.</p><p>2 boys playing disc golf in a forest.</p><p>A man wearing blue shorts.</p><p>Boy wearing blue shirt.</p><p>A blue shirt on a man.</p><p>Two people playing with a frisbee.</p><p>Boy holding blue frisbee.</p><p>Blue shorts on the man.</p><p>2 boys playing disc golf in a forest.</p><p>A man wearing blue shorts.</p><p>Boy wearing blue shirt.</p><p>A blue shirt on a man.</p><p>Two people playing with a frisbee.</p><p>Boy holding blue frisbee.</p><p>Blue shorts on the man.</p><p>2 boys playing disc golf in a forest. Girl with blue hair.</p><p>The girl has long hair.</p><p>A dog with a collar.</p><p>A white and black dog.</p><p>A dog with a black and white collar.</p><p>A girl and dog on a motorcycle dressed in costumes. C: 2 boys playing disc golf in a forest. <ref type="figure">Figure 3</ref>: Visualization for DualVD. Visual module highlights the most relevant subject (red box) according to attention weights of each object (γ v i in Eq. 9) and the objects (orange and blue boxes) with the top two attended relationships (β ij in Eq. 5). Semantic module shows the attention distribution (δ q i in Eq. 11) over the global caption (first row) and the local captions (rest rows), where darker green color indicates bigger attention weight. The yellow thermogram on the top visualizes the gate values (gate s in Eq. 16) of the visual embedding (left) and the caption embedding (right) in visual-semantic fusion. The ratio of gate values for the visual module and semantic module is computed from Eq. 16.</p><p>We further show another two examples with a current question and the dialogue history (first two rows in <ref type="figure">Figure 3</ref>) to reveal DualVD's mode in information selection. We observe that the amount of information derived from each module highly depends on the complexity of the question and the relevance of the content. More information will come from the semantic module when the question involves complex relationships or the semantic module explicitly contains question-relevant clues. In <ref type="figure">Figure 3</ref>, ratio of total gate values reveals the amount of information derived from each module. In the first example, more visual information is required. Similar observation exists for the second question in the third example. Such questions referring to object appearance depend more clues from the visual module. In the second example, the current question is about the relationship between the girl and the hair. The amount of semantic information remarkably increases since there exists explicit evidence "The girl has long hair". This observation holds for the third question in the third example. Since language is a higher-level encoding of the visual content after complex reasoning involved with prior knowledge, it provides more useful clues for semantic-level questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, inspired by the dual-coding theory in cognitive science, we propose a novel DualVD model for visual dialogue. DualVD mainly consists of a visual module and a semantic module, which encodes image information at appearance-level and semantic-level, respectively. Desired clues for answer inference are adaptively selected from the two modules via gate mechanism. Results from extensive experiments on benchmarks demonstrate that deriving visual information from visual-semantic representations can achieve superior performance compared to other state-ofthe-art approaches. Another major advantage of DualVD is its interpretability via progressive visualization. It can give us insight of how information from different modalities is used for inferring answers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview structure of the DualVD model for visual dialogue. The model mainly contains two parts: Visual Module and Semantic Module, where "G" represents gate operation given inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>we compare DualVD with state-ofthe-art discriminative models, namely LF (Das et al. 2017), HRE (Das et al. 2017), MN (Das et al. 2017), SAN-QI (Yang et al. 2016), HieCoAtt-QI (Lu et al. 2016), AMEM (Seo et al. 2017), HCIAE (Lu et al. 2017), SF (Jain, Lazebnik, and Schwing 2018), CoAtt (Qi et al. 2018), CorefMN (Kottur et al. 2018), VGNN (Zheng et al. 2019), LF-Att (Das et al. 2017), MN-Att (Das et al. 2017), RvA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Object Representation (ObjRep): this model uses the averaged object features to represent an image. Object representations are enhanced by question-driven attention. Relation Representation (RelRep): this model applies averaged relation-aware object representations via questionguided relation attention and question-guided graph convolution as the image representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>History H Current Question Q Current Question Q Dialogue History H . Softmax Answer Candidate A Object Regions</head><label></label><figDesc></figDesc><table><row><cell>Is he wearing shorts?</cell><cell></cell><cell></cell></row><row><cell>C: A man doing a trick on his</cell><cell></cell><cell></cell></row><row><cell>skateboard on a wall.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Visual Module</cell></row><row><cell>Faster R-CNN</cell><cell></cell><cell>Visual Embedding</cell></row><row><cell>Image Caption C</cell><cell>Question-Guided Semantic Attention</cell><cell>Semantic Module</cell></row><row><cell>Dense Caption Z</cell><cell></cell><cell>Caption Embedding</cell></row><row><cell></cell><cell>…</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Visual Knowledge</cell></row><row><cell></cell><cell></cell><cell>Embedding</cell></row><row><cell></cell><cell></cell><cell>Answer</cell></row><row><cell>Dialogue</cell><cell>Fused Feature</cell><cell>Candidates</cell></row><row><cell></cell><cell></cell><cell>Score</cell></row><row><cell></cell><cell>A 1</cell><cell>0.20</cell></row><row><cell></cell><cell>A 2</cell><cell>0.10</cell></row><row><cell></cell><cell>A 100</cell><cell>0.05</cell></row></table><note>Q1: Is he young or old? A1: Young, late teens.Q2: Is he in mid-air? A2: Yes.… Q5: What color is his shirt? A5: Purple.! "</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Then C and Z are separately encoded with two different LSTMs, denoted as C and Z = { z 1 , z 2 , ..., z k }, respectively.</figDesc><table /><note>). The words in both C and Z are represented by con- catenated GloVe (Pennington, Socher, and Manning 2014) and ELMo (Peters et al. 2018) word embeddings.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison on validation split of VisDial v0.9.</figDesc><table><row><cell>Model</cell><cell>MRR R@ 1 R@ 5 R@ 10 Mean</cell></row><row><cell>LF</cell><cell>58.07 43.82 74.68 84.07 5.78</cell></row><row><cell>HRE</cell><cell>58.46 44.67 74.50 84.22 5.72</cell></row><row><cell>MN</cell><cell>59.65 45.55 76.22 85.37 5.46</cell></row><row><cell>SAN-QI</cell><cell>57.64 43.44 74.26 83.72 5.88</cell></row><row><cell>HieCoAtt-QI</cell><cell>57.88 43.51 74.49 83.96 5.84</cell></row><row><cell>AMEM</cell><cell>61.60 47.74 78.04 86.84 4.99</cell></row><row><cell>HCIAE</cell><cell>62.22 48.48 78.75 87.59 4.81</cell></row><row><cell>SF</cell><cell>62.42 48.55 78.96 87.75 4.70</cell></row><row><cell>CoAtt</cell><cell>63.98 50.29 80.71 88.81 4.47</cell></row><row><cell>CorefMN</cell><cell>64.10 50.92 80.18 88.81 4.45</cell></row><row><cell>VGNN</cell><cell>62.85 48.95 79.65 88.36 4.57</cell></row><row><cell>DualVD</cell><cell>62.94 48.64 80.89 89.94 4.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison on test-standard split of VisDial v1.0.</figDesc><table><row><cell>Model</cell><cell>MRR R@ 1 R@ 5 R@ 10 Mean NDCG</cell></row><row><cell>LF</cell><cell>55.42 40.95 72.45 82.83 5.95 45.31</cell></row><row><cell>HRE</cell><cell>54.16 39.93 70.47 81.50 6.41 45.46</cell></row><row><cell>MN</cell><cell>55.49 40.98 72.30 83.30 5.92 47.50</cell></row><row><cell>LF-Att</cell><cell>57.07 42.08 74.82 85.05 5.41 40.76</cell></row><row><cell>MN-Att</cell><cell>56.90 42.43 74.00 84.35 5.59 49.58</cell></row><row><cell>CorefMN</cell><cell>61.50 47.55 78.10 88.80 4.40 54.70</cell></row><row><cell>VGNN</cell><cell>61.37 47.33 77.98 87.83 4.57 52.82</cell></row><row><cell>RvA</cell><cell>63.03 49.03 80.40 89.83 4.18 55.59</cell></row><row><cell>DL-61</cell><cell>62.20 47.90 80.43 89.95 4.17 57.32</cell></row><row><cell>DualVD</cell><cell>63.23 49.25 80.23 89.70 4.11 56.32</cell></row><row><cell cols="2">semantic-rich relationships and language priors. It should be</cell></row><row><cell cols="2">noted that our model and the compared approaches all be-</cell></row><row><cell cols="2">long to single-step models. With the success of multi-step</cell></row><row><cell cols="2">reasoning, ReDAN (Gan et al. 2019) achieves 1% boost over</cell></row><row><cell cols="2">our model on most metrics. We believe that stacking our vi-</cell></row><row><cell cols="2">sual encoder to achieve multi-step visual understanding is a</cell></row><row><cell cols="2">promising future work. DL-61 (Guo, Xu, and Tao 2019) is</cell></row><row><cell cols="2">a two-stage network for candidate selection and re-ranking</cell></row><row><cell cols="2">while FGA (Schwartz et al. 2019) conducts attention across</cell></row><row><cell cols="2">all the data parts, which gain relatively high performance on</cell></row><row><cell cols="2">some metrics compared with our model. We believe that our</cell></row><row><cell cols="2">model for the visual part and existing works for the dialogue</cell></row><row><cell cols="2">or answer parts have complementary advantages.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of DualVD on VisDial v1.0.</figDesc><table><row><cell>Model</cell><cell>MRR R@ 1 R@ 5 R@ 10 Mean NDCG</cell></row><row><cell>ObjRep</cell><cell>63.84 49.83 81.27 90.29 4.07 55.48</cell></row><row><cell>RelRep</cell><cell>63.63 49.25 81.01 90.34 4.07 55.12</cell></row><row><cell>VisNoRel</cell><cell>63.97 49.87 81.74 90.60 4.00 56.73</cell></row><row><cell>VisMod</cell><cell>64.11 50.04 81.78 90.52 3.99 56.67</cell></row><row><cell>GlCap</cell><cell>60.02 45.34 77.66 87.27 4.78 50.04</cell></row><row><cell>LoCap</cell><cell>60.95 46.43 78.45 88.17 4.62 51.72</cell></row><row><cell>SemMod</cell><cell>61.07 46.69 78.56 88.09 4.59 51.10</cell></row><row><cell>w/o ELMo</cell><cell>63.67 49.89 80.44 89.84 4.14 56.41</cell></row><row><cell>DualVD</cell><cell>64.64 50.74 82.10 91.00 3.91 57.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>sbj, 0.10 have, 0.20 have, 0.25 obj, 0.06 obj, 0.02 C:</head><label></label><figDesc>A woman sits on a bench holding a guitar in her lap. Q1: Is this in a park? A1: Yes, I believe it is. Q2: Are there others around? A2: No, she is alone. Q3: Does she have a collection bucket? A3: No. Q4: Is her hair long? A4: Yes, pretty long. Q5: Is she wearing a dress? A5: I don't think so, hard to tell. Q6: Does she have shoes on? A6: Yes, flip flops.</figDesc><table><row><cell>Image</cell><cell>Dialogue History</cell><cell>Visual Module</cell><cell>Semantic Module</cell></row><row><cell></cell><cell></cell><cell>Ratio of total gate values: 60.93%</cell><cell></cell></row><row><cell>Question</cell><cell>Is there grass nearby?</cell><cell></cell><cell></cell></row><row><cell>Answer</cell><cell>Yes, there is.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Ratio of total gate values: 52.09%</cell><cell>Ratio of total gate values: 47.91%</cell></row><row><cell>Answer</cell><cell>Yes.</cell><cell></cell><cell></cell></row><row><cell>obj, 0.42</cell><cell></cell><cell></cell><cell></cell></row><row><cell>have, 0.12</cell><cell></cell><cell></cell><cell></cell></row><row><cell>stand, 0.13</cell><cell></cell><cell></cell><cell></cell></row><row><cell>sbj, 0.06</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by the National Key Research and Development Program (Grant No.2017YFB0803301).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
		<title level="m">Visual dialog. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image-question-answer synergistic network for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10434" to="10443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
	<note>Hudson and Manning 2019</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two can play this game: Visual dialog with discriminative question generation and answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazebnik</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schwing ; Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5754" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karpathy</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei ;</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual coreference resolution in visual dialog using neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kottur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="153" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual question answering as reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6319" to="6328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relation-aware graph attention network for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recursive visual attention in visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6679" to="6688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paivio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rinehart</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winston ; Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>Peters et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>NAACL. [Qi et al. 2017] Qi, W.</editor>
		<editor>Chunhua, S.</editor>
		<editor>Peng, W.</editor>
		<editor>Anthony, D.</editor>
		<editor>and Anton van den</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Pennington, Socher, and Manning</publisher>
			<date type="published" when="1971" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1367" to="1381" />
		</imprint>
	</monogr>
	<note>Deep contextualized word representations</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are you talking to me? reasoned visual dialog generation through adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6106" to="6115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiros</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2953" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Factor graph attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2039" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neighbourhood watch: Referring expression comprehension via language-guided graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Making history matter: Gold-critic sequence training for visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zha</forename><surname>Zhang ; Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09326</idno>
		<idno>Zhang et al. 2019</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6669" to="6678" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
