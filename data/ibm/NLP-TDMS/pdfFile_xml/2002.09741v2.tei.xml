<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VFlow: More Expressive Generative Flows with Variational Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqi</forename><surname>Chenli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Tian</surname></persName>
						</author>
						<title level="a" type="main">VFlow: More Expressive Generative Flows with Variational Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative flows are promising tractable models for density modeling that define probabilistic distributions with invertible transformations. However, tractability imposes architectural constraints on generative flows. In this work, we study a previously overlooked constraint that all the intermediate representations must have the same dimensionality with the data due to invertibility, limiting the width of the network. We propose VFlow to tackle this constraint on dimensionality.</p><p>VFlow augments the data with extra dimensions and defines a maximum evidence lower bound (ELBO) objective for estimating the distribution of augmented data jointly with the variational data augmentation distribution. Under mild assumptions, we show that the maximum ELBO solution of VFlow is always better than the original maximum likelihood solution. For image density modeling on the CIFAR-10 dataset, VFlow achieves a new state-of-the-art 2.98 bits per dimension.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative flows <ref type="bibr" target="#b5">(Dinh et al., 2014;</ref><ref type="bibr" target="#b19">Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b14">Ho et al., 2019)</ref> are a promising class of generative models. They define a probability distribution p(x) by applying an invertible transformation x = f −1 ( ) to some simple and known distribution p( ). Stacking a sequence f 1 . . . , f L of deep neural networks as the transformation, generative flows can model complicated highdimensional data. Comparing with generative adversarial networks (GANs) <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref> and variational autoencoders (VAEs) <ref type="bibr" target="#b20">(Kingma &amp; Welling, 2014)</ref>, generative flows are particularly attractive because their sampling process and density estimation are tractable. Due to these Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).    <ref type="bibr" target="#b14">(Ho et al., 2019)</ref> for CIFAR-10. Dimensionality of the transformed data (red) limits the model capacity. (b) Our solution VFlow, where DZ is the dimensionality of the augmented random variable. Only the transformation step f 1 is shown due to space constraint. advantages, generative flows have been applied to a wide range of problems including image generation <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref>, speech synthesis <ref type="bibr" target="#b27">(Prenger et al., 2019)</ref>, 3D point cloud generation <ref type="bibr" target="#b33">(Yang et al., 2019)</ref>, semi-supervised learning <ref type="bibr" target="#b25">(Nalisnick et al., 2019)</ref>, anomaly detection <ref type="bibr" target="#b4">(Choi et al., 2018)</ref>, and ray tracing <ref type="bibr" target="#b24">(Müller et al., 2019)</ref>.</p><p>However, tractability comes with a cost of model expressiveness. To be tractable, generative flows have more architectural constraints compared with other non-invertible models, such as GANs and VAEs. One particular constraint is that the determinant of the Jacobian of f must be efficient to compute. While previous work typically adopts transformations with diagonal <ref type="bibr" target="#b5">(Dinh et al., 2014;</ref><ref type="bibr" target="#b19">Kingma &amp; Dhariwal, 2018)</ref> or triangular Jacobian <ref type="bibr" target="#b26">(Papamakarios et al., 2017)</ref>, there has been lots of recent work developing transformations with free-form Jacobians, including invertible 1x1 convolution <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref>, continuous time flows <ref type="bibr" target="#b1">(Chen et al., 2018;</ref><ref type="bibr" target="#b10">Grathwohl et al., 2019)</ref>, invertible residual blocks <ref type="bibr" target="#b2">Chen et al., 2019)</ref>, and emerging convolutions <ref type="bibr" target="#b15">(Hoogeboom et al., 2019)</ref>.</p><p>In this paper, we study another orthogonal architectural constraint, the bottleneck problem. To be invertible, all the transformation steps f 1 , . . . , f L must output the same dimensionality with the input data x, although each transformation (i.e., neural network) can have internal hidden layers of higher dimensionality. This contradicts with the commonly adopted wisdom of deep learning to learn overcomplete features, i.e., higher dimensional features than the data. As an example, <ref type="figure" target="#fig_2">Fig. 1(a)</ref> presents a state-of-the-art Flow++ <ref type="bibr" target="#b14">(Ho et al., 2019)</ref> architecture. Although each transformation f l has internal higher-dimensional hidden layers (green), its input and output (red) still lie on the lowerdimensional data space. This makes the generative flow highly inefficient because the high-dimensional features extracted within a transformation step cannot be reused by subsequent steps.</p><p>We propose VFlow as a solution to the bottleneck problem. VFlow augments the data x by extra dimensions z, which are interpreted as latent variables. We develop a variational inference framework to learn a generative flow p(x, z) in the augmented data space jointly with the augmented data distribution q(z|x). We show that VFlow is a generalization of the vanilla generative flows, so the augmented dimensions always help. VFlow improves existing generative flows, and achieves a state-of-the-art 2.98 bits per dimension likelihood on the CIFAR-10 dataset.</p><p>On the efficiency side, the additional q(z|x) network and higher data dimensionality of VFlow only add marginal overhead to the vanilla generative flows. Meanwhile, VFlow can be more compact, since more information can be shared between individual transformation steps. Thus, each transformation step can be simpler by avoiding extracting highdimensional features from scratch. We show that VFlow can be 2.6 times more compact than vanilla generative flows, while achieving similar model quality. Our code is opensourced at https://github.com/thu-ml/vflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Backgrounds</head><p>In this section, we review the basics of generative flows and formally define the bottleneck problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generative Flows</head><p>Given a distribution of D X -dimensional data x on the space R D X , the task of generative modeling aims to learn a model distribution p(x; θ) parameterized by θ that approximates the data distribution. The model can be learned with the maximum likelihood principle</p><formula xml:id="formula_0">max θ Ep (x) [log p(x; θ)],<label>(1)</label></formula><p>wherep(x) is the empirical data distribution.</p><p>Generative flows define a sequence of invertible transformation steps f 1 , . . . , f L , that transform a datum x to some random variable ,</p><formula xml:id="formula_1">x f 1 ←→ h 1 f 2 ←→ h 2 · · · f L ←→ ,</formula><p>where follows a simple factorized distribution that p ( ) = i p ( i ), such as the standard normal distribution. For notational simplicity, we define h 0 = x and h L = . Let f be the composition of all the L transformations, such that = f (x; θ), a generative flow defines the model distribution with the change-of-variables formula</p><formula xml:id="formula_2">log p(x; θ) = log p ( ) + log ∂ ∂x ,</formula><p>where log ∂ ∂x is the log-absolute-determinant of the Jacobian of f . Samples from p(x; θ) can be obtained by taking the inverse transformation from p :</p><formula xml:id="formula_3">∼ p , x = f −1 ( ; θ).</formula><p>One popular invertible transformation is the affine coupling layer <ref type="bibr" target="#b6">(Dinh et al., 2017)</ref>, where each transformation h l = f l (h l−1 ; θ) is defined as</p><formula xml:id="formula_4">x 1 , x 2 = split(h l−1 ), y 1 = x 1 , y 2 = µ(x 1 ; θ) + exp(s(x 1 ; θ)) • x 2 , (2) h l = f l (h l−1 ; θ) = concat(y 1 , y 2 ),</formula><p>where split(·) is any operation that splits the input into two disjoint parts, concat(·) is its inverse operation, and µ, s are neural networks with B hidden layers and D H units per layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Bottleneck Problem</head><p>Starting from the work on universal approximation theorems <ref type="bibr" target="#b12">(Gybenko, 1989;</ref><ref type="bibr" target="#b22">Mhaskar, 1993)</ref> of multi-layer perceptrons, it is well known that network width plays an important role on the model capacity. The impact of network width is also verified empirically by recent works such as Wide ResNet <ref type="bibr" target="#b34">(Zagoruyko &amp; Komodakis, 2016)</ref> and EfficientNet <ref type="bibr" target="#b31">(Tan &amp; Le, 2019)</ref>. Almost all existing non-invertible deep models, such as residual networks <ref type="bibr" target="#b13">(He et al., 2016)</ref> and generative adversarial networks <ref type="bibr" target="#b9">(Goodfellow et al., 2014)</ref> have features in a higher-dimensional space than the original data space.</p><p>However, for generative flows, all the transformed data h 0 , . . . , h L must have the same dimensionality D X with the input x due to invertibility, as illustrated in <ref type="figure" target="#fig_2">Fig. 1</ref>. This architecture is ineffective for three reasons:</p><p>1. Few features (green in <ref type="figure" target="#fig_2">Fig. 1</ref>) extracted within each transformation step can pass through the bottleneck (red in <ref type="figure" target="#fig_2">Fig. 1</ref>), so subsequent transformation steps must extract their own features from scratch; 2. For fixed dimensional h l , the benefit of increasing the hidden layer size D H is limited. Unlike non-invertible deep networks, which can approximate arbitrary functions with large D H , the capacity of a single transformation step is intrinsically limited by architectural constraints, even with infinite D H . For example, an affine coupling layer <ref type="bibr" target="#b6">(Dinh et al., 2017)</ref> can not alter all the dimensions at once, while an invertible residual block  has a bounded Lipschitz constant; 3. Due to the limited capacity of a single transformation step, a sufficiently powerful generative flow needs to have many transformation steps, which is expensive.</p><p>We refer to this issue as the bottleneck problem. To reflect the impact of the bottleneck width on model capacity, we denote a generative flow with D-dimensional bottleneck as a D-dimensional flow. Ideally, a D H -dimensional flow completely eliminates the bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VFlow</head><p>We present VFlow, a variational data augmentation framework and compare it with the vanilla generative flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Variational Data Augmentation</head><p>The bottleneck problem can be tackled by increasing the dimensionality of the original data, so that the dimensionality of the flow is also increased. To achieve this, we augment the data x with an additional D Z -dimensional random variable z ∈ R D Z , and model the augmented data distribution p(x, z; θ) with a (D X + D Z )-dimensional flow. The new flow p(x, z; θ) is more powerful since its dimensionality can be adjusted freely by setting D Z . The underlying</p><formula xml:id="formula_5">invertible transformation becomes = f (x, z; θ) where ∈ R D X +D Z .</formula><p>By modeling the augmented data distribution, the log marginal likelihood log p(x; θ) = log p(x, z; θ)dz and optimization problem (1) become intractable in general. Thus, we resort to the variational methods and establish a lower bound of the marginal likelihood with a variational distribution of the augmented data q(z|x; φ):</p><formula xml:id="formula_6">log p(x; θ) ≥ E q(z|x;φ) [log p(x, z; θ) − log q(z|x; φ)],<label>(3)</label></formula><p>which is known as evidence lower bound (ELBO) in variational inference literature. VFlow optimizes the following maximum ELBO objective as a surrogate of the maximum likelihood objective Eq. (1):</p><formula xml:id="formula_7">max θ,φ Ep (x)q(z|x;φ) [log p(x, z; θ) − log q(z|x; φ)]. (4)</formula><p>After training, density estimation can be achieved with importance sampling</p><formula xml:id="formula_8">log p(x; θ) ≈ log 1 S S i=1 p(x, z i ; θ) q(z i |x; φ) ,<label>(5)</label></formula><p>where z 1 , . . . , z S ∼ q(z|x; φ) are the S samples.</p><p>The augmented data distribution q(z|x; φ) is modeled with another conditional flow defined with an invertible transfor-</p><formula xml:id="formula_9">mation z = g −1 ( q ; x, φ): log q(z|x; φ) = log p ( q ) − log ∂z ∂ q ,</formula><p>where q follows the same distribution p with . Given that z = g −1 ( q ; x, φ) is a differentible reparameterization of q , the ELBO in Eq.</p><p>(3) can be optimized with the reparameterization trick <ref type="bibr" target="#b20">(Kingma &amp; Welling, 2014)</ref>. VFlow is illustrated in <ref type="figure" target="#fig_2">Fig. 1(b)</ref>. By choosing different architectures for p(x, z; θ) and q(z|x; φ), VFlow can be combined with various existing generative flows <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b14">Ho et al., 2019;</ref><ref type="bibr" target="#b2">Chen et al., 2019)</ref>, and improve their expressiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Connection to Vanilla Generative Flows</head><p>While VFlow tackles the bottleneck problem, it only maximizes a lower bound of the likelihood. It is thus worth studying whether the gain from increased dimensionality of the flow surpasses the gap between the marginal likelihood and the ELBO.</p><p>We now show that VFlow is indeed better even it only optimizes a lower bound. Before presenting the theoretical results, we need to clarify the parameter space of different flow models.</p><formula xml:id="formula_10">• A vanilla generative flow defines p x (x; θ x ), where θ x ∈ Θ x , and Θ x is the parameter space. • For any D Z &gt; 0, a VFlow defines p a (x, z; θ a ), where z ∈ R D Z , θ a ∈ Θ a , and Θ a is the parameter space. Marginalizing z yields p a (x; θ a ). • For any D Z &gt; 0, the variational distribution is q(z|x; φ), where z ∈ R D Z , φ ∈ Φ, and Φ is the parameter space.</formula><p>With these notations, the maximum likelihood solution of vanilla generative flows (Eq. 1) can be written as max θx Ep (x) [log p x (x; θ x )], and the maximum ELBO solution of VFlow can be written as</p><formula xml:id="formula_11">max θa,φ Ep (x)q(z|x;φ) [log p a (x, z; θ a ) − log q(z|x; φ)].</formula><p>Our analysis is based on the following assumptions:</p><p>A1 (high-dimensional flow can emulate low-dimensional flow) For all θ x ∈ Θ x and D Z &gt; 0, there exists θ a ∈ Θ a , such that for all x and z,</p><formula xml:id="formula_12">p a (x, z; θ a ) = p x (x; θ x )p (z).</formula><p>A2 (the variational family has an identity transformation) For all D Z &gt; 0, there exists φ ∈ Φ, such that for all x and z, q(z|x; φ) = p (z), where p (z) is the simple factorized distribution defined in Sec. 2.1.</p><p>Assumptions A1 and A2 can be verified for most existing invertible transformation steps <ref type="bibr" target="#b6">(Dinh et al., 2017;</ref><ref type="bibr" target="#b19">Kingma &amp; Dhariwal, 2018;</ref><ref type="bibr" target="#b2">Chen et al., 2019)</ref>. Consider the simplest case of a linear flow = xθ x , where</p><formula xml:id="formula_13">θ x ∈ Θ x is an orthonormal matrix. Taking θ a = θ x 0 0 I yields p a (x, z; θ a ) = p x z θ x 0 0 I = p (xθ x )p (z), satisfying Assumption A1. Moreover, q(z|x; I) = p (zI) = p (z), satisfying Assumption A2.</formula><p>We leave the detailed verification for Glow <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref> and Residual Flow  in Appendix A.</p><p>The following theorem compares the maximum ELBO solution Eq. (4) of VFlow with the maximum likelihood solution Eq.</p><p>(1) of vanilla generative flows. Theorem 1. Under Assumptions A1 and A2, for any D Z &gt; 0, we have</p><formula xml:id="formula_14">max θx∈Θx Ep (x) [log px(x; θx)] ≤ max θa∈Θa,φ∈Φ Ep (x)q(z|x;φ) [log pa(x, z; θa) − log q(z|x; φ)].</formula><p>Proof. Our proof is based on a simple construction. Given any vanilla flow model p x (x; θ x ), according to Assumptions A1 and A2, for any D Z &gt; 0, we can construct</p><formula xml:id="formula_15">• θ(θ x ) ∈ Θ a , such that p a (x, z; θ(θ x )) is a factorized distribution p a (x, z; θ(θ x )) = p x (x; θ x )p (z)</formula><p>. This is a very weak model that does not utilize z at all.</p><formula xml:id="formula_16">• φ ∈ Φ, such that the variational distribution is trivial q(z|x; φ) = p (z).</formula><p>Even using these special models, we have</p><formula xml:id="formula_17">log p a (x, z; θ(θ x )) − log q(z|x; θ x ) = log p x (x; θ x ). Now, starting from max θx∈Θx Ep (x) [log px(x; θ)] = max θa∈Θa,φ∈Φ Ep (x)p (z) [log px(x; θ) + log p (z) − log p (z)],</formula><p>considering the special θ(θ x ), we have</p><formula xml:id="formula_18">= max θx∈Θx Ep (x) [log pa(x, z; θ(θx)) − log p (z)],</formula><p>allowing the parameter of p a to be chosen freely from Θ a , not just θ(θ x ) ⊂ Θ a , we have</p><formula xml:id="formula_19">≤ max θa∈Θa Ep (x) [log pa(x, z; θa) − log p (z)],</formula><p>replacing p (z) by the trivial variational distribution, we have</p><formula xml:id="formula_20">= max θa∈Θa Ep (x) [log pa(x, z; θa) − log q(z|x; φ)],<label>(6)</label></formula><p>allowing φ to be chosen freely from Φ, we have</p><formula xml:id="formula_21">≤ max θa∈Θa,φ∈Φ Ep (x) [log pa(x, z; θa) − log q(z|x; φ)].</formula><p>Remark 1: Theorem 1 does not consider optimization issues, such as convergence speed. However, as we shall see in Appendix A, it is rather simple for a VFlow to mimic a vanilla generative flow by setting some parameters to zero, due to the residual structure of transformation steps. Therefore, we hypothesize that VFlow should still be better than vanilla generative flows under the same number of optimizer iterations. This is empirically verified in <ref type="figure" target="#fig_6">Fig. 6</ref>.</p><p>Remark 2: Variational inference-based models such as VAEs rely heavily on the quality of the variational posterior q(z; x, φ) to work well. Unlike VAE, VFlow is better than vanilla generative flows even with a trivial variational distribution q(z|x) = p (z). This can be seen from Eq. (6).</p><p>Finally, combining Theorem 1 with the variational bound Eq.</p><p>(3), we have Corollary 1. Under Assumptions A1 and A2, for any D Z &gt; 0, we have</p><formula xml:id="formula_22">max θx∈Θx Ep (x) [log px(x; θx)] ≤ max θa∈Θa Ep (x) [log pa(x; θa)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Efficiency</head><p>While VFlow makes the model more expressive, its overhead is only marginal. To see this, note that the overhead of VFlow arises from two parts: (1) the cost of computing q(z|x; φ) and (2) the increase of the cost for computing p(x, z; θ) due to the increase of the dimensionality. The first cost can be small by using a much smaller network for q(z|x; φ) than p(x, z; θ). As an extreme case, one can eliminate the cost by adopting q(z|x; φ) = p (z), according to Remark 2. The second cost is small because most computation of p(x, z; θ) is spent on the internal hidden layers (green layers in <ref type="figure" target="#fig_2">Fig. 1)</ref>, whose time complexity is only related to the hidden layer size D H instead of the flow dimensionality D X + D Z .</p><p>On the other hand, VFlow can be more compact and efficient than a vanilla generative flow to achieve similar modeling quality, as it alleviates the ineffectiveness listed in Sec. 2.2 caused by the bottleneck problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Modeling Discrete Data</head><p>The discussion so far is limited to continuous data x. If the data follow a discrete distribution P (x), an additional dequantization step is needed to convert the data from discrete to continuous. <ref type="bibr" target="#b14">Ho et al. (2019)</ref> propose to bound the discrete density with a variational dequantization distribution r(u|x):</p><formula xml:id="formula_23">log P (x) ≥ E r(u|x) [log p(x + u) − log r(u|x)],</formula><p>where u is continuous and p(x + u) is a generative flow for continuous data. Combining with the ELBO Eq. <ref type="formula" target="#formula_6">(3)</ref>, we obtain a lower bound for discrete data</p><formula xml:id="formula_24">log P (x) ≥ E r(u|x),q(z|x+u) [ log p(x + u, z) − log r(u|x) − log q(z|x + u)].<label>(7)</label></formula><p>Estimating the marginal density log P (x) involves similar importance sampling procedure with Eq. <ref type="formula" target="#formula_8">(5)</ref>, but the samples are drawn from the joint distribution r(u|x)q(z|x + u) of dequantization noise and augmented data.</p><p>Although both variational dequantization and VFlow introduce variational distributions, their purposes are different.</p><p>Variational dequantization aims to reduce the gap between the discrete data distribution and continuous model distribution, while VFlow aims to increase the dimensionality of the flow. These approaches are orthogonal to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Works</head><p>There exists a large bulk of works on developing more flexible transformation steps, such as transformations with free-form Jacobians <ref type="bibr" target="#b0">Behrmann et al., 2019;</ref><ref type="bibr" target="#b2">Chen et al., 2019)</ref>, fast fourier transformationbased invertible convolutions <ref type="bibr" target="#b15">(Hoogeboom et al., 2019)</ref>, flexible coupling functions <ref type="bibr" target="#b14">(Ho et al., 2019;</ref><ref type="bibr" target="#b8">Durkan et al., 2019;</ref><ref type="bibr" target="#b24">Müller et al., 2019)</ref>, and masked convolutional layers <ref type="bibr" target="#b15">(Hoogeboom et al., 2019;</ref><ref type="bibr" target="#b30">Song et al., 2019)</ref>. VFlow is orthogonal with these approaches since it tackles a different bottleneck of dimensionality, and can be combined with these works to create better models.</p><p>The bottleneck problem is studied for discriminative invertible models including neural ODEs <ref type="bibr" target="#b7">(Dupont et al., 2019)</ref> and i-RevNets <ref type="bibr" target="#b17">(Jacobsen et al., 2018)</ref>, where zeros are padded to the input data to increase the number of dimensions. In contrast, VFlow studies the much more challenging generative modeling problem. For generative modeling, zero padding does not work because the padded data (x, 0) still lies on a D X -dimensional manifold, while the distribution p ( ) is defined on a D X + D Z dimensional space. Therefore, an invertible transformation does not exist. Similarly, simply replicating the data does not help. Another possible solution is reducing the number of transformations L to one. While this does eliminate the bottleneck problem, the capacity of a single transformation is limited, as discussed in Sec. 2.2.</p><p>Variational autoencoders <ref type="bibr" target="#b20">(Kingma &amp; Welling, 2014)</ref> can be understood as VFlows where both p(x, z; θ) and q(z|x; φ) are generative flows with a single affine coupling layer. Particularly, a Gaussian VAE p(x, z; θ) = N (z; 0, I)N (x; µ(z), exp(s(z)) 2 ) is equivalent with</p><formula xml:id="formula_25">Z ∼ N (0, I), X ∼ N (0, I), z = Z , x = µ( Z ) + exp(s( Z )) • X ,</formula><p>which shares the same form with the affine coupling layer defined in Eq. (2), despite in the opposite direction. VFlows are more general than VAEs by not assuming the hierarchical structure p(x, z) = p(z)p(x|z). Though it is possible for VAEs to implement both p(z) and p(x|z) with generative flows <ref type="bibr" target="#b23">(Morrow &amp; Chiu, 2020;</ref><ref type="bibr" target="#b3">Chen et al., 2017)</ref>, the flow p(x|z) is still D X -dimensional, so the bottleneck problem persists. Another line of work implement q(z|x) with generative flows <ref type="bibr" target="#b21">(Kingma et al., 2016;</ref><ref type="bibr" target="#b28">Rezende &amp; Mohamed, 2015)</ref> but leaves p(x, z) unchanged. VFlow has identical q(z|x) but more powerful p(x, z) than these works. There are also a number of works combining VAEs with autoregressive models <ref type="bibr" target="#b3">(Chen et al., 2017;</ref><ref type="bibr" target="#b11">Gulrajani et al., 2017)</ref>. However they suffer from slow sampling due to the sequential nature of autoregressive models. Finally, while a powerful q(z|x) is critical for VAEs, it is less important for VFlows, since p(x, z) itself is powerful even with q(z|x) = p (z), as discussed in Sec. 3.2.</p><p>Augmented Normalizing Flow (ANF) <ref type="bibr">(Huang et al., 2020)</ref> is an independent parallel work of VFlow. Both ANF and VFlow combine generative flows and variational inference. The main difference is the theoretical guarantee. We view VFlow as a general improvement of flow models, so our theory compares the modeling quality of original and augmented data. In contrast, ANF is more closely related to variational autoencoders, and it rather focuses on the universal approximation of probability distributions in the asymptotic case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Toy Data Experiments</head><p>We first evaluate VFlow on a toy D X = 2 Checkerboard dataset , which is multimodal and its density is shown in <ref type="figure" target="#fig_3">Fig. 2(a)</ref>. The baseline model is Glow <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref>, where each transformation step consists of an affine coupling layer with 2 hidden layers and D H = 50 hidden units per layer. VFlow further augments Glow with a conditional Glow q(z|x; φ) and various number of extra dimensions D Z . All the models are trained for 100,000 iterations with Adam <ref type="bibr" target="#b18">(Kingma &amp; Ba, 2015)</ref> and a batch size of 64, and each experiment is repeated 5 times with different random seeds. Model quality is measured with the log-likelihood log p(x) on a 1,000- sample test set. For VFlow, likelihood is evaluated with 100-sample importance sampling by Eq. (5).</p><p>We study the impact of the dimensionality of the flow D X + D Z ∈ {2, 4, 6, 8, 10}, where D X + D Z = 2 is the baseline Glow and D X + D Z &gt; 2 is VFlow. To control the model size, we vary the total number of transformation steps L ∈ {2, 3, 4, 5, 10, 15, 20}. For baseline Glow, the p-network has all the L p = L transformation steps; and for VFlow, p-network has L p = L − 1 transformation steps and q-network has one transformation step. The result is shown in <ref type="figure">Fig. 3</ref>, VFlow significantly outperforms Glow under similar model size. For example, a 3-step, 10dimensional VFlow achieves −3.51 ± 0.01 log-likelihood ( <ref type="figure" target="#fig_3">Fig. 2(b)</ref>), outperforming the baseline 3-step Glow with −3.67 ± 0.03 log-likelihood ( <ref type="figure" target="#fig_3">Fig. 2(c)</ref>) by a large margin. The 3-layer, 10-dimensional VFlow even outperforms a much larger 20-step Glow, which achieves −3.54 ± 0.05 log-likelihood ( <ref type="figure" target="#fig_3">Fig. 2(d))</ref>, showing that the model can be much more compact by solving the bottleneck problem.</p><p>To further understand why the dimensionality of is important, we visualize the learnt representation for a 2-step Glow and a 2-step VFlow, which has a single transformation step for both p and q. To make visualization possible, z is only one-dimensional, so D X + D Z = 3. Note that having odd number of dimensions is suboptimal because the affine coupling layer cannot split the data into two parts with equal number of dimensions. Moreover, affine coupling layer cannot represent the one-dimensional distribution q(z|x), so we replace it with a Gaussian layer N (z; µ(x), σ(x)) without changing the architecture of µ(x) and σ(x). The  <ref type="figure">Figure 3</ref>. Impact of the dimensionality on the toy dataset. learnt transformations are visualized in <ref type="figure">Fig. 4</ref>. While Glow struggles to map different modes to the compact space of , VFlow does a much better job. VFlow learns a pile of "pies" in the space, where each mode is a pie, and different modes are directly distinguished by the extra dimension z. 1 By shifting the pies to different positions on the x-plane based on z, VFlow can easily handle the multi-modality of the data. Comparing with Glow which needs to map a irregular shape in the space to a square in the x space, VFlow requires a much simpler transformation, because each mode is already a regular pie in the space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Density Estimation of Images</head><p>In this section, we evaluate VFlow on CIFAR-10 2 and Ima-geNet <ref type="bibr" target="#b29">(Russakovsky et al., 2015)</ref> for density estimation of images. VFlow augments a state-of-the-art generative flow, Flow++ <ref type="bibr" target="#b14">(Ho et al., 2019)</ref> by introducing extra dimensions and another variational distribution q(z|x). More specifically, the p(x, z) network is similar with Flow++ shown in <ref type="figure" target="#fig_2">Fig. 1, and</ref> the main difference is the dimensionality of the flow. Variational dequantization is deployed according to Sec. 3.4. We choose the network architecture for q(z|x) to be similar with the variational dequantization network r(u|x) of Flow++. A detailed description of the model architecture is in Appendix B. While we only consider Flow++ for this section due to its impressive density estimation result, our variational data augmentation framework is general and can be combined with future advances of the model architecture.</p><p>Model density <ref type="bibr">(-3.80</ref> <ref type="figure">Figure 4</ref>. Visualization of learnt transformation on toy data. Top row: 2-step Glow. Bottom row: 2-step, 3-dimensional VFlow. Loglikelihood is shown in parenthesis. We sample and visualize the transformed density in x, h1 and space. The density is estimated from samples by kernel density estimation, and we show the 50% probability contour / isosurface for each mode in different color. Glow <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref> 3.35 4.09 3.81 FFJORD  3.40 Residual Flow  3.28 4.01 3.76 MintNet <ref type="bibr" target="#b30">(Song et al., 2019)</ref> 3.32 4.06 Flow++ <ref type="bibr" target="#b14">(Ho et al., 2019)</ref> 3.08 3.86 3.69 VFlow 2.98 3.83 3.66</p><formula xml:id="formula_26">) x h 1 Model density (-3.69) x (top view) x (front view) f 1 f 2 f 1</formula><p>The model size is controlled by three main hyper-parameters, (1) the dimensionality of the flow; (2) the hidden layer size D H ; and (3) the number of hidden layers B per transformation step. For brevity, we refer to a 32×32×C-dimensional flow as a C-channel flow, where a 3-channel flow is the baseline Flow++.</p><p>The model is trained with an Adam optimizer <ref type="bibr" target="#b18">(Kingma &amp; Ba, 2015)</ref> with a batch size 64 for 2,000 epochs. Following <ref type="bibr" target="#b14">(Ho et al., 2019)</ref>, the learning rate linearly warms up to 0.0012 during the first 2,000 training steps, and exponentially decays at a rate of 0.99999 per step starting from the 50,000-th step until it reaches 0.0003. All the experiments are run on 16 RTX 2080Ti GPUs. The model quality is measured by bits per dimension (bpd) <ref type="bibr" target="#b32">(Van Oord et al., 2016)</ref>, where smaller bpd implies higher likelihood and better modeling quality. The likelihood P (x; θ) is evalu-ated with importance sampling as Eq. (5) with S = 4096 samples for CIFAR-10 and S = 1024 for ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Improving Existing Models</head><p>We compare a 6-channel VFlow with existing generative flows in <ref type="table" target="#tab_0">Table 1</ref>, where the hyperparameters D H = 96 and B = 10 are set identical to Flow++. By augmenting the number of channels from 3 to 6, For CIFAR-10, VFlow improves the bpd from 3.08 of Flow++ to 2.98. Samples from Flow++ and VFlow are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Study under Fixed Parameter Budget</head><p>To further investigate the impact of the dimensionality, we vary the number of channels under a fixed 4 million parameter budget on the CIFAR-10 dataset. In this set of experiments, we randomly hold out 10,000 samples from the training set for validation. As the dimensionality grows, we reduce the number of hidden layers B to stay within the parameter budget. The training curve and final bpd are reported in <ref type="figure" target="#fig_6">Fig. 6</ref> and <ref type="table" target="#tab_2">Table 2</ref>  Interestingly, <ref type="figure" target="#fig_6">Fig. 6</ref> suggests that besides improved model capacity, the generalization gap of VFlow is also slightly  smaller than Flow++. We suspect the additional randomness introduced by z acts as an implicit regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Parameter Efficiency</head><p>The last set of experiments aims for more compact models with similar model capacity. As shown by <ref type="table" target="#tab_3">Table 3</ref>, we can reduce the number of parameters of the baseline Flow++ from 31.4 million to 11.9 million, which is a 2.6 times reduction. This reduction of model size is achieved by reducing the hidden layer size D H from 96 to 56. As we argued in Sec. 2.2, the excessive number of hidden units does not help much for a network with merely 3 channels. Increasing the dimensionality of the network (i.e., the bottleneck width) is much more efficient than increasing D H . Therefore, VFlow is not only more expressive but also more compact than vanilla low-dimensional flows. We emphasize that the reduction of model size come solely from resolving the bottleneck problem. Even smaller models can be forged by combining with potentially more compact architectures, such as MintNet <ref type="bibr" target="#b30">(Song et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We identify the bottleneck problem which limits the capacity of generative flows. To tackle this problem, we propose VFlow, a variational data augmentation framework that pads extra dimensions to the data with learnable variational distributions for the padded data. VFlow is a generalization of vanilla generative flows, and can be combined with existing generative flows to improve their expressiveness and compactness. In our experiments on the CIFAR-10 dataset, VFlow achieves a new state-of-the-art 2.98 bpd, while retaining the 3.08 bpd of vanilla Flow++ with 2.6 times less parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VFlow: More Expressive Generative Flows with Variational Data Augmentation Supplementary Material</head><p>A. Verification of Assumption A1 and A2 A1 For all p(x; θ D X ) ∈ P D X and D Z &gt; 0, there exists p(x, z; θ D X +D Z ) ∈ P D X +D Z , such that for all x and z,</p><formula xml:id="formula_27">p(x, z; θ D X +D Z ) = p(x; θ D X )p (z).</formula><p>A2 For all D Z &gt; 0, there exists q(z|x; φ) ∈ Q D Z , such that for all x and z,</p><formula xml:id="formula_28">q(z|x; φ) = p (z).</formula><p>Let x, z be row vectors, and x z be the horizontal concatenation of x and z. We first show that the following conditions are sufficient for Assumption A1 and A2.</p><p>B1 For all θ D X ∈ Θ D X and D Z &gt; 0, there exists θ D X +D Z ∈ Θ D X +D Z , such that for all l, x and z,</p><formula xml:id="formula_29">f l ( x z ; θ D X +D Z ) = f l (x; θ D X ) z .</formula><p>B2 For all D Z &gt; 0, there exists φ ∈ Φ D Z , such that for all l, x and z,</p><formula xml:id="formula_30">g l ( q ; x, φ) = q .</formula><p>Proof. Under condition B1,</p><formula xml:id="formula_31">f ( x z ) =f 1 (. . . (f L ( x z ))) =f 1 (. . . (f L−1 ( f L (x) z ))) =f 1 (. . . (f L−2 ( f L−1 (f L (x)) z ))) = . . . = f 1 (. . . (f L (x))) z .</formula><p>Then,</p><formula xml:id="formula_32">p(x, z; θ D X +D Z ) = p (f ( x z ; θ D X +D Z )) ∂f ( x z ; θ D X +D Z ) ∂ x z = p ( f (x; θ D X ) z ) ∂ f (x; θ D X ) z ∂ x z = p (f (x; θ D X ))p (z) ∂f (x;θ D X ) ∂x 0 0 I = p (f (x; θ D X )) ∂f (x; θ D X ) ∂x p (z) = p(x; θ D X )p (z).</formula><p>VFlow: More Expressive Generative Flows with Variational Data Augmentation Similarly, under condition B2,</p><formula xml:id="formula_33">g( q ; x, φ) = g 1 (. . . (g L ( q ))) = q . So q(z; x, φ) = q(g( q ; x, φ)|x; φ) = p ( q )/ ∂z ∂ q = p ( q )/ |I| = p ( q ).</formula><p>Therefore, we only need to verify condition B1 and B2 separately for each transformation step . For Glow <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref> and Residual Flow , the transformations to verify includes affine coupling layer <ref type="bibr" target="#b6">(Dinh et al., 2017)</ref>, invertible 1×1 convolution <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref>, and invertible residual blocks . In this section we only verify condition B1 and B2 for fully-connected transformations, but they readily generalize to convolutional transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Invertible Residual Blocks</head><p>An invertible residual block  </p><formula xml:id="formula_34">f l (x; θ D X ) for D X -dimensional input x is defined as a 1 = xθ (1) D X , a 2 = n(a 1 ; θ (2) D X ), ∆ x = a 2 θ (3) D X , f l (x; θ D X ) = y = x + ∆ x ,</formula><p>where we explicitly write the first and last linear layer, and leave all the internal hidden layers as n(a 1 ; θ</p><p>(2)</p><formula xml:id="formula_35">D X ). We construct a D X + D Z -dimensional invertible residual block f l ( x z ; θ D X +D Z ) as a 1 = x z θ (1) D X +D Z , a 2 = n(a 1 ; θ (2) D X +D Z ), ∆ x 0 = a 2 θ (3) D X +D Z , f l ( x z ; θ D X +D Z ) = x + ∆ x z + 0 = f l (x; θ D X ) z , satisfying condition B1, where θ (1) D X +D Z = θ (1) D X 0 , θ<label>(2)</label></formula><formula xml:id="formula_36">D X +D Z = θ (2) D X , θ<label>(3)</label></formula><formula xml:id="formula_37">D X +D Z = θ (3) D X 0 .</formula><p>This construction is demonstrated by <ref type="figure" target="#fig_7">Fig. 7</ref>. Intuitively, due to the residual structure, we only need to output 0 for all the z dimensions. Similarly, condition B2 can be satisfied by taking θ</p><p>D Z = 0, so that all the residuals are zero and the network outputs identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Affine Coupling Layer</head><p>An affine coupling layer <ref type="bibr" target="#b6">(Dinh et al., 2017)</ref> </p><formula xml:id="formula_39">f l (x; θ D X ) for D X -dimensional input x is defined as x 1 , x 2 = split(x), y 1 = x 1 , y 2 = µ(x 1 ; θ D X ) + exp(s(x 1 ; θ D X )) • x 2 , f l (x; θ D X ) = concat(y 1 , y 2 ).</formula><p>The case of affine coupling layer is almost identical to the invertible residual block, because both transformations have residual structures. This can be seen by noticing when µ(x 1 ; θ D X ) = s(x 1 ; θ D X ) = 0, f l (x; θ D X ) = x. We explicitly write out the first and last linear layers of µ(·) and s(·):</p><p>x 1 , x 2 = split(x), y 1 = x 1 ,</p><formula xml:id="formula_40">a 1 = x 1 θ (a1) D X , a 2 = µ (a 1 ; θ (a2) D X ), a 3 = a 2 θ (a3) D X , b 1 = x 1 θ (b1) D X , b 2 = s (b 1 ; θ (b2) D X ), b 3 = b 2 θ (b3) D X , y 2 = a 3 + b 3 • x 2 , f l (x; θ D X ) = concat(y 1 , y 2 ).</formula><p>A D X + D Z -dimensional affine coupling layer has the form</p><formula xml:id="formula_41">x 1 z 1 , x 2 z 2 = split( x z ), y 1 = x 1 , a 1 = x 1 z 1 θ (a1) D X +D Z , a 2 = µ (a 1 ; θ (a2) D X +D Z ), a 3 u 3 = a 2 θ (a3) D X +D Z , b 1 = x 1 z 1 θ (b1) D X +D Z , b 2 = s (b 1 ; θ (b2) D X +D Z ), b 3 w 3 = b 2 θ (b3) D X +D Z , y 2 = a 3 + b 3 • x 2 u 3 + w 3 • z 2 , f l ( x z ; θ D X +D Z ) = concat( y 1 z 1 , y 2 ),</formula><p>We want the networks µ(·; θ D X +D Z ) and s(·; θ D X +D Z ) to ignore the z 1 part from the input, and output zero for u 3 , w 3 , so that</p><formula xml:id="formula_42">a3 u 3 = µ(x 1 ; θ D X ) 0 , b3 w 3 = s(x 1 ; θ D X ) 0 y 2 = µ(x 1 ; θ D X ) + exp(s(x 1 ; θ D X )) • x 2 z 2 , f l ( x z ; θ D X +D Z ) = f l (x; θ D X ) z ,</formula><p>so condition B1 is satisfied. We can easily achieve this by setting θ (a1)</p><formula xml:id="formula_43">D X +D Z = θ (a1) D X 0 , θ<label>(a2)</label></formula><formula xml:id="formula_44">D X +D Z = θ (a2) D X , θ<label>(a3)</label></formula><formula xml:id="formula_45">D X +D Z = θ (a3) D X 0 θ (b1) D X +D Z = θ (b1) D X 0 , θ<label>(b2)</label></formula><formula xml:id="formula_46">D X +D Z = θ (b2) D X , θ<label>(b3)</label></formula><formula xml:id="formula_47">D X +D Z = θ (b3) D X 0 .</formula><p>Similarly, condition B2 can be satistied by setting θ (a3)</p><formula xml:id="formula_48">D Z = θ (b3) D Z = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Invertible 1×1 Convolution</head><p>For fully-connected cases, invertible 1×1 convolution <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref> degenerates to a regular matrix multiplication</p><formula xml:id="formula_49">f l (x; θ D X ) = xθ D X ,</formula><p>where θ D X is a non-singular matrix. We construct f l (x; θ D X +D Z ) such that</p><formula xml:id="formula_50">θ D X +D Z = θ D X 0 0 I .</formula><p>Clearly, θ D X +D Z is also non-singular, and f l ( x z ; θ D X +D Z ) = f l (x; θ D X ) z . On the other hand, condition B2 can be satisfied by setting θ D Z = I. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Architecture</head><p>Our model architecture is directly taken from Flow++ <ref type="bibr" target="#b14">(Ho et al., 2019)</ref>, with some minor changes to make best use of the increased dimensionality.</p><p>Flow++ has three types of invertible transformation steps, activation normalization ActNorm <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref>, invertible 1×1 convolution Pointwise <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref> and mixture-of-logistic attention coupling layer MixLogisticAttnCoupling <ref type="bibr" target="#b14">(Ho et al., 2019)</ref>. Each coupling layers is controlled by the number of convolution-attention hidden layers B, number of filters D H , and number of logistic mixture components K, as mentioned in Sec. 6. There are two types of input splits for coupling layer, where ChannelSplit partitions input by channel, and CheckerboardSplit partitions input by space. Squeezing operation SpaceToDepth <ref type="bibr" target="#b6">(Dinh et al., 2017</ref>) is adopted for multiscale modeling. Conditional distributions, including augmented data distribution q(z|x + u) and dequantization distribution r(u|x), are implemented by adding a transformed version of x to the input of every coupling layer. Further denoting TupleFlip as flipping the two split inputs, Inverse(·) as the inverse transformation, and MixLogisticCoupling as MixLogisticAttnCoupling without attention, Flow++ consists the following building blocks:</p><formula xml:id="formula_51">f checker (B, D H , K) =CheckerboardSplit −→ ActNorm −→ Pointwise −→ MixLogisticAttnCoupling(B, D H , K) −→ TupleFlip −→ Inverse(CheckerboardSplit) f channel (B, D H , K) =ChannelSplit −→ ActNorm −→ Pointwise −→ MixLogisticAttnCoupling(B, D H , K) −→ TupleFlip −→ Inverse(ChannelSplit) g checker (B, D H , K) =CheckerboardSplit −→ ActNorm −→ Pointwise −→ MixLogisticCoupling(B, D H , K) −→ TupleFlip −→ Inverse(CheckerboardSplit) g channel (B, D H , K) =ChannelSplit −→ ActNorm −→ Pointwise −→ MixLogisticCoupling(B, D H , K) −→ TupleFlip −→ Inverse(ChannelSplit)</formula><p>We show the model architectures used in Sec. 6.1 and Sec. 6.3 in <ref type="table" target="#tab_4">Table 4</ref>, where the architecture of VFlow is almost identical with the baseline Flow++, except we use both f checker and f channel for the 32 × 32 scale, while Flow++ uses only f checker . Flow++ cannot use f channel for the 32 × 32 scale because there are odd number (3) of channels. The model architectures under 4-million-parameter budget used in Sec. 6.2 are listed in <ref type="table" target="#tab_5">Table 5</ref>. In this experiment, we use a special affine coupling where µ and s are R D Z → R D X functions. We empirically find that adding this special affine coupling layer accelerates the convergence for small networks. The building block with this affine coupling layer with B hidden layers and D H hidden units is denoted as f affine (B, D H ) in <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fixing Gradient Explosion</head><p>In our experiments, we find that the implementation of mixture-of-logistic attention coupling layer <ref type="bibr" target="#b14">(Ho et al., 2019)</ref> sometimes produces huge gradients, leading the training to diverge. To see this, note that the mixture-of-logistic attention coupling layer for a given input x = (x 1 , x 2 ) and the output y = (y 1 , y 2 ) is defined by:</p><p>MixLogCDF(x; π, µ, s) = K i=1 π i σ((x − µ i ) · exp(−s i )), where K i=1 π i = 1 y 1 = x 1 , y 2 = σ −1 (MixLogCDF(x 2 ; π θ (x 1 ), µ θ (x 1 ), s θ (x 1 ))) • exp(a θ (x 1 )) + b θ (x 1 ),</p><p>where σ(x) = 1 1+e −x is the sigmoid function. However, the inverse sigmoid may cause gradient explosion. For example, if x = 1 − 10 −N , then σ −1 (x) = 1 x(1−x) ≈ 10 N . If for each component, x − µ i is large and s i is small, then (x − µ i ) · exp(−s i ) is large, and MixLogCDF(x 2 ; π θ (x 1 ), µ θ (x 1 ), s θ (x 1 ) will be close to 1, leading to gradient explosion of the inverse sigmoid function. For example, if π i = 1, x − µ i = 4 and s i = −1, we have MixLogCDF = σ((x − µ i ) · exp(−s i )) ≈ 1 − 2 · 10 −5 and then the gradient can be very large. We fix this issue by scaling the input of the inverse sigmoid function to [0.05, 0.95]: y 2 = σ −1 (0.05 + 0.9 * MixLogCDF(x 2 ; π θ (x 1 ), µ θ (x 1 ), s θ (x 1 ))) • exp(a θ (x 1 )) + b θ (x 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extra Experiments</head><p>We further study whether it is better to put more parameters on p(x, z) or q(z|x). Under a fixed 4 million total parameter budget, we vary the parameter allocation between p(x, z) or q(z|x), and list the corresponding result and model architecture in <ref type="table">Table 6</ref>. The result implies that it is better to put most parameters on p(x, z), supporting our claim in Sec. 4 that the variational distribution of VFlow is not necessarily as complicated as those in VAEs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>(a) Bottleneck problem in a Flow++</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>3-step Glow (-3.66) (d) 20-step Glow (-3.52) Data and model density on toy data, log-likelihood is shown in parenthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Random samples, where (a) is reprinted from<ref type="bibr" target="#b14">(Ho et al., 2019)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Bpd on training (light) and validation (dark) dataset of Flow++ and VFlow under a 4-million parameter budget (not fully converged). Here bpd is only a upper bound because we evaluate it with ELBO as Eq.(7)instead of the marginal likelihood.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Constructing f l ( x z ; θD X +D Z ) based on f l (x; θD X ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Density modeling results in bits/dim (bpd). We report testing bpd for CIFAR-10 and validation bpd for ImageNet.</figDesc><table><row><cell>Model</cell><cell>CIFAR-10 ImageNet 32x32 ImageNet 64x64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Impact of dimensionality on the CIFAR-10 dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">bpd Parameters DH</cell><cell>B</cell></row><row><cell cols="2">3-channel Flow++ 3.23</cell><cell>4.02M</cell><cell>32</cell><cell>13</cell></row><row><cell>4-channel VFlow</cell><cell>3.16</cell><cell>4.03M</cell><cell>32</cell><cell>11</cell></row><row><cell>6-channel VFlow</cell><cell>3.13</cell><cell>4.01M</cell><cell>32</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Parameter efficiency on CIFAR-10.</figDesc><table><row><cell>Model</cell><cell cols="3">bpd Parameters DH</cell><cell>B</cell></row><row><cell cols="2">3-channel Flow++ 3.08</cell><cell>31.4M</cell><cell>96</cell><cell>10</cell></row><row><cell>6-channel VFlow</cell><cell>2.98</cell><cell>37.8M</cell><cell>96</cell><cell>10</cell></row><row><cell>6-channel VFlow</cell><cell>3.03</cell><cell>16.5M</cell><cell>64</cell><cell>10</cell></row><row><cell>6-channel VFlow</cell><cell>3.08</cell><cell>11.9M</cell><cell>56</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Model architecture for improving existing models experiment and parameter efficiency experiment. ×2 f checker (10, 64, 16) ×2 f checker (10, 56, 10) ×2 f channel (10, 96, 32) ×2 f channel (10, 64, 16) ×2 f channel (10, 56, 10) ×2 ×2 f checker (10, 96, 32) ×2 f checker(10, 64, 16) ×2 f checker (10, 56, 10) ×2 f checker (10, 96, 32) ×3 f channel (10, 96, 32) ×3 f channel (10, 64, 16) ×3 f channel (10, 56, 10) ×3</figDesc><table><row><cell>Model</cell><cell>3-channel Flow++</cell><cell>6-channel VFlow</cell><cell>6-channel VFlow</cell><cell>6-channel VFlow</cell></row><row><cell>Parameters</cell><cell>31.4M</cell><cell>37.8M</cell><cell>16.5M</cell><cell>11.9M</cell></row><row><cell>bpd</cell><cell>3.08</cell><cell>2.98</cell><cell>3.03</cell><cell>3.08</cell></row><row><cell></cell><cell cols="3">Architecture for p(x, z): direction (x, z) →</cell><cell></cell></row><row><cell>32 × 32</cell><cell cols="2">f checker (10, 96, 32) ×4 f checker (10, 96, 32) SpaceToDepth SpaceToDepth</cell><cell>SpaceToDepth</cell><cell>SpaceToDepth</cell></row><row><cell>16 × 16</cell><cell cols="3">f channel (10, 96, 32) Architecture for q(z|x): direction q → z</cell><cell></cell></row><row><cell>32 × 32</cell><cell>N/A</cell><cell>g checker (3, 96, 32) ×4 Sigmoid</cell><cell>g checker (3, 64, 16) ×4 Sigmoid</cell><cell>g checker (3, 56, 10) ×4 Sigmoid</cell></row><row><cell></cell><cell></cell><cell cols="2">Architecture for r(u|x): direction r → u</cell><cell></cell></row><row><cell>32 × 32</cell><cell>g checker (2, 96, 32) ×4 Sigmoid</cell><cell>g checker (2, 96, 32) ×4 Sigmoid</cell><cell>g checker (2, 64, 16) ×4 Sigmoid</cell><cell>f checker (2, 56, 10) ×4 Sigmoid</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Model architecture for ablation experiment under fixed parameter budget. Architecture for p(x, z): direction (x, z) → 32 × 32 f checker (13, 32, 4) ×4 f affine (3, 32) ×1 f affine (3, 32) ×1 f checker (11, 32, 4) ×2 f checker (10, 32, 4) ×2 f channel (11, 32, 4) ×2 f channel (10, 32, 4) ×2 ×2 f checker (11, 32, 4) ×2 f checker (10, 32, 4) ×2 f checker (13, 32, 4) ×3 f channel (11, 32, 4) ×3 f channel (10, 32, 4) ×3 Architecture for q(z|x): direction q → z</figDesc><table><row><cell>Model</cell><cell>3-channel Flow++</cell><cell>4-channel VFlow</cell><cell>6-channel VFlow</cell></row><row><cell>Parameters</cell><cell>4.02M</cell><cell>4.03M</cell><cell>4.01M</cell></row><row><cell>bpd</cell><cell>3.21</cell><cell>3.15</cell><cell>3.12</cell></row><row><cell></cell><cell>SpaceToDepth</cell><cell>SpaceToDepth</cell><cell>SpaceToDepth</cell></row><row><cell cols="2">16 × 16 f channel (13, 32, 4) 32 × 32 N/A</cell><cell>g checker (3, 32, 4) ×4 Sigmoid</cell><cell>g checker (3, 32, 4) ×4 Sigmoid</cell></row><row><cell></cell><cell cols="2">Architecture for r(u|x): direction r → u</cell><cell></cell></row><row><cell>32 × 32</cell><cell>f checker (2, 32, 4) ×4 Sigmoid</cell><cell>f checker (2, 32, 4) ×4 Sigmoid</cell><cell>f checker (2, 32, 4) ×4 Sigmoid</cell></row><row><cell>layer to mix z and x forcibly:</cell><cell></cell><cell></cell><cell></cell></row></table><note>y 1 = z, y 2 = µ(z) + exp(s(z)) • x, f affine = concat(y 1 , y 2 )</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Department of Computer Science and Technology, Institute for AI, BNRist Center, Tsinghua University 2 RealAI. Correspondence to: Jun Zhu &lt;dcszj@mail.tsinghua.edu.cn&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The three axes x0, x1 and z do not distinguish from each other in the space because the invertible 1x1 convolution in Glow can rotate them arbitrarily.2 https://www.cs.toronto.edu/˜kriz/cifar. html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Residual flows for invertible generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9913" to="9923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational lossy autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Waic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01392</idno>
		<title level="m">but why? generative ensembles for robust anomaly detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Augmented neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3134" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural spline flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bekasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Papamakarios</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7509" to="7520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betterncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A latent variable model for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pixelvae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximation by superposition of sigmoidal functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flow++</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Emerging convolutions for generative normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2771" to="2780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07101</idno>
		<title level="m">Augmented normalizing flows: Bridging the gap between generative flows and latent variable models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>VFlow: More Expressive Generative Flows with Variational Data Augmentation Huang</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<title level="m">Deep invertible networks. In International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Approximation properties of a multilayered feedforward artificial neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Mhaskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1eh30NFwB" />
		<title level="m">Variational autoencoders with normalizing flow decoders</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousselle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Novák</surname></persName>
		</author>
		<title level="m">Neural importance sampling. ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">145</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hybrid models with deep and invertible features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4723" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building invertible neural networks with masked convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mintnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11002" to="11012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4541" to="4550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
