<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Sense Disambiguation using a Bidirectional LSTM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-11-18">18 Nov 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kågebäck</surname></persName>
							<email>kageback@chalmers.se</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<addrLine>SE-412 96</addrLine>
									<settlement>Göteborg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Salomonsson</surname></persName>
							<email>hans.salomonsson@chalmers.se</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<addrLine>SE-412 96</addrLine>
									<settlement>Göteborg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Word Sense Disambiguation using a Bidirectional LSTM</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-11-18">18 Nov 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a clean, yet effective, model for word sense disambiguation. Our approach leverage a bidirectional long short-term memory network which is shared between all words. This enables the model to share statistical strength and to scale well with vocabulary size. The model is trained end-to-end, directly from the raw text to sense labels, and makes effective use of word order. We evaluate our approach on two standard datasets, using identical hyperparameter settings, which are in turn tuned on a third set of held out data. We employ no external resources (e.g. knowledge graphs, part-of-speech tagging, etc), language specific features, or hand crafted rules, but still achieve statistically equivalent results to the best state-of-the-art systems, that employ no such limitations. *</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Words are in general ambiguous and can have several related or unrelated meanings depending on context. For instance, the word rock can refer to both a stone and a music genre, but in the sentence "Without the guitar, there would be no rock music" the sense of rock is no longer ambiguous. The task of assigning a word token in a text, e.g. rock, to a well defined word sense in a lexicon is called word sense disambiguation (WSD). From the rock example above it is easy to see that the context surrounding the word is what disambiguates the sense. However, it may not be so obvious that this is a difficult task. To see this, consider instead the phrase "Solid rock" where changing the order of words completely changes the meaning, or "Hard rock crushes heavy metal" where individual words seem to indicate stone but together they actually define the word token as music. With this in mind, our thesis is that to do WSD well we need to go beyond bag of words and into the territory of sequence modeling.</p><p>Improved WSD would be beneficial to many natural language processing (NLP) problems, e.g. machine translation <ref type="bibr" target="#b17">(Vickrey et al., 2005)</ref>, information Retrieval, information Extraction <ref type="bibr" target="#b11">(Navigli, 2009)</ref>, and sense aware word representations <ref type="bibr" target="#b12">(Neelakantan et al., 2015;</ref><ref type="bibr" target="#b6">Kågebäck et al., 2015;</ref><ref type="bibr" target="#b0">Bovi et al., 2015)</ref>. However, though much progress has been made in the area, many current WSD systems suffer from one or two of the following deficits. (1) Disregarding the order of words in the context which can lead to problems as described above.</p><p>(2) Relying on complicated and potentially language specific hand crafted features and resources, which is a big problem particularly for resource poor languages. We aim to mitigate these problems by (1) modeling the sequence of words surrounding the target word, and (2) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation, i.e. word embeddings. Using word embeddings has previously been shown to improve WSD <ref type="bibr" target="#b16">(Taghipour and Ng, 2015;</ref>. However, these works did not consider the order of words or their operational effect on each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The main contributions of this work include:</head><p>• A purely learned approach to WSD that achieves results on par with state-of-the-art resource heavy systems, employing e.g. knowledge graphs, parsers, part-of-speech tagging, etc.</p><p>• Parameter sharing between different word types to make more efficient use of labeled data and make full vocabulary scaling plausible without the number of parameters exploding.</p><p>• Empirical evidence that highlights the importance of word order for WSD.</p><p>• A WSD system that, by using no explicit window, is allowed to combine local and global information when deducing the sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section we introduce the most important underlying techniques for our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bidirectional LSTM</head><p>Long short-term memory (LSTM) is a gated type of recurrent neural network (RNN). LSTMs were introduced by <ref type="bibr">Hochreiter and Schmidhuber (1997)</ref> to enable RNNs to better capture long term dependencies when used to model sequences. This is achieved by letting the model copy the state between timesteps without forcing the state through a non-linearity. The flow of information is instead regulated using multiplicative gates which preserves the gradient better than e.g. the logistic function. The bidirectional variant of LSTM, (BLSTM) <ref type="bibr" target="#b1">(Graves and Schmidhuber, 2005)</ref> is an adaptation of the LSTM where the state at each time step consist of the state of two LSTMs, one going left and one going right. For WSD this means that the state has information about both preceding words and succeeding words, which in many cases are absolutely necessary to correctly classify the sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word embeddings by GloVe</head><p>Word embeddings is a way to represent words as real valued vectors in a semantically meaningful space. Global Vectors for Word Representation (GloVe), introduced by Pennington et al. <ref type="formula">(2014)</ref> is a hybrid approach to embedding words that combine a log-linear model, made popular by <ref type="bibr" target="#b10">Mikolov et al. (2013)</ref>, with counting based co-occurrence statistics to more efficiently capture global statistics. Word embeddings are trained in an unsupervised fashion, typically on large amounts of data, and is able to capture fine grained semantic and syntactic information about words. These vectors can subsequently be used to initialize the input layer of a neural network or some other NLP model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Model</head><p>Given a document and the position of the target word, i.e. the word to disambiguate, the model computes a probability distribution over the possible senses corresponding to that word. The architecture of the model, depicted in <ref type="figure">Figure 1</ref>, consist of a softmax layer, a hidden layer, and a BLSTM. See Section 2.1 for more details regarding the BLSTM. The BLSTM and the hidden layer share parameters over all word types and senses, while the softmax is parameterized by word type and selects the corresponding weight matrix and bias vector for each word type respectively. This structure enables the model to share statistical strength across different word types while remaining computationally efficient even for a large total number of senses and realistic vocabulary sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model definition</head><p>The input to the BLSTM at position n in document D is computed as</p><formula xml:id="formula_0">x n = W x v(w n ), n ∈ {1, . . . , |D|}.</formula><p>Here, v(w n ) is the one-hot representation of the word type corresponding to w n ∈ D. A one-hot representation is a vector with dimension V consisting of |V | − 1 zeros and a single one which index</p><formula xml:id="formula_1">x n−1 x n−2 x n−3 x 0 ...</formula><p>x n+1</p><p>x n+2</p><p>x n+3 x |D| ... a y(n) <ref type="figure">Figure 1</ref>: A BLSTM centered around a word at position n. Its output is fed to a neural network sense classifier consisting of one hidden layer with linear units and a softmax. The softmax selects the corresponding weight matrix and bias vector for the word at position n. .</p><p>indicate the word type. This will have the effect of picking the column from W x corresponding to that word type. The resulting vector is referred to as a word embedding. Further, W x can be initialized using pre-trained word embeddings, to leverage large unannotated datasets. In this work GloVe vectors are used for this purpose, see Section 4.1 for details. The model output,</p><formula xml:id="formula_2">y(n) = softmax(W ay wn a + b ay wn ),</formula><p>is the predicted distribution over senses for the word at position n, where W ay wn and b ay wn are the weights and biases for the softmax layer corresponding to the word type at position n. Hence, each word type will have its own softmax parameters, with dimensions depending on the number of senses of that particular word. Further, the hidden layer a is computed as</p><formula xml:id="formula_3">a = W ha [h L n−1 ; h R n+1 ] + b ha where [h L n−1 ; h R n+1</formula><p>] is the concatenated outputs of the right and left traversing LSTMs of the BLSTM at word n. W ha and b ha are the weights and biases for the hidden layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function</head><p>The parameters of the model, Ω = {W x , Θ BLST M , W ha , b ha , {W ay w , b ay w } ∀w∈V , }, are fitted by minimizing the cross entropy error</p><formula xml:id="formula_4">L(Ω) = − i∈I j∈S(w i ) t i,j log y j (i)</formula><p>over a set of sense labeled tokens with indices I ⊂ {1, . . . , |C|} within a training corpus C, each labeled with a target sense t i , ∀i ∈ I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dropword</head><p>Dropword is a regularization technique very similar to word dropout introduced by <ref type="bibr" target="#b4">Iyyer et al. (2015)</ref>. Both methods are word level generalizations of dropout <ref type="bibr" target="#b15">(Srivastava et al., 2014)</ref> but in word dropout the word is set to zero while in dropword it is replaced with a &lt;dropped&gt; tag. The tag is subsequently treated just like any other word in the vocabulary and has a corresponding word embedding that is trained. This process is repeated over time, so that the words dropped change over time. The motivation for doing dropword is to decrease the dependency on individual words in the training context. This technique can be generalized to other kinds of sequential inputs, not only words.</p><p>To evaluate our proposed model we perform the lexical sample task of SensEval 2 (SE2) <ref type="bibr" target="#b8">(Kilgarriff, 2001)</ref> and SensEval 3 (SE3) <ref type="bibr" target="#b9">(Mihalcea et al., 2004)</ref>, part of the SensEval <ref type="bibr" target="#b7">(Kilgarriff and Palmer, 2000)</ref> workshops organized by Special Interest Group on the Lexicon at ACL. For both instances of the task training and test data are supplied, and the task consist of disambiguating one indicated word in a context. The words to disambiguate are sampled from the vocabulary to give a range of low, medium and high frequency words, and a gold standard sense label is supplied for training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental settings</head><p>The hyperparameter settings used during the experiments, presented in <ref type="table">Table 1</ref>, were tuned on a separate validation set with data picked from the SE2 training set. The source code, implemented using TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>, has been released as open source 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter</head><p>Range  <ref type="table">Table 1</ref>: Hyperparameter settings used for both experiments and the ranges that were searched during tuning. "-" indicates that no tuning were performed on that parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embeddings</head><p>The embeddings are initialized using a set of freely available 2 GloVe vectors trained on Wikipedia and Gigaword. Words not included in this set are initialized from N (0, 0.1). To keep the input noise proportional to the embeddings it is scaled by σ i which is the standard deviation in embedding dimension i for all words in the embeddings matrix, W x . σ i is updated after each weight update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data preprocessing</head><p>The only preprocessing of the data that is conducted is replacing numbers with a &lt; number &gt; tag. This result in a vocabulary size of |V | = 50817 for SE2 and |V | = 37998 for SE3. Words not present in the training set are considered unknown during test. Further, we limit the size of the context to max 140 words centered around the target word to facilitate faster training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The results of our experiments and the state-of-the-art are shown in <ref type="table" target="#tab_2">Table 2</ref>. 100JHU(R) was developed by <ref type="bibr" target="#b18">Yarowsky et al. (2001)</ref> and achieved the best score on the English lexical sample task of SE2 with a F1 score of 64.2. Their system utilized a rich feature space based on raw words, lemmas, POS tags, bag-of-words, bi-gram, and tri-gram collocations, etc. as inputs to an ensemble classifier. htsa3 by <ref type="bibr" target="#b2">Grozea (2004)</ref> was the winner of the SE3 lexical sample task with a F1 score of 72.9. This system was based mainly on raw words, lemmas, and POS tags. These were used as inputs to a regularized least square classifier. IMS+adapted CW is a more recent system, by <ref type="bibr" target="#b16">Taghipour and Ng (2015)</ref>, that uses separately trained word embeddings as input. However, it also relies on a rich set of other features including POS tags, collocations and surrounding words to achieve their reported result. Our proposed model achieves the top score on SE2 and are tied with IMS+adapted CW on SE3. Moreover, we see that dropword consistently improves the results on both SE2 and SE3. Randomizing the order of the input words yields a substantially worse result, which provides evidence for our hypothesis that the order of the words are significant. We also see that the system effectively makes use of the information in the pre-trained word embeddings and that they are essential to the performance of our system on these datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1 score</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions &amp; future work</head><p>We presented a BLSTM based model for WSD that was able to effectively exploit word order and achieve results on state-of-the-art level, using no external resources or handcrafted features. As a consequence, the model is largely language independent and applicable to resource poor languages. Further, the system was designed to generalize to full vocabulary WSD by sharing most of the parameters between words. For future work we would like to provide more empirical evidence for language independence by evaluating on several different languages, and do experiments on large vocabulary all words WSD, where every word in a sentence is disambiguated. Further, we plan to experiment with unsupervised pre-training of the BLSTM, encouraged by the substantial improvement achieved by incorporating word embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for Senseval 2 and 3 on the English lexical sample task.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source for all experiments is available at: https://bitbucket.org/salomons/wsd 2 The employed GloVe vectors are available for download at: http://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to acknowledge the project Towards a knowledge-based culturomics supported by a framework grant from the Swedish Research Council (2012-2016; dnr 2012-5738). Further, the work was partly funded by the BigData@Chalmers initiative. Finally. the authors would like to thank the reviewers for their insightful and constructive feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="726" to="736" />
		</imprint>
	</monogr>
	<note>Knowledge base unification via sense embeddings and disambiguation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding optimal parameter settings for high performance word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Grozea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Senseval-3 Workshop</title>
		<meeting>Senseval-3 Workshop</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and Jürgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Iyyer et al.2015</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining relational and distributional knowledge for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Nieto Piña</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nordic Conference of Computational Linguistics NODALIDA 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural context embeddings for automatic discovery of word senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kågebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on senseval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">English lexical sample task description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The senseval-3 english lexical sample task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mihalcea</surname></persName>
		</author>
		<editor>Rada Mihalcea, Timothy Anatolievich Chklovski, and Adam Kilgarriff</editor>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient non-parametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neelakantan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06654</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple and efficient method to generate word sense representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Nieto Piña</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johansson2015] Luis Nieto</forename><surname>Piña</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Recent Advances in Natural Language Processing</title>
		<meeting>Recent Advances in Natural Language Processing<address><addrLine>Hissar, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Ruslan Salakhutdinov</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised word sense disambiguation using word embeddings in general and specific domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
	<note>Taghipour and Ng2015</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Word-sense disambiguation for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vickrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="771" to="778" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The johns hopkins senseval2 system descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems, SENSEVAL &apos;01</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="163" to="166" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
