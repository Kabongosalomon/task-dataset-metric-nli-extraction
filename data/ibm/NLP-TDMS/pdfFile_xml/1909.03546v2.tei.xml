<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entity, Relation, and Event Extraction with Contextualized Span Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
							<email>dwadden@cs.washington.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
							<email>luanyi@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<email>hannaneh@cs.washington.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Language</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Entity, Relation, and Event Extraction with Contextualized Span Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We examine the capabilities of a unified, multitask framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DYGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (withinsentence) and global (cross-sentence) context. Our framework achieves state-of-theart results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range crosssentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github. com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many information extraction tasks -including named entity recognition, relation extraction, event extraction, and coreference resolution -can benefit from incorporating global context across sentences or from non-local dependencies among phrases. For example, knowledge of a coreference relationship can provide information to help infer the type of a difficult-to-classify entity mention. In event extraction, knowledge of the entities present in a sentence can provide information that is useful for predicting event triggers.</p><p>To model global context, previous works have used pipelines to extract syntactic, discourse, and other hand-engineered features as inputs to structured prediction models <ref type="bibr" target="#b10">(Li et al., 2013;</ref><ref type="bibr" target="#b23">Yang and</ref>  Mitchell, 2016; <ref type="bibr" target="#b9">Li and Ji, 2014)</ref> and neural scoring functions <ref type="bibr" target="#b14">(Nguyen and Nguyen, 2019)</ref>, or as a guide for the construction of neural architectures <ref type="bibr" target="#b15">(Peng et al., 2017;</ref><ref type="bibr" target="#b25">Zhang et al., 2018;</ref><ref type="bibr" target="#b22">Sha et al., 2018;</ref><ref type="bibr" target="#b2">Christopoulou et al., 2018)</ref>. Recent end-toend systems have achieved strong performance by dynmically constructing graphs of spans whose edges correspond to task-specific relations <ref type="bibr" target="#b12">(Luan et al., 2019;</ref>.</p><p>Meanwhile, contextual language models (Dai and Le, 2015; <ref type="bibr" target="#b17">Peters et al., , 2018</ref><ref type="bibr" target="#b4">Devlin et al., 2018)</ref> have proven successful on a range of natural language processing tasks <ref type="bibr" target="#b1">(Bowman et al., 2015;</ref><ref type="bibr" target="#b21">Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b20">Rajpurkar et al., 2016)</ref>. Some of these models are also capable of modeling context beyond the sentence boundary. For instance, the attention mechanism in BERT's transformer architecture can capture relationships between tokens in nearby sentences.</p><p>In this paper, we study different methods to incorporate global context in a general multi-task IE framework, building upon a previous span-based IE method <ref type="bibr" target="#b12">(Luan et al., 2019)</ref>. Our DYGIE++ framework, shown in <ref type="figure" target="#fig_0">Figure 1</ref>, enumerates candidate text spans and encodes them using contextual language models and task-specific message updates passed over a text span graph. Our framework achieves state-of-the results across three IE tasks, leveraging the benefits of both contextualization methods.</p><p>We conduct experiments and a thorough analysis of the model on named entity, relation, and event extraction. Our findings are as follows: (1) Our general span-based framework produces stateof-the-art results on all tasks and all but one subtasks across four text domains, with relative error reductions ranging from 0.2 -27.9%. (2) BERT encodings are able to capture important within and adjacent-sentence context, achieving improved performance by increasing the input window size.</p><p>(3) Contextual encoding through message passing updates enables the model to incorporate crosssentence dependencies, improving performance beyond that of BERT alone, especially on IE tasks in specialized domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task and Model</head><p>Our DYGIE++framework extends a recent spanbased model for entity and relation extraction <ref type="bibr" target="#b12">(Luan et al., 2019)</ref> as follows: (1) We perform event extraction as an additional task and propagate span updates across a graph connecting event triggers to their arguments. (2) We build span representations on top of multi-sentence BERT encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task definitions</head><p>The input is a document represented as a sequence of tokens D, from which our model constructs spans S = {s 1 , . . . , s T }, the set of all possible within-sentence phrases (up to a threshold length) in the document.</p><p>Named Entity Recognition involves predicting the best entity type label e i for each span s i . For all tasks, the best label may be a "null" label. Relation Extraction involves predicting the best relation type r ij for all span pairs (s i , s j ). For the data sets studied in this work, all relations are between spans within the same sentence. The coreference resolution task is to predict the best coreference antecedent c i for each span s i . We perform coreference resolution as auxiliary task, to improve the representations available for the "main" three tasks.</p><p>Event Extraction involves predicting named entities, event triggers, event arguments, and argument roles. Specifically, each token d i is predicted as an event trigger by assigning it a label t i . Then, for each trigger d i , event arguments are assigned to this event trigger by predicting an argument role a ij for all spans s j in the same sentence as d i . Unlike most work on event extraction, we consider the realistic setting where gold entity labels are not available. Instead, we use predicted entity mentions as argument candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DyGIE++ Architecture</head><p>Figure 1 depicts the four-stage architecture. For more details, see <ref type="bibr" target="#b12">(Luan et al., 2019)</ref>. Token encoding: DYGIE++ uses BERT for token representations using a "sliding window" approach, feeding each sentence to BERT together with a size-L neighborhood of surrounding sentences. Span enumeration: Spans of text are enumerated and constructed by concatenating the tokens representing their left and right endpoints, together with a learned span width embedding. Span graph propagation: A graph structure is generated dynamically based on the model's current best guess at the relations present among the spans in the document. Each span representation g t j is updated by integrating span representations from its neighbors in the graph according to three variants of graph propagation. In coreference propagation, a span's neighbors in the graph are its likely coreference antecedents. In relation propagation, neighbors are related entities within a sentence. In event propagation, there are event trigger nodes and event argument nodes; trigger nodes pass messages to their likely arguments, and arguments pass messages back to their probable triggers. The whole procedure is trained end-to-end, with the model learning simultaneously how to identify important links between spans and how to share information between those spans.</p><p>More formally, at each iteration t the model generates an update u t x (i) for span s t ∈ R d :</p><formula xml:id="formula_0">u t x (i) = j∈Bx(i) V t x (i, j) g t j ,<label>(1)</label></formula><p>where denotes elementwise multiplication and V t x (i, j) is a measure of similarity between spans i and j under task x -for instance, a score indicating the model's confidence that span j is the coreference antecedent of span i. For relation extraction, we use a ReLU activation to enforce sparsity. The final updated span representation g t+1 j is computed as a convex combination of the previous representation and the current update, with weights determined by a gating function.</p><p>Multi-task classification: The re-contextualized representations are input to scoring functions which make predictions for each of the end tasks. We use a two-layer feedforward neural net (FFNN) as the scoring function. For trigger and named entity prediction for span g i , we compute FFNN task (g i ). For relation and argument role prediction, we concatenate the relevant pair of embeddings and compute FFNN task ([g i , g j ]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>Data We experiment on four different datasets: ACE05, SciERC, GENIA and WLPC (Statistics and details on all data sets and splits can be found in Appendix A.). The ACE05 corpus provides entity, relation, and event annotations for a collection of documents from a variety of domains such as newswire and online forums. For named entity and relation extraction we follow the train / dev / test split from <ref type="bibr" target="#b13">Miwa and Bansal (2016)</ref>. Since the ACE data set lacks coreference annotations, we train on the coreference annotations from the OntoNotes dataset <ref type="bibr" target="#b18">(Pradhan et al., 2012)</ref>. For event extraction we use the split described in <ref type="bibr" target="#b23">Yang and Mitchell (2016)</ref>; <ref type="bibr" target="#b24">Zhang et al. (2019)</ref>. We refer to this split as ACE05-E in what follows. The SciERC corpus <ref type="bibr" target="#b11">(Luan et al., 2018)</ref> provides entity, coreference and relation annotations from 500 AI paper abstracts. The GENIA corpus <ref type="bibr" target="#b6">(Kim et al., 2003)</ref> provides entity tags and coreferences for 1999 abstracts from the biomedical research literature with a substantial portion of entities (24%) overlapping some other entity. The WLPC dataset provides entity, relation, and event annotations for 622 wet lab protocols <ref type="bibr" target="#b7">(Kulkarni et al., 2018)</ref>. Rather than treating event extraction as a separate task, the authors annotate event triggers as an entity type, and event arguments as relations between an event trigger and an argument.</p><p>Evaluation We follow the experimental setups of the respective state-of-the-art methods for each dataset: <ref type="bibr" target="#b12">Luan et al. (2019)</ref> for entities and relations, and <ref type="bibr" target="#b24">Zhang et al. (2019)</ref> for event extraction. An entity prediction is correct if its label and span matches with a gold entity; a relation is correct if both the span pairs and relation labels match with a gold relation triple. An event trigger is correctly identified if its offsets match a gold trigger. An argument is correctly identified if its offsets and event type match a gold argument. Triggers and arguments are correctly classified if their event types and event roles are also correct, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Variations</head><p>We perform experiments with the following variants of our model architecture.</p><p>BERT + LSTM feeds pretrained BERT embeddings to a bi-directional LSTM layer, and the LSTM parameters are trained together with task specific layers. BERT Finetune uses supervised fine-tuning of BERT on the end-task. For each variation, we study the effect of integrating different task-specific message propagation approaches.</p><p>Comparisons For entity and relation extraction, we compare DYGIE++ against the DYGIE system it extends. DYGIE is a system based on ELMo (Peters et al., 2018) that uses dynamic span graphs to propagate global context. For event extraction, we compare against the method of Zhang et al. <ref type="formula" target="#formula_0">(2019)</ref>, which is also an ELMo-based approach that relies on inverse reinforcement learning to focus the model on more difficult-to-detect events. Implementation Details Our model is implemented using AllenNLP <ref type="bibr" target="#b5">(Gardner et al., 2017)</ref>. We use BERT BASE for entity and relation extraction tasks and use BERT LARGE for event extraction. For BERT finetuning, we use BertAdam with the learning rates of 1 × 10 −3 for the task specific layers, and 5.0 × 10 −5 for BERT. We use a longer warmup period for BERT than the warmup period for task specific-layers and perform linear decay of the learning rate following the warmup       this occurs because all relations are within a single sentence, and thus BERT can be trained to model these relationships well. Our best event extraction results did not use any propagation techniques <ref type="table" target="#tab_4">(Table 4</ref>). We hypothesize that event propagation is not helpful due to the asymmetry of the relationship between triggers and arguments. Methods to model higher-order interactions among event arguments and triggers represent an interesting direction for future work. <ref type="table" target="#tab_7">Table 6</ref> shows that both variations of our BERT model benefit from wider context windows. Our model achieves the best performance with a 3sentence window across all relation and event extraction tasks. Pre-training or Fine Tuning BERT Under Limited Resources <ref type="table" target="#tab_5">Table 5</ref> shows that fine-tuning BERT generally performs slightly better than using the pre-trained BERT embeddings combined with a final LSTM layer. 1 Named entity recognition improves by an average of 0.32 F1 across the four datasets tested, and relation extraction improves by an average of 1.0 F1, driven mainly by the performance gains on SciERC. On event extraction, fine-tuning decreases performance by 1.6 F1 on average across tasks. We believe that this is due to the high sensitivity of both BERT finetuning and event extraction to the choice of optimization hyperparameters -in particular, the trigger detector begins overfitting before the argument detector is finished training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits of Graph Propagation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits of Cross-Sentence Context with BERT</head><p>Pretrained BERT combined with an LSTM layer and graph propagation stores gradients on 15 million parameters, as compared to the 100 million pa-   <ref type="figure">Figure 2</ref> shows an example where span updates passed along a coreference chain corrected an overly-specific entity identification for the acronym "CCRs". We observed similar context sharing via CorefProp in the GENIA data set, and include an example in Appendix C. Coreference propagation updated the span representations of all but one of 44 entities, and in 68% of these cases the update with the largest corefer-(a) The green span CCRs in sentence 2 is updated based on its predicted coreference antecedent.</p><p>(b) The mention of CCRs in sentence 2 serves as a bridge to propagate information from sentence 1 to the mention of CCRs in sentence 3 (c) Coreference link strength. Red is strong. <ref type="figure">Figure 2</ref>: CorefProp enables a correct entity prediction. In each subplot, the green token is being updated by coreference propagation. The preceeding tokens are colored according to the strength of their predicted coreference links with the green token. Tokens in bold are part of a gold coreference cluster discussing CCRs. During the CorefProp updates, the span CCRs in sentence 2 is updated based on its antecedent Category Cooccurrence Restrictions. Then, it passes this information along to the span CCRs in sentence 3. As a result, the model changes its prediction for CCRs in sentence 3 from Method -which is overly specific according to the SciERC annotation guideline -to the correct answer Other Scientific Term. ence "attention weight" came from a text span in a different sentence that was itself a named entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we provide an effective plug-and-play IE framework that can be applied to many information extraction tasks. We explore the abilities of BERT embeddings and graph propagation to capture context relevant for these tasks. We find that combining these two approaches improves performance compared to using either one alone, with BERT building robust multi-sentence representations and graph propagations imposing additional structure relevant to the problem and domain under consideration. Future work could extend the framework to other NLP tasks and explore other approaches to model higher-order interactions like those present in event extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Data</head><p>A.1 Dataset statistics  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ACE event data preprocessing and evaluation</head><p>There is some inconsistency in the ACE event literature on how to handle "time" and "value" event arguments, which are not technically named entities. Some authors, for instance <ref type="bibr" target="#b23">Yang and Mitchell (2016)</ref> leave them in and create new entity types for them. We follow the preprocessing of <ref type="bibr" target="#b24">Zhang et al. (2019)</ref>, who ignore these arguments entirely, since these authors shared their preprocessing code with us and report the current state of the art. We will be releasing code at https://github. com/dwadden/dygiepp to exactly reproduce our data preprocessing, so that other authors can compare their approaches on our data. Due to this discrepancy in the literature, however, our results for named entity and event argument classification are not directly comparable with some previous works.</p><p>In addition, there is some confusion on what constitutes an "Event argument identification". Following <ref type="bibr" target="#b23">Yang and Mitchell (2016)</ref> and <ref type="bibr" target="#b24">Zhang et al. (2019)</ref>, we say that an argument is identified correctly if its offsets and event type are correct. Some other works seem to require require only that an argument's offsets be identified, not its event type. We do not compare against these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Graph Propagation</head><p>We model relation and coreference interactions similarly to <ref type="bibr" target="#b12">Luan et al. (2019)</ref>, and extend the approach to incorporate events. We detail the event propagation procedure here. While the relation and coreference span graphs consist of a single type of node, the event graph consists of two types of nodes: triggers and arguments.</p><p>The intuition behind the event graph is to provide each trigger with information about its potential arguments, and each potential argument with information about triggers for the events in which it might participate. <ref type="bibr">2</ref> The model iterates between updating the triggers based on the representations of their likely arguments, and updating the arguments based on the representations of their likely triggers. More formally, denote the number of possible semantic roles played by an event argument (i.e. the number of argument labels) as L A , B T as a beam of candidate trigger tokens, and B A as a beam of candidate argument spans. These beams are selected by learned scoring functions. For each trigger h t i ∈ B T and argument g t j ∈ B A , the model computes a similarity vector V t A (i, j) by concatenating the trigger and argument embeddings and running them through a feedforward neural network. The k th element of V t A (i, j) scores how likely it is that argument g j plays role k in the event triggered by h i .</p><p>Extending Equation 1, the model updates the trigger h i by taking an average of the candidate argument embeddings, weighted by the likelihood that each candidate plays a role in the event:</p><formula xml:id="formula_1">u t A→T (i) = j∈B A A A→T f (V t A (i, j)) g t j ,<label>(2)</label></formula><p>where A A→T ∈ R d×L A is a learned projection matrix, f is a ReLU function, is an elementwise product, and d is the dimension of the span embeddings. Then, the model computes a gate determining how much of the update from (2) to apply to the trigger embedding:</p><formula xml:id="formula_2">f t A→T (i) = σ W A→T [h t i , u t A→T (i)] ,<label>(3)</label></formula><p>where W A→T ∈ R d×2d is a learned projection matrix and σ is the logistic sigmoid function. Finally, the updated trigger embeddings are computed as follows:</p><formula xml:id="formula_3">h t+1 i = f t A→T (i) h t i +(1−f t A→T (i)) u t A→T (i).</formula><p>(4) Similarly, an update for argument span g j is computed via messages u t T →A (j) as a weighted average over the trigger spans. The update is computed analgously to Equation 2, with a new trainable matrix A T →A . Finally, the gate f t T →A (j) and the updated argument spans g t+1 j are computed in the same fashion as <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula">(4)</ref> respectively. This process represents one iteration of event graph propagation. The outputs of the graph propagation are contextualized trigger and argument representations. When event propagation is performed, the final trigger scorer takes the contextualized surrogate spans h i as input rather than the original token embeddings d i .  <ref type="figure">Figure 4</ref> shows a visualization of the coreference attention weights for a named entity in the GENIA data set. The acronym "v-erbA" is correctly identified as a protein, due to a span update from its coreference antecedent "v-erbA oncoprotein".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C CorefProp visualizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Statistical significance of results</head><p>For a subset of the results in <ref type="table" target="#tab_0">Table 1</ref>, we evaluated statistical significance by re-training a model with 5 random seeds and computing the mean and standard error of the mean of the F1 scores. For ensemble models (trigger detection), we trained 3 ensembles instead of 5 due to the large computational demands of training ensemble models. Due to the large number of experiments performed, it was impractical to perform these tests for every experiment. We report means and standard errors in <ref type="table">Table 9</ref>. For most results, our mean is more than two standard errors above the previous state of the art; those results are significant. Our event argument results are not significant. For trigger classification, our mean F1 is a little less than two standard errors above the state of the art, indicating moderate significance.   <ref type="figure" target="#fig_1">(Fig. 3a)</ref>, or correct to incorrect <ref type="figure" target="#fig_1">(Fig. 3b)</ref>. CorefProp leads to more corrections than mistakes, and tends to make less specific predictions (i.e. OtherScientificTerm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Implementation Details</head><p>Learning rate schedules For BERT finetuning, we used BertAdam with a maximum learning rate of 1×10 −3 for the task specific layers and 5×10 −5 for BERT. For the learning rate schedule, we had an initial warmup period of 40000 batches for the BERT parameters, and 20000 batches for the task specific layers. Following the warmup period, we linearly decayed the learning rate.</p><p>For event extraction models with no finetuning we found that SGD with momentum performed better than Adam. We used the PyTorch implementation of SGD, with an initial learning rate of 0.02, momentum of 0.9, weight decay of 1 × 10 −6 and a batch size of 15 sentences. We cut the learning rate in half whenever dev set F1 had not decreased (a) To classify the mention of v-erbA in Sentence 4, the model can share information from its coreference antecedents (in red).</p><p>v -erbA overexpression c -erbA function the erbA the erbA target gene the erbA target gene CAII CAII The v -erbA oncoprotein erythrocyte -specific genes  <ref type="figure">Figure 4</ref>: CorefProp aggregates information from informative text spans. By using the representation of the span v-erbA oncoprotein in Sentence 2 to update the representation of v-erbA in sentence 4, the model is able to correctly classify the latter entity mention as a protein.</p><p>for 3 epochs.</p><p>For all models, we used early stopping based on dev set loss.</p><p>Hyperparameter selection We experimented with both BERT BASE and BERT LARGE on all tasks. We found that BERT LARGE provided improvement on event extraction with a final LSTM layer, but not on any of the other tasks or event extraction with BERT fine-tuning. In our final experiments we used BERT BASE except in the one setting mentioned were BERT LARGE was better.</p><p>We experiment with hidden layer sizes of 150, 300, and 600 for our feedforward scoring functions. We found that 150 worked well for all tasks except event extraction, where 600 hidden units were used.</p><p>Event extraction modeling details For event extraction we experimented with a final "decoding" module to ensure that event argument assignments were consistent with the types of the events in which they participated -for instance, an entity participating in a "Personnel.Nominate" event can play the role of "Agent", but not the role of "Prosecutor". However, we found in practice that the model learned which roles were compatible with each event type, and constrained decoding did not improve performance. For argument classification, we included the entity label of each candidate argu-  <ref type="table">Table 9</ref>: Mean and standard error of the mean. Trig-C is moderately significant. Arg-ID and Arg-C do not improve SOTA when averaging across five models. The remaining results are highly significant. Note that our means here differ from the numbers in <ref type="table" target="#tab_0">Table 1</ref>, where we report our best single run to be consistent with previous literature.</p><p>ment as an additional feature. At train time we used gold entity labels and at inference time we used the softmax scores for each entity class as predicted by the named entity recognition model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event model ensembling</head><p>For the event extraction experiments summarized in <ref type="table" target="#tab_4">Table 4</ref> we performed early stopping based on dev set argument role classification performance. However, our trigger detector tended to overfit before the argument classifier had finished training. We also found stopping based on dev set error to be unreliable, due to the small size and domain shift between dev and test split on the ACE05-E data set. Therefore, for our final predictions reported in <ref type="table" target="#tab_0">Table 1</ref>, we trained a four-model ensemble optimized for trigger detections rather than event argument classification, and combined the trigger predictions from this model with the argument role predictions from our original model. This combination improves both trigger detection and argument classification, since an argument classification is only correct if the trigger to which it refers is also classified correctly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our framework: DYGIE++. Shared span representations are constructed by refining contextualized word embeddings via span graph updates, then passed to scoring functions for three IE tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>shows confusion matrices for cases where CorefProp corrects and introduces a mistake on SciERC named entity recognition. It tends to correct mistakes where the base model either missed an entity, or was overly specific -classifying an entity as a Material or Method when it should have been classified with the more general label Other-ScientificTerm. Similarly, CorefProp introduces mistakes by assigning labels that are too general, or by missing predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )</head><label>a</label><figDesc>CorefProp corrects a mistake. (b) CorefProp makes a mistake.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Confusion matrix of cases in the SciERC dev set where coreference propagation changed a prediction from incorrect to correct</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(b) Coreference attention weights. Darker is larger. The biggest update comes from The v-erbA oncoprotein</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>DYGIE++ achieves state-of-the-art results. Test set F1 scores of best model, on all tasks and datasets. We define the following notations for events:</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell>SOTA</cell><cell>Ours</cell><cell>∆%</cell></row><row><cell>ACE05</cell><cell>Entity Relation</cell><cell>88.4 63.2</cell><cell>88.6 63.4</cell><cell>1.7 0.5</cell></row><row><cell></cell><cell>Entity</cell><cell>87.1</cell><cell>90.7</cell><cell>27.9</cell></row><row><cell></cell><cell>Trig-ID</cell><cell>73.9</cell><cell>76.5</cell><cell>9.6</cell></row><row><cell>ACE05-Event*</cell><cell>Trig-C</cell><cell>72.0</cell><cell>73.6</cell><cell>5.7</cell></row><row><cell></cell><cell>Arg-ID</cell><cell>57.2</cell><cell>55.4</cell><cell>-4.2</cell></row><row><cell></cell><cell>Arg-C</cell><cell>52.4</cell><cell>52.5</cell><cell>0.2</cell></row><row><cell>SciERC</cell><cell>Entity Relation</cell><cell>65.2 41.6</cell><cell>67.5 48.4</cell><cell>6.6 11.6</cell></row><row><cell>GENIA</cell><cell>Entity</cell><cell>76.2</cell><cell>77.9</cell><cell>7.1</cell></row><row><cell>WLPC</cell><cell>Entity Relation</cell><cell>79.5 64.1</cell><cell>79.7 65.9</cell><cell>1.0 5.0</cell></row></table><note>Trig: Trigger, Arg: argument, ID: Identification, C: Classification. * indicates the use of a 4-model ensem- ble for trigger detection. See Appendix E for details. The results of the single model are reported in Table 2 (c). We ran significance tests on a subset of results in Appendix D. All were statistically significant except Arg-C and Arg-ID on ACE05-Event.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>F1 scores on NER.</figDesc><table><row><cell></cell><cell>ACE05</cell><cell>SciERC</cell><cell>WLPC</cell></row><row><cell>BERT + LSTM</cell><cell>60.6</cell><cell>40.3</cell><cell>65.1</cell></row><row><cell>+RelProp</cell><cell>61.9</cell><cell>41.1</cell><cell>65.3</cell></row><row><cell>+CorefProp</cell><cell>59.7</cell><cell>42.6</cell><cell>-</cell></row><row><cell>BERT FineTune</cell><cell>62.1</cell><cell>44.3</cell><cell>65.4</cell></row><row><cell>+RelProp</cell><cell>62.0</cell><cell>43.0</cell><cell>65.5</cell></row><row><cell>+CorefProp</cell><cell>60.0</cell><cell>45.3</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>F1 scores on Relation.</figDesc><table><row><cell></cell><cell>Entity</cell><cell cols="2">Trig-C Arg-ID</cell><cell>Arg-C</cell></row><row><cell>BERT + LSTM</cell><cell>90.5</cell><cell>68.9</cell><cell>54.1</cell><cell>51.4</cell></row><row><cell>+EventProp</cell><cell>91.0</cell><cell>68.4</cell><cell>52.5</cell><cell>50.3</cell></row><row><cell>BERT FineTune</cell><cell>89.7</cell><cell>69.7</cell><cell>53.0</cell><cell>48.8</cell></row><row><cell>+EventProp</cell><cell>88.7</cell><cell>68.2</cell><cell>50.4</cell><cell>47.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>F1 scores on ACE05-E.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of contextualization methods. All ablations are performed on the dev set except for ACE05-E, where the precedent in the literature is to ablate on test.period. Each of the feed-forward neural networks has two hidden layers and ReLU activations and 0.4 dropout. We use 600 hidden units for event extraction and 150 for entity and relation extraction (more details in Appendix E).</figDesc><table><row><cell>4 Results and Analyses</cell></row></table><note>State-of-the-art Results Table 1 shows test set F1 on the entity, relation and event extraction tasks. Our framework establishes a new state-of-the-art on all three high-level tasks, and on all subtasks except event argument identification. Relative error reductions range from 0.2 -27.9% over previous state of the art models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>shows</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Effect of BERT cross-sentence context. F1 score of relation F1 on ACE05 dev set and entity, arg, trigger extraction F1 on ACE05-E test set, as a function of the BERT context window size.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>In-domain pre-training: SciBERT vs. BERT LSTM + CorefProp model from Table 2 was different from the BERT + LSTM model. We found 44 cases where the CorefProp model corrected an error made by the base model, and 21 cases where it introduced an error. The model without CorefProp was often overly specific in the label it assigned, labeling entities as Material or Method when it should have given the more general label Other Scientific Term. Visualizations of the disagreements between the two model variants can be found in Appendix C.</figDesc><table><row><cell>rameters in BERT BASE . Since the BERT + LSTM +</cell></row><row><cell>Propagation approach requires less memory and is</cell></row><row><cell>less sensitive to the choice of optimization hyperpa-</cell></row><row><cell>rameters, it may be appealing for non-experts or for</cell></row><row><cell>researchers working to quickly establish a reason-</cell></row><row><cell>able baseline under limited resources. It may also</cell></row><row><cell>be desirable in situations where fine-tuning BERT</cell></row><row><cell>would be prohibitively slow or memory-intensive,</cell></row><row><cell>for instance when encoding long documents like</cell></row><row><cell>scientific articles.</cell></row><row><cell>Importance of In-Domain Pretraining We re-</cell></row><row><cell>placed BERT (Devlin et al., 2018) with SciB-</cell></row><row><cell>ERT (Beltagy et al., 2019) which is pretrained on</cell></row><row><cell>a large multi-domain corpus of scientific publica-</cell></row><row><cell>tions. Table 7 compares the results of BERT and</cell></row><row><cell>SciBERT with the best-performing model configu-</cell></row><row><cell>rations. SciBERT significantly boosts performance</cell></row><row><cell>for scientific datasets including SciERC and GE-</cell></row><row><cell>NIA. These results indicate that introducing unla-</cell></row><row><cell>beled text of similar domains for pre-training can</cell></row><row><cell>significantly improve performance.</cell></row><row><cell>Qualitative Analysis To better understand the</cell></row><row><cell>mechanism by which graph propagation improved</cell></row><row><cell>performance, we examined all named entities in the</cell></row><row><cell>SciERC dev set where the prediction made by the</cell></row><row><cell>BERT +</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc>provides summary statistics for all datasets used in the paper.</figDesc><table><row><cell></cell><cell cols="3">Domain Docs Ent</cell><cell cols="3">Rel Trig Arg</cell></row><row><cell>ACE05</cell><cell>News</cell><cell>511</cell><cell>7</cell><cell>6</cell><cell>-</cell><cell>-</cell></row><row><cell>ACE05-E</cell><cell>News</cell><cell>599</cell><cell>7</cell><cell>-</cell><cell>33</cell><cell>22</cell></row><row><cell>SciERC</cell><cell>AI</cell><cell>500</cell><cell>6</cell><cell>7</cell><cell>-</cell><cell>-</cell></row><row><cell>GENIA</cell><cell cols="2">Biomed 1999</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>WLP</cell><cell>Bio lab</cell><cell>622</cell><cell>18</cell><cell>13</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Datasets for joint entity and relation extraction and their statistics. Ent: Number of entity categories.</figDesc><table><row><cell>Rel: Number of relation categories. Trig: Number of</cell></row><row><cell>event trigger categories. Arg: Number of event argu-</cell></row><row><cell>ment categories.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Pre-trained BERT without a final LSTM layer performed substantially worse than either fine-tuning BERT, or using pre-trained BERT with a final LSTM layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Event propagation is a somewhat different idea from<ref type="bibr" target="#b22">(Sha et al., 2018)</ref>, who model argument-argument interactions using a learned tensor. We experimented with adding a similar tensor to our architecture, but did not see any clear performance improvements.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the ONR MURI N00014-18-1-2670, ONR N00014-18-1-2826, DARPA N66001-19-2-4031, NSF (IIS 1616112), Allen Distinguished Investigator Award, and Samsung GRO. We thank Mandar Joshi for his valuable BERT finetuning advice, Tongtao Zhang for sharing the ACE data code, anonymous reviewers, and the UW-NLP group for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scibert: Pretrained contextualized embeddings for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<idno>abs/1903.10676</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A walk-based model on entity graphs for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In NeurIPs</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Allennlp: A deep semantic natural language processing platform</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Genia corpus -a semantically annotated corpus for bio-textmining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="182" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An annotated corpus for machine reading of instructions in wet lab protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Machiraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarse-to-fine inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">One for all: Neural joint modeling of entities and events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huu Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Graphie: A graph-based framework for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Squad: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jointly extracting event triggers and arguments by dependency-bridge rnn and tensor-based argument interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint extraction of events and entities within a document context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint entity and event extraction with generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="99" to="120" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
