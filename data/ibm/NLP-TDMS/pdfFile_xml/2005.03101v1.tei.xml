<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scale-Equalizing Pyramid Convolution for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
							<email>wangxinjiang@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
							<email>zhangshilong@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Yu</surname></persName>
							<email>yuzhuoran@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
							<email>fenglitong@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
							<email>wayne.zhang@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Scale-Equalizing Pyramid Convolution for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature pyramid has been an efficient method to extract features at different scales. Development over this method mainly focuses on aggregating contextual information at different levels while seldom touching the inter-level correlation in the feature pyramid. Early computer vision methods extracted scale-invariant features by locating the feature extrema in both spatial and scale dimension. Inspired by this, a convolution across the pyramid level is proposed in this study, which is termed pyramid convolution and is a modified 3-D convolution. Stacked pyramid convolutions directly extract 3-D (scale and spatial) features and outperforms other meticulously designed feature fusion modules. Based on the viewpoint of 3-D convolution, an integrated batch normalization that collects statistics from the whole feature pyramid is naturally inserted after the pyramid convolution. Furthermore, we also show that the naive pyramid convolution, together with the design of RetinaNet head, actually best applies for extracting features from a Gaussian pyramid, whose properties can hardly be satisfied by a feature pyramid. In order to alleviate this discrepancy, we build a scale-equalizing pyramid convolution (SEPC) that aligns the shared pyramid convolution kernel only at high-level feature maps. Being computationally efficient and compatible with the head design of most single-stage object detectors, the SEPC module brings significant performance improvement (&gt; 4AP increase on MS-COCO2017 dataset) in state-of-the-art one-stage object detectors, and a light version of SEPC also has ∼ 3.5AP gain with only around 7% inference time increase. The pyramid convolution also functions well as a stand-alone module in two-stage object detectors and is able to improve the performance by ∼ 2AP. The source code can be found at https://github.com/jshilong/SEPC. * equal contribution 60 65 70 75 80 85 90 35 36 37 38 39 40 41 42 AP FSAF FreeAnchor Reppoints RetinaNet FCOS Time (ms) C-Faster Faster L-Faster D-Faster Baseline SEPC-lite Two-stage detectors</p><p>Figure 1: Performance on COCO-minival dataset of pyramid convolution in various single-stage detectors including RetinaNet [20], FCOS [38], FSAF [48], Reppoints [44], FreeAnchor [46]. Reference points of two-stage detectors such as Faster R-CNN (Faster) [31], Libra Faster R-CNN (L-Faster) [29], Cascade Faster R-CNN (C-Faster) [1] and Deformable Faster R-CNN (D-Faster) [49] are also provided. All models adopt ResNet-50 backbone and use the 1x training strategy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>An object may appear in vastly different scales in natural images and yet should be recognized as the same. The scales can easily vary by more than 1 magnitude in natural images <ref type="bibr" target="#b34">[33]</ref>, which presents as a challenging task in various computer vision tasks such as object detection. Extensive research has focused on this issue. Multi-scale training <ref type="bibr">[4]</ref> is a direct solution to scale changes by letting the network memorize the patterns at different scales. Multi-scale inference <ref type="bibr" target="#b27">[27]</ref> shares the same idea with traditional image pyramid methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30]</ref>. However, the image pyramid method is time-consuming since multiple inputs are necessary. Intrinsic feature pyramid <ref type="bibr" target="#b24">[24]</ref> in CNNs at different stages provides an efficient alternative to image pyramid. Each level of the downsampled convolutional features corresponds to a specific scale in the original image. However, there exists a semantic gap between each two levels in feature pyramid. To alleviate the discrepancy, different feature fusion strategies have been proposed, including top-down information flow <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7]</ref>, an extra bottom-up information flow path <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b13">14]</ref>, multiple hourglass structures <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b48">47]</ref>, concatenating features from different layers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b9">10]</ref>, feature refinements using non-local attention module <ref type="bibr" target="#b29">[29]</ref>, gradual multi-stage local information fusions <ref type="bibr" target="#b46">[45,</ref><ref type="bibr" target="#b36">35]</ref>. However, the design of feature fusion is intuitive by directly summing up feature maps after resizing them to the same resolution. Intrinsic properties of the feature pyramid are not explored to let all feature maps contribute equally without distinction. Scale-space theory has been studied for decades in traditional computer vision. Effective feature point detection methods <ref type="bibr" target="#b22">[22]</ref> were proposed by detecting scale-space extrema in the pyramid. Motivated by this, we propose to capture the inter-scale interactions through an explicit convolution in the scale dimension, forming a 3-D convolution in the feature pyramid, termed pyramid convolution <ref type="bibr">(PConv)</ref>.</p><p>Convolution in the scale dimension is a natural choice compare to summing up all feature maps directly. For instance, feature maps of neighboring scales on a feature pyramid should correlate the most, which is however neglected in previous methods. A feature pyramid is built by extracting intermediate outputs after each downsample operation of a feature extraction network (backbone), such as VGG <ref type="bibr" target="#b33">[32]</ref>, ResNet <ref type="bibr" target="#b12">[13]</ref> and ResNext <ref type="bibr" target="#b44">[43]</ref>. <ref type="figure" target="#fig_0">Fig. 2</ref> demonstrates the correlation matrix between feature maps extracted from the backbone before and after FPN in Reti-naNet. Values close to the diagonal are larger than remote ones. This is similar to the prior of using spatial convolutions for handling natural image that neighboring pixels on an image correlate stronger than distant pairs. However, this property is not directly captured in previous feature fusion designs <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b37">36]</ref>.</p><p>Further, we also show that the head design of RetinaNet is a special case of PConv with scale kernel = 1, and it actually best suits for extracting features from a Gaussian pyramid. A Gaussian pyramid is generated by consecutively blurring an image with a Gaussian kernel followed by a subsampling. The kernel size of the Gaussian blur should be proportional to the subsampling ratio, so as to remove highfrequency noise during subsampling and not big enough to remove much detail. Conducting PConv in this Gaussian pyramid helps extract scale-invariant features. However, the feature pyramid constructed from a deep backbone network is usually far from a Gaussian pyramid. First, the multiple convolutional layers in the backbone between two feature pyramid levels make a larger effective Gaussian kernel; Second, the theoretical value of effective Gaussian kernel should vary from pixel to pixel due to the non-linearity operations such as ReLU in obtaining the next pyramid features. As a result, we explore the possibility of relaxing these two discrepancies by devising a scaleequalizing module. Using the idea of deformable convolution <ref type="bibr" target="#b4">[5]</ref>, the kernel size at the bottom pyramid is fixed and deforms as the shared kernel strides in the scale dimension. Such a modification over PConv now enables it to equalize different pyramid levels (scales) by aligning its kernels when convolving higher layers, and is thus termed as scaleequalizing pyramid convolution (SEPC). It can be shown to extract scale-invariance features from feature pyramid and only brings a modest computational cost increase since the deformable kernels are only applied to high-level features. Equipped with the SEPC module, the detection performance boosts for various models. For example, SEPC module reaches as high as 4.3AP increase in state-of-the-art single stage detectors, such as FreeAnchor <ref type="bibr" target="#b47">[46]</ref>, FSAF <ref type="bibr" target="#b49">[48]</ref>, Reppoints <ref type="bibr" target="#b45">[44]</ref> and FCOS <ref type="bibr" target="#b39">[38]</ref>, making them even surpass most two-stage detectors. A light version of SEPC (SEPClite) can also reach a performance gain of around 3.5AP with only ∼7% increase in computational cost.</p><p>This study mainly contributes in the following aspects.</p><p>(1). We propose a light-weighted pyramid convolution (PConv) to conduct 3-D convolution inside the feature pyramid to cater for inter-scale correlation.</p><p>(2). We also develop a scale-equalizing pyramid convolution (SEPC) to relax the discrepancy between the feature pyramid and the Gaussian pyramid by aligning the shared PConv kernel only at high-level feature maps.</p><p>(3). The module boosts the performance (∼ 3.5AP increase on state-of-the-art single stage object detectors) with negligible inference speed compromise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object detection</head><p>Modern object detection architectures are generally divided into one-stage and two-stage ones. Two-stage detec-stride=1 stride=1 stride=2 stride=2 stride=0.5 stride=0.5 ... <ref type="figure">Figure 3</ref>: Pyramid convolution as a 3-D convolution. Three convolutional kernels (in red, yellow and cyan) are used for this 3-D convolution. The convolutional stride of each kernel scales as the size of the feature map. Feature maps of the same frame color (e.g. blue and pink) generate the feature map at the right side of the same frame color. The image is only used to show the scale and does not represent feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pyramid convolution</head><p>tion representatives like SPP <ref type="bibr" target="#b11">[12]</ref>, Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>, Faster R-CNN <ref type="bibr" target="#b32">[31]</ref> first extract region proposals and then classify each of them. The scale variance problem is somewhat mitigated in two-stage detectors where objects of different sizes are rescaled to be the same size during the ROI pooling process. On the other hand, single-stage object detection <ref type="bibr" target="#b24">[24]</ref> directly utilizes the intrinsic sliding-window trait of convolutions to build feature pyramids and directly predict objects based on each pixel. Though having earned advantage in real-time tasks due to its fast inference, single-stage detectors has been lagging behind two-stage ones as for the performance. RetinaNet <ref type="bibr" target="#b19">[20]</ref> is a milestone single-stage detector since it boosts detection performance by adopting focal loss and new design of detection head. Following works further accelerate the model and improve its performance simultaneously by viewing object detection as key point localization tasks and thus removing the dependency on multiple anchors at each feature map <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b39">38]</ref>. But the design of FPN and head remains the same as RetinaNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature fusion</head><p>In deep networks, low-level features are generally deemed lacking in semantic information but rich in keeping geometric details, which is the opposite for high-level features. Therefore, feature fusion plays a crucial rule in combining both semantic and geometric information. Several backbone structures have designs of fusing information from different scales such as Inception network <ref type="bibr" target="#b38">[37]</ref> and ScaleNet <ref type="bibr" target="#b16">[17]</ref>. FPN <ref type="bibr" target="#b18">[19]</ref> and its contemporary works leverage high-level feature maps when detecting small objects.</p><p>The following works further the efficiency of feature fusion from different aspects. As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, PA-Net <ref type="bibr" target="#b23">[23]</ref> directly creates a short path for low-level feature maps since detecting large objects also needs the assistance of location-sensitive feature maps. Following the same philosophy, multiple bidirectional information fusion paths were also proposed in <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b48">47]</ref>. Apart from normal approaches of direct summation, some other methods also adopted concatenation to project all feature maps to a common space followed by a back distribution. Pang et al. <ref type="bibr" target="#b29">[29]</ref> furthered the level of feature diffusion by adding a non-local block to fine-tune the combined feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Cross-scale correlation</head><p>There have also been several other methods considering the cross-scale correlation in both traditional and recent research. Cross-scale difference was calculated to approximate the Laplacian operator in SIFT <ref type="bibr" target="#b25">[25]</ref> to extract scaleinvariant features. Worrall &amp; Welling <ref type="bibr" target="#b43">[42]</ref> also extended group convolution to deep neural networks using dilated convolution. Wang et al. <ref type="bibr" target="#b41">[40]</ref> fused feature maps with the neighboring scales to capture inter-scale correlation after all feature maps are transferred to be of the same size as the largest one. In these work, either repeated computations over different transformation of the input image is needed <ref type="bibr" target="#b43">[42]</ref> or scale correlation is conducted on a high-resolution feature map <ref type="bibr" target="#b41">[40]</ref>, both of which incur undesirable increase in computational resources. In this study, the pyramidal structure of feature maps is maintained when conducting convolution across different scales, which is much more ef-  <ref type="bibr" target="#b18">[19]</ref> (c) PA-Net <ref type="bibr" target="#b23">[23]</ref>, (d) Libra R-CNN <ref type="bibr" target="#b29">[29]</ref> and (e) HR-Net <ref type="bibr" target="#b37">[36]</ref>. Each feature map upward has a spatial sized scaled down by two by default. Dotted lines represent interpolation operations, meaning that they can be upsampling, downsampling or shortcut depending on the respective feature map sizes. Each black solid line is an independent convolution, and colored solid lines of the same color are shared convolution operations. ficient in computation. Actually, the original design of the head structure of RetinaNet and its descendants can also be viewed as a PConv with scale kernel of 1. Therefore, our design of PConv is compatible with the state-of-the-art singlestage object detectors with minimal computational cost increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pyramid convolution</head><p>The pyramid convolution (PConv) is indeed a 3-D convolution across both scale and spatial dimensions. If we represent the features in each level as a dot as in <ref type="figure" target="#fig_1">Fig. 4a</ref>, the PConv can be represented as N different 2-D convolutional kerels. Nevertheless, as shown in <ref type="figure">Fig. 3</ref> there is a size mismatch across different pyramid levels. The spatial size is scaled down as the pyramid level goes up. In order to accommodate the mismatch, we set different strides for the K different kernels when convolving in different layers. For example, for PConv with N = 3, the first kernel should have a stride of 2 while the last one should have a stride of 0.5. Then the output of the PConv is</p><formula xml:id="formula_0">y l = w 1 * s0.5 x l+1 + w 0 * x l + w −1 * s2 x l−1 , (1)</formula><p>where l denotes pyramid level, w 1 , w 0 and w −1 are three independent 2-D convolutional kernels, x is the input feature map and * s2 means a convolution with stride 2. The kernel of stride 0.5 is further replaced by a normal convolution with stride of 1 and a consecutive bilinear upsampling layer. That is,</p><formula xml:id="formula_1">y l = Upsample(w 1 * x l+1 ) + w 0 * x l + w −1 * s2 x l−1 (2)</formula><p>Similar to conventional convolutions, zero-padding is also used for PConv. As for the bottom pyramid level (l = 1), the last term in Eq. 2 is unnecessary while for the top-most level (l = L), the first term is ignored. Despite the 3 convolution operations at each layer, the total FLOPs of PConv is actually only around 1.5 times as much as the original head (see Appen. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pipeline</head><p>Apart from the ability of extracting scale-correlated features, PConv also benefits from its compatibility with the head design of RetinaNet and its descendants. As seen from <ref type="figure" target="#fig_3">Fig. 5a</ref>, RetinaNet head is actually also a PConv with a scale kernel of one. Therefore, the 4 convolutional heads can be directly replaced by our PConv module with a scale kernel of 3. The stacked PConv echoes the stacked convolutions modules in 3-D deep networks <ref type="bibr" target="#b40">[39]</ref>, so as to gradually increase correlation distance without much computational burden.</p><p>However, each PConv still brings some additional computation. As an alternative, the 4 PConv modules are shared by both classification and localization branch, forming a combined head structure as shown in <ref type="figure" target="#fig_3">Fig. 5b</ref>. In order to cater for the difference in the classification and localization tasks, an extra normal convolution are also added after the shared 4 PConv modules. It can be calculated that this design has even less FLOPs than the original RetinaNet head (see Appen. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Integrated batch normalization (BN) in the head</head><p>In this study, we also retrieve the use of BN in detection head. A shared BN follows the PConv module and collects statistics from all feature maps inside the feature pyramid, instead of from a single layer. This design comes naturally as we view PConv as a 3-D convolution. Since the statistics are collected from all the feature maps inside the pyramid, the variance becomes smaller, especially for high-level features having small feature map sizes. This enables us to train BN in the head even in a small batch size ∼ 4 and achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Scale-equalizing pyramid convolution</head><p>In designing the pyramid convolution, we have used a naive implementation. The kernel size of each 2-D convo-   lution used in PConv keeps constant when the kernel strides along the scale dimension, even though the feature map size shrinks. This is reasonable when PConv is conducted on a Gaussian pyramid (a Gaussian pyramid is built by consecutively Gaussian blurring an image followed by a downsampling) since Remark 1 Pyramid convolution is able to extract scaleinvariant features from a Gaussian pyramid.</p><p>The detailed mathematical proof can be found in Appen. 3. It is shown intuitively in <ref type="figure" target="#fig_4">Fig. 6a</ref>. When a PConv with N = 1 extracts features from the pyramid, objects of different scales can be captured by the same kernel at different level. Moreover, the Gaussian blur is also necessary in generating the pyramid so as to avoid high-frequency noises in extracting features in downsampled images. On the other hand, too strong blur conceals details. The optimal blurring kernel in the Gaussian pyramid is around the size of the downsampling ratio between two pyramid levels.</p><p>In the naive implementation of PConv, and also in the design of RetinaNet head, such a fashion is directly used to process feature pyramid. However, the optimal blurring kernel is hardly satisfied for feature pyramid. In <ref type="figure" target="#fig_4">Fig. 6b</ref>, we see that the blurring effect of feature maps in high-level features becomes much more serious than that in an image pyramid. This is due to the many layers of convolution and non-linearity operations in the backbone between two feature maps in the feature pyramid.</p><p>In order to compromise the stronger blurring effect and extract scale-invariant features, some studies advocated using dilated convolution <ref type="bibr" target="#b43">[42]</ref>. That is, as the PConv module strides in the scale dimension, the kernel should also be Instead, we borrow the idea of deformable convolution to directly predict the offset of the convolutional kernel as the shared kernel strides upward in the scale dimension. As shown in <ref type="figure" target="#fig_4">Fig. 6b</ref>, the kernel convolving with the the bottom feature map is fixed as a normal 3 × 3 convolution. As it processes high-level feature maps in the feature pyramid, a deformation offset is predicted based on the current layer of feature map. In this way, features in each pyramid level (scale) are equalized by the deformation offset and is ready to be convolved by the shared PConv kernels. Therefore, it is termed scale-equalizing pyramid convolution (SEPC). The pseudo-code for both PConv and SEPC can be found in Appen. 2.</p><p>There are multiple benefits in SEPC. 1) The larger blurring effect between two layers of feature pyramid is considered due to its dilating ability of a deformable convolution kernel; 2) The discrepancy of a feature pyramid from the Gaussian pyramid is alleviated. 3) Since the computational cost of a convolution reduces by 4 from one layer to its upper feature pyramid level, adding deformable convolution only to the high-level feature maps incurs minimal computations. In this study, we study the effect of both SEPC-full that applies SEPC to both the combined and the extra head in <ref type="figure" target="#fig_3">Fig. 5b</ref>, and SEPC-lite that applies SEPC only to the extra head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The experiments in this study were performed on the MS-COCO2017 detection dataset <ref type="bibr" target="#b20">[21]</ref> of 80 categories. The training set is composed of around 118k images, and the validation set consists of 5k images (minival). The detection metrics are reported by default on minival. The results on the test set (test-dev) are also reported for several models in this study. More details in regards to the experimental settings can be found in Appen. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Single-stage object detectors</head><p>The single stage object detectors in this study are mostly the latest and state-of-the-art models, using either anchorbased such as RetinaNet, <ref type="bibr" target="#b19">[20]</ref> and FreeAnchor <ref type="bibr" target="#b47">[46]</ref> or anchor-free methods such as FSAF <ref type="bibr" target="#b49">[48]</ref> and Reppoints <ref type="bibr" target="#b45">[44]</ref>. The results between the proposed SEPC and their original baselines are compared in Tab. 1. SEPC-full is found able to boosts the performance by more than 4AP, yet incurring unnecessary inference time increase due to the deformable operations involved. The improvement of SEPC-lite in each network is also substantial, increasing 3.1 ∼ 3.8AP with only 7% latency increase. A direct comparison of SEPClite on more detectors is shown in <ref type="figure" target="#fig_7">Fig. 1</ref>. It should be noted that our own implemented baseline of FSAF (see details in Appen. 6) already achieves 36.9AP, 1.1AP higher than the original results. And the performance of FSAF is further boosted to 40.9 by SEPC-lite, which is even 0.5AP higher than that of Cascade and Deformable Faster-RCNN while maintaining more than 20% faster. The improvement of SEPC-lite on FreeAnchor, one of the best single-stage detectors, is also surprising, making it achieve 41.7 and painlessly furthering the state of the art by 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Effect of each component</head><p>The replacement of normal convolution in the head with PConv brings around ∼ 1.5AP increase in various models. As for the speed of PConv, the total FLOPs of PConv is actually smaller than the original head, and the latency still increases by around 3%, due to the more convolution kernels involved.</p><p>The insertion of integrated BN (iBN) in the head also benefits the model by 0.2 ∼ 1.2AP for different architectures. Several other studies also advocated group normalization (GN) when training detection networks <ref type="bibr" target="#b39">[38]</ref>. However, one trait of BN that is missing in GN is that BN does not require calculating on-site statistics when conducting inference and can be merged in the former convolutional layer. This brings significant advantage in inference speed, as revealed by the same forward latency with iBN. The increased performance is a natural result of the faster optimization and better generalization of BN. (see Appen. 5)</p><p>As for the effect of scale-equalizing module, we also compare the results of SEPC with PConv+iBN and find significant improvement (1.6 ∼ 2.5AP increase), indicating that the scale equalizing module can help align features at different levels and functions well in various object detectors.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Comparison with DCN head</head><p>If all convolutions are replaced by deformable convolutional kernels (DCN) <ref type="bibr" target="#b4">[5]</ref> in the original head of RetinaNet-alike models (RetinaNet <ref type="bibr" target="#b19">[20]</ref>, FSAF <ref type="bibr" target="#b49">[48]</ref>, FCOS <ref type="bibr" target="#b39">[38]</ref>, FreeAnchor <ref type="bibr" target="#b47">[46]</ref> et al.), the increase in AP varies. For most models such as RetinaNet and FreeAnchor, the increase in AP is limited (∼ 1AP). The performance increase in FSAF is more significant, possibly due to the combination of the adaptive kernel in DCN and the adaptive loss function designed in FSAF. Nontheless, the runtime costs of DCN head in all these models are huge. As shown in Tab. 1, the AP gain of PConv+iBN(no DCN involved) in RetinaNet &amp; FreeAnchor already significantly outperforms DCN head significantly in both AP and time efficiency. SEPC-lite and SEPC brings further AP gains and outperforms DCN head in all these models, while bringing only 1/5 and 1/2 as much runtime overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Comparison of different BN implementations in the head</head><p>There are different implementations of BN in dealing with feature pyramids, which are shown in <ref type="figure" target="#fig_5">Fig. 7a</ref>. The output after each BN module is y = γ x−µ σ + β, where γ and β are parameters, and µ and σ are batch statistics for normalization. Single BN adds a BN module after each feature pyramid level with shared parameters γ and β across the pyramid. But each feature map collects statistics on its own during training. Independent BN makes both parameters and statistics of BN in a feature layer exclusive to itself and is used in <ref type="bibr" target="#b7">[8]</ref>. Integrated BN, as discussed in section 3.2, calculates batch statistics over all feature maps from feature pyramid networks.</p><p>The effects of these four BN designs are demonstrated in <ref type="figure" target="#fig_5">Fig. 7</ref>  decline in performance of detectors, due to the mismatch between the shared parameters and non-shared statistics. Integrated BN and independent BN both improves AP, and integrated BN outperforms independent BN because of the more stable statistics during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Comparison with other feature fusion modules</head><p>In regards to different feature fusion methods, Tab. 2 presents our comparison of PConv to other state-of-the-art feature fusion modules on FreeAnchor. It is obvious that PConv provides a dramatic performance increase compared to common feature pyramid networks, including NAS-FPN <ref type="bibr" target="#b7">[8]</ref> and Libra <ref type="bibr" target="#b29">[29]</ref>. Moreover, the designed PConv stack head also earns the minimum FLOPs increase among the feature fusion modules. Results of this section validate the effectiveness of PConv in feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with state-of-the-art object detectors</head><p>In this section, we compare our method to other stateof-the-art object detectors on COCO2017 benchmarking dataset. The training strategies followed 2x with 640-800 scale jitter and the results were obtained with only a single scale, unless specified otherwise. Details can be found in Appen. 4. We only report FreeAnchor equipped with with SEPC-lite and SEPC for the purpose of real potential applications since SEPC incurs intangible computation cost for large backbones such as ResNext-101. It is ob-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Extension to two-stage object detectors</head><p>We also present that PConv (without scale-equalizing module) can still be effective when it is applied to two-stage object detectors. As shown in Tab 4, PConv provides remarkable improvement of AP on different two-stage detec-   tors. PConv provides the most AP increase to Mask-RCNN, which improves the AP by 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this study, we explore considering the inter-scale correlation through a pyramid convolution (PConv), which runs a 3-D convolution on both the scale and spatial dimension of the feature pyramid. The striding pattern for this PConv in both spatial and scale dimension is quite different from conventional ones. First, due to the different spatial sizes in the pyramid, the striding step of the spatial slices of PConv kernels is proportional to the convolved feature map size in the pyramid level. This pendulum-alike striding pattern of the PConv kernel helps align the spatial position of neighboring feature maps as they are involved in one PConv. Second, when PConv strides up in the scale dimension, the kernel should also adjust its spatial deformation as well, which is then called scale-equalizing pyramid convolution (SEPC). The naive striding pattern with a fixed spatial kernel size actually best suits for extracting features in a Gaussian pyramid, which is quite far from the feature pyramid generated by deep networks. And SEPC helps relax this discrepancy and extracts more robust features. Being light-weighted and compatible with most object detectors, SEPC is able to significantly improve the detection performance with minimal computational cost increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>We would like to thank Wenqi Shao and Zhanghui Kuang from SenseTime for the inspiring discussions and suggestions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material for</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present details of FLOPS calculation in Sec. 1. The pseudo-code of pyramid convolution is in Sec. 2. The detailed mathematical proof that pyramid convolution can extract scale-invariant features from Gaussian pyramid is in Sec. 3. Sec. 4 shows the details of experiments including ablation study and test of inference time respectively. Extra ablation experiments of integrated batch normalization(iBN) and the effect of the number of PConv layers are presented in Sec. 5. Finally, We present implementation details of Feature Selective Anchor Free module in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">FLOPs in the head</head><p>The input image size of all models is 3 × 1280 × 800. we follow the computation of FLOPs in mmdetection <ref type="bibr">[1]</ref> for normal convolution, where one multiplication-addition pair is counted for a single operation. This yields</p><formula xml:id="formula_2">FLOPs = C in × K h × K w × H × W × C out<label>(1)</label></formula><p>for a single convolution operation, where C in and C out are the number of input and output channels of a convolution, which are all fixed as 256 in this study, K h and K w are the convolutional kernel sizes and set as 3, H and W are the width and height of a feature map. PConv fuses features of adjacent levels to the work-on level by 3-D convolutions with stride of 0.5(for upper level) and of 2(for lower level). For the convolution with stride of 0.5, we implemented by performing a regular convolution with stride of 1 followed by an upsample operation, which can be represented in formula as:</p><formula xml:id="formula_3">y l = Upsample(w 1 * x l+1 ) + w 0 * x l + w −1 * s2 x l−1 ,<label>(2)</label></formula><p>where x l is the feature map at level l and w 1 , w 0 , w −1 are three independent convolutional kernels. Since the feature map size of x l+1 is half of x l and the size of x l−1 is twice as much as x l , the computational cost of the first term in Eqn.2 is a quarter of that of the second term whereas the last two terms share the same computational cost. Note that at top-most level(P7), the first term is eliminated and at bottom-most level(P3), the last term is eliminated. We abbreviate the analysis of upsample operation as its cost is relatively small compared to convolutions in this place. Therefore, the ratio c i of the FLOPs after applying PConv to that of the original ones in feature pyramids are 2, 2.25, 2.25, 2.25, 1.25 in top-down order. Additionally, the FLOPS associated with each level is also proportional to H × W . The ratio between the spatial size of each feature map and total spatial size can be represented by r i = = 0.99925 times of that using default head design. When SEPC or SEPC-Lite is used, we discuss how FLOPs in deformable conv are calculated. One forwarding of deformable conv is composed of a normal convolution with output channels of 2 × K h × K w for offset prediction, a bilinear * equal contribution </p><formula xml:id="formula_4">F LOP S ≈ (1 + 8 + 2 × K h × Kw Cout ) × Cin × K h × Kw × H × W × Cout</formula><p>In SEPC-Lite, we use a normal conv2D on P3 and deform it through P4-P7. Notice that</p><formula xml:id="formula_5">7 i=4 H i ×W i 7 i=3 H i ×W i ≈ 0.</formula><p>249, using each deformable convolution only introduces 0.249 × 26 256 = 0.025 times more computation comparted to using normal conv in feature pyramid. When it comes to SEPC, the actual method for evaluating computation cost is slightly different from calculation in SEPC-Lite due to P3 and P7, however, following a similar trajectory, one can easily justify that the extra computation cost is still marginal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Implementation pseudocode of PConv and SEPC</head><p>The pseudocodes of pyramid conv (PConv) are attached as follows, which also corresponds to  <ref type="figure" target="#fig_4">Fig.  6</ref> in the manuscript. Note that when convolving the lowest-level features (P3), a normal Conv2D is utilized, whose weights are shared by deformable convs used in higher layers.</p><p>Therefore, SEPC is an improved version of pconv, to relax the discrepancy of feature pyramid from a Gaussian pyramid by aligning the feature map of higher layers with the lowest layer. In one word, pconv only uses plain Conv2D modules while SEPC leverages deformable convs in an efficient way. Different from a naive implementation of deformable convolution in the head, SEPC only deforms the kernel when convolving higher layers, which is motivated by the perspective from scale space theory and is much more effective w.r.t. computational cost gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Discussion about remark 1</head><p>Remark 1. Pyramid convolution is able to extract scale-invariant features from a Gaussian pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian scale space.</head><p>Consider an image f : Z 2 → R 2 , where the input domain represents the pixel coordinate and f (x) is pixel intensity, a Gaussian scale space is generated by consecutively blurring the initial image f0 with an isotropic 2-D Gauss-Weierstrass kernel G(x, t) = (4πt) −1 exp( x 2 /4t) of variable width √ t and spatial position x. A set of responses f (t, x), t ≥ 0 represents blurred images, forming a Gaussian scale space <ref type="bibr">[3]</ref> (GSS), as written by:</p><formula xml:id="formula_6">f (t, x) = [G(·, t) * f0](x), t &gt; 0<label>(3)</label></formula><p>where a higher t indicates a larger blur. Gaussian pyramid. With the above introduction of GSS, a Gaussian pyramid is denoted as</p><formula xml:id="formula_7">p(a, x) = f (t(a, s0), a −1 x)<label>(4)</label></formula><p>where s0 is the initial scale, a is the downsizing ratio 0 &lt; a ≤ 1 and</p><formula xml:id="formula_8">t = s0 a 2 − s0<label>(5)</label></formula><p>is the Gaussian kernel variance corresponding to the downsizing ratio a in order to keep the same frequency limit after downsizing <ref type="bibr">[4]</ref>. In practice, a is chosen to be 2 −l , where l is the level of Gaussian pyramid with l = 0 denoting no sub-sampling on the original image. Then the Gaussian pyramid is also written as</p><formula xml:id="formula_9">p l (x) = f (t(2 −l , s0), 2 l x).<label>(6)</label></formula><p>In fact, we can also define an action Sn that transfer from the original level into another by,</p><formula xml:id="formula_10">[Sn[p]](x) = pn(x) = [G(·, t(2 −n , s0)) * p0](2 n x).<label>(7)</label></formula><p>Lemma 1. The actions Sm and Sn satisfy SmSn = Sm+n</p><p>Proof. Since the sub-sampling ratio 2 m 2 n = 2 m+n naturally satisfies this argument, we mainly focus on the Gaussian convolution in Eqn. 7. In order to prove the associativity property of Sn and Sm, we only need to calculate the associativity property of convolution (G(·, t(2 −m , s0)) * [G(·, t(2 −n , s0)) * f (·)](2 n x))(2 m x) = [G(·, t(2 −m−n , s0)) * f (·)](2 m+n x)</p><p>This process contains four sub-processes: (1) Gaussian convolution with variance t(2 −n ), (2) sub-sampling by 2 n ; (3) Gaussian convolution with variance t(2 −m ); (4) sub-sampling by 2 m . Then we will follow this process and apply Fourier transform on after another. Applying Fourier transform to G(·, t(2 −n , s0)) * f (·), we obtain</p><formula xml:id="formula_12">F1 = F [G(·, t(2 −n , s0) * f (·)]) = exp(−t(2 −n , s0) ω 2 ))F0(ω),<label>(9)</label></formula><p>where ω ∈ R 2 represents 2-D frequency in the Fourier-transformed domain, F0(ω) is the Fourier transform of f (x). Substituting Eqn. 5 into the above equation acquires F1 = exp((2 2n s0 − s0) ω 2 ))F0(ω),</p><p>After process (2) (sub-sampling by 2 n ), the Fourier transform is now</p><formula xml:id="formula_14">F2 = F [G(·, t(2 −n , s0) * f (·)])(2 n x) = 2 −n exp(−t(2 −n , s0) 2 −n ω 2 ))F0(2 −n ω) = 2 −n exp((s0−2 −2n s0) ω 2 ))F0(2 −n ω),<label>(11)</label></formula><p>As for process (3) (Gaussian convolution with variance t(2 −m )),</p><formula xml:id="formula_15">F3 = F [G(·, t(2 −m , s0)]) · F2 = 2 −n exp((2 2m s0 − 2 −2n s0) ω 2 ))F0(2 −n ω),<label>(12)</label></formula><p>As for process (4) (sub-sampling by 2 −m ),</p><formula xml:id="formula_16">F4 = 2 −m−n exp((s0 − 2 −2n−2m s0) ω 2 ))F0(2 −m−n ω),<label>(13)</label></formula><p>It is obvious that after process 4), the Fourier-transformed expression is equivalent to the Fourier transform of RHS of Eqn. 8</p><p>F [G(·, t(2 −m−n , s0) * f (·)])(2 m+n x) = 2 −m−n exp((s0 − 2 −2n−2m s0) ω 2 ))F0(2 −m−n ω) = F4</p><p>The above lemma is very useful in a Gaussian pyramid, since it means that one level in the Gaussian pyramid is able to be transferred to another by a simple jumping action, such that pm+n = Sm[pn] <ref type="bibr" target="#b14">(15)</ref> Now recall that the expression of pyramid convolution is given by</p><formula xml:id="formula_18">y l = w1 * s0.5 x l+1 + w0 * x l + w−1 * s2 x l−1 ,<label>(16)</label></formula><p>where w1, w0 and w−1 are three independent kernels, * 2 denotes a convolution with stride 2, x l represents the feature pyramid in the l th layer. Once the feature pyramid x l can be viewed as a Gaussian pyramid, the pyramid convolution is written as</p><formula xml:id="formula_19">y l (z) = 1 k=−1 [w k * p l+k ](2 −k z) = 1 k=−1 u∈Z d w k (u)p l+k (u + 2 −k z).<label>(17)</label></formula><p>Note that the stride option at neighboring layers is now represented by 2 −k . For k = −1 with larger feature map size, the stride is 2 and for k = 1 with smaller feature map size, the stride is now 1 2 . If we apply a jumping action Sm on the output PConv and leverage Eq. 15, Sm(y l (z)) = Sm </p><p>The above equation shows an important property of using PConv in a Gaussian pyramid. That is, PConv commutes with the jumping action on the pyramid, which is conventionally called scale equivariance. It can be rephrased in another way. When the scale of an object changes in the original image, the extracted feature can also be found by shifting the convolved pyramid after using PConv, which also fits into the usual definition of scale invariance in object detection <ref type="bibr">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment details 4.1. Training details</head><p>We trained the model with backbone ResNet-50 and ResNet-101 and mini-batch size of 16 on 8 Nvidia Titan XP GPUs. The training budget for the strategy 1x was 12 epochs. The initial learning rate was 0.01, and was decreased by 0.1 after 8 and 11 epochs. When the 2x schedule was adapted, we used 24 epochs for training and kept the same learning rate and decreased it by 0.1 after 16 and 22 epochs. All models with ResNext101-64-4d backbone were trained on Nvidia V100 GPUs under the same setting. When using BN in all experiments,we set 4 images per gpu with the same batch size to get more accurate statistics.</p><p>In the experiments of evaluating other feature fusion modules, all models used the same backbone ResNet-50 with 1x schedule. We used 4 PConvs in a combined way with one extra head to get better trade-off.In the HRNet,PANet, and Libra, and only replaced the origin FPN with the feature fusion module in the origin paper. In NAS-FPN,we used 7 merging-cells and keep channel 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Speed test details</head><p>We compared the speed of our method (include pre-precossing, forwarding and nms) with other proposed one stage detectors. All evaluation was performed on one Nvidia 1080Ti GPU with i7-7700k@4.2GHz. We set batch size to 8 and started the timer at 100-th iteration to make sure I/O is stable. Then we used the means of next 200 iteration in computation of speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Supplementary ablation experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of the number of pyramid convolution stacks</head><p>The total number of pyramid convolutions is adjusted from 2 to 6. The recorded AP of different detectors is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. The figure illustrates that all these three detectors benefit from increasing the number of PConv from 2 to 4 due to the gradual information flow from top to bottom by PConv stacks. Using four stacked PConv in heads is rational as it provides descent average precision without causing much redundancy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training curves using iBN</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Correlation matrix of feature maps in the feature pyramid of RetinaNet. The upper and lower triangle represent the correlation before and after FPN respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of pyramid convolution (a) with other feature fusion modules, including (b) FPN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>(a) Head design of the original RetinaNet; (b) Head design with PConv. In the final output convolution, K is the number of anchor boxes, which is 1 for anchor-free methods, and C is the number of classes in classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>(a) PConv on Gaussian pyramid; (b) SEPC on Feature pyramid larger than the one used in the bottom-most features. However, because of the non-linearity operations in the backbone, the dilation ratios of different pixels are also different, making it difficult to directly use a constant one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>(a) Different batch normalization implementations in the feature pyramid levels. Only 2 feature levels are given as an illustration; (b) Comparison of AP results of architectures with different batch normalization implementations in the feature pyramid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Hi×Wi 7 j=3</head><label>7</label><figDesc>Hj ×Wj . The numeric values of such ratios are calculated as: 0.0029,0.0117,0.0469,0.1877,0.7507 in top-down order. Thus, the total computation of using 4 stacked PConv is C = 7 i=3 c i × r i = 1.4985 times of using 4 stakced convolution in regular settings. When classification and regression subsets use combined PConv structure with one extra non-combined for each subnet, the total computation is (4×C+2×1) 2×4×1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of pyramid convolution on the feature pyramid interpolation for every sampling point which involves 8 matiplications and 7 additions, and another normal convolution. So we get</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 &amp; 3 # 4 #</head><label>334</label><figDesc>4(a) in the manuscript. Note that only normal Conv2D modules are used in pconv. 1 def pconv_module_forward(x, conv2D_list): 2 # x: input feature list [p3,p4,p5,p6,p7] conv2D_list: conv2D module list, [nn.Conv2D(stride=2),nn.Conv2D(),nn.Conv2D()] 5 out_x = [] 6 for level in range(len(x)): 7 tmp = conv2D_list[1](x[level]) As for Scale-equalizing pyramid conv (SEPC), within the for loop of the pseudocodes, conv2d_list[i] changes to DeformableConv (conv2d_list[i].weight), where i ∈ {0, 1, 2}, only when level &gt; 0 (i.e. excluding P3 layer). The main idea is illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 k=− 1 [ 1 k=− 1 [ 1 k=− 1 w</head><label>11111</label><figDesc>w k * [p l+k ]](2 −k z) = w k * [p l+k+m ]](2 m−k z) = k * [Sm[p l+k ]](2 −k z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 :</head><label>2</label><figDesc>AP change with number of pyramid convolution in different architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 3 (Figure 3 :</head><label>33</label><figDesc>a-c) displays the training loss of different models and Fig. 3(d-f) presents how AP changes as training goes. When iBN is used, in general the losses reduce faster in the early stage of training especially for FreeaAchor and RetinaNet. However, at the end of training, models with iBN result in a slightly higher training loss, yet the mAP is higher than models without IBN. This observation follows the better generalization property of batch normalization. Comparison of training loss and validation AP with and without iBN. (a-c) shows the training loss of FreeAnchor, FSAF and RetinaNet, and (d-f) are the validation AP of FreeAnchor, FSAF and RetinaNet, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Trivially using single BN results in a catastrophic</figDesc><table><row><cell>Detector</cell><cell>Note</cell><cell cols="2">FLOPS(G) Time(ms)</cell><cell>AP</cell><cell cols="5">AP50 AP75 APS APM APL</cell></row><row><cell>RetinaNet</cell><cell>baseline DCN head PConv PConv+iBN SEPC SEPC-lite</cell><cell>239.32 249.55 239.29 239.36 242.22 240</cell><cell>73.2 102.1 76.5 76.4 89.6 78.5</cell><cell>35.7 36.8 37.0 37.9 39.7 38.8</cell><cell>55.0 56.8 57.7 59.3 60.4 59.9</cell><cell>38.5 39.6 39.4 40.6 42.7 41.8</cell><cell>18.9 20.4 22.3 22.5 23.1 22.6</cell><cell>38.9 40.3 40.8 42.2 44 42.8</cell><cell>46.3 49.0 48.9 49.1 52.2 51</cell></row><row><cell>FSAF</cell><cell>baseline  *  DCN head PConv PConv+iBN SEPC SEPC-lite</cell><cell>205.2 215.42 205.18 205.25 208.11 205.88</cell><cell>62.4 85.2 66.0 66.1 77.4 68.2</cell><cell>36.9 40.1 38.7 38.9 41.3 40.7</cell><cell>56.1 58.5 58.9 59.1 60.4 60</cell><cell>39 42.8 41.1 41.8 43.6 43.4</cell><cell>20.6 22.4 22.2 22.2 23 22.4</cell><cell>40.1 43.3 42 42.5 44.8 44.6</cell><cell>48.2 54.7 51 51 57.8 55.1</cell></row><row><cell>FreeAnchor</cell><cell>baseline DCN head PConv PConv+iBN SEPC SEPC-lite</cell><cell>239.32 249.55 239.29 239.36 242.22 240</cell><cell>76.4 100.4 79.4 79.7 89.9 81.2</cell><cell>38.5 39.4 40.0 41.2 42.8 41.7</cell><cell>57.3 58.0 59.1 60.5 61.9 61</cell><cell cols="2">41.2 42.4 43 44.3 45.9 25.6 21.1 21.7 22.8 24.3 45.1 24.2</cell><cell>41.8 43.0 43.8 44.6 46.4 45.2</cell><cell>51.5 52.7 53.3 54.6 57.4 54.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* : our own implementation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison of detection AP results of different architectures. All models were trained using ResNet-50 backbone and adopted the 1x training strategy. Results were evaluated on COCO minival set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparison of PConv with other feature fusion modules</figDesc><table><row><cell>including FPN [19], HR-Net [36], PA-Net [23], NAS-FPN [8] and</cell></row><row><cell>Libra [29] on FreeAnchor. Results evaluated on COCO minival</cell></row><row><cell>are reported.</cell></row><row><cell>served that SEPC boosts the original baselines by a signif-icant margin and achieves the state-of-the-art 47.7AP us-ing ResNext-101 backbone without utilizing bells and whis-tles (e.g. multi-scale test, sync BN, deformable backbone), surpassing even most two-stage detectors with deformable backbones and multi-scale test. If DCN backbone and stronger training scale jitter (480-960) is applied, the AP performance reaches 50.1, the best reported detection result on a single-stage model with single-scale test.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparing of the single-model &amp; single-scale test results of SEPC with other state-of-the-art object detectors. Results are evaluated on test-dev.</figDesc><table><row><cell>Detector</cell><cell>Note</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell>Faster</cell><cell cols="2">Baseline 36.5 PConv 38.5</cell><cell>58.4 59.9</cell><cell>39.1 41.4</cell></row><row><cell>Mask</cell><cell cols="2">Baseline 37.3 PConv 39.6</cell><cell>59 60.1</cell><cell>40.2 43.5</cell></row><row><cell>HTC</cell><cell cols="2">Baseline 42.1 PConv 43.6</cell><cell>60.8 62.0</cell><cell>45.9 47.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Extension of only PConv module to two-stage detectors including Faster R-CNN [31], Mask R-CNN [11] and HTC [2].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Scale-equalizing Pyramid Convolution for object detection Xinjiang Wang * , Shilong Zhang * , Zhuoran Yu, Litong Feng, Wayne Zhang Sensetime Research {wangxinjiang, zhangshilong, yuzhuoran, fenglitong, wayne.zhang}@sensetime.com</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Details of FSAF</head><p>We only implemented the anchor-free branch in original paper. In the re-implementation process, we found that in label assigning phase, removing the ignore region could effectively improve model's ability to distinguish between positive and hard negative samples. Thus, in experiments, we set effective area as regions within 0.2 from center of projected area of object on the feature map, and assigned negative labels to all outside areas. This effort resulted in 1.1mAP improvement.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Revisiting feature alignment for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01570</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ron: Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5936" to="5944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-driven neuron allocation for scale aggregation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11526" to="11534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fssd: feature fusion single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqiang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00960</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scale-space theory: A basic tool for analyzing structures at different scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied statistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="225" to="270" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sift-the scale invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Autofocus: Efficient multi-scale inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">High-speed human detection using a multiresolution cascade of histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan José</forename><surname>Villanueva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9310" to="9320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">Wenyu Liu, and Jingdong Wang. High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV), October</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dsrn: a deep scale relationship network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="947" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Gated bidirectional feature pyramid network for accurate one-shot detection. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soonmin</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho-Deok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="543" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep scale-spaces: Equivariance over scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">FreeAnchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">M2det: A single-shot object detector based on multi-level feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scale-space theory: A basic tool for analyzing structures at different scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied statistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="225" to="270" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep scale-spaces: Equivariance over scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
