<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Generalization by Solving Jigsaw Puzzles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
							<email>fabio.maria.carlucci@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>D&amp;apos;innocente</surname></persName>
							<email>antonio.dinnocente@iit.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rome Sapienza</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Italian Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bucci</surname></persName>
							<email>silvia.bucci@iit.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Italian Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>barbara.caputo@polito.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Italian Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
							<email>tatiana.tommasi@polito.it</email>
							<affiliation key="aff2">
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huawei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>London</surname></persName>
						</author>
						<title level="a" type="main">Domain Generalization by Solving Jigsaw Puzzles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human adaptability relies crucially on the ability to learn and merge knowledge both from supervised and unsupervised learning: the parents point out few important concepts, but then the children fill in the gaps on their own. This is particularly effective, because supervised learning can never be exhaustive and thus learning autonomously allows to discover invariances and regularities that help to generalize. In this paper we propose to apply a similar approach to the task of object recognition across domains: our model learns the semantic labels in a supervised fashion, and broadens its understanding of the data by learning from self-supervised signals how to solve a jigsaw puzzle on the same images. This secondary task helps the network to learn the concepts of spatial correlation while acting as a regularizer for the classification task. Multiple experiments on the PACS, VLCS, Office-Home and digits datasets confirm our intuition and show that this simple method outperforms previous domain generalization and adaptation solutions. An ablation study further illustrates the inner workings of our approach. * This work was done while at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the current gold rush towards artificial intelligent systems it is becoming more and more evident that there is little intelligence without the ability to transfer knowledge and generalize across tasks, domains and categories <ref type="bibr" target="#b10">[11]</ref>. A large portion of computer vision research is dedicated to supervised methods that show remarkable results with convolutional neural networks in well defined settings, but still struggle when attempting these types of generalizations. Focusing on the ability to generalize across domains, <ref type="bibr">Figure 1</ref>. Recognizing objects across visual domains is a challenging task that requires high generalization abilities. Other tasks, based on intrinsic self-supervisory image signals, allow to capture natural invariances and regularities that can help to bridge across large style gaps. With JiGen we learn jointly to classify objects and solve jigsaw puzzles, showing that this supports generalization to new domains. the community has attacked this issue so far mainly by supervised learning processes that search for semantic spaces able to capture basic data knowledge regardless of the specific appearance of input images. Existing methods range from decoupling image style from the shared object content <ref type="bibr" target="#b2">[3]</ref>, to pulling data of different domains together and imposing adversarial conditions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, up to generating new samples to better cover the space spanned by any future target <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b46">47]</ref>. With the analogous aim of getting general purpose feature embeddings, an alternative research direction has been recently pursued in the area of unsupervised learning. The main techniques are based on the definition of tasks useful to learn visual invariances and regularities captured by spatial co-location of patches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38]</ref>, counting primitives <ref type="bibr" target="#b36">[37]</ref>, image coloring <ref type="bibr" target="#b49">[50]</ref>, video frame ordering <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b47">48]</ref> and other self-supervised signals.</p><p>Since unlabeled data are largely available and by their very nature are less prone to bias (no labeling bias issue arXiv:1903.06864v2 [cs.CV] 14 Apr 2019 <ref type="bibr" target="#b44">[45]</ref>), they seem the perfect candidate to provide visual information independent from specific domain styles. Despite their large potential, the existing unsupervised approaches often come with tailored architectures that need dedicated finetuning strategies to re-engineer the acquired knowledge and make it usable as input for a standard supervised training process <ref type="bibr" target="#b37">[38]</ref>. Moreover, this knowledge is generally applied on real-world photos and has not been challenged before across large domain gaps with images of other nature like paintings or sketches.</p><p>This clear separation between learning intrinsic regularities from images and robust classification across domains is in contrast with the visual learning strategies of biological systems, and in particular of the human visual system. Indeed, numerous studies highlight that infants and toddlers learn both to categorize objects and about regularities at the same time <ref type="bibr" target="#b1">[2]</ref>. For instance, popular toys for infants teach to recognize different categories by fitting them into shape sorters; jigsaw puzzles of animals or vehicles to encourage learning of object parts' spatial relations are equally widespread among 12-18 months old. This type of joint learning is certainly a key ingredient in the ability of humans to reach sophisticated visual generalization abilities at an early age <ref type="bibr" target="#b15">[16]</ref>.</p><p>Inspired by this, we propose the first end-to-end architecture that learns simultaneously how to generalize across domains and about spatial co-location of image parts <ref type="bibr">(Figure 1,</ref><ref type="bibr" target="#b1">2)</ref>. In this work we focus on the unsupervised task of recovering an original image from its shuffled parts, also known as solving jigsaw puzzles. We show how this popular game can be re-purposed as a side objective to be optimized jointly with object classification over different source domains and improve generalization with a simple multi-task process <ref type="bibr" target="#b6">[7]</ref>. We name our Jigsaw puzzle based Generalization method JiGen. Differently from previous approaches that deal with separate image patches and recombine their features towards the end of the learning process <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38]</ref>, we move the patch re-assembly at the image level and we formalize the jigsaw task as a classification problem over recomposed images with the same dimension of the original one. In this way object recognition and patch reordering can share the same network backbone and we can seamlessly leverage over any convolutional learning structure as well as several pretrained models without the need of specific architectural changes.</p><p>We demonstrate that JiGen allows to better capture the shared knowledge among multiple sources and acts as a regularization tool for a single source. In the case unlabeled samples of the target data are available at training time, running the unsupervised jigsaw task on them contributes to the feature adaptation process and shows competing results with respect to state of the art unsupervised domain adaptation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Solving Jigsaw Puzzles The task of recovering an original image from its shuffled parts is a basic pattern recognition problem that is commonly identified with the jigsaw puzzle game. In the area of computer science and artificial intelligence it was first introduced by <ref type="bibr" target="#b16">[17]</ref>, which proposed a 9-piece puzzle solver based only on shape information and ignoring the image content. Later, <ref type="bibr" target="#b22">[23]</ref> started to make use of both shape and appearance information. The problem has been mainly cast as predicting the permutations of a set of squared patches with all the challenges related to number and dimension of the patches, their completeness (if all tiles are/aren't available) and homogeneity (presence/absence of extra tiles from other images). The application field for algorithms solving jigsaw puzzles is wide, from computer graphics and image editing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b40">41]</ref> to re-compose relics in archaeology <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39]</ref>, from modeling in biology <ref type="bibr" target="#b31">[32]</ref> to unsupervised learning of visual representations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b9">10]</ref>. Existing assembly strategies can be broadly classified into two main categories: greedy methods and global methods. The first ones are based on sequential pairwise matches, while the second ones search for solutions that directly minimize a global compatibility measure over all the patches. Among the greedy methods, <ref type="bibr" target="#b17">[18]</ref> proposed a minimum spanning tree algorithm which progressively merges components while respecting the geometric consistent constraint. To eliminate matching outliers, <ref type="bibr" target="#b41">[42]</ref> introduced loop constraints among the patches. The problem can be also formulated as a classification task to predict the relative position of a patch with respect to another as in <ref type="bibr" target="#b14">[15]</ref>. Recently, <ref type="bibr" target="#b38">[39]</ref> expressed the patch reordering as the shortest path problem on a graph whose structure depends on the puzzle completeness and homogeneity. The global methods consider all the patches together and use Markov Random Field formulations <ref type="bibr" target="#b8">[9]</ref>, or exploit genetic algorithms <ref type="bibr" target="#b40">[41]</ref>. A condition on the consensus agreement among neighbors is used in <ref type="bibr" target="#b42">[43]</ref>, while <ref type="bibr" target="#b35">[36]</ref> focuses on a subset of possible permutations involving all the image tiles and solves a classification problem. The whole set of permutations is instead considered in <ref type="bibr" target="#b9">[10]</ref> by approximating the permutation matrix and solving a bi-level optimization problem to recover the right ordering.</p><p>Regardless of the specific approach and application, all the most recent deep-learning jigsaw-puzzle solvers tackle the problem by dealing with the separate tiles and then finding a way to recombine them. This implies designing tilededicated network architectures then followed by some specific process to transfer the collected knowledge in more standard settings that manage whole image samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Generalization and Adaptation</head><p>The goal of domain generalization (DG) is that of learning a system that can perform uniformly well across multiple data distribu- . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convnet</head><p>Shuffled Images <ref type="figure">Figure 2</ref>. Illustration of the proposed method JiGen. We start from images of multiple domains and use a 3 × 3 grid to decompose them in 9 patches which are then randomly shuffled and used to form images of the same dimension of the original ones. By using the maximal Hamming distance algorithm in <ref type="bibr" target="#b35">[36]</ref> we define a set of P patch permutations and assign an index to each of them. Both the original ordered and the shuffled images are fed to a convolutional network that is optimized to satisfy two objectives: object classification on the ordered images and jigsaw classification, meaning permutation index recognition, on the shuffled images.</p><p>tions. The main challenge is being able to distill the most useful and transferrable general knowledge from samples belonging to a limited number of population sources. Several works have reduced the problem to the domain adaptation (DA) setting where a fully labeled source dataset and an unlabeled set of examples from a different target domain are available <ref type="bibr" target="#b10">[11]</ref>. In this case the provided target data is used to guide the source training procedure, that however has to run again when changing the application target. To get closer to real world conditions, recent work has started to focus on cases where the source data are drawn from multiple distributions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b48">49]</ref> and the target covers only a part of the source classes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref>. For the more challenging DG setting with no target data available at training time, a large part of the previous literature presented model-based strategies to neglect domain specific signatures from multiple sources. They are both shallow and deep learning methods that build over multi-task learning <ref type="bibr" target="#b21">[22]</ref>, low-rank network parameter decomposition <ref type="bibr" target="#b26">[27]</ref> or domain specific aggregation layers <ref type="bibr" target="#b13">[14]</ref>. Alternative solutions are based on source model weighting <ref type="bibr" target="#b29">[30]</ref>, or on minimizing a validation measure on virtual tests defined from the available sources <ref type="bibr" target="#b25">[26]</ref>. Other feature-level approaches search for a data representation able to capture information shared among multiple domains. This was formalized with the use of deep learning autoencoders in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref>, while <ref type="bibr" target="#b33">[34]</ref> proposed to learn an embedding space where images of same classes but different sources are projected nearby. The recent work of <ref type="bibr" target="#b28">[29]</ref> adversarially exploits class-specific domain classification modules to cover the cases where the covariate shift assumption does not hold and the sources have different class conditional distributions. Data-level methods propose to augment the source domain cardinality with the aim of covering a larger part of the data space and possibly get closer to the target. This solution was at first presented with the name of domain randomization <ref type="bibr" target="#b43">[44]</ref> for samples from simulated environments whose variety was extended with random renderings. In <ref type="bibr" target="#b39">[40]</ref> the augmentation is obtained with domain-guided perturbations of the original source instances. Even when dealing with a single source domain, <ref type="bibr" target="#b46">[47]</ref> showed that it is still possible to add adversarially perturbed samples by defining fictitious target distributions within a certain Wasserstein distance from the source. Our work stands in this DG framework, but proposes an orthogonal solution with respect to previous literature by investigating the importance of jointly exploiting supervised and unsupervised inherent signals from the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The JiGen Approach</head><p>Starting from the samples of multiple source domains, we wish to learn a model that can perform well on any new target data population covering the same set of categories. Let us assume to observe S domains, with the ith domain containing N i labeled instances {(x i j , y i j )} Ni j=1 , where x i j indicates the j-th image and y i j ∈ {1, . . . , C} is its class label. The first basic objective of JiGen is to minimize the loss L c (h(x|θ f , θ c ), y) that measures the error between the true label y and the label predicted by the deep model function h, parametrized by θ f and θ c . These parameters define the feature embedding space and the final classifier, respectively for the convolutional and fully connected parts of the network. Together with this objective, we ask the network to satisfy a second condition related to solving jigsaw puzzles. We start by decomposing the source images using a regular n × n grid of patches, which are then shuffled and re-assigned to one of the n 2 grid positions. Out of the n 2 ! possible permutations we select a set of P elements by following the Hamming distance based algorithm in <ref type="bibr" target="#b35">[36]</ref>, and we assign an index to each en-try. In this way we define a second classification task on</p><formula xml:id="formula_0">K i labeled instances {(z i k , p i k )} Ki k=1</formula><p>, where z i k indicates the recomposed samples and p i k ∈ {1, . . . , P } the related permutation index, for which we need to minimize the jigsaw loss L p (h(z|θ f , θ p ), p). Here the deep model function h has the same structure used for object classification and shares with that the parameters θ f . The final fully connected layer dedicated to permutation recognition is parametrized by θ p . Overall we train the network to obtain the optimal model through</p><formula xml:id="formula_1">argmin θ f ,θc,θp S i=1 Ni j=1 L c (h(x i j |θ f , θ c ), y i j )+ Ki k=1 αL p (h(z i k |θ f , θ p ), p i k )<label>(1)</label></formula><p>where both L c and L p are standard cross-entropy losses. We underline that the jigsaw loss is also calculated on the ordered images. Indeed, the correct patch sorting corresponds to one of the possible permutations and we always include it in the considered subset P . On the other way round, the classification loss is not influenced by the shuffled images, since this would make object recognition tougher. At test time we use only the object classifier to predict on the new target images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extension to Unsupervised Domain Adaptation</head><p>Thanks to the unsupervised nature of the jigsaw puzzle task, we can always extend JiGen to the unlabeled samples of target domain when available at training time. This allows us to exploit the jigsaw task for unsupervised domain adaptation. In this setting, for the target ordered images we minimize the classifier prediction uncertainty through the empirical entropy loss</p><formula xml:id="formula_2">L E (x t ) = y∈Y h(x t |θ f , θ c )log{h(x t |θ f , θ c )}, while for the shuffled target images we keep optimizing the jigsaw loss L p (h(z t |θ f , θ p ), p t ).</formula><p>Implementation Details Overall JiGen 1 has two parameters related to how we define the jigsaw task, and three related to the learning process. The first two are respectively the grid size n × n used to define the image patches and the cardinality of the patch permutation subset P . As we will detail in the following section, JiGen is robust to these values and for all our experiments we kept them fixed, using 3 × 3 patch grids and P = 30. The remaining parameters are the weights α of the jigsaw loss, and η assigned to the entropy loss when included in the optimization process for unsupervised domain adaptation. The final third parameter regulates the data input process: the shuffled images enter the network together with the original ordered ones, hence each image batch contains both of them. We define a data bias parameter β to specify their relative ratio. For instance <ref type="bibr" target="#b0">1</ref> Code available at https://github.com/fmcarlucci/JigenDG β = 0.6 means that for each batch, 60% of the images are ordered, while the remaining 40% are shuffled. These last three parameters were chosen by cross validation on a 10% subset of the source images for each experimental setting. We designed the JiGen network making it able to leverage over many possible convolutional deep architectures. Indeed it is sufficient to remove the existing last fully connected layer of a network and substitute it with the new object and jigsaw classification layers. JiGen is trained with SGD solver, 30 epochs, batch size 128, learning rate set to 0.001 and stepped down to 0.0001 after 80% of the training epochs. We used a simple data augmentation protocol by randomly cropping the images to retain between 80 − 100% and randomly applied horizontal flipping. Following <ref type="bibr" target="#b37">[38]</ref> we randomly (10% probability) convert an image tile to grayscale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets To evaluate the performance of JiGen when training over multiple sources we considered three domain generalization datasets. PACS <ref type="bibr" target="#b26">[27]</ref> covers 7 object categories and 4 domains (Photo, Art Paintings, Cartoon and Sketches). We followed the experimental protocol in <ref type="bibr" target="#b26">[27]</ref> and trained our model considering three domains as source datasets and the remaining one as target. VLCS <ref type="bibr" target="#b44">[45]</ref> aggregates images of 5 object categories shared by the PASCAL VOC 2007, LabelMe, Caltech and Sun datasets which are considered as 4 separated domains. We followed the standard protocol of <ref type="bibr" target="#b19">[20]</ref> dividing each domain into a training set (70%) and a test set (30%) by random selection from the overall dataset. The Office-Home dataset <ref type="bibr" target="#b45">[46]</ref> contains 65 categories of daily objects from 4 domains: Art, Clipart, Product and Real-World. In particular Product images are from vendor websites and show a white background, while Real-World represents object images collected with a regular camera. For this dataset we used the same experimental protocol of <ref type="bibr" target="#b13">[14]</ref>. Note that Office-Home and PACS are related in terms of domain types and it is useful to consider both as test-beds to check if JiGen scales when the number of categories changes from 7 to 65. Instead VLCS offers different challenges because it combines object categories from Caltech with scene images of the other domains.</p><p>To understand if solving jigsaw puzzles supports generalization even when dealing with a single source, we extended our analysis to digit classification as in <ref type="bibr" target="#b46">[47]</ref>. We trained a model on 10k digit samples of the MNIST dataset <ref type="bibr" target="#b24">[25]</ref> and evaluated on the respective test sets of MNIST-M <ref type="bibr" target="#b18">[19]</ref> and SVHN <ref type="bibr" target="#b34">[35]</ref>. To work with comparable datasets, all the images were resized to 32 × 32 treated as RGB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch-Based Convolutional Models for Jigsaw Puzzles</head><p>We start our experimental analysis by evaluating the application of existing jigsaw related patch-based convolu-  <ref type="table">Table 1</ref>. Domain Generalization results on PACS. The results of JiGen are average over three repetitions of each run. Each column title indicates the name of the domain used as target. We use the bold font to highlight the best results of the generalization methods, while we underline a result when it is higher than all the others despite produced by the naïve Deep All baseline. Top: comparison with previous methods that use the jigsaw task as a pretext to learn transferable features using a context-free siamese-ennead network (CFN). Center and Bottom: comparison of JiGen with several domain generalization methods when using respectively Alexnet and Resnet-18 architectures. tional architectures and models to the domain generalization task. We considered two recent works that proposed a jigsaw puzzle solver for 9 shuffled patches from images decomposed by a regular 3 × 3 grid. Both <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b37">[38]</ref> use a Context-Free Network (CFN) with 9 siamese branches that extract features separately from each image patch and then recompose them before entering the final classification layer. Specifically, each CFN branch is an Alexnet <ref type="bibr" target="#b23">[24]</ref> up to the first fully connected layer (f c6) and all the branches share their weights. Finally, the branches' outputs are concatenated and given as input to the following fully connected layer (f c7). The jigsaw puzzle task is formalized as a classification problem on a subset of patch permutations and, once the network is trained on a shuffled version of Imagenet <ref type="bibr" target="#b11">[12]</ref>, the learned weights can be used to initialize the conv layers of a standard Alexnet while the rest of the network is trained from scratch for a new target task. Indeed, according to the original works, the learned representation is able to capture semantically relevant content from the images regardless of the object labels. We followed the instructions in <ref type="bibr" target="#b35">[36]</ref> and started from the pretrained Jigsaw CFN (J-CFN) model provided by the authors to run finetuning for classification on the PACS dataset with all the source domain samples aggregated together. In the top part of <ref type="table">Table 1</ref> we indicate with J-CFN-Finetune the results of this experiment using the jigsaw model proposed in <ref type="bibr" target="#b35">[36]</ref>, while with J-CFN-Finetune++ the results from the advanced model proposed in <ref type="bibr" target="#b37">[38]</ref>. In both cases the average classification accuracy on the domains is lower than what can be obtained with a standard Alexnet model pre-trained for object classification on Imagenet and finetuned on all the source data aggregated together. We indicate this baseline approach with Deep All and we can use as reference the corresponding values in the following central part of <ref type="table">Table 1</ref>. We can conclude that, despite its power as an unsupervised pretext task, completely disregarding the object labels when solving jigsaw puzzles induces a loss of semantic information that may be crucial for generalization across domains.</p><p>To demonstrate the potentialities of the CFN architecture, the authors of <ref type="bibr" target="#b35">[36]</ref> used it also to train a supervised object Classification model on Imagenet (C-CFN) and demonstrated that it can produce results analogous to the standard Alexnet. With the aim of further testing this network to understand if and how much its peculiar siamese-ennead structure can be useful to distill shared knowledge across domains, we considered it as the main convolutional backbone for JiGen. Starting from the C-CFN model provided by the authors, we ran the obtained C-CFN-JiGen on PACS data, as well as its plain object classification version with the jigsaw loss disabled (α = 0) that we indicate as C-CFN-Deep All. From the obtained recognition accuracy we can state that combining the jigsaw puzzle with the classification task provides an average improvement in performance, which is the first result to confirm our intuition. However, C-CFN-Deep All is still lower than the reference results obtained with standard Alexnet.</p><p>For all the following experiments we consider the convolutional architecture of JiGen built with the same main structure of Alexnet or Resnet, using always the image as a whole (ordered or shuffled) instead of relying on separate patch-based network branches. A detailed comparison of per-class results on the challenging sketches domain for J-CFN-Finetune++ and JiGen based on Alexnet reveals that for four out of seven categories, J-CFN-Finetune++ is actually doing a good job, better than Deep All. With JiGen we improve over Deep All for the same categories by solving jigsaw puzzles at image level and we keep the advantage of Deep All for the remaining categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Source Domain Generalization</head><p>We compare the performance of JiGen against several recent domain generalization methods. TF is the low-rank parametrized network that was presented together with the dataset PACS in <ref type="bibr" target="#b26">[27]</ref>. CIDDG is the conditional invariant deep domain generalization method presented in <ref type="bibr" target="#b28">[29]</ref> that trains for image classification with two adversarial constraints: one that maximizes the overall domain confusion following <ref type="bibr" target="#b18">[19]</ref> and a second one that does the same per-class. In the DeepC variant, only this second condition is enabled. MLDG <ref type="bibr" target="#b25">[26]</ref> is a meta-learning approach that simulates train/test domain shift during training and exploit them to optimize the learning model. CCSA <ref type="bibr" target="#b33">[34]</ref> learns an embedding subspace where mapped visual domains are semantically aligned and yet maximally separated. MMD-AAE <ref type="bibr" target="#b27">[28]</ref> is a deep method based on adversarial autoencoders that learns an invariant feature representation by aligning the data distributions to an arbitrary prior through the Maximum Mean Discrepancy (MMD). SLRC <ref type="bibr" target="#b12">[13]</ref> is based on a single domain invariant network and multiple domain specific ones and it applies a low rank constraint among them. D-SAM <ref type="bibr" target="#b13">[14]</ref> is a method based on the use of domain-specific aggregation modules combined to improve model generalization: it provides the current sota results on PACS and Office-Home. For each of these methods, the Deep All baseline indicates the performance of the corresponding network when all the introduced domain adaptive conditions are disabled.</p><p>The central and bottom parts of <ref type="table">Table 1</ref> show the results of JiGen on the dataset PACS when using as backbone architecture Alexnet and Resnet-18 2 . On average Ji-Gen produces the best result when using Alexnet and it is just slightly worse than the D-SAM reference for Resnet-18. Note however, that in this last case, JiGen outperforms D-SAM in three out of four target cases and the average advantage of D-SAM originate only from its result on sketches. On average, JiGen outperforms also the competing methods on the VLCS and on the Office-Home datasets (see respectively <ref type="table">Table 2</ref>  presented small gain in accuracy with respect to the corresponding Deep All baseline (e.g. TF). Since <ref type="bibr" target="#b13">[14]</ref> did not present the results of D-SAM on the VLCS dataset, we used the code provided by the authors to run these experiments. The obtained results show that, although generally able to close large domain gaps across images of different styles as in PACS and Office-Home, when dealing with domains all coming from real-world images, the use of aggregative modules does not support generalization.</p><p>Ablation We focus on the Alexnet-PACS DG setting for an ablation analysis on the respective roles of the jigsaw and of the object classification task in the learning model. For these experiments we kept the jigsaw hyperparameters fixed with a 3 × 3 patch grid and P = 30 jigsaw classes. {α = 0, β = 1} means that the jigsaw task is off, and the data batches contain only original ordered images, which corresponds to Deep All. The value assigned to the data bias β drives the overall training: it moves the focus from jigsaw when using low values (β &lt; 0.5) to object classification when using high values (β ≥ 0.5). By setting the data bias to β = 0.6 we feed the network with more ordered than shuffled images, thus keeping the classification as the primary goal of the network. In this case, when changing the jigsaw loss weight α in {0.1, 1}, we observe results which are always either statistically equal or better than the Deep All baseline as shown in the first plot of   ure 4. The second plot indicates that, for high values of α, tuning β has a significant effect on the overall performance. Indeed {α ∼ 1, β = 1} means that jigsaw task is on and highly relevant in the learning process, but we are feeding the network only with ordered images: in this case the jigsaw task is trivial and forces the network to recognize always the same permutation class which, instead of regularizing the learning process, may increase the risk of data memorization and overfitting. Further experiments confirm that, for β = 1 but lower α values, JiGen and Deep All perform equally well. Setting β = 0 means feeding the network only with shuffled images. For each image we have P variants, only one of which has the patches in the correct order and is allowed to enter the object classifier, resulting in a drastic reduction of the real batch size. In this condition the object classifier is unable to converge, regardless of whether the jigsaw classifier is active (α &gt; 0) or not (α = 0). In those cases the accuracy is very low (&lt; 20%), so we do not show it in the plots to ease the visualization.</p><p>Jigsaw hyperparameter tuning By using the same experimental setting of the previous paragraph, the third plot in <ref type="figure" target="#fig_3">Figure 4</ref> shows the change in performance when the number of jigsaw classes P varies between 5 and 1000. We started from a low number, with the same order of magnitude of the number of object classes in PACS, and we grew till 1000 which is the number used for the experiments in <ref type="bibr" target="#b35">[36]</ref>. We observe an overall variation of 1.5 percentage points in the accuracy which still remains (almost always) higher than the Deep All baseline. Finally, we ran a test to check the accuracy when changing the grid size and consequently the patch number. Even in this case, the range of variation is limited when passing from a 2 × 2 to a 4 × 4 grid, confirming the conclusions of robustness already obtained for this parameter in <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b9">[10]</ref>. Moreover all the results are better than the Deep All reference. It is also interesting to check whether the jigsaw classifier is producing meaningful results per-se, besides supporting generalization for the object classifier. We show its recognition accuracy when testing on the same images used to evaluate the object classifier but with shuffled patches. In <ref type="figure" target="#fig_4">Figure 5</ref>, the first plot shows the accuracy over the learning epochs for the object and jigsaw classifiers indicating that both grows simultaneously (on different scales). The second plot shows the jigsaw recognition accuracy when changing the number of permutation classes P : of course the performance decreases when the task becomes more difficult, but overall the obtained results indicate that the jigsaw model is always effective in reordering the shuffled patches.</p><p>Single Source Domain Generalization The generalization ability of a model depends both on the chosen learning process and on the used training data. To investigate the former and better evaluate the regularization effect provided by the jigsaw task, we consider the case of training data from a single source domain. For these experiments we compare against the generalization method based on adversarial data augmentation (Adv.DA) recently presented in <ref type="bibr" target="#b46">[47]</ref>. This work proposes an iterative procedure that perturbs the samples to make them hard to recognize under the current model and then combine them with the original ones while solving the classification task. We reproduced the experimental setting used in <ref type="bibr" target="#b46">[47]</ref> and adopt a similar result display style with bar plots for experiments on the MNIST-M and SVHN target datasets when training on MNIST. In <ref type="figure" target="#fig_5">Figure  6</ref> we show the performance of JiGen when varying the data bias β and the jigsaw weight α. With the red background shadow we indicate the overall range covered by Adv.DA results when changing its parameters 3 , while the horizontal line is the reference Adv.DA results around which the authors of <ref type="bibr" target="#b46">[47]</ref> ran their parameter ablation analysis. The figure indicates that, although Adv.DA can reach high peak <ref type="bibr" target="#b2">3</ref> The whole set of results is provided as supplementary material of <ref type="bibr" target="#b46">[47]</ref>.   <ref type="table">Table 4</ref>. Multi-source Domain Adaptation results on PACS obtained as average over three repetitions for each run. Besides considering the same jigsaw loss weight for source and target samples α s = α t , we also tuned the target jigsaw loss weight while keeping α s = 0.7 showing that we can get even higher results. values, it is also very sensitive to the chosen hyperparameters. On the other hand, JiGen is much more stable and it is always better than the lower accuracy value of Adv.DA with a single exception for SVHN and data bias 0.5, but we know from the ablation analysis, that this corresponds to a limit case for the proper combination of object and jigsaw classification. Moreover, JiGen gets close to Adv.DA reference results for MNIST-M and significantly outperform it for SVHN.</p><p>Unsupervised Domain Adaptation When unlabeled target samples are available at training time we can let the jigsaw puzzle task involve these data. Indeed patch reordering does not need image labels and running the jigsaw optimization process on both source and target data may positively influence the source classification model for adaptation. To verify this intuition we considered again the PACS dataset and used it in the same unsupervised domain adaptation setting of <ref type="bibr" target="#b30">[31]</ref>. This previous work proposed a method to first discover the existence of multiple latent domains in the source data and then differently adapt their knowledge to the target depending on their respective similarity. It has been shown that this domain discovery (DDiscovery) technique outperforms other powerful adaptive approaches as Dial <ref type="bibr" target="#b5">[6]</ref> when the source actually includes multiple domains. Both these methods exploit the minimization of the entropy loss as an extra domain alignment condition: in this way the source model when predicting on the target samples is encouraged to assign maximum prediction probability to a single label rather than distributing it over multiple class options. For a fair comparison we also turned on the entropy loss for JiGen with weight η = 0.1. Moreover, we considered two cases for the jigsaw loss: either keeping the weight α already used for the PACS-Resnet-18 DG experiments for both the source and target data (α = α s = α t = 0.7), or treating the domain separately with a dedicated weight for the jigsaw target loss (α s = 0.7, α t = [0.1, 0.3, 0.5, 0.9]). The results for this setting are summarized in <ref type="table">Table 4</ref>. The obtained accuracy indicates that JiGen outperforms the competing methods on average and in particular on the difficult task of recognizing sketches. Furthermore, the advantage remains true regardless of the specific choice of the target jigsaw loss weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we showed for the first time that generalization across visual domains can be achieved effectively by learning to classify and learning intrinsic image invariances at the same time. We focused on learning spatial colocation of image parts, and proposed a simple yet powerful framework that can accommodate a wide spectrum of pretrained convolutional architectures. Our method JiGen can be seamlessly and effectively used for domain adaptation and generalization as shown by the experimental results.</p><p>We see this paper as opening the door to a new research thread in domain adaptation and generalization. While here we focused on a specific type of invariance, several other regularities could be learned possibly leading to an even stronger benefit. Also, the simplicity of our approach calls for testing its effectiveness in applications different from object categorization, like semantic segmentation and person re-identification, where the domain shift effect strongly impact the deployment of methods in the wild. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>We provide here some further analysis and experimental results on using jigsaw puzzle and other self-supervised tasks as auxiliary objectives to improve generalization across visual domains.</p><p>Visual explanation and Failure cases The relative position of each image patch with respect to the others captures visual regularities which are at the same time shared among domains and discriminative with respect to the object classes. Thus, by solving jigsaw puzzles we encourage the network to localize and re-join relevant object subparts regardless of the visual domain. This helps to focus on the most informative image areas. For an in-depth analysis of the learned model we adopted the Class Activation Mapping (CAM, <ref type="bibr" target="#b50">[51]</ref>) method on ResNet-18, with which we produced the activation maps in <ref type="figure" target="#fig_6">Figure 7</ref> for the PACS dataset. The first two rows show that JiGen is better at localizing the object class with respect to Deep All. The last row indicates that the mistakes are related to some flaw in data interpretation, while the localization remains correct.</p><p>Self-supervision by predicting image rotations Reordering image patches to solve jigsaw puzzle is not the only self-supervised approach that can be combined with supervised learning for domain generalization. We ran experiments by using as auxiliary self-supervised task the rotation classifier (four classes [0 • , 90 • , 180 • , 270 • ]) proposed in <ref type="bibr" target="#b20">[21]</ref>. We focused on the PACS dataset with the Alexnet-based architecture, following the same protocol used for JiGen. The obtained accuracy <ref type="table">(Table 5)</ref> is higher than the Deep All baseline, but still lower than what obtained with our method. Indeed object 2d orientation provides useful semantic information when dealing with real photos, but it becomes less critical for cartoons and sketches.  <ref type="table">Table 5</ref>. Top: results obtained by using Rotation recognition as auxiliary self-supervised task. Bottom: three cartoons and three sketches that show objects with odd orientations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Confusion matrices on Alexnet-PACS DG setting, when sketches is used as target domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Ablation results on the Alexnet-PACS DG setting. The reported accuracy is the global average over all the target domains with three repetitions for each run. The red line represents our Deep All average fromTable 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Analysis of the behaviour of the jigsaw classifier on the Alexnet-PACS DG setting. For the plot on the left each axes refers to the color matching curve in the graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Single Source Domain Generalization experiments. We analyze the performance of JiGen in comparison with the method Adv.DA<ref type="bibr" target="#b46">[47]</ref>. The shaded background area covers the overall range of results of Adv.DA obtained when changing the hyper-parameters of the method. The reference result of Adv.DA (γ = 1, K = 2) together with its standard deviation is indicated here by the horizontal red line. The blue histogram bars show the performance of JiGen when changing the jigsaw weight α and data bias β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>CAM activation maps: yellow corresponds to high values, while dark blue corresponds to low values. JiGen is able to localize the most informative part of the image, useful for object class prediction regardless of the visual domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>and 3). In particular we remark that VLCS is a tough setting where the most recent works have only Domain Generalization results on VLCS. For details about number of runs, meaning of columns and use of bold/underline fonts, seeTable 1. Domain Generalization results on Office-Home. For details about number of runs, meaning of columns and use of bold/underline fonts, seeTable 1.</figDesc><table><row><cell></cell><cell>VLCS</cell><cell cols="3">Caltech Labelme Pascal Sun</cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell cols="2">Alexnet</cell><cell></cell></row><row><cell></cell><cell>Deep All</cell><cell>85.73</cell><cell>61.28</cell><cell cols="2">62.71 59.33 67.26</cell></row><row><cell>[29]</cell><cell>DeepC</cell><cell>87.47</cell><cell>62.60</cell><cell cols="2">63.97 61.51 68.89</cell></row><row><cell></cell><cell>CIDDG</cell><cell>88.83</cell><cell>63.06</cell><cell cols="2">64.38 62.10 69.59</cell></row><row><cell>[34]</cell><cell>Deep All CCSA</cell><cell>86.10 92.30</cell><cell>55.60 62.10</cell><cell cols="2">59.10 54.60 63.85 67.10 59.10 70.15</cell></row><row><cell>[13]</cell><cell>Deep All SLRC</cell><cell>86.67 92.76</cell><cell>58.20 62.34</cell><cell cols="2">59.10 57.86 65.46 65.25 63.54 70.97</cell></row><row><cell>[27]</cell><cell>Deep All TF</cell><cell>93.40 93.63</cell><cell>62.11 63.49</cell><cell cols="2">68.41 64.16 72.02 69.99 61.32 72.11</cell></row><row><cell cols="3">[28] MMD-AAE 94.40</cell><cell>62.60</cell><cell cols="2">67.70 64.40 72.28</cell></row><row><cell>[14]</cell><cell>Deep All D-SAM</cell><cell>94.95 91.75</cell><cell>57.45 56.95</cell><cell cols="2">66.06 65.87 71.08 58.59 60.84 67.03</cell></row><row><cell></cell><cell>Deep All</cell><cell>96.93</cell><cell>59.18</cell><cell cols="2">71.96 62.57 72.66</cell></row><row><cell></cell><cell>JiGen</cell><cell>96.93</cell><cell>60.90</cell><cell cols="2">70.62 64.30 73.19</cell></row><row><cell cols="6">Office-Home Art Clipart Product Real-World Avg.</cell></row><row><cell></cell><cell></cell><cell cols="2">Resnet-18</cell><cell></cell></row><row><cell>[14]</cell><cell cols="2">Deep All 55.59 42.42 D-SAM 58.03 44.37</cell><cell>70.34 69.22</cell><cell>70.86 71.45</cell><cell>59.81 60.77</cell></row><row><cell></cell><cell cols="2">Deep All 52.15 45.86</cell><cell>70.86</cell><cell>73.15</cell><cell>60.51</cell></row><row><cell></cell><cell cols="2">JiGen 53.04 47.51</cell><cell>71.47</cell><cell>72.79</cell><cell>61.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>PACS art paint. cartoon sketches photoAvg.</figDesc><table><row><cell></cell><cell></cell><cell>Alexnet</cell><cell></cell><cell></cell></row><row><cell>Deep All</cell><cell>66.68</cell><cell>69.41</cell><cell>60.02 89.98</cell><cell>71.52</cell></row><row><cell>Rotation</cell><cell>67.67</cell><cell>69.83</cell><cell>61.04 89.98</cell><cell>72.13</cell></row><row><cell>JiGen</cell><cell>67.63</cell><cell>71.71</cell><cell>65.18 89.00</cell><cell>73.38</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">With Resnet18, to put JiGen on equal footing with D-SAM we follow the same data augmentation protocol in<ref type="bibr" target="#b13">[14]</ref> and enabled color jittering.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported by the ERC grant 637076 RoboExNovo and a NVIDIA Academic Hardware Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive deep learning through visual domain localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Angeletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotic Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Bisanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gay</forename><forename type="middle">L</forename><surname>Bisanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kail</surname></persName>
		</author>
		<title level="m">Learning in Children: Progress in Cognitive Development Research</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain Separation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Andreas Vlachopoulos, Christos Doumas, Szymon Rusinkiewicz, and Tim Weyrich. A system for high-volume acquisition and matching of fresco fragments: Reassembling Theran wall paintings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benedict</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corey</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Toler-Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Nehab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dobkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Partial adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijia</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europeran Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Just dial: domain alignment layers for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel Rota</forename><surname>Bulò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The patch transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Taeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1489" to="1501" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A probabilistic image jigsaw puzzle solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Taeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual permutation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Domain Adaptation in Computer Vision Applications. Advances in Computer Vision and Pattern Recognition</title>
		<editor>Gabriela Csurka</editor>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li Jia Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep domain generalization with structured low-rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain generalization with domain-specific aggregation modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Antonio D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caputo</surname></persName>
		</author>
		<ptr target="https://github.com/VeloDC/D-SAM_public" />
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Waxman. Very young infants learn abstract rules in the visual modality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brock</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Apictorial jigsaw puzzles: The computer solution of a problem in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Garder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electronic Computers</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="118" to="127" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jigsaw puzzles with pieces of unknown orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision, (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An automatic jigsaw puzzle solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Kosiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">M</forename><surname>Devaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarak</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangachar</forename><surname>Kasturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV), September</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust place categorization with deep domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimilano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mitochondrial dna as a genomic jigsaw puzzle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Marande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertraud</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">5849</biblScope>
			<biblScope unit="page" from="415" to="415" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Shuffle and Learn: Unsupervised Learning using Temporal Order Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image reassembly combining deep learning and shortest path problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Morgane</forename><surname>Paumard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A generalized genetic algorithm-based solver for very large jigsaw puzzles of complex types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dror</forename><surname>Sholomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><forename type="middle">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Solving square jigsaw puzzles with loop constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilho</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Solving small-piece jigsaw puzzles by growing consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilho</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
