<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUPERVISED COMMUNITY DETECTION WITH LINE GRAPH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Rosebud AI</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SUPERVISED COMMUNITY DETECTION WITH LINE GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditionally, community detection in graphs can be solved using spectral methods or posterior inference under probabilistic graphical models. Focusing on random graph families such as the stochastic block model, recent research has unified both approaches and identified both statistical and computational detection thresholds in terms of the signal-to-noise ratio. By recasting community detection as a node-wise classification problem on graphs, we can also study it from a learning perspective. We present a novel family of Graph Neural Networks (GNNs) for solving community detection problems in a supervised learning setting. We show that, in a data-driven manner and without access to the underlying generative models, they can match or even surpass the performance of the belief propagation algorithm on binary and multi-class stochastic block models, which is believed to reach the computational threshold. In particular, we propose to augment GNNs with the non-backtracking operator defined on the line graph of edge adjacencies. Our models also achieve good performance on real-world datasets. In addition, we perform the first analysis of the optimization landscape of training linear GNNs for community detection problems, demonstrating that under certain simplifications and assumptions, the loss values at local and global minima are not far apart.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph inference problems encompass a large class of tasks and domains, from posterior inference in probabilistic graphical models to community detection and ranking in generic networks, image segmentation or compressed sensing on non-Euclidean domains. They are motivated both by practical applications, such as in the case of PageRank <ref type="bibr" target="#b31">(Page et al., 1999)</ref>, and also by fundamental questions on the algorithmic hardness of solving such tasks.</p><p>From a data-driven perspective, these problems can be formulated in supervised, semi-supervised and unsupervised learning settings. In the supervised case, one assumes a dataset of graphs with labels on their nodes, edges or the entire graphs, and attempts to perform node-wise, edge-wise and graph-wise classification by optimizing a loss over a certain parametric class, e.g. neural networks. Graph Neural Networks (GNNs) are natural extensions of Convolutional Neural Networks (CNN) to graph-structured data, and have emerged as a powerful class of algorithms to perform complex graph inference leveraging labeled data <ref type="bibr" target="#b11">(Gori et al., 2005;</ref><ref type="bibr" target="#b5">Bruna et al., 2013b;</ref><ref type="bibr" target="#b9">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b3">Bronstein et al., 2017;</ref><ref type="bibr" target="#b20">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b8">Defferrard et al., 2016;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017)</ref>. In essence, these neural networks learn cascaded linear combinations of intrinsic graph operators interleaved with node-wise (or edge-wise) activation functions. Since they utilize intrinsic graph operators, they can be applied to varying input graphs, and they offer the same parameter sharing advantages as their CNN counterparts.</p><p>In this work, we focus on community detection problems, a wide class of node classification tasks that attempt to discover a clustered, segmented structure within a graph. The traditional algorithmic approaches to this problem include a rich class of spectral methods, which take advantage of the spectrum of certain operators defined on the graph, as well as approximate message-passing methods such as belief propagation (BP), which performs approximate posterior inference under predefined graphical models <ref type="bibr" target="#b7">(Decelle et al., 2011)</ref>. Focusing on the supervised setting, we study the ability of GNNs to approximate, generalize and even improve upon these class of algorithms. Our motivation is two-fold. On the one hand, this problem exhibits algorithmic hardness on some settings, opening up the possibility to discover more efficient algorithms than the current ones. On the other hand, many practical scenarios fall beyond pre-specified probabilistic models, hence calling for data-driven solutions.</p><p>We propose modifications to the GNN architecture, which allow it to exploit edge adjacency information, by incorporating the non-backtracking operator of the graph. This operator is defined over the edges of the graph and allows a directed flow of information even when the original graph is undirected. It was introduced to community detection problems by <ref type="bibr" target="#b22">Krzakala et al. (2013)</ref>, who propose a spectral method based on the non-backtracking operator. We refer to the resulting GNN model as a Line Graph Neural Network <ref type="bibr">(LGNN)</ref>. Focusing on important random graph families exhibiting community structure, such as the stochastic block model (SBM) and the geometric block model (GBM), we demonstrate improvements in the performance by our GNN and LGNN models compared to other methods including spectral methods and BP in regimes within the computational-to-statistical gap <ref type="bibr" target="#b0">(Abbe, 2017)</ref>. In fact, some gains can already be obtained with linear LGNNs, which can be interpreted as data-driven versions of power iteration algorithms.</p><p>Besides community detection tasks, <ref type="bibr">GNN and</ref> LGNN can be applied to other node-wise classification problems too. The reason we are focusing on community detection is that it has a rich theoretical literature where different algorithms have been proposed and fundamental limits in terms of computational and statistical (or information-theoretic) thresholds have been established in several scenarios. Moreover, synthetic datasets can be easily generated for community detection tasks. Therefore, besides the practical value of community detection, we think it is a nice platform for comparing against traditional non-data-driven algorithms.</p><p>The good performances of GNN and LGNN motivate our second main contribution: an analysis of the optimization landscape of simplified linear GNN models when trained under a given graph distribution. Under reparametrization, we provide an upper bound on the energy gap controlling the loss difference between local and global minima. With some assumptions on the spectral concentration of certain random matrices, this energy gap will shrink as the size of the input graphs increases, which would mean that the optimization landscape is benign on large enough graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Main Contributions:</head><p>• We define a GNN model based on a family of multiscale graph operators, and propose to augment it using the line graph and the non-backtracking operator, which yields improvements in supervised community detection tasks.</p><p>• For graphs generated from stochastic block models (SBMs), our models reach detection thresholds in a purely data-driven fashion, outperforming belief propagation (BP) in hard regimes. Our models also succeed on graphs from the geometric block model (GBM).</p><p>• Our models perform well in detecting communities in real datasets from SNAP.</p><p>• We perform the first analysis of the learning landscape of linear GNN models, showing that under certain simplifications and assumptions, the local minima are confined in low-loss regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM SETUP</head><p>Community detection is a specific type of node-classification tasks in which given an input graph G = (V, E), we want to predict an underlying labeling function y : V → {1, . . . , C} that encodes a partition of V into C communities. We consider the supervised learning setting, where a training set {(G t , y t )} t≤T is given, with which we train a model that predictsŷ = Φ θ (G) by minimizing a loss function of the form</p><formula xml:id="formula_0">L(θ) = 1 T t≤T (Φ θ (G t ), y t )</formula><p>Since y encodes a partition of C groups, the specific label of each node is only important up to a global permutation of {1, . . . , C}. Section 4.3 describes how to construct loss functions with such a property. Moreover, a permutation of the node indices translates into the same permutation applied to the labels, which justifies using models Φ that are equivariant to node permutations. Also, we are interested in inferring properties of community detection algorithms that do not depend on the specific size of the graphs, and therefore require that the model Φ accepts graphs of variable size for the same set of parameters. We also assume that C is known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORKS</head><p>GNN was first proposed in <ref type="bibr" target="#b11">Gori et al. (2005)</ref>; <ref type="bibr" target="#b36">Scarselli et al. (2009)</ref>. <ref type="bibr" target="#b4">Bruna et al. (2013a)</ref> generalize convolutional neural networks on general undirected graphs by using the graph Laplacian's eigenbasis. This was the first time the Laplacian operator was used in a neural network architecture to perform classification on graph inputs. <ref type="bibr" target="#b8">Defferrard et al. (2016)</ref> consider a symmetric Laplacian generator to define a multiscale GNN architecture, demonstrated on classification tasks. Similarly, Kipf &amp; Welling (2016) use a similar generator as effective embedding mechanisms for graph signals and applies it to semi-supervised tasks. This is the closest application of GNNs to our current contribution. However, we highlight that semi-supervised learning requires bootstrapping the estimation with a subset of labeled nodes, and is mainly interested in transductive learning within a single, fixed graph. In comparison, our setup considers inductive community detection across a distribution of input graphs and assumes no initial labeling on the graphs in the test dataset except for the adjacency information.</p><p>There have been several extensions of GNNs by modifying their non-linear activation functions, parameter sharing strategies, and choice of graph operators <ref type="bibr" target="#b24">(Li et al., 2015;</ref><ref type="bibr" target="#b39">Sukhbaatar et al., 2016;</ref><ref type="bibr" target="#b9">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b30">Niepert et al., 2016)</ref>. In particular, <ref type="bibr" target="#b10">Gilmer et al. (2017)</ref> interpret the GNN architecture as learning an approximate message-passing algorithm, which extends the learning of hidden representations to graph edges in addition to graph nodes. Recently, <ref type="bibr" target="#b42">Velickovic et al. (2017)</ref> relate adjacency learning with attention mechanisms, and <ref type="bibr" target="#b41">Vaswani et al. (2017)</ref> propose a similar architecture in the context of machine translation. Another recent and related piece of work is by <ref type="bibr" target="#b21">Kondor et al. (2018)</ref>, who propose a generalization of GNN that captures high-order node interactions through covariant tensor algebra. Our approach to extend the expressive power of GNN using the line graph may be seen as an alternative to capture such high-order interactions.</p><p>Our energy landscape analysis is related to the recent paper by <ref type="bibr" target="#b37">Shamir (2018)</ref>, which establishes an energy bound on the local minima arising in the optimization of ResNets. In our case, we exploit the properties of the community detection problem to produce an energy bound that depends on the concentration of certain random matrices, which one may hope for as the size of the input graphs increases. Finally, Zhang (2016)'s work on data regularization for clustering and rank estimation is also motivated by the success of using Bethe-Hessian-like perturbations to improve spectral methods on sparse networks. It finds good perturbations via matrix perturbations and also has successes on the stochastic block model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LINE GRAPH NEURAL NETWORKS</head><p>This section introduces our GNN architectures that include the power graph adjacency (Section 4.1) and its extension to line graphs using the non-backtracking operator (Section 4.2), as well as the design of losses invariant to global label permutations (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GRAPH NEURAL NETWORKS USING A FAMILY OF MULTISCALE GRAPH OPERATORS</head><p>Given a graph G = (V, E) and a vector x ∈ R |V |×b of node features, we consider intrinsic linear operators of the graph that act locally on x, which can be represented as |V |-by-|V | matrices. For example, the adjacency matrix A is defined entry-wise by A i1i2 = 1 if (i 1 , i 2 ) ∈ E and A i1i2 = 0 if (i 1 , i 2 ) / ∈ E, for every pair (i 1 , i 2 ) ∈ V × V . The degree matrix D is a diagonal matrix with D ii being the degree of the ith node, and it can be expressed as diag(A1). We can also define power graph adjacency matrices as A J = min(1, A 2 J ), which encodes 2 J -hop neighborhoods into a binary graph for J ∈ N * . Finally, there is also the identity matrix, I. . Given a graph G, we construct its line graph L(G) with the non-backtracking operator ( <ref type="figure" target="#fig_1">Figure 2</ref>). In every layer, the states of all nodes in G and L(G) are updated according to (2). The final states of nodes in G are used to predict node-wise labels, and the trainining is performed end-to-end using standard backpropagation with a label permutation invariant loss (Section 4.3).</p><p>Having a family of such matrices, F = {I, D, A, A 2 , ..., A J } with a certain J, we can define a multiscale GNN layer that maps x (k) ∈ R |V |×b k to x (k+1) ∈ R |V |×b k+1 as follows. First, we compute</p><formula xml:id="formula_1">z (k+1) = ρ Oi∈F O i x (k) θ i , z (k+1) = Oi∈F O i x (k) θ i (1) where θ j ∈ R b k × b k+1 2</formula><p>are trainable parameters and ρ(·) is a point-wise nonlinear activation function, chosen in this work to be the ReLU function, i.e. ρ(z) = max(0, z) for z ∈ R. Then we define x (k+1) = [z (k+1) , z (k+1) ] ∈ R |V |×b k+1 as the concatenation of z (k+1) and z (k+1) . The layer thus includes linear skip connections via z (k) , both to ease with the optimization when using large number of layers (similar to residual connections <ref type="bibr" target="#b14">(He et al., 2016)</ref>) and to increase the expressive power of the model by enabling it to perform power iterations. Since the spectral radius of the learned linear operators in (1) can grow as the optimization progresses, the cascade of GNN layers can become unstable to training. In order to mitigate this effect, we perform instance normalization (or spatial batch normalization with one graph per batch) <ref type="bibr" target="#b15">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b40">Ulyanov et al., 2016)</ref> at each layer. The initial node states x (0) are set to be the node attributes if they are present in the data, and otherwise the degrees of the nodes, i.e., x</p><formula xml:id="formula_2">(0) i = D ii .</formula><p>Note that the model Φ(G, x (0) ) = x (K) satisfies the permutation equivariance property required for node classification: given a permutation π among the nodes in the graph, Φ(π • G, Πx (0) ) = ΠΦ(G, x (0) ), where Π is the |V | × |V | permutation matrix associated with π.</p><p>Analogy with power iterations In our setup, instance normalization not only prevents gradient blowup, but also performs the orthogonalisation relative to the constant vector, which reinforces the analogy with the spectral methods for community detection, some background of which is described in Appendix B.1. In short, under certain conditions, the community structure of the graph is correlated with both the eigenvector of A corresponding to its second largest eigenvalue and the eigenvector of the Laplacian matrix, L = D − A, corresponding to its second smallest eigenvalue (the latter often called the Fiedler vector). Thus, spectral methods for community detection performs power iterations on these matrices to obtain the eigenvectors of interest and predicts the community structure based on them. For example, to extract the Fiedler vector, after finding the eigenvector v corresponding to the smallest eigenvalue of L, one can then perform projected power iterations onL := L I − L by iteratively computing y (n+1) =Lx (n) and</p><formula xml:id="formula_3">x (n+1) = y (n+1) −v T vy (n+1) y (n+1) −v T vy (n+1) .</formula><p>As v is in fact a constant vector, the normalization here is analogous to the instance normalization step in the GNN layer defined above. As explained in Appendix B.1, the graph Laplacian is not ideal for spectral clustering to operate well in the sparse regime as compared to the Bethe Hessian matrix, which explores the space of matrices generated by {I, D, A}, just like our GNN model. Moreover, the expressive power of our GNN is further increased by adding multiscale versions of A. We can choose the depth of the GNN to be of the order of the graph diameter, so that all nodes obtain information from the entire graph. In sparse graphs with small diameter, this architecture offers excellent scalability and computational complexity. Indeed, in many social networks diameters are constant (due to hubs) or log(|V |), as in the stochastic block model in the constant or log(|V |) average degree regime <ref type="bibr" target="#b33">(Riordan &amp; Wormald, 2010)</ref>. This results in a model with computational complexity on the order of |V | log(|V |), making it amenable to large-scale graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LGNN: GNN ON LINE GRAPHS WITH THE NON-BACKTRACKING OPERATOR</head><p>Belief propagation (BP) is a dynamical-programming-style algorithm for computing exactly or approximating marginal distributions in graphical model inferences <ref type="bibr" target="#b32">(Pearl, 1982;</ref><ref type="bibr" target="#b44">Yedidia et al., 2003)</ref>. BP operates by passing messages iteratively on the non-backtracking edge adjacency structure, which we introduce in details in Appendix B.2. In this section, we describe an upgraded GNN model that exploits the non-backtracking structure, which can be viewed as a data-driven generalization of BP.</p><p>Given an undirected graph G = (V, E), its line graph L(G) = (V L , E L ) encodes the directed edge adjacency structure of G. The vertices of L(G) consist of the ordered edges in E, i.e., V L = {(i → j) : (i, j) ∈ E}, which means that |V L | = 2|E|. The edge set E L of L(G) is given by the non-backtracking matrix B ∈ R 2|E|×2|E| defined as</p><formula xml:id="formula_4">B (i→j),(i →j ) = 1 if j = i and j = i , 0 otherwise.</formula><p>This matrix enables the directed propagation of information on the line graph, and was first proposed in the context of community detection on sparse graphs in <ref type="bibr" target="#b22">Krzakala et al. (2013)</ref>. The messagepassing rules of BP can be expressed as a diffusion in the line graph L(G) using this non-backtracking operator, with specific choices of activation function that turn product of beliefs into sums. Detailed explanations are given in Appendix B.3.</p><p>Thus, a natural extension of the GNN architecture presented in Section 4.1 is to consider a second GNN defined on L(G), where B and D B = diag(B1) play the role of the adjacency and the degree matrices, respectively. Analogous to A J , we also define B J = min(1, B 2 J ). These operators allow us to consider edge states that update according to the edge adjacency of G. Moreover, edge and node states communicate at each layer using the unsigned and signed incidence matrices P,P ∈ {0, 1} |V |×2|E| , defined as P i,(i→j) = 1, P j,(i→j) = 1,P i,(i→j) = 1,P j,(i→j) = −1 and 0 otherwise. Together with skip linear connections z (k+1) and w (k+1) defined in a way analogous to (1), the update rule at each layer can be written as</p><formula xml:id="formula_5">z (k+1) = ρ   Oi∈F O i x (k) θ i + O j ∈F O j y (k) θ i   w (k+1) = ρ   O l ∈F O l y (k) θ i + O j ∈F (O j ) T x (k+1) θ j   (2) where F = {I, D, A, A 2 , . . . , A J }, F = {I B , D B , B, B 2 , . . . , B J }, F = {P,P }, and the train- able parameters are θ i , θ i , θ i ∈ R b k ×b k+1 and θ i ∈ R b k+1 ×b k+1 .</formula><p>We call such a model a Line Graph Neural Network (LGNN).</p><p>In our experiments, we set x (0) = deg(A) and y (0) = deg(B). For graph families whose average degree d remains constant as |V | grows, the line graph has size of O(d|V |), and therefore the model is feasible computationally. Furthermore, the construction of line graphs can be iterated to generate L(L(G)), L(L(L(G))), etc. to yield a line graph hierarchy, which can capture higher-order interactions among nodes of G. Such a hierarchical construction is related to other recent efforts to generalize GNNs <ref type="bibr" target="#b21">(Kondor et al., 2018;</ref><ref type="bibr" target="#b27">Morris et al., 2019)</ref>.</p><p>Learning directed edge features from an undirected graph Several authors have proposed to combine node and edge feature learning, such as <ref type="bibr" target="#b2">(Battaglia et al., 2016;</ref><ref type="bibr" target="#b17">Kearnes et al., 2016;</ref><ref type="bibr" target="#b10">Gilmer et al., 2017;</ref><ref type="bibr" target="#b42">Velickovic et al., 2017)</ref>. However, we are not aware of works that consider the edge adjacency structure provided by the non-backtracking matrix on the line graph. With non-backtracking matrix, our LGNN can be interpreted as learning directed edge features from an undirected graph. Indeed, if each node i contains two distinct sets of features x s (i) and x r (i), the non-backtracking operator constructs edge features from node features while preserving orientation: For an edge e = (i, j), our model is equivalent to constructing oriented edge features f i→j = g(x s (i), x r (j)) and f j→i = g(x r (i), x s (j)) (where g is trainable and not necessarily commutative on its arguments) that are subsequently propagated through the graph. To demonstrate the benefit of incorporating such local oriented structures, we will compare LGNN with a modified version, symmetric LGNN (LGNN-S).</p><p>LGNN-S is based on an alternative line graph of size |E| whose nodes are the undirected edges of the original graph, and where two such undirected edges of G are adjacent if and only if they share one common node in G; also, we set F = {P } in LGNN-S, with P ∈ R |V |×|E| defined as P i,(j,k) = 1 if i = j or k and 0 otherwise. In addition, we also define linear LGNN (LGNN-L) as the LGNN that drops the nonlinear activation functions ρ in (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A LOSS FUNCTION INVARIANT UNDER LABEL PERMUTATION</head><p>Let G = (V, E) be the input graph and y i be the ground truth community label of node i. Let [C] := {1, . . . , C} denote the set of all community labels, and consider first the case where communities do not overlap. After applying the softmax function at the end of the model, for each c ∈ [C], we interpret the cth dimension of the model's output at node i as the conditional probability that the node belongs to community c: o i,c = p(y i = c |θ, G). Since the community structure is defined up to global permutations of the labels, we define the loss function as</p><formula xml:id="formula_6">(θ) = min π∈S C − i∈V log o i,π(yi) ,<label>(3)</label></formula><p>where S C denotes the permutation group of C elements. This is essentially taking the the cross entropy loss minimized over all possible permutations of [C]. In our experiments, we consider examples with small numbers of communities such as 2 and 5. In general scenarios where C is much larger, the evaluation of the loss function (3) can be impractical due to the minimization over S C . A possible solution is to randomly partition [C] intoC and then marginalize the model outputs</p><formula xml:id="formula_7">{o i,c } c∈[C] into {õ i,c := c∈c o i,c }c ∈C .</formula><p>Finally, we can use (θ) = min π∈SC − i∈V logõ i,π(ỹi) as an approximate loss value, which only involves a permutation group of size |C|!.</p><p>Finally, if communities may overlap, we can enlarge the label set to include subsets of communities and define the permutation group accordingly. For example, if there are two overlapping communities, we let C = {{1}, {2}, {1, 2}} be the label set, and only allow the permutation between {1} and {2} when computing the loss function (as well as the overlap to be introduced in Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LOSS LANDSCAPE OF LINEAR GNN OPTIMIZATION</head><p>As described in the numerical experiments, we found that the GNN models without nonlinear activations already provide substantial gains relative to baseline (non-trainable) algorithms. This section studies the optimization landscape of linear GNNs. Despite defining a non-convex objective, we prove that the landscape is benign under certain further simplifications, in the sense that the local minima are confined in sublevel sets of the loss function.</p><p>For simplicity, we consider only the binary (i.e., C = 2) case where we replace the node-wise binary cross-entropy loss by the squared cosine distance (which also accounts for the invariance up to a global flip of labels), assume a single dimension of hidden states (b k = 1 for all k), and focus on the GNN described in Section 4.1 (although our analysis carries equally to describe the line graph version; see remarks below). We also make the simplifying assumption of replacing the layer-wise instance normalization by a simpler projection onto the unit 2 ball (thus we do not remove the mean). Without loss of generality, assume that the input graph G has size n, and denote by F = {A 1 , . . . , A Q } the family of graph operators appearing in (1). Each layer thus applies an arbitrary polynomial</p><formula xml:id="formula_8">Q q=1 θ (k)</formula><p>q A q to the incoming node feature vector x <ref type="bibr">(k)</ref> . Given an input node vector w ∈ R n , the network output can thus be written aŝ</p><formula xml:id="formula_9">Y = e e , with e =   K k=1 q≤Q θ (k) q A q   w .<label>(4)</label></formula><p>We highlight that this linear GNN setup is fundamentally different from the linear fully-connected neural networks (that is, neural networks with linear activation function), whose landscape has been analyzed in <ref type="bibr" target="#b16">Kawaguchi (2016)</ref>. First, the output of the GNN is on the unit sphere, which has a different geometry. Next, the operators in F depend on the input graph, which introduce fluctuations in the landscape. In general, the operators in F are not commutative, but by considering the generalized Krylov subspace generated by powers of F,</p><formula xml:id="formula_10">F K = {O 1 = A K 1 , O 2 = A 1 A K−1 2 , O 3 = A 1 A 2 A K−2 1 , . . . O Q K = A K Q }, one can reparametrize (4) as e = Q K j=1 β j O j w with β ∈ R M , with M = Q K .</formula><p>Given a graph instance with label vector y ∈ R n , the loss it incurs is 1 − | e,y | 2 e 2 , and therefore the population loss, when expressed as a function of β, equals</p><formula xml:id="formula_11">L n (β) = 1 − E Xn,Yn β Y n β β X n β , with (5) Y n = z n z n ∈ R M ×M , (z n ) j = O j w, y and X n = U n U n ∈ R M ×M , U n =   (O 1 w) . . . (O M w)   .</formula><p>Thus, to study the loss landscape, we examine the properties of the pair of random matrices Y n , X n ∈ R M ×M . Assuming that EX n 0, we write the Cholesky decomposition of EX n as EX n = R n R T n ,</p><formula xml:id="formula_12">and define A n = R −1 n Y n (R −1 n ) T ,Ā n = EA n = R −1 n EY n (R −1 n ) T , B n = R −1 n X n (R −1 n ) T , and ∆B n = B n − I n . Given a symmetric matrix K ∈ R M ×M , we let λ 1 (K), λ 2 (K), .</formula><p>.., λ M (K) denote the eigenvalues of K in nondecreasing order. Then, the following theorem establishes that under appropriate assumptions, the concentration of relevant random matrices around their mean controls the energy gaps between local and global minima of L. Theorem 5.1. For a given n, let</p><formula xml:id="formula_13">η n = (λ 1 (Ā n ) − λ 2 (Ā n )) −1 , µ n = E[|λ 1 (A n )| 6 ], ν n = E[|λ 1 (B n )| −6 ], δ n = E[ ∆B n 6 ]</formula><p>, and assume that all four quantities are finite. Then if β l ∈ S M −1 is a local minimum of L n , and β g ∈ S M −1 is a global minimum of L n , we have L n (β l ) ≤ (1 + ηn,µn,νn,δn ) · L n (β g ), where ηn,µn,νn,δn = O(δ n ) for given η n , µ n , ν n as δ n → 0 and its formula is given in the appendix. Corollary 5.2. If (η n ) n∈N * , (µ n ) n∈N * , (ν n ) n∈N * are all bounded sequences, and lim n→∞ δ n = 0, then ∀ &gt; 0, ∃n such that ∀n &gt; n , |L n (β l ) − L n (β g )| ≤ · L n (β g ).</p><p>The main strategy of the proof is to consider the actual loss function L n as a perturbation of L n (β) = 1 − E Xn,Yn</p><formula xml:id="formula_14">β T Ynβ β T EXnβ = 1 − β T EYnβ β T EXnβ ,</formula><p>which has a landscape that is easier to analyze and does not have poor local minima, since it is equivalent to a quadratic form defined over the sphere S M −1 . Applying this theorem requires estimating spectral fluctuations of the pair X n , Y n , which in turn involve the spectrum of the C * algebras generated by the non-commutative family F. For example, for stochastic block models, it is an open problem how the bound behaves as a function of the parameters p and q. Another interesting question is to understand how the asymptotics of our landscape analysis relate to the hardness of estimation as a function of the signal-to-noise ratio.</p><p>Finally, another open question is to what extent our result could be extended to the non-linear residual GNN case, perhaps leveraging ideas from <ref type="bibr" target="#b37">Shamir (2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We present experiments on community detection in synthetic datasets (Sections 6.1, 6.2 and Appendix C.1) as well as real-world datasets (Section 6.3). In the synthetic experiments, the performance is measured by the overlap between predicted (ŷ) and true labels (y), which quantifies how much better than random guessing a prediction is, given by</p><formula xml:id="formula_15">overlap(y,ŷ) = max π∈S C 1 n u δ π(y(u)),ŷ(u) − 1 C /(1 − 1 C )<label>(6)</label></formula><p>where δ is the Kronecker delta, and the maximization is performed over permutations of all the labels. In the real-world datasets, as the communities are overlapping and unbalanced, the prediction accuracy is measured by max π 1 n u δ π(y(u)),ŷ(u) , and the set of permutations to be maximized over is described in Section 4.3. We use Adamax <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2014)</ref> with learning rate 0.004 for optimization across all experiments. The neural network models have 30 layers and 8 dimensions of hidden states in the middle layers (i.e., b k = 8) for experiments in Sections 6.1 and 6.2, and 20 layers and 6 dimensions of hidden states for Section 6.3. GNNs and LGNNs have J = 2 across the experiments except the ablation experiments in Section C.3. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">STOCHASTIC BLOCK MODELS</head><p>The stochastic block model (SBM) is a random graph model with planted community structure. A graph sampled from SBM (n, p, q, C) consists of |V | = n nodes partitioned into C communities, that is, each node is assigned a label y ∈ {1, ..., C}. An edge connecting any two vertices u, v is drawn independently at random with probability p if y(v) = y(u), and with probability q otherwise. We consider the sparse regime of constant average degree, where p = a/n, q = b/n for some a, b ≥ 0 that do not depend on n. As explained in Appendix B.3, the difficulty of recovering the community labels is indicated by the signal-to-noise ratio (SNR). We compare our GNN and LGNN with belief propagation (BP) as well as spectral methods using the normalized Laplacian and the Bethe Hessian, which we introduce in details in Appendix B. In particular, the spectral methods involve performing power iterations for as many times as the number of layers in the GNN and LGNN (which is 30). We also implement Graph Attention Networks (GAT) as a baseline model 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">BINARY SBM</head><p>For binary SBM (i.e., C = 2), the SNR has the expression SN R(a, b) = (a − b) 2 /(2(a + b)). Thus, we test the models on different choices of SNR by choosing five different pairs of a i and b i (or equivalently, p i and q i ) while fixing a i + b i , thereby maintaining the average degree. In particular, we vary the SNRs around 1 because for binary SBM under the sparse regime, SN R = 1 is the exact threshold for the detection of y to be possible asymptotically in n (Abbe, 2017).</p><p>We consider two learning scenarios. In the first scenario, for each pair of (a i , b i ), we sample 6000 graphs under G ∼ SBM (n = 1000, p i = a i /n, q i = b i /n, C = 2) and train the model separately for each i. <ref type="figure" target="#fig_2">Figure 3</ref> reports the performances of of the different models in the first learning scenario. We observe that both GNN and LGNN reach the performance of BP, which is known to be asymptotically optimal <ref type="bibr" target="#b6">Coja-Oghlan et al. (2016)</ref> when p and q are known, while GNN and LGNN are agnostic to these parameters. In addition, even the linear LGNN achieves a performance that is quite close to that of BP, in accordance to the spectral approximations of BP given by the Bethe Hessian (see Appendix B.3) together with the ability of linear LGNN to express power iterations. These models all significantly outperforms the spectral methods that perform 30 power iterations on the Bethe Hessian or the normalized Laplacian. We also notice that our models outperform GAT in this task.</p><p>In the second scenario, whose results are reported in Appendix C.2, we train a single model from a set of 6000 graphs sampled from a mixture of SBMs parameterized by the different pairs of (p i , q i ). This setup demonstrates that our models are more powerful than applying known algorithms such as BP or spectral clustering using Bethe Hessian using SBM parameters learned from data, since the parameters vary in the dataset.</p><p>We also ran experiments in the dissociative case (q &gt; p) as well as with C = 3 communities and obtained similar results, which are not reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">PROBING THE COMPUTATIONAL-TO-STATISTICAL THRESHOLD IN 5-CLASS SBM</head><p>In SBM with fewer than 4 communities, it is known that BP provably reaches the information-theoretic threshold <ref type="bibr" target="#b0">(Abbe, 2017;</ref><ref type="bibr" target="#b25">Massoulié, 2014;</ref><ref type="bibr" target="#b6">Coja-Oghlan et al., 2016)</ref>. The situation is different for k &gt; 4, where it is conjectured that when the SNR falls into a certain gap, called the computationalto-statistical gap, there will be a discrepancy between the theoretical performance of the maximum likelihood estimator and the performance of any polynomial-time algorithm including BP <ref type="bibr" target="#b7">(Decelle et al., 2011)</ref>. In this context, one can use the GNN models to search in the space of generalizations of BP, attempting to improve upon the detection performance of BP for scenarios where the SNR falls within this gap. <ref type="table">Table 1</ref> presents results for the 5-community disassortative SBM with n = 400, p = 0 and q = 18/n, in which case the SNR lies within the computational-to-statistical gap. Note that since p = 0, this also amounts to a graph coloring problem.</p><p>We see that the GNN and LGNN models outperform BP in this experiment, indeed opening up the possibility to bridge the computation-information gap in a data-driven fashion. That said, our model may be taking advantage of finite-size effects that would vanish as n → ∞. The asymptotic study of these gains is left for future work. In terms of the average test accuracy, LGNN has the best performance. In particular, it outperforms the symmetric version of LGNN, emphasizing the importance of the non-backtracking matrix used in LGNN. Although equipped with the attention mechanism, GAT does not explicitly incorporate in itself the degree matrix, the power graph adjacency matrices or the line graph structure, and has inferior performance compared with the GNN and LGNN models. Further ablation studies on GNN and LGNN are described in Section C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">REAL DATASETS FROM SNAP</head><p>We now compare the models on the SNAP datasets <ref type="bibr" target="#b23">(Leskovec &amp; Krevl, 2014)</ref>, whose domains range from social networks to hierarchical co-purchasing networks. We obtain the training set as follows.  <ref type="table">Table 1</ref>: Performance of different models on 5-community dissociative SBM graphs with n = 400, C = 5, p = 0, q = 18/n, corresponding to an average degree of 14.5. The first row gives the average overlap across test graphs, and the second row gives the graph-wise standard deviation of the overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN LGNN LGNN-L LGNN-S GAT BP</head><p>For each SNAP dataset, we select only on the 5000 top quality communities provided in the dataset. We then identify edges (i, j) that cross at least two different communities. For each of such edges, we consider pairs of communities C 1 , C 2 such that i ∈ C 1 , j ∈ C 2 , i / ∈ C 2 and j / ∈ C 1 , and extract the subset of nodes determined by C 1 ∪ C 2 together with the edges among them to form a graph. The resulting graph is connected since each community is connected. Finally, we divide the dataset into training and testing sets by enforcing that no community belongs to both the training and the testing set. In our experiment, due to computational limitations, we restrict our attention to the three smallest datasets in the SNAP collection (Youtube, DBLP and Amazon), and we restrict the largest community size to 200 nodes, which is a conservative bound.</p><p>We compare the performance of GNN and LGNN models with GAT as well as the Community-Affiliation Graph Model (AGM), which is a generative model proposed in <ref type="bibr" target="#b43">Yang &amp; Leskovec (2012)</ref> that captures the overlapping structure of real-world networks. Community detection can be achieved by fitting AGM to a given network, which was shown to outperform some state-of-the-art algorithms. <ref type="table">Table 2 compares</ref>   <ref type="figure">GNN, LGNN,</ref> LGNN-S and GAT yield similar results and outperform AGMfit, with the first three achieving the highest average accuracies. It further illustrates the benefits of data-driven models that strike the right balance between expressivity and structural design.  <ref type="table">Table 2</ref>: Comparison of the node classification accuracy by different models on the three SNAP datasets. Note that the average accuracy is computed graph-wise with each graph weighted by its size, while the standard deviation is computed graph-wise with equal weights among the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this work, we have studied data-driven approaches to supervised community detection with graph neural networks. Our models achieve similar performance to BP in binary SBM for various choices of SNR in the data, and outperform BP in the sparse regime of 5-class SBM that falls between the computational-to-statistical gap. This is made possible by considering a family of graph operators including the power graph adjacency matrices, and importantly by introducing the line graph equipped with the non-backtracking matrix. We also provide a theoretical analysis of the optimization landscapes of simplified linear GNN for community detection and showed the gap between the loss values at local and global minima are bounded by quantities related to the concentration of certain random matricies.</p><p>One word of caution is that our empirical results are inherently non-asymptotic. Whereas models trained for given graph sizes can be used for inference on arbitrarily sized graphs (owing to the parameter sharing of GNNs), further work is needed in order to understand the generalization properties as |V | increases. Nevertheless, we believe our work the study of computational-tostatistical gaps, where our model can be used to inquire about the form of computationally tractable approximations. Moreover, our work also opens up interesting questions, including whether the network parameters can be interpreted mathematically, and how our results on the energy landscape depend upon specific signal-to-noise ratios. Another current limitation of our model is that it presumes a fixed number of communities to be detected. Thus, other directions of future research include the extension to the case where the number of communities is unknown and varied, or even increasing with |V |, as well as applications to ranking and edge-cut problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGEMENT</head><p>This work was partially supported by the Alfred P. Sloan Foundation and DOA W911NF-17-1-0438.</p><p>Pan Zhang. Robust spectral detection of global structures in the data by learning a regularization. In Arxiv preprint, pp. 541-549, 2016.</p><p>A PROOF OF THEOREM 5.1</p><p>For simplicity and with an abuse of notation, in the remaining part we redefine L andL in the following way, to be the negative of their original definition in the main section: L n (β) = E Xn,Yn β Ynβ β Xnβ , L n (β) = E Xn,Yn β T Ynβ β T EXnβ . Thus, minimizing the loss function (5) is equivalent to maximizing the function L n (β) redefined here.</p><p>We write the Cholesky decomposition of EX n as EX n = R n R T n , and define</p><formula xml:id="formula_16">A n = R −1 n Y n (R −1 n ) T , A n = EA n = R −1 n EY n (R −1 n ) T , B n = R −1 n X n (R −1 n ) T</formula><p>, and ∆B n = B n − I n . Given a symmetric matrix K ∈ R M ×M , we let λ 1 (K), λ 2 (K), ..., λ M (K) denote the eigenvalues of K in nondecreasing order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First, we have</head><formula xml:id="formula_17">|L n (β l ) − L n (β g )| ≤ |L n (β l ) −L n (β l )| + |L n (β l ) −L n (β g )| + |L n (β g ) − L n (β g )| (7)</formula><p>Let us denote byβ g a global minimum of the mean-field lossL n . Taking a step further, we can extend this bound to the following one (the difference is in the second term on the right hand side): Lemma A.1.</p><formula xml:id="formula_18">|L n (β l ) − L n (β g )| ≤ |L n (β l ) −L n (β l )| + |L n (β l ) −L n (β g )| + |L n (β g ) − L n (β g )| (8)</formula><p>Proof of Lemma A.1. We consider two separate cases: The first case is whenL n (β l ) ≥L n (β g ). ThenL n (β l ) −L n (β g ) ≥L n (β l ) −L n (β g ) ≥ 0, and so |L n (β l ) − L n (β g )| ≤ |L n (β l ) −L n (β l )| + |L n (β l ) −L n (β g )| + |L n (β g ) − L n (β g )|.</p><p>The other case is whenL n (β l ) &lt;L n (β g ). Note that L n (β l ) ≥ L n (β g ).</p><formula xml:id="formula_19">Then |L n (β l ) − L n (β g )| ≤ |L n (β l ) −L n (β l )| + |L n (β g ) − L n (β g )| ≤ |L n (β l ) −L n (β l )| + |L n (β l ) −L n (β g )| + |L n (β g ) − L n (β g )|.</formula><p>Hence, to bound the "energy gap" |L n (β l ) − L n (β g )|, if suffices to bound the three terms on the right hand side of Lemma A.1 separately. First, we consider the second term, |L n (β l ) −L n (β g )|.</p><p>Let γ l = R T n β l , γ g = R T n β g andγ g = R T nβg . Define S n (γ) = L n (R −T n γ) andS n (γ) = L n (R −T n γ), for any γ ∈ R M . Thus, we apply a change-of-variable and try to bound |S n (γ l ) − S n (γ g )|.</p><p>Since β l is a local maximum of L n , λ 1 (∇ 2 L n (β l )) ≤ 0. Since ∇ 2 S n (γ l ) = R −1 n ∇ 2 L n (β l )R −T n , where R n is invertible, we know that λ 1 (∇ 2 S n (γ l )) ≤ 0, thanks to the following lemma:</p><formula xml:id="formula_20">Lemma A.2. If R, Q ∈ R M ×M , R is invertible, Q is symmetric and λ q &gt; 0 is an eigenvalue of Q, then λ 1 (RQR T ) ≥ λ · λ M (RR T ) Proof of Lemma A.2. Say Qw = λw for some vector w ∈ R M . Let v = R −T w. Then v T (RQR T )v = w T Qw = λ w 2 . Note that w 2 = v T RR T v ≥ v 2 λ M (RR T ). Hence λ 1 (RQR T ) ≥ v T (RQR T )v v 2 ≥ λ w 2 w 2 /λ M (RR T ) ≥ λ · λ M (RR T ) Since ∇ 2 S n (γ l ) = ∇ 2S n (γ l ) + (∇ 2 S n (γ l ) − ∇ 2S n (γ l )), there is 0 ≥ λ 1 (∇ 2 S n (γ l )) ≥ λ 1 (∇ 2S n (γ l )) − ∇ 2 S n (γ l ) − ∇ 2S n (γ l ) . Hence, λ 1 (∇ 2S n (γ l )) ≤ ∇ 2 S n (γ l ) − ∇ 2S n (γ l )<label>(9)</label></formula><p>Next, we relate the left hand side of the inequality above to cos(γ l ,γ g ), thereby obtaining an upper bound on [1 − cos 2 (γ l ,γ g )], which will then be used to bound |S n (γ l ) −S n (γ g )|.</p><formula xml:id="formula_21">Lemma A.3. ∀γ ∈ R d , λ 1 (∇ 2S n (γ)) ≥ 2 γ 2 {[1 − cos 2 (γ,γ g )] · [λ 1 (Ā n ) − λ 2 (Ā n )] − 2 γ · ∇S n (γ) } Proof of Lemma A.3. ∇ 2S n (γ) =2E (γ T γ)A n − (γ T A n γ)I (γ T γ) 2 + 4(γ T A n γ)γγ T − 4(γ T γ)A n γγ T (γ T γ) 3 =2E (γ T γ)A n − (γ T A n γ)I (γ T γ) 2 + 4[(γ T γ)A n − (γ T A n γ)I]γγ T (γ T γ) 3 =2 (γ T γ)Ā n − (γ TĀ n γ)I (γ T γ) 2 + 4[(γ T γ)Ā n − (γ TĀ n γ)I]γγ T (γ T γ) 3 (10) Thus, if we define Q 1 = (γ T γ)[(γ T γ)Ā n − (γ TĀ n γ)I], Q 2 = 4[(γ T γ)Ā n − (γ TĀ n γ)I]γγ T , we have ∇ 2S n (γ) = 2 γ 6 (Q 1 − Q 2 )<label>(11)</label></formula><p>To bound λ 1 (∇ 2S n (γ)), we bound λ 1 (Q 1 ) and Q 2 as follows: SinceĀ n is symmetric, letγ 1 , . . .γ M be the orthonormal eigenvectors ofĀ n corresponding to nonincreasing eigenvalues l 1 , . . . l M . Note that the global minimum satisfiesγ g = ±γ 1 . Write</p><formula xml:id="formula_22">γ = M i=1 α iγi , and letᾱ i = αi √ M i=1 α 2 i</formula><p>. Then | cos(γ,γ g )| = | cos(γ,γ 1 )| = |ᾱ 1 |.</p><formula xml:id="formula_23">l i α 2 i )]α kγk =4 (γ T γ) 2 2 γ T ∇S(γ) ≤2(γ T γ) 2 γ ∇S(γ)<label>(14)</label></formula><p>Thus,</p><formula xml:id="formula_24">λ 1 (Q 1 − Q 2 ) ≥λ 1 (Q 1 ) − Q 2 ≥(γ T γ) 2 ([(1 −ᾱ 2 1 )(l 1 − l 2 )] − 2 γ ∇ γ S(γ) )<label>(15)</label></formula><p>This yields the desired lemma.</p><p>Combining inequality 9 and Lemma A.3, we get</p><formula xml:id="formula_25">1 − cos 2 (γ l ,γ g ) ≤ 2 γ l · ∇S n (γ l ) + γ l 2 2 ∇ 2 S n (γ l ) − ∇ 2S n (γ l ) λ 1 (Ā n ) − λ 2 (Ā n )<label>(16)</label></formula><p>Thus, to bound the angle between γ l andγ g , we can aim to bound ∇S n (γ l ) and ∇ 2 S n (γ l ) − ∇ 2S n (γ l ) as functions of the quantities µ n , ν n and δ n . Lemma A.4. γ l · ∇S n (γ l ) ≤ 2µ n ν n δ n (1 + 3ν n + δν n )</p><p>Proof of Lemma A.4.</p><formula xml:id="formula_27">∇S n (γ) = 2E A n γ γ T B n γ − 2E (γ T A n γ)B n γ (γ T B n γ) 2 (18) ∇S n (γ) = 2E A n γ γ T γ − 2E (γ T A n γ)γ (γ T γ) 2<label>(19)</label></formula><p>Combining equations 18 and 19, we get</p><formula xml:id="formula_28">∇S n (γ) − ∇S n (γ) = E 2(γ T γ − γ T B n γ)A n γ (γ T B n γ)(γ T γ) − 2(γ T A n γ)[(γ T γ) 2 B n γ − (γ T B n γ) 2 γ] (γ T B n γ) 2 (γ T γ) 2<label>(20)</label></formula><p>Since ∇S n (γ l ) = 0, we have</p><formula xml:id="formula_29">∇S n (γ l ) = E 2(γ T l γ l − γ T l B n γ l )A n γ l (γ T l B n γ l )(γ T l γ l ) − 2(γ T l A n γ l )[(γ T l γ l ) 2 B n γ l − (γ T l B n γ l ) 2 γ l ] (γ T l B n γ l ) 2 (γ T l γ l ) 2 ≤ 2 γ l E |λ 1 (A n )| ∆B n |λ M (B n )| + 3 |λ 1 (A n )| ∆B n λ 2 M (B n ) + |λ 1 (A n )| ∆B n 2 λ 2 M (B n )<label>(21)</label></formula><p>Then, by the generalized Hölder's inequality,</p><formula xml:id="formula_30">∇S n (γ l ) ≤ 2 γ l E|λ 1 (A n )| 3 E ∆B n 3 E 1 |λ M (B n )| 3 1 3 + 3 E|λ 1 (A n )| 3 E ∆B n 3 E 1 |λ M (B n )| 6 1 3 + E|λ 1 (A n )| 3 E ∆B n 6 E 1 |λ M (B n )| 6 1 3 .<label>(22)</label></formula><p>Hence, written in terms of the quantities µ n , ν n and δ n , we have γ l · ∇S n (γ l ) ≤2(µ n ν n δ n + 3µ n ν 2 n δ n + µ n δ 2 n ν 2 n ) =2µ n ν n δ n (1 + 3ν n + δν n ) </p><p>Note that</p><formula xml:id="formula_33">gma E(1 + X) 6 = EX 6 + 6EX 5 + 15EX 4 + 20EX 3 + 15EX 2 + 6EX + 1<label>(25)</label></formula><p>and for k ∈ {1, 2, 3, 4, 5}, if X is a nonnegative random variable,</p><formula xml:id="formula_34">EX k =1 X&gt;1 EX k + 1 X≤1 EX k ≤1 + 1 X≤1 EX 6 ≤1 + EX 6<label>(26)</label></formula><p>Therefore, E|λ 1 (B n )| 6 ≤ 64 + 63E ∆B n 6 .</p><p>From now on, for simplicity, we introduce δ n = (64 + 63δ 6 n ) 1 6 , as a function of δ n . Lemma A.6. ∀γ ∈ R M , γ l 2 · ∇ 2 S n (γ) − ∇ 2S n (γ) ≤µ n ν n δ n (10 + 14ν n + 2δ n ν n + 16ν 2 n + 16δ n ν n + 8δ n ν 2 n + 8δ n ν n + 8δ n δ n ν)</p><p>Hence,</p><formula xml:id="formula_36">H 3 =(γ T A n γ) (γ T γ) 3 ∆B n γγ T B n + (γ T γ) 3 γγ T ∆B n + (−γ T ∆B n γ)γγ T (γ T B n γ) 3 (γ T γ) 3 + (−γ T ∆B n γ)γγ T (γ T B n γ) 2 (γ T γ) + (−γ T ∆B n γ)γγ T (γ T B n γ)(γ T γ) 2<label>(39)</label></formula><p>Thus,</p><formula xml:id="formula_37">H 3 ≤ |λ 1 (A n )| γ 2 1 |λ 3 M (B n )| ( ∆B n |λ 1 (B n )| + 2 ∆B n ) + 1 λ 2 M (B n ) ∆B n + 1 |λ M (B n )| ∆B n<label>(40)</label></formula><p>Applying generalized Hölder's inequality, we obtain</p><formula xml:id="formula_38">γ 2 · E H 3 ≤ E 1 |λ M (B n )| 6 1 2 (E|λ 1 (A n )| 6 ) 1 6 (E ∆B n 6 ) 1 6 (E|λ 1 (B n )| 6 ) 1 6 + 2 E 1 |λ M (B n )| 6 1 2 (E|λ 1 (A n )| 3 ) 1 3 (E ∆B n 6 ) 1 6 + E 1 |λ M (B n )| 6 1 3 (E|λ 1 (A n )| 3 ) 1 3 (E ∆B n 3 ) 1 3 + E 1 |λ M (B n )| 3 1 3 (E|λ 1 (A n )| 3 ) 1 3 (E ∆B n 3 ) 1 3</formula><p>≤µ n ν n δ n (δ n ν 2 n + 2ν 2 n + ν n + 1)</p><p>For the last term,</p><formula xml:id="formula_40">H 4 = [−2(γ T γ)(γ T ∆B n γ)I − (γ T ∆B n γ) 2 I]A n γγ T B n + (γ T B n γ) 2 A n γγ T ∆B n (γ T B n γ) 2 (γ T γ) 2<label>(42)</label></formula><p>Thus,</p><formula xml:id="formula_41">H 4 ≤ 1 γ 2 1 λ 2 M (B n ) (2 ∆B n + ∆B n 2 )|λ 1 (A n )||λ 1 (B n )| + 1 λ 2 M (B n ) |λ 2 1 (B n )||λ 1 (A n )| ∆B n<label>(43)</label></formula><p>Applying generalized Hölder's inequality, we obtain</p><formula xml:id="formula_42">γ 2 · E H 4 ≤2 E 1 |λ M (B n )| 6 1 3 (E|λ 1 (A n )| 3 ) 1 3 (E ∆B n 6 ) 1 6 (E|λ 1 (B n )| 6 ) 1 6 + E 1 |λ M (B n )| 6 1 3 (E|λ 1 (A n )| 6 ) 1 6 (E ∆B n 6 ) 1 3 (E|λ 1 (B n )| 6 ) 1 6 + E 1 |λ M (B n )| 6 1 3 (E|λ 1 (A n )| 6 ) 1 6 (E ∆B n 6 ) 1 6 (E|λ 1 (B n )| 6 ) 1 3</formula><p>≤µ n ν n δ n (2ν n δ n + δ n δ n ν n + δ n 2 ν n )</p><p>Therefore, summing up the bounds above, we obtain γ l 2 · ∇ 2 S n (γ) − ∇ 2S n (γ) ≤µ n ν n δ n (10 + 14ν n + 2δ n ν n + 16ν 2 n + 16δ n ν n + 8δ n ν 2 n + 8δ n ν n + 8δ n δ n ν)</p><p>Hence, combining inequality 16, Lemma A.4 and Lemma A.6, we get 1 − cos 2 (γ l ,γ g ) ≤η n [4µ n ν n δ n (1 + 3ν n δ n µ n ) + 1 2 µ n ν n δ n (10 + 14ν n + 2δ n ν n + 16ν 2 n + 16δ n ν n + 8δ n ν 2 n + 8δ n ν n + 8δ n δ n ν)] =µ n ν n δ n η n (9 + 19ν n + 5δ n ν n + 8ν 2 n + 8δ n ν n + 4δ n ν n 2 + 4δ n ν n + 4δ n δ n ν n )</p><p>, or λ 1 (Ā n ) ≤ L n (β g ) + µ n ν n δ n (55) Therefore, |L n (β l ) − L n (β g )| ≤2µ n ν n δ n + (1 − cos 2 (γ l ,γ g ))[L n (β g ) + µ n ν n δ n ] ≤µ n ν n δ n [2 + η n µ n ν n δ n C(δ n , ν n )] + η n µ n ν n δ n C(δ n , ν n )L n (β g ) ≤L n (β g ) µ n ν n δ n [2 + η n µ n ν n δ n C(δ n , ν n )] η −1 n − µ n ν n δ n + η n µ n ν n δ n C(δ n , ν n ) = 2η n µ n ν n δ n [2 + C(δ n , ν n )] 1 − η n µ n ν n δ n L n (β g )</p><p>Hence, we have proved the theorem, with ηn,µn,νn,δn = 2ηnµnνnδn[2+C(δn,νn)] 1−ηnµnνnδn .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B BACKGROUND B.1 GRAPH MIN-CUTS AND SPECTRAL CLUSTERING</head><p>We consider graphs G = (V, E), modeling a system of N = |V | elements presumed to exhibit some form of community structure. The adjacency matrix A associated with G is the N × N binary matrix such that A i,j = 1 when (i, j) ∈ E and 0 otherwise. We assume for simplicity that the graphs are undirected, therefore having symmetric adjacency matrices. The community structure is encoded in a discrete label vector s : V → {1, . . . , C} that assigns a community label to each node, and the goal is to estimate s from observing the adjacency matrix.</p><p>In the binary case, we can set s(i) = ±1 without loss of generality. Furthermore, we assume that the communities are associative, which means two nodes from the same community are more likely to be connected than two nodes from the opposite communities. The quantity i,j</p><p>(1 − s(i)s(j))A i,j measures the cost associated with cutting the graph between the two communities encoded by s, and we wish to minimize it under appropriate constraints <ref type="bibr" target="#b29">(Newman, 2006)</ref>. Note that i,j A i,j = s T Ds, with D = diag(A1) being the degree matrix, and so the cut cost can be expressed as a positive semidefinite quadratic form min</p><formula xml:id="formula_47">s(i)=±1 s T (D − A)s = s T ∆s</formula><p>that we wish to minimize. This shows a fundamental connection between the community structure and the spectrum of the graph Laplacian ∆ = D − A, which provides a powerful and stable relaxation of the discrete combinatorial optimization problem of estimating the community labels for each node. The eigenvector of ∆ associated with the smallest eigenvalue is, trivially, 1, but its Fiedler vector (the eigenvector associated with the second smallest eigenvalue) reveals important community information of the graph under appropriate conditions <ref type="bibr" target="#b29">(Newman, 2006)</ref>, and is associated with the graph conductance under certain normalization schemes <ref type="bibr" target="#b38">(Spielman, 2015)</ref>.</p><p>Given linear operator L(A) extracted from the graph (that we assume symmetric), we are thus interested in extracting eigenvectors at the edge of its spectrum. A particularly simple algorithm is the power iteration method. Indeed, the Fiedler vector of L(A) can be obtained by first extracting the leading eigenvector v ofÃ = L(A) I − L(A), and then iteratively compute</p><formula xml:id="formula_48">y (n) =Ãw (n−1) , w (n) = y (n) − y (n) , v v y (n) − y (n) , v v .</formula><p>Unrolling power iterations and recasting the resulting model as a trainable neural network is akin to the LISTA sparse coding model, which unrolled iterative proximal splitting algorithms <ref type="bibr" target="#b12">(Gregor &amp; LeCun, 2010)</ref>.</p><p>Despite the appeal of graph Laplacian spectral approaches, it is known that these methods fail in sparsely connected graphs <ref type="bibr" target="#b22">(Krzakala et al., 2013)</ref> . Indeed, in such scenarios, the eigenvectors of the graph Laplacian concentrate on nodes with dominant degrees, losing their correlation with the community structure. In order to overcome this important limitation, people have resorted to ideas inspired from statistical physics, as explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 PROBABILISTIC GRAPHICAL MODELS AND BELIEF-PROPAGATION (BP)</head><p>Graphs with labels on nodes and edges can be cast as a graphical model where the aim of clustering is to optimize label agreement. This can be seen as a posterior inference task. If we simply assume the graphical model is a Markov Random Field (MRF) with trivial compatibility functions for cliques greater than 2, the probability of a label configuration σ is given by</p><formula xml:id="formula_49">P(σ) = 1 Z i∈V φ i (σ i ) ij∈E ψ ij (σ i , σ j ).<label>(57)</label></formula><p>Generally, computing marginals of multivariate discrete distributions is exponentially hard. For instance, if X is the state space that we assume to be discrete, naively we have to sum over |X| n−1 terms in order to compute P(σ i ). But if the graph is a tree, we can factorize the MRF efficiently to compute the marginals in linear time via a dynamic programming method called the sum-product algorithm, also known as belief propagation (BP). An iteration of BP is given by</p><formula xml:id="formula_50">b i→j (σ i ) = 1 Z i→j φ i (σ i ) k∈N (i)\{j} σ k ∈X ψ ik (σ i , σ k )b k→i (σ k ).<label>(58)</label></formula><p>When the graph is a tree, the BP equations above converge to a fixed point <ref type="bibr" target="#b26">(Mezard &amp; Montanari, 2009)</ref></p><formula xml:id="formula_51">. Moreover, if we define b i (σ i ) = k∈N (i) b k→i (σ i ) ,<label>(59)</label></formula><p>then at the fixed point, we can recover the single-variable marginals as P i (σ i ) = b i (σ i ). For graphs that are not trees, BP is not guaranteed to converge. However, graphs generated from SBM are locally tree-like so that such an approximation is reasonble <ref type="bibr" target="#b0">(Abbe, 2017)</ref>.</p><p>In order to apply BP for community detection, we need a generative model of the graph. If the parameters of the underlying model are unknown, they parameters can be estimated using expectation maximization, which introduces further complexity and instability to the method since it is possible to learn parameters for which BP does not converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 SPECTRAL METHOD WITH THE NON-BACKTRACKING AND BETHE HESSIAN MATRICES</head><p>The BP equations have a trivial fixed-point where every node takes equal probability in each group. Linearizing the BP equation around this point is equivalent to spectral clustering using the nonbacktracking matrix (NB), a matrix defined on the directed edges of the graph that indicates whether two edges are adjacent and do not coincide. Spectral clustering using NB gives significant improvements over spectral clustering with different versions of the Laplacian matrix L and the adjacency matrix A ( <ref type="bibr" target="#b22">Krzakala et al., 2013)</ref>. High degree fluctuations drown out the signal of the informative eigenvalues in the case of A and L, whereas the eigenvalues of NB are confined to a disk in the complex plane except for the eigenvalues that correspond to the eigenvectors that are correlated with the community structure, which are therefore distinguishable from the rest.</p><p>However, spectral method with NB is still not optimal in that, firstly, NB is defined on the edge set; and secondly, NB is asymmetric, therefore unable to enjoy tools of numerical linear algebra for symmetric matrices. <ref type="bibr" target="#b34">Saade et al. (2014)</ref> showed that a spectral method can do as well as BP in the sparse SBM regime using the Bethe Hessian matrix defined as BH(r) := (r 2 − 1)I − rA + D, where r is a scalar parameter. This is thanks to a one-to-one correspondence between the fixed points of BP and the stationary points of the Bethe free energy (corresponding Gibbs energy of the Bethe approximation) <ref type="bibr" target="#b34">(Saade et al., 2014)</ref>. The Bethe Hessian is a scaling of the Hessian of the Bethe free energy at an extremum corresponding to the trivial fixed point of BP. Negative eigenvalues of BH(r) correspond to phase transitions in the Ising model where new clusters become identifiable.</p><p>The success of the spectral method using the Bethe Hessian gives a theoretical motivation for having a family of matrices including I, D and A in our GNN defined in Section 4, because in this way the GNN is capable of expressing the algorithm of performing power iteration on the Bethe Hessian matrices. On the other hand, while belief propagation requires a generative model and the spectral method using the Bethe Hessian requires the selection of the parameter r, whose optimal value also depends on the underlying generative model, the GNN does not need a generative model and is able to learn and then make predictions in a data-driven fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 STOCHASTIC BLOCK MODEL</head><p>We briefly review the main properties needed in our analysis, and refer the interested reader to <ref type="bibr" target="#b0">Abbe (2017)</ref> for an excellent review. The stochastic block model (SBM) is a generative model of random undirected graph denoted by SBM (n, p, q, C). To generate a graph according to this model, one starts with a set V of n vertices each belong to one of C communities, represented by a labeling function F : V → {1, . . . , C}. We say the SBM is balanced if the communities are of the same size. Then, edges are constructed independently at random such that two vertices u, v share an edge with probability p if F (v) = F (u), and with probability q if F (v) = F (u).</p><p>The goal of community detection is then to estimate F from the edge set. LetF : V → {1, C} be the estimate of F obtained by a certain method. For a sequence of (p n , q n ), we say the method achieves exact recovery if P(F n =F n ) converges to 1 as n → ∞, and achieves weak recovery or detection if ∃ &gt; 0 such that P(|F −F | ≥ 1/C + ) converges to 1 as n → ∞ (i.e the method performs better than random guessing).</p><p>It is harder to tell communities apart if p is close to q (for example, if p = q we get an Erdős-Renyi random graph, which has no communities). In the binary case, it has been shown that exact recovery is possible on SBM (n, (a log n)/n, (b log n)/n, 2) if and only if (a + b)/2 ≥ 1 + √ ab <ref type="bibr" target="#b28">(Mossel et al., 2014;</ref><ref type="bibr" target="#b1">Abbe et al., 2014)</ref>. While this is an information-theoretic result, it is also known that when this inequality is satisfied, there exist polynomial time algorithms that achieves exact recovery <ref type="bibr" target="#b0">(Abbe, 2017;</ref><ref type="bibr" target="#b28">Mossel et al., 2014)</ref>. For this reason, we say that there is no information-computation gap in this regime.</p><p>Note that for exact recovery to be possible, p, q must grow at least O(log n) or else the graphs will likely not be connected, and thus the vertex labels will be underdetermined. Hence, in the sparser regime of constant degree, SBM (n, a/n, b/n, C), detection is the best we could hope for. The constant degree regime is also of most interest to us for real world applications, as most large datasets have bounded degree and are extremely sparse. It is also a very challenging regime; spectral approaches using the adjacency matrix or the graph Laplacian in its various (un)normalized forms, as well as semidefinite programming (SDP) methods do not work well in this regime due to large fluctuations in the degree distribution that prevent eigenvectors from concentrating on the clusters <ref type="bibr" target="#b0">(Abbe, 2017)</ref>. <ref type="bibr" target="#b7">Decelle et al. (2011)</ref> first proposed the BP algorithm on the SBM, which was proven to yield Bayesian optimal values in <ref type="bibr" target="#b6">Coja-Oghlan et al. (2016)</ref>. In the constant degree regime with C balanced communities, the signal-to-noise ratio is defined as SN R = (a−b) 2 /(C(a+(C −1)b)), and SN R = 1 is called the Kesten-Stigum (KS) threshold <ref type="bibr" target="#b18">(Kesten &amp; Stigum, 1966)</ref>. When SN R &gt; 1, detection can be solved in polynomial time by BP <ref type="bibr" target="#b0">(Abbe, 2017;</ref><ref type="bibr" target="#b7">Decelle et al., 2011)</ref>. For C = 2, it has been shown that when SN R &lt; 1, detection is information-theoretically unsolvable, and therefore SN R = 1 is both the computational and the information theoretic threshold <ref type="bibr" target="#b0">(Abbe, 2017)</ref>. For C ≥ 4, it conjectured that for certain SN R &lt; 1, there exist non-polynomial time algorithms able to solve the detection problem, while no polynomial time algorithm can solve detection when SN R &lt; 1, in which case a gap would exist between the information theoretic threshold and the computational threshold <ref type="bibr" target="#b0">(Abbe, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C FURTHER EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 GEOMETRIC BLOCK MODEL</head><p>The success of BP on the SBM relies on its locally hyperbolic property, which makes the graph tree-like with high probability. This behavior is completely different if one considers random graphs with locally Euclidean geometry. The Geometric Block Model <ref type="bibr" target="#b35">(Sankararaman &amp; Baccelli, 2018</ref>) is a random graph generated as follows. We start by sampling n points x 1 , . . . , x n i.i.d. from a Gaussian mixture model whose means are µ 1 , . . . µ k ∈ R d at distances S apart and covariances are the identity.  . left: k = 2. We verify that BH(r) models cannot perform detection at both ends of the spectrum simultaneously.</p><p>The label of each sampled point corresponds to which Gaussian it belongs to. We then draw an edge between two nodes i, j if x i − x j ≤ T / √ n. Due to the triangle inequality, the model contains a large number of short cycles, which affects the performance of BP <ref type="bibr" target="#b26">(Mezard &amp; Montanari, 2009</ref>). This motivates other estimation algorithms based on motif-counting that require knowledge of the model likelihood function <ref type="bibr" target="#b35">(Sankararaman &amp; Baccelli, 2018)</ref>. <ref type="table" target="#tab_3">Table 3</ref> shows the performance of GNN and LGNN on the binary GBM model, obtained with d = 2, n = 500, T = 5 √ 2 and varying S, compared to those of two spectral methods, using respectively the normalized Laplacian and the Bethe Hessian, with the latter introduced in Appendix B.3. We note that LGNN model, thanks to its added flexibility and the multiscale nature of its generators, is able to significantly outperform both spectral methods and the baseline GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 MIXTURE OF BINARY SBM</head><p>We report here our experiments on the SBM mixture, generated with G ∼ SBM (n = 1000, p = kd − q, q ∼ Unif(0,d − d ), C = 2) , where the average degreed is either fixed constant or also randomized withd ∼ Unif(1, t). <ref type="figure" target="#fig_4">Figure  4</ref> shows the overlap obtained by our model compared with several baselines. Our GNN model is either competitive with or outperforming the spectral method using Bethe Hessian (BH), which is the state-of-the-art spectral method <ref type="bibr" target="#b34">Saade et al. (2014)</ref>, despite not having any access to the underlying generative model (especially in cases where GNN was trained on a mixture of SBM and thus must be able to generalize the r parameter in BH). They all outperform by a wide margin spectral clustering methods using the symmetric Laplacian and power method applied to BH I − BH using the same number of layers as our model. Thus GNN's ability to predict labels goes beyond approximating spectral decomposition via learning the optimal r for BH(r). The model architecture could allow it to learn a higher dimensional function of the optimal perturbation of the multiscale adjacency basis, as well as nonlinear power iterations, that amplify the informative signals in the spectrum.  Compared to f , each of h, i and k has one fewer operator in F, and j has two fewer. We see that with the absence of A 2 , k has much worse performance than the other four, indicating the importance of the power graph adjacency matrices. Interestingly, with the absence of I, i actually has better average accuracy than f . One possibly explanation is that in SBM, each node has the same expected degree, and hence I may be not very far from D, which might make having both I and D in the family redundant to some extent.</p><p>Comparing GNN models a, b and c, we see it is not the case that having larger J will always lead to better performance. Compared to f , GNN models c, d and e have similar numbers of parameters but all achieve worse average test accuracy, indicating that the line graph structure is essential for the good performance of LGNN in this experiment. In addition, l also performs worse than f , indicating the significance of the non-backtracking line graph compared to the symmetric line graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the architecture of LGNN (Section 4.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Construction of the line graph L(G) using the non-backtracking matrix. The nodes of L(G) correspond to oriented edges of G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Binary assortative SBM detection (i.e. C = 2 and p &gt; q). X-axis corresponds to SNR, and Y-axis to overlap between the prediction and the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the performance, measured with a 3-class (C = {{1}, {2}, {1, 2}}) classification accuracy up to global permutation of {1} ↔ {2}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>GNN mixture (Graph Neural Network trained on a mixture of SBM with average degree 3), GNN full mixture (GNN trained over different SNR regimes), BH( √d ) and BH(− √d )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance measured by the overlap (in percentage) of GNN and LGNN on graphs generated by the Geometric Block Model compared with two spectral methods</figDesc><table><row><cell>Model</cell><cell>S = 1</cell><cell>S = 2</cell><cell>S = 4</cell></row><row><cell>Norm. Laplacian</cell><cell>1 ± 0.5</cell><cell>1 ± 0.6</cell><cell>1 ± 1</cell></row><row><cell>Bethe Hessian</cell><cell>18 ± 1</cell><cell>38 ± 1</cell><cell>38 ± 2</cell></row><row><cell>GNN</cell><cell>20 ± 0.4</cell><cell>39 ± 0.5</cell><cell>39 ± 0.5</cell></row><row><cell>LGNN</cell><cell cols="3">22 ± 0.4 50 ± 0.5 76 ± 0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>C.3 ABLATION STUDIES OF GNN AND LGNN</figDesc><table><row><cell></cell><cell></cell><cell cols="3">#layers #features J</cell><cell>F</cell><cell cols="2">#parameters Avg.</cell><cell>Std. Dev.</cell></row><row><cell cols="2">(a) GNN</cell><cell>30</cell><cell>8</cell><cell>2</cell><cell>I, D, A, A2</cell><cell>8621</cell><cell cols="2">0.1792 0.0385</cell></row><row><cell cols="2">(b) GNN</cell><cell>30</cell><cell>8</cell><cell>4</cell><cell>I, D, A, ..., A4</cell><cell>12557</cell><cell cols="2">0.1855 0.0438</cell></row><row><cell cols="2">(c) GNN</cell><cell>30</cell><cell>8</cell><cell cols="2">11 I, D, A, ..., A11</cell><cell>26333</cell><cell cols="2">0.1794 0.0359</cell></row><row><cell cols="2">(d) GNN</cell><cell>30</cell><cell>15</cell><cell>2</cell><cell>I, D, A, A2</cell><cell>28760</cell><cell cols="2">0.1894 0.0388</cell></row><row><cell cols="2">(e) GNN</cell><cell>30</cell><cell>12</cell><cell>4</cell><cell cols="2">I, D, A, ..., A (4) 27273</cell><cell cols="2">0.1765 0.0371</cell></row><row><cell cols="2">(f) LGNN</cell><cell>30</cell><cell>8</cell><cell>2</cell><cell>I, D, A, A2</cell><cell>25482</cell><cell cols="2">0.2073 0.0481</cell></row><row><cell cols="3">(g) LGNN-L 30</cell><cell>8</cell><cell>2</cell><cell>I, D, A, A2</cell><cell>25482</cell><cell cols="2">0.1822 0.0395</cell></row><row><cell cols="2">(h) LGNN</cell><cell>30</cell><cell>8</cell><cell>2</cell><cell>I, A, A2</cell><cell>21502</cell><cell cols="2">0.1981 0.0529</cell></row><row><cell>(i)</cell><cell>LGNN</cell><cell>30</cell><cell>8</cell><cell>2</cell><cell>D, A, A2</cell><cell>21502</cell><cell cols="2">0.2212 0.0581</cell></row><row><cell>(j)</cell><cell>LGNN</cell><cell>30</cell><cell>8</cell><cell>2</cell><cell>A, A2</cell><cell>17622</cell><cell cols="2">0.1954 0.0441</cell></row><row><cell cols="2">(k) LGNN</cell><cell>30</cell><cell>8</cell><cell>1</cell><cell>I, D, A</cell><cell>21502</cell><cell cols="2">0.1673 0.0437</cell></row><row><cell>(l)</cell><cell cols="2">LGNN-S 30</cell><cell>8</cell><cell>2</cell><cell>I, D, A, A2</cell><cell>21530</cell><cell cols="2">0.1776 0.0398</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The effects of different architectures and choices of the operator family for GNN and LGNN, as demonstrated by their performance on the 5-class disassortative SBM experiments with the exact setup as in Section 6.2. For LGNN, F is the same as F except for changing A to B.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is available at https://github.com/zhengdao-chen/GNN4CD 2 Implemented based on https://github.com/Diego999/pyGAT. Similar to our GNN and LGNN, we add instance normalization to every layer. The model contains 30 layers and 8 dimensions of hidden states.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To bound Q 2 :</p><p>Note that given vectors v,</p><p>Proof of Lemma A.6.</p><p>where</p><p>H 4 , and we try to bound each term on the right hand side separately.</p><p>For the first term, there is</p><p>Applying generalized Hölder's inequality, we obtain</p><p>For the second term, there is</p><p>Hence,</p><p>Applying generalized Hölder's inequality, we obtain</p><p>For H 3 , note that</p><p>Published as a conference paper at ICLR 2019</p><p>For simplicity, we define C(δ n , ν n ) = 9+19ν n +5δ n ν n +8ν 2 n +8δ n ν n +4δ n ν n 2 +4δ n ν n +4δ n δ n ν n . Thus, 1 − cos 2 (γ l ,γ g ) ≤ µ n ν n δ n η n C(δ n , ν n )</p><p>Following the notations in the proof of Lemma A.3, we write</p><p>Since Y n is positive semidefinite, EY n is also positive semidefinite, and henceĀ n = R T n EY n (R −1 n ) T is positive semidefinite as well. This means that l i ≥ 0, ∀i ∈ {1, ..., M }. SinceL n (β g ) =S n (γ g ) = S n (γ 1 ) = l 1 , there is</p><p>Next, we bound the first and the third term on the right hand side of the inequality in Lemma A.1. Lemma A.7. ∀β,</p><p>Thus, we get the desired lemma by the generalized Hölder's inequality.</p><p>Combining inequality 47, inequality 49 and Lemma A.7, we get</p><p>≤2µ n ν n δ n + µ n ν n δ n η n C(δ n , ν n ) · λ 1 (Ā n )</p><p>Meanwhile, |L n (β g ) −L n (β g )| ≤ max{|L n (β g ) −L n (β g )|, |L n (β g ) −L n (β g )|}</p><p>Hence, L n (β g ) ≥L n (β g ) − µ n ν n δ n ≥λ 1 (Ā n ) − µ n ν n δ n ≥η −1 n − µ n ν n δ n</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Community detection and stochastic block models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10146</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">recent developments. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afonso</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgina</forename><surname>Hall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3267v4</idno>
		<title level="m">Exact recovery in the stochastic block model</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Information-theoretic thresholds from the cavity method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Coja-Oghlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenka</forename><surname>Zdeborova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00814</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Decelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenka</forename><surname>Zdeborová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66106</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A limit theorem for multidimensional galton-watson processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Kesten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bernt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stigum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1211" to="1223" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hy</forename><surname>Truong Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02144</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spectral redemption in clustering sparse networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elchanan</forename><surname>Mossel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Neeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Sly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenka</forename><surname>Zdeborová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">52</biblScope>
			<biblScope unit="page" from="20935" to="20940" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<title level="m">SNAP Datasets: Stanford large network dataset collection</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Community detection thresholds and the weak ramanujan property</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Massoulié</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-sixth annual ACM symposium on Theory of computing</title>
		<meeting>the forty-sixth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="694" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Information, Physics, and Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Mezard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Oxford University Press, Inc., USA</publisher>
			<biblScope unit="volume">019857083</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A proof of the block model threshold conjecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elchanan</forename><surname>Mossel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Neeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Sly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.4115</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modularity and community structure in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="8577" to="8582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<idno>1999-66</idno>
		<ptr target="http://ilpubs.stanford.edu:8090/422/.Previousnumber=SIDL-WP-1999-0120" />
		<imprint>
			<date type="published" when="1999-11" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reverend bayes on inference engines: A distributed hierarchical approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second AAAI Conference on Artificial Intelligence, AAAI&apos;82</title>
		<meeting>the Second AAAI Conference on Artificial Intelligence, AAAI&apos;82</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1982" />
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The diameter of sparse random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Riordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Wormald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorics, Probability and Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="835" to="926" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spectral clustering of graphs with the bethe hessian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenka</forename><surname>Zdeborová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Community detection on euclidean random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abishek</forename><surname>Sankararaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Baccelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2181" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06739</idno>
		<title level="m">Are resnets provably better than linear predictors? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Spectral graph theory, am 561, cs 662</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Spielman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Community-affiliation graph model for overlapping network community detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding ICDM &apos;12 Proceedings of the 2012 IEEE 12th International Conference on Data Mining</title>
		<meeting>eeding ICDM &apos;12 eedings of the 2012 IEEE 12th International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="page" from="1170" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Understanding belief propagation and its generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="236" to="239" />
		</imprint>
	</monogr>
	<note>Exploring artificial intelligence in the new millennium</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
