<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Arabic Text Diacritization: State of the Art Results and a Novel Approach for Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fadel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<settlement>Irbid</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Tuffaha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<settlement>Irbid</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bara</forename><forename type="middle">&amp;apos;</forename><surname>Al-Jawarneh</surname></persName>
							<email>baraaaljawarneh@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<settlement>Irbid</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Al-Ayyoub</surname></persName>
							<email>malayyoub@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<settlement>Irbid</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Arabic Text Diacritization: State of the Art Results and a Novel Approach for Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present several deep learning models for the automatic diacritization of Arabic text. Our models are built using two main approaches, viz. Feed-Forward Neural Network (FFNN) and Recurrent Neural Network (RNN), with several enhancements such as 100-hot encoding, embeddings, Conditional Random Field (CRF) and Block-Normalized Gradient (BNG). The models are tested on the only freely available benchmark dataset and the results show that our models are either better or on par with other models, which require language-dependent postprocessing steps, unlike ours. Moreover, we show that diacritics in Arabic can be used to enhance the models of NLP tasks such as Machine Translation (MT) by proposing the Translation over Diacritization (ToD) approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In Arabic and many other languages, diacritics are added to the characters of a word (as short vowels) in order to convey certain information about the meaning of the word as a whole and its place within the sentence. Arabic Text Diacritization (ATD) is an important problem with various applications such as text to speech (TTS). At the same time, this problem is a very challenging one even to native speakers of Arabic due to the many subtle issues in determining the correct diacritic for each character from the list shown in <ref type="figure" target="#fig_1">Figure 2</ref> and the lack of practice for many native speakers. Thus, the need to build automatic Arabic text diacritizers is high <ref type="bibr" target="#b26">(Zitouni and Sarikaya, 2009)</ref>.</p><p>The meaning of a sentence is greatly influenced by the diacritization which is determined by the context of the sentence as shown in the following example:</p><p>Buckwalter Transliteration: klm &gt;Hmd ... Incomplete sentence without diacritization.</p><p>Buckwalter Transliteration: kal∼ama &gt;aHomadN Sadiyqahu Translation: Ahmad talked to his friend.</p><p>Buckwalter Transliteration: kalama &gt;aHomadN Eaduw∼ahu Translation: Ahmad wounded his enemy.</p><p>The letters "klm" manifests into two different words when given two different diacritizations. As shown in this example, "kal∼ama" in the first sentence is the verb 'talked' in English, while "kalama" in the second sentence is the verb 'wounded' in English.</p><p>To formulate the problem in a formal manner: Given a sequence of characters representing an Arabic sentence S, find the correct diacritic class (from <ref type="figure" target="#fig_1">Figure 2</ref>) for each Arabic character S i in S.</p><p>Despite the problem's importance, it received limited attention. One of the reasons for this is the scarcity of freely available resources for this problem. To address this issue, the Tashkeela Corpus 1 <ref type="bibr" target="#b25">(Zerrouki and Balla, 2017)</ref> has been released to the community. Unfortunately, there are many problems with the use of this corpus for benchmarking purposes. A very recent study <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref> discussed in details these issues and provided a cleaned version of the dataset with predefined split into training, testing and validation sets. In this work, we use this dataset and provide yet another extension of it with a larger training set and a new testing set to circumvent the issue that some of the existing systems have already be-en trained on the entire Tashkeela Corpus.</p><p>According to <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref>, existing approaches to ATD are split into two groups: traditional rule-based approaches and machine learning based approaches. The former was the main approach by many researchers such as <ref type="bibr" target="#b26">(Zitouni and Sarikaya, 2009;</ref><ref type="bibr" target="#b20">Pasha et al., 2014;</ref><ref type="bibr">Darwish et al., 2017)</ref> while the latter has started to receive attention only recently <ref type="bibr" target="#b9">(Belinkov and Glass, 2015;</ref><ref type="bibr" target="#b1">Abandah et al., 2015;</ref><ref type="bibr" target="#b7">Barqawi and Zerrouki, 2017;</ref><ref type="bibr" target="#b19">Mubarak et al., 2019)</ref>. Based on the extensive experiments of <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref>, deep learning approaches (aka neural approaches) are superior to non-neural approaches especially when large training data is available. In this work, we present several neural ATD models and compare their performance with the state of the art (SOTA) approaches to show that our models are either on par with the SOTA approaches or even better. Finally, we present a novel way to utilize diactritization in order to enhance the accuracy of Machine Translation (MT) models in what we call Translation over Diacritization (ToD) approach.</p><p>The rest of the paper is organized as follows. The following section discusses the dataset proposed by <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref>. Sections 3 and 4 discuss our two main approaches: Feed-Forward Neural Network (FFNN) and Recurrent Neural Network (RNN), respectively. Section 5 brielfy discusses the related work and presents a comparison with the SOTA approaches while Section 6 describes our novel approach to integrate diacritization into translation tasks. The paper is concluding in Section 7 with final remarks and future directions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The dataset of <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref> (which is an adaptation of the Tashkeela Corpus) consists of about 2.3M words spread over 55K lines. Basic statistics about this dataset size, content and diacritics usage are given in <ref type="table" target="#tab_0">Table 1</ref>. Among the resources provided with this dataset are new definitions of the Diacritic Error Rate (DER), which is "the percentage of misclassified Arabic characters regardless of whether the character has 0, 1 or 2 diacritics", and the Word Error Rate (WER), which is "the percentage of Arabic words which have at least one misclassified Arabic character". <ref type="bibr">2</ref> The redefinition of these measures is to exclu-2 DER/WER are computed with diacritization stat.py </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Feed-Forward Neural Network (FFNN) Approach</head><p>This is our first approach and we present three models based on it. In this approach, we consider diacritizing each character as an independent problem. To do so, the model takes a 100-dimensional vector as an input representing features for a single character in the sentence. The first 50 elements in the vector represent the 50 non-diacritic characters before the current character and the last 50 elements represent the 50 non-diacritic characters after it including the current character. For example, the sentence ' ', the vector related to the character ' ' is as shown in Figure 1. As the figure shows, there are two characters before the character ' ' and four after it (including the whitespace). The special token '&lt;PAD&gt;' is used as a filler when there are no characters to feed. Note that the dataset contains 73 unique characters (without the diacritics) which are mapped to unique integer values from 0 to 74 after sorting them based on their unicode representations including the special padding and unknown ('&lt;UNK&gt;') tokens.</p><p>Each example belongs to one of the 15 classes under consideration, which are shown in Figure 2. The model outputs probabilities for each  class. Using a Softmax output unit, the class with maximum probability is considered as the correct output. The number of training, validation and testing examples from converting the dataset into examples as described earlier are 9,017K, 488K and 488K respectively.</p><p>Basic Model. The basic model consists of 17 hidden layers of different sizes. The activation function used in all layers is Rectified Linear Unit (Re-LU) and the number of trainable parameters is about 1.5M. For more details see Appendix A. The model is trained for 300 epochs on an Nvidia Ge-Force GTX 970M GPU for about 16 hours using AdaGrad optimization algorithm <ref type="bibr" target="#b13">(Duchi et al., 2011)</ref> with 0.01 learning rate, 512 batch size, and categorical cross-entropy loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>100-Hot</head><p>Model. In this model, each integer from the 100-integer inputs is converted into its 1-hot representation as a 75-dimensional vector. Then, the 100 vectors are concatenated forming a 7,500dimensional vector. Based on empirical exploration, the model is structured to have five hidden layers with dropout. It has close to 2M trainable parameters. For more details see Appendix A. The model is trained for 50 epochs on an Nvidia GeForce GTX 970M GPU for about 3 hours using Adam optimization algorithm (Kingma and Ba, 2014) with 0.001 learning rate, 0.9 beta1, 0.999 beta2, 512 batch size, and categorical crossentropy loss function.</p><p>Embeddings Model. In this model, the 100-hot layer is replaced with an embeddings layer to learn feature vectors for each character through the training process. Empirically determined, the model has five hidden layers with only 728K trainable parameters. For more details see Appendix A. The model is trained with the same configurations as the 100-hot model and the training time is about 2.5 hours only.</p><p>Results and Analysis. Although the idea of diacritizing each character independently is counterintuitive, the results of the FFNN models on the test set (shown in <ref type="table" target="#tab_1">Table 2</ref>) are very promising with the embeddings model having an obvious advantage over the basic and 100-hot models and performing much better than the best rule-based diacritization system Mishkal 3 among the systems reviewed by <ref type="bibr" target="#b14">(Fadel et al., 2019</ref>) (Mishakl DER: 13.78% vs FFNN Embeddings model DER: 4.06%). However, these models are still imperfect. More detailed error analysis of these models is available in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Recurrent Neural Network (RNN) Approach</head><p>Since RNN models usually need huge data to train on and learn high-level linguistic abstractions, we prepare an external training dataset following the guidelines of <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref>. The extra training dataset is extracted from the Classical Arabic (CA) part of the Tashkeela Corpus and the Holy Quran (HQ). We exclude the lines that already exist in the previously mentioned dataset. Note that, with the extra training dataset the number of unique characters goes up to 87 (without the diacritics). <ref type="table" target="#tab_2">Table 3</ref> shows the statistics for the extra training dataset. The lines in the dataset are split using the following 14 punctuations ('.', ',', ' ', ':', ';', ' ' , '(', ')', '[', ']', '{', '}', ' ' and ' '). After that, the lines with length more than 500 characters (without counting diacritics) are split into lines of length no more than 500. This step is necessary for the training phase to limit memory usage within a single batch. Note that the splitting procedure is omitted within the prediction phase, e.g., when calculating DER/WER on the validation and test sets. Moreover, four special tokens ('&lt;SOS&gt;', '&lt;EOS&gt;', '&lt;UNK&gt;' and '&lt;PAD&gt;') are used to prepare the input data before feeding it to the model. '&lt;SOS&gt;' and '&lt;EOS&gt;' are added to the start and the end of the sequences, respectively. '&lt;UNK&gt;' is used to represent unknown characters not seen in the training dataset. Finally, '&lt;PAD&gt;' is appended to pad the sequences within the same batch.  Four equivalent special tokens are used as an output in the target sequences.</p><p>Basic Model. Several model architectures are trained without the extra training dataset. After some exploration, the best model architecture is chosen to experiment with different techniques as described in details throughout this section. The exploration is done to tune different hyperparameters and find the structure that gives the best DER, which, in most cases, leads to better WER. Because the neural network size have a great impact on performance, we primarily experiment with the number of Bidirectional CuD-NN Long Short-Term Memory (BiCuDNNLSTM) <ref type="bibr" target="#b4">(Appleyard et al., 2016)</ref> layers and their hidden units. By using either one, two or three layers, the error significantly decreases going from one layer to two layers. However, it shows slight improvement (if any) when going from two layers to three layers while increasing the training time. So, we decide to use two BiCuDNNLSTMs in further experiments as well as 256 hidden units per layer as using less units will increase the error rate while using more units does not significantly improve it. Then, we experiment with the size and depth of the fully connected feed-forward network. The results show that the depth is not as important as the size of each layer. The best results are produced with the model using two layers with 512 hidden units each. All experiments are done using Adam optimization algorithm, because different optimizers like Stochastic Gradient Descent, Adagrad and Adadelta do not converge to the optimal minimal fast enough and RMSprop, Nadam and Adamax give the same or slightly worse results. The number of character features to learn in the embedding layer that gives the best results is 25, where more features leads to little improvement and more overfitting, and less features makes the training harder for the network. This is probably due to the input vocabulary being limited to 87 different characters. We also experiment with training the models for more than 50 epochs, but the return is very little or it makes the learning unstable and eventually causes exploding gradients leaving the network with useless predictions, unable to learn anymore. The best model is structured as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>The training is done twice: with and without the extra training dataset, in order to explore the impact of the dataset size on the training phase for the diacritization problem. This has led to reduced overfitting. A weights averaging technique over the last few epochs is applied to partially overcome the overfitting issue and obtain a better generalization.</p><p>Models in all following experiments are trained on Google Colab 4 <ref type="bibr" target="#b10">(Carneiro et al., 2018)</ref> environment for 50 epochs using an Nvidia Tesla T4 GPU, Adam optimization algorithm with 0.001 learning rate, 0.9 beta1, 0.999 beta2, 10 −7 epsilon, 256 batch size, and categorical cross-entropy loss function.</p><p>Conditional Random Field (CRF) Model. A CRF classifier is used in this model instead of the Softmax layer to predict the network output. CRF is usually more powerful than Softmax in terms of sequence dependencies in the output layer which exist in the diacritization problem. It is worth mentioning that CRF is considered to be "a best practice" in sequence labeling problems. However, in this particular problem, the results show that CRF performs worse than Softmax in most cases except for WER results when training without the extra dataset which indicates that, even with worse DER results, CRF is able to make more consistent predictions within the same word.</p><p>Block-Normalized Gradient (BNG) Model. In this model, <ref type="bibr" target="#b24">(Yu et al., 2017)</ref>'s BNG method is applied to normalize gradients within each batch. This can help accelerate the training process. According to <ref type="bibr" target="#b24">(Yu et al., 2017)</ref>, this method performs better in RNN when using optimizers with adaptive step sizes, such as Adam. It can also lead to solutions with better generalization. This coincides with our results.</p><p>Discussion and Analysis. The results of the RNN models on the test set (shown in <ref type="table" target="#tab_3">Table 4</ref>) are much better than the FFNN models by about 67%. To show the effect of the weights averaging technique, <ref type="table" target="#tab_4">Table 5</ref> reports the DER/WER statistics related to the BNG model after averaging its weights over the last 1, 5, 10, and 20 epochs. Studying the confusion matrices for all the models suggests that the Shadda class and the composite classes (i.e., Shadda + another diacritic) are harder to learn for the network compared to other classes. However, with the extra training dataset, the network is able to find significantly better results compared to the results without the extra training dataset, especially for the Shadda class.</p><p>The comparison method for calculating DER/WER without case ending skips comparing the diacritization on the end of each word. This skip improves the best DER to 1.34% (vs 1.69%) and best WER to 2.91% (vs 5.09%) which is a 26% improvement in DER and 43% improvement in WER. This is because the diacritic of the last character of the word usually depends on the part of speech tag making it harder to diacritize. However, we note that the actual last character of the word may come before the end of the word if the word has some suffix added to it. Consider the example shown in <ref type="figure" target="#fig_3">Figure 4</ref>. The word ' ' means 'his book' where the last character ' ' is the suffix representing the pronoun 'his', and the letter before it may take three different diacritics depending on its part of speech tagging. More detailed error analysis of these models available in Appendix B. Furthermore, an Encoder-Decoder structure (seq2seq) was built using BiCuDNNLSTMs to encode a sequence of characters and generate a sequence of diacritics, but the model was not able to successfully learn the alignment between inputted characters and outputted diacritics. Other attempts tried encoding the sentences as sequences of words and generate a sequences of diacritics also terribly failed to learn.</p><p>The BNG model performs the best compared to other models described above. So, it is used for comparison with other systems in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison with Existing Systems</head><p>As mentioned earlier, the efforts on building automatic ATD is limited. A recent study <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref> surveyed existing approaches and tools for ATD. After discussing the limitations in closedsource tools, they divided existing approaches to ATD into two groups: traditional rule-based approaches <ref type="bibr" target="#b26">(Zitouni and Sarikaya, 2009;</ref><ref type="bibr" target="#b20">Pasha et al., 2014;</ref><ref type="bibr" target="#b22">Shahrour et al., 2015;</ref><ref type="bibr" target="#b2">Alnefaie and Azmi, 2017;</ref><ref type="bibr" target="#b8">Bebah et al., 2014;</ref><ref type="bibr" target="#b5">Azmi and Almajed, 2015;</ref><ref type="bibr" target="#b11">Chennoufi and Mazroui, 2017;</ref><ref type="bibr">Darwish et al., 2017;</ref><ref type="bibr" target="#b15">Fashwan and Alansary, 2017;</ref><ref type="bibr" target="#b3">Alqahtani et al., 2019)</ref> and machine learning based approaches <ref type="bibr" target="#b9">(Belinkov and Glass, 2015;</ref>    <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref> showed that neural ATD models are superior to their competitors especially when large training data is available. Thus, we limit our attention in this work to such models. According to <ref type="bibr" target="#b14">(Fadel et al., 2019</ref>), the Shakkala system <ref type="bibr" target="#b7">(Barqawi and Zerrouki, 2017)</ref> performs the best compared to other existing systems using the test set and the evaluation metrics proposed in <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref>. Considering our best model's results mentioned previously, it is clear that our model outperforms Shakkala on the testing set after splitting the lines to be at most 315 characters long (Shakkala system limit), which causes a slight drop in our best model's results. However, since Shakkala was also trained on Tashkeela Corpus, we develop an auxiliary test set extracted from three books from Al-Shamela Library 5 ' ', ' ' and ' ' using the 5 http://shamela.ws same extraction and cleaning method proposed by <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref> while keeping only lines with more than 80% "diacritics to Arabic characters" rate. The extracted lines are each split into lines of lengths no more than 315 characters (without counting diacritics) which is the input limit of the Shakkala system. This produces a test set consisting of 443K words. <ref type="table" target="#tab_5">Table 6</ref> shows the results comparison with Shakkala.</p><p>A comparison with the pre-trained model of <ref type="bibr" target="#b9">(Belinkov and Glass, 2015)</ref> is also done using the test set and the evaluation metrics of <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref> while splitting the lines into lines of lengths no more than 125 characters (without counting diacritics) since any input with length more than that causes an error in their system. The results show that <ref type="bibr" target="#b9">(Belinkov and Glass, 2015)</ref>'s model performs poorly. However, we note that (Belinkov and Glass, 2015)'s system was trained and tested on the Arabic TreeBank (ATB) dataset which consists of text in Modern Standard Arabic (MSA). So, to make a fair comparison with <ref type="bibr" target="#b9">(Belinkov and Glass, 2015)</ref>'s system, an auxiliary dataset is built from the MSA part of the Tashkeela Corpus using the same extraction and cleaning method proposed  <ref type="bibr" target="#b7">(Barqawi and Zerrouki, 2017)</ref>   by <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref> keeping only lines with more than 80% "diacritics to Arabic characters" rate. This test set consists of 111K words. The results are reported in <ref type="table" target="#tab_7">Table 7</ref>. In addition to the poor results of (Belinkov and Glass, 2015)'s system, its output has a large number of special characters inserted randomly. These characters are removed manually to make the evaluation of the system possible.</p><p>Finally, we compare our model with <ref type="bibr" target="#b1">(Abandah et al., 2015)</ref>'s model which, to our best knowledge, is the most recent deep-learning work announcing the best results so far. To do so, we employ a similar comparison method to <ref type="bibr" target="#b11">(Chennoufi and Mazroui, 2017)</ref>'s by using the 10 books from the Tashkeela Corpus and the HQ that were excluded from <ref type="bibr" target="#b1">(Abandah et al., 2015)</ref>'s test set. The sentences used for testing our best model are all sentences that are not included in the training dataset of <ref type="bibr" target="#b14">(Fadel et al., 2019)</ref> or extra training dataset on which our model is trained. To make the comparison fair, we use the same evaluation metric as <ref type="bibr" target="#b1">(Abandah et al., 2015)</ref>, which is <ref type="bibr" target="#b26">(Zitouni and Sarikaya, 2009</ref>)'s. Moreover, the characters with no diacritics in the original text are skipped similarly to <ref type="bibr" target="#b1">(Abandah et al., 2015)</ref>. The results are shown in <ref type="table" target="#tab_8">Table 8</ref>. It is worth mentioning that the results of <ref type="bibr" target="#b1">(Abandah et al., 2015)</ref> include post-processing techniques, which improved DER by 23.8% as re-ported in <ref type="bibr" target="#b1">(Abandah et al., 2015)</ref>. It can be easily shown that, without this step, our model's results are actually superior.</p><p>All codes related to the diacritization work are publicly available on GitHub, 6 and are also implemented into a web application 7 for testing purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Translation over Diacritization (ToD)</head><p>Word's diacritics can carry various types of information about the word itself, like its part of speech tag, the semantic meaning and the pronunciation. Intuitively, providing such extra features in NLP tasks has the potential to improve the results of any system. In this section, we show how we benefit from the integration of diacritics into Arabic-English (Ar-En) Neural Machine Translation (NMT) creating what we call Translation over Diacritization (ToD).</p><p>Dataset Extraction and Preparation. Due to the lack of free standardized benchmark datasets for Ar-En MT, we create a mid-size dataset using the following corpora: GlobalVoices v2017q3, MultiUN v1, News-Commentary v11, Tatoeba v2, TED2013 v1.1, Ubuntu v14.10, Wikipedia v1.0 <ref type="bibr" target="#b23">(Tiedemann, 2012)</ref> downloaded from  <ref type="bibr" target="#b1">(Abandah et al., 2015)</ref>    <ref type="bibr" target="#b21">(Sennrich et al., 2015)</ref> is applied separately on both English and original (undiacritized) Arabic sequences to segment the words into subwords. This step overcomes the Out Of Vocabulary (OOV) problem and reduces the vocabulary size. Then, diacritics are added to Arabic subwords to create the diacritized version. <ref type="table" target="#tab_10">Table 9</ref> shows the number of tokens before and after BPE step for English, Original Arabic and Diacritized Arabic as well as the Diacritics forms when removing the Arabic characters.</p><p>Model Structure The model used in the experiments is a basic Encoder-Decoder sequence to sequence (seq2seq) model that consists of a BiCuD-NNLSTM layer for encoding and a CuDNNLSTM layer for decoding with 512 units each (256 per direction for the encoder) while applying additive attention <ref type="bibr" target="#b6">(Bahdanau et al., 2014)</ref> on the outputs of the encoder. As for the embeddings layer, a single randomly initialized embeddings layer with vector size 64 is used to represent the subwords when training without diacritics. Another layer with the same configuration is used to represent subwords' diacritics, which is concatenated with the subwords embeddings when training with diacritics. The model structure shown in <ref type="figure" target="#fig_4">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>To explore the effect of the Arabic diacritization on the NMT task, we experiment with training both with and without diacritics. The models are trained for 50 epochs using an Nvidia Titan Xp GPU, Adam optimization algorithm with 0.001 learning rate, 0.9 beta1, 0.999 beta2, 10 −7 epsilon and 256 batch size.</p><p>The structure for training the model with diacritics may vary. We experiment with two variations where the first one uses the diacritized version of the sequences, while the other one uses the original sequences and the diacritics sequences in parallel. When merging diacritics with their sequences, we get more variations of each word depending on its different forms of diacritization, therefore expanding the vocabulary size. On the other hand, when separating diacritics from their sequences, the vocab size stays the same, and diacritics are added separately as extra input.</p><p>The results in <ref type="table" target="#tab_0">Table 10</ref> show that training the model with diacritization compared to without diacritization improves marginally by 0.31 BLEU score 10 when using the 'with diacritics (merged)' data and improves even more when using the 'with diacritics (separated)' data by 1.33 BLEU score. Moreover, the training time and model size increases by about 20.6% and 41.4%, respectively, for using the 'with diacritics (merged)' data, while they only increase by about 3.4% and 4.5%, respectively, for using the 'with diacritics (separated)' data. By observing <ref type="figure" target="#fig_5">Figure 6</ref>, which reports the BLEU score on all three models every 5 epochs, it is clear that, although the 'with diacritics (merged)' model converges better at the start of the training, it starts diverging after 15 epochs, which might be due to the huge vocab size and the training data size.</p><p>By analysing <ref type="figure" target="#fig_5">Figure 6</ref>, we find that BLUE score converges faster when training with diacritics (merged) compared to the other two approaches. However, it starts diverging later on due to vocabulary sparsity. As for with diacritics (separated), the BLUE score has higher convergence compared to without diacritics while also maintaining stability compared to with diacritics (merged). This is because separating diacritics solves the vocabulary sparsity issue while also providing the information needed to disambiguate homonym words.</p><p>We note that, concurrently to our work, another work on utilizing diacritization for MT has recently appeared. <ref type="bibr" target="#b3">(Alqahtani et al., 2019)</ref> used diacritics with text in three downstream tasks, namely Semantic Text Similarity (STS), NMT and Part of Speech (POS) tagging, to boost the performance of their systems. They applied different techniques to disambiguate homonym words through diacritization. They achieved 27.1 and 27.3 BLUE scores without and with diacritics, respectively, using their best disambiguation technique. This is a very small improvement of 0.74% compared to our noticeable improvement of 4.03%. Moreover, our approach is simpler and it does not require to drop any diacritical information.</p><p>All codes related to the ToD work are publicly available on GitHub 11 . 10 BLEU scores are computed with multi-bleu.perl 11 https://github.com/AliOsm/ translation-over-diacritization  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we explored the ATD problem. Our models, which follow two main approaches: FF-NN and RNN, proved to be very effective as they performed on par with or better than SOTA approaches. In the future, we plan on investigating the sequence to sequence models such as RNN Seq2seq, Conv Seq2seq and Transformer. In another contribution of this work, we showed that diacritics can be integrated into other systems to attain enhanced versions in NLP tasks. We used MT as a case study and showed how our idea of ToD improved the results of the SOTA NMT system. the test set accuracy compared to the basic model. <ref type="figure" target="#fig_7">Figure 8</ref> shows the loss and accuracy values on the training and validation datasets while training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Embeddings Model</head><p>Using a very similar structure as the 100-hot model structure <ref type="table" target="#tab_0">(Table 12)</ref>, this model is structured as shown in <ref type="table" target="#tab_0">Table 13</ref>. It achieves the best results compared to the basic and 100-hot models with 94.88%, 94.53% and 94.49% accuracies for training, validation, and testing datasets, respectively. This model improves the accuracy by 1.04% on the test set compared to the 100-hot model while reducing the number of trainable parameters (model size) by 51.46% and 62.66% compared to the basic and 100-hot models, respectively. <ref type="figure" target="#fig_8">Figure 9</ref> shows the loss and accuracy values on the training and validation datasets while training. <ref type="figure" target="#fig_0">Figure 10</ref> shows the best diacritization examples diacritized using each FFNN model, while Figure 11 shows the worst diacritization examples. It is worth mentioning that the worst examples (listed in <ref type="figure" target="#fig_0">Figure 11</ref>) are from old Arabic poetry, which is very hard to diacritize flawlessly even for native speakers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RNN Models in Details</head><p>This section provides details for the trained RNN models. First of all, <ref type="figure" target="#fig_0">Figure 12</ref> shows the validation DER of each model while training, reported every 5 epochs. This clarifies the importance of the dataset size, where any model significantly improves their DER when trained with the extra train dataset compared to any other model trained without it.</p><p>Moreover, to explore the embeddings learnt by our best model, the weights vectors from the embeddings layer were extracted and reduced to 2 dimensions instead of 25 using t-SNE dimensionality reduction algorithm <ref type="bibr" target="#b17">(Maaten and Hinton, 2008)</ref>, then plotted in 2D space as shown in Figure 13. The embeddings are able to capture meaningful information where digits appear together at the bottom-left, the majority of the punctuati- ons appear at the middle and the top-left side, and finally, the Arabic letters appear at the right side. <ref type="figure" target="#fig_0">Figures 14 and 15</ref> show both best and worst examples from diacritizing using each RNN model. An important note is that the old Arabic poetry lines are no longer the majority in the worst examples, in contrast to the FFNN models.</p><p>Finally, <ref type="figure" target="#fig_0">Figures 16 and 17</ref> shows the confusion matrices related to our best model when trained without and with the extra train dataset, respectively. By comparing them, it is easy to see that the Shadda class is the worst one in both cases. However, the case with the extra train dataset shows dramatic improvement in this class, as well as other classes like Shadda + another diacritic and the Dammatan. A justification for this improvement is that there is a larger number of examples in the extra train dataset related to these classes as shown in <ref type="table" target="#tab_0">Table 14</ref>. Another insight can be concluded from the confusion matrices is that the model usually misclassifies the Shadda class as Shadda + another diacritic class due to different diacritization conventions, which in many cases would be a grammatically correct guess.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Vector representation of a FFNN example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The 15 classes under consideration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>RNN basic model structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Case ending different diacritization with different part of speech tag.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>ToD model structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Testing dataset BLEU score while training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>FFNN basic model training and validation accuracy and loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>FFNN 100-Hot model training and validation accuracy and loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>FFNN Embeddings model training and validation accuracy and loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>FFNN models good diacritization examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>FFNN models bad diacritization examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Recurrent models validation DER while training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Embeddings plotted in 2D space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>RNN models good diacritization examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>RNN models bad diacritization examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Without extra train confusion matrix for the best BNG model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 17 :</head><label>17</label><figDesc>With extra train confusion matrix for the best BNG model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics about the size, content and diacritics usage of<ref type="bibr" target="#b14">(Fadel et al., 2019)</ref>'s Dataset</figDesc><table><row><cell></cell><cell>Train</cell><cell>Valid</cell><cell>Test</cell></row><row><cell>Words Count</cell><cell cols="3">2,103K 102K 107K</cell></row><row><cell>Lines Count</cell><cell>50K</cell><cell cols="2">2.5K 2.5K</cell></row><row><cell>Avg Chars/Word</cell><cell>3.97</cell><cell>3.97</cell><cell>3.97</cell></row><row><cell>Avg Words/Line</cell><cell cols="3">42.06 40.97 42.89</cell></row><row><cell>0 Diacritics (%)</cell><cell cols="3">17.78 17.75 17.80</cell></row><row><cell>1 Diacritic (%)</cell><cell cols="3">77.17 77.19 77.22</cell></row><row><cell>2 Diacritics (%)</cell><cell>5.03</cell><cell>5.05</cell><cell>4.97</cell></row><row><cell>Error Diacritics (%)</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="4">de counting irrelevant characters such as numbers</cell></row><row><cell cols="4">and punctuations, which were included in (Zitouni</cell></row><row><cell cols="4">and Sarikaya, 2009)'s original definitions of DER</cell></row><row><cell cols="4">and WER. It is worth mentioning that DER/WER</cell></row><row><cell cols="4">are computed in four different ways in the lite-</cell></row><row><cell cols="4">rature depending on whether the last character of</cell></row><row><cell cols="4">each word (referred to as case ending) is counted</cell></row><row><cell cols="4">or not and whether the characters with no diacriti-</cell></row><row><cell cols="2">zation are counter or not.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>DER/WER comparison of the different FFNN models on the test set</figDesc><table><row><cell>DER/WER</cell><cell cols="4">w/ case ending w/o case ending w/ case ending w/o case ending Including 'no diacritic' Excluding 'no diacritic'</cell></row><row><cell>Basic model</cell><cell>9.33% / 25.93%</cell><cell>6.58% / 13.89%</cell><cell>10.85% / 25.39%</cell><cell>7.51% / 13.53%</cell></row><row><cell>100-Hot model</cell><cell>6.57% / 20.21%</cell><cell>4.83% / 11.14%</cell><cell>7.75% / 19.83%</cell><cell>5.62% / 10.93%</cell></row><row><cell cols="2">Embeddings model 5.52% / 17.12%</cell><cell>4.06% / 9.38%</cell><cell>6.44% / 16.63%</cell><cell>4.67% / 9.10%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Extra training dataset statistics</figDesc><table><row><cell></cell><cell>Extra Train</cell></row><row><cell>Words Count</cell><cell>22.4M</cell></row><row><cell>Lines Count</cell><cell>533K</cell></row><row><cell>Avg Chars/Word</cell><cell>3.97</cell></row><row><cell>Avg Words/Line</cell><cell>42.1</cell></row><row><cell>0 Diacritics (%)</cell><cell>17.79</cell></row><row><cell>1 Diacritic (%)</cell><cell>77.16</cell></row><row><cell>2 Diacritics (%)</cell><cell>5.03</cell></row><row><cell>Error Diacritics (%)</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>DER/WER comparison of the different RNN models on the test set</figDesc><table><row><cell>DER/WER</cell><cell cols="4">w/ case ending w/o case ending w/ case ending w/o case ending Including 'no diacritic' Excluding 'no diacritic'</cell></row><row><cell></cell><cell cols="3">Without Extra Train Dataset</cell><cell></cell></row><row><cell>Basic model</cell><cell>2.68% / 7.91%</cell><cell>2.19% / 4.79%</cell><cell>3.09% / 7.61%</cell><cell>2.51% / 4.66%</cell></row><row><cell>CRF model</cell><cell>2.67% / 7.73%</cell><cell>2.19% / 4.69%</cell><cell>3.08% / 7.46%</cell><cell>2.52% / 4.60%</cell></row><row><cell>BNG model</cell><cell>2.60% / 7.69%</cell><cell>2.11% / 4.57%</cell><cell>3.00% / 7.39%</cell><cell>2.42% / 4.44%</cell></row><row><cell></cell><cell></cell><cell cols="2">With Extra Train Dataset</cell><cell></cell></row><row><cell>Basic model</cell><cell>1.72% / 5.16%</cell><cell>1.37% / 2.98%</cell><cell>1.99% / 4.96%</cell><cell>1.59% / 2.92%</cell></row><row><cell>CRF model</cell><cell>1.84% / 5.42%</cell><cell>1.47% / 3.17%</cell><cell>2.13% / 5.22%</cell><cell>1.69% / 3.09%</cell></row><row><cell>BNG model</cell><cell>1.69% / 5.09%</cell><cell>1.34% / 2.91%</cell><cell>1.95% / 4.89%</cell><cell>1.54% / 2.83%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>DER/WER comparison showing the effect of the weights averaging technique on BNG model</figDesc><table><row><cell>DER/WER</cell><cell>Averaged Epochs</cell><cell cols="4">w/ case ending w/o case ending w/ case ending w/o case ending Including 'no diacritic' Excluding 'no diacritic'</cell></row><row><cell>Without</cell><cell>1</cell><cell>2.73% / 8.08%</cell><cell>2.21% / 4.80%</cell><cell>3.16% / 7.79%</cell><cell>2.54% / 4.68%</cell></row><row><cell>extra</cell><cell>5</cell><cell>2.64% / 7.80%</cell><cell>2.14% / 4.64%</cell><cell>3.04% / 7.49%</cell><cell>2.46% / 4.52%</cell></row><row><cell>train</cell><cell>10</cell><cell>2.60% / 7.69%</cell><cell>2.11% / 4.57%</cell><cell>3.00%/ 7.39%</cell><cell>2.42% / 4.44%</cell></row><row><cell>dataset</cell><cell>20</cell><cell>2.61% / 7.73%</cell><cell>2.11% / 4.56%</cell><cell>3.01% / 7.42%</cell><cell>2.42% / 7.42%</cell></row><row><cell>With</cell><cell>1</cell><cell>1.97% / 5.85%</cell><cell>1.61% / 3.55%</cell><cell>2.20% / 5.61%</cell><cell>1.82% / 3.45%</cell></row><row><cell>extra</cell><cell>5</cell><cell>1.73% / 5.20%</cell><cell>1.38% / 3.02%</cell><cell>1.98% / 4.98%</cell><cell>1.58% / 2.92%</cell></row><row><cell>train</cell><cell>10</cell><cell>1.70% / 5.13%</cell><cell>1.35% / 2.94%</cell><cell>1.96% / 4.92%</cell><cell>1.55% / 2.85%</cell></row><row><cell>dataset</cell><cell>20</cell><cell>1.69% / 5.09%</cell><cell>1.34% / 2.91%</cell><cell>1.95% / 4.89%</cell><cell>1.54% / 2.83%</cell></row><row><cell cols="4">et al., 2015, 2017; Barqawi and Zerrouki, 2017;</cell><cell></cell><cell></cell></row><row><cell cols="4">Moumen et al., 2018; Mubarak et al., 2019). The</cell><cell></cell><cell></cell></row><row><cell cols="2">extensive experiments of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparing the BNG model with</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparing the BNG model with<ref type="bibr" target="#b9">(Belinkov and Glass, 2015)</ref> in terms of DER/WER on the test set</figDesc><table><row><cell>DER/WER</cell><cell cols="4">w/ case ending w/o case ending w/ case ending w/o case ending Including 'no diacritic' Excluding 'no diacritic'</cell></row><row><cell></cell><cell cols="3">Classical Arabic Testing Dataset Results</cell><cell></cell></row><row><cell>Our best model</cell><cell>1.99% / 6.10%</cell><cell>1.48% / 3.25%</cell><cell>2.30% / 5.88%</cell><cell>1.70% / 3.17%</cell></row><row><cell cols="2">Belinkov, 2015 31.26% / 75.29%</cell><cell>29.66% / 59.46%</cell><cell>35.78% / 74.37%</cell><cell>33.67% / 57.66%</cell></row><row><cell></cell><cell cols="3">Modern Standard Arabic Testing Dataset Results</cell><cell></cell></row><row><cell cols="2">Our best model 8.05% / 23.56%</cell><cell>6.85% / 16.12%</cell><cell>8.29% / 21.10%</cell><cell>7.16% / 14.41%</cell></row><row><cell cols="2">Belinkov, 2015 31.77% / 75.02%</cell><cell>29.21% / 59.40%</cell><cell>37.13% / 73.93%</cell><cell>33.82% / 58.03%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparing the BNG model with</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="3">: Vocab size for all sequences types before and</cell></row><row><cell>after BPE step</cell><cell></cell><cell></cell></row><row><cell>Language</cell><cell cols="2">Vocab Size Before BPE After BPE</cell></row><row><cell>English</cell><cell>113K</cell><cell>31K</cell></row><row><cell>Original Arabic</cell><cell>224K</cell><cell>32K</cell></row><row><cell>Diacritized Arabic</cell><cell>402K</cell><cell>186K</cell></row><row><cell>Diacritics Forms</cell><cell>41K</cell><cell>15K</cell></row><row><cell cols="3">the OPUS 8 project. The dataset contains 1M Ar-</cell></row><row><cell cols="3">En sentence pairs split into 990K pairs for training</cell></row><row><cell cols="3">and 10K pairs for testing. The extracted 1M pairs</cell></row><row><cell cols="3">follow these conventions: (i) The maximum length</cell></row><row><cell cols="3">for each sentence in the pair is 50 tokens, (ii) Ara-</cell></row><row><cell cols="3">bic sentences contain Arabic letters only, (iii) Eng-</cell></row><row><cell cols="3">lish sentences contain English letters only, and (iv)</cell></row><row><cell cols="2">the sentences do not contain any URLs.</cell><cell></cell></row><row><cell cols="3">The Arabic sentences in the training and testing</cell></row><row><cell cols="3">datasets are diacritized using the best BNG model.</cell></row><row><cell cols="2">After that, Byte Pair Encoding (BPE) 9</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Translation over Diacritization (ToD) results on the test set</figDesc><table><row><cell>Model</cell><cell>Training Time</cell><cell>Model Size</cell><cell>Best BLEU Score</cell></row><row><cell cols="3">Without 29 Hours 285MB</cell><cell>33.01</cell></row><row><cell>Merged</cell><cell cols="2">35 Hours 403MB</cell><cell>33.32</cell></row><row><cell cols="3">Separated 30 Hours 298MB</cell><cell>34.34</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>FFNN 100-Hot model structure</figDesc><table><row><cell>Layer Name</cell><cell cols="2">Neurons Activation Func</cell></row><row><cell>One Hot</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Flatten</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">Dropout 1 (2.5%) N/A</cell><cell>N/A</cell></row><row><cell>Hidden 1</cell><cell>250</cell><cell>ReLU</cell></row><row><cell cols="2">Dropout 2 (2.5%) N/A</cell><cell>N/A</cell></row><row><cell>Hidden 2</cell><cell>200</cell><cell>ReLU</cell></row><row><cell cols="2">Dropout 3 (2.5%) N/A</cell><cell>N/A</cell></row><row><cell>Hidden 3</cell><cell>150</cell><cell>ReLU</cell></row><row><cell cols="2">Dropout 4 (2.5%) N/A</cell><cell>N/A</cell></row><row><cell>Hidden 4</cell><cell>100</cell><cell>ReLU</cell></row><row><cell cols="2">Dropout 5 (2.5%) N/A</cell><cell>N/A</cell></row><row><cell>Hidden 5</cell><cell>50</cell><cell>ReLU</cell></row><row><cell cols="2">Dropout 6 (2.5%) N/A</cell><cell>N/A</cell></row><row><cell>Output</cell><cell>15</cell><cell>Softmax</cell></row><row><cell cols="3">Trainable Parameters: 1,951,515</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>FFNN Embeddings model structure</figDesc><table><row><cell>Layer Name</cell><cell cols="2">Neurons Activation Func</cell></row><row><cell cols="2">Embedding (25) N/A</cell><cell>N/A</cell></row><row><cell>Flatten</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Dropout (10%)</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Hidden 1</cell><cell>250</cell><cell>ReLU</cell></row><row><cell>Hidden 2</cell><cell>200</cell><cell>ReLU</cell></row><row><cell>Hidden 3</cell><cell>150</cell><cell>ReLU</cell></row><row><cell>Hidden 4</cell><cell>100</cell><cell>ReLU</cell></row><row><cell>Hidden 5</cell><cell>50</cell><cell>ReLU</cell></row><row><cell>Output</cell><cell>15</cell><cell>Softmax</cell></row><row><cell cols="3">Trainable Parameters: 728,590</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Number of examples for each class</figDesc><table><row><cell></cell><cell>Train</cell><cell cols="3">Valid Test Extra Train</cell><cell>Total</cell><cell>%</cell></row><row><cell>No Diacritic</cell><cell cols="3">4,366K 213K 222K</cell><cell>46,647K</cell><cell cols="2">51,449K 38.87</cell></row><row><cell>Fatha</cell><cell cols="3">2,932K 144K 150K</cell><cell>31,287K</cell><cell cols="2">34,514K 26.07</cell></row><row><cell>Fathatah</cell><cell>58K</cell><cell>3K</cell><cell>3K</cell><cell>626K</cell><cell>691K</cell><cell>00.52</cell></row><row><cell>Damma</cell><cell>812K</cell><cell>39K</cell><cell>41K</cell><cell>8,648K</cell><cell cols="2">9,539K 07.20</cell></row><row><cell>Dammatan</cell><cell>58K</cell><cell>3K</cell><cell>3K</cell><cell>622K</cell><cell>686K</cell><cell>00.51</cell></row><row><cell>Kasra</cell><cell cols="2">1,265K 62K</cell><cell>64K</cell><cell>13,533K</cell><cell cols="2">14,924K 11.27</cell></row><row><cell>Kasratan</cell><cell>88K</cell><cell>4K</cell><cell>4K</cell><cell>941K</cell><cell cols="2">1,037K 00.78</cell></row><row><cell>Sukun</cell><cell cols="2">1,230K 60K</cell><cell>63K</cell><cell>13,135K</cell><cell cols="2">14,487K 10.94</cell></row><row><cell>Shaddah</cell><cell>6K</cell><cell>254</cell><cell>471</cell><cell>66K</cell><cell>73K</cell><cell>00.05</cell></row><row><cell>Shaddah + Fatha</cell><cell>300K</cell><cell>15K</cell><cell>15K</cell><cell>3,202K</cell><cell cols="2">3,532K 02.66</cell></row><row><cell>Shaddah + Fathatah</cell><cell>3K</cell><cell>189</cell><cell>132</cell><cell>36K</cell><cell>40K</cell><cell>00.03</cell></row><row><cell>Shaddah + Damma</cell><cell>43K</cell><cell>2K</cell><cell>2K</cell><cell>463K</cell><cell>511K</cell><cell>00.38</cell></row><row><cell>Shaddah + Dammatan</cell><cell>5K</cell><cell>238</cell><cell>222</cell><cell>51K</cell><cell>56K</cell><cell>00.04</cell></row><row><cell>Shaddah + Kasra</cell><cell>64K</cell><cell>3K</cell><cell>3K</cell><cell>679K</cell><cell>749K</cell><cell>00.56</cell></row><row><cell>Shaddah + Kasratan</cell><cell>6K</cell><cell>298</cell><cell>273</cell><cell>63K</cell><cell>69K</cell><cell>00.05</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://tahadz.com/mishkal</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://colab.research.google.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/AliOsm/shakkelha 7 https://shakkelha.herokuapp.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">http://opus.nlpl.eu 9 https://github.com/rsennrich/ subword-nmt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of the Deanship of Research at the Jordan University of Science and Technology for supporting this work via Grant #20180193 in addition to NVIDIA Corporation for the donation of the Titan Xp GPU that was used for this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A FFNN Models in Details</head><p>This section discusses the details of the FFNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Basic Model</head><p>After a massive exploration for finding the best hyperparameters to structure the model (like the number of layers, number of neurons in each layer, and the activation function), the final structure is shown in <ref type="table">Table 11</ref>. This model results in 92.87%, 90.72% and 90.67% accuracies for training, validation, and testing datasets, respectively. <ref type="figure">Figure 7</ref> shows the loss and accuracy values on the training and validation datasets while training. The model is still able to slightly learn as well as generalize even after 300 epochs with no signs of overfitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 100-Hot Model</head><p>Starting from the the basic model structure (Table 11), and by tuning the hyperparameters and using extra techniques, such as applying Dropout regularization, the model is able to learn better using the 100-hot representations. The model is structured as shown in <ref type="table">Table 12</ref>. This model results in 94.25%, 93.49% and 93.45% accuracies for training, validation, and testing datasets, respectively. This is an improvement of 2.78% on</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Investigating hybrid approaches for arabic text diacritization with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gheith</forename><surname>Abandah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Arabiyat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Jordan Conference on Applied Electrical Engineering and Computing Technologies (AEECT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic diacritization of arabic text using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gheith A Abandah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balkees</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Al-Shagoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuad</forename><surname>Arabiyat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Jamour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Taee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="197" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic minimal diacritization of arabic texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rehab</forename><surname>Alnefaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="169" to="174" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Homograph disambiguation through selective diacritic restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sawsan</forename><surname>Alqahtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Aldarmaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Arabic Natural Language Processing Workshop</title>
		<meeting>the Fourth Arabic Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Optimizing performance of recurrent neural networks on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Appleyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>ar- Xiv:1604.01946</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of automatic arabic diacritization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aqil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reham S Almajed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="495" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ar- Xiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Barqawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taha</forename><surname>Zerrouki</surname></persName>
		</author>
		<title level="m">Shakkala, arabic text vocalization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mazroui Azzeddine, and Lakhouaja Abdelhak</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Bebah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chennoufi</forename><surname>Amine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.2646</idno>
	</analytic>
	<monogr>
		<title level="m">Hybrid approaches for automatic vowelization of arabic texts</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Arabic diacritization with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2281" to="2285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance analysis of google colaboratory as a tool for accelerating deep learning applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Victor Medeiros Da Nóbrega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Nepomuceno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Bin</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hugo C De Albuquerque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Pedrosa Reboucas</forename><surname>Filho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="61677" to="61685" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Morphological, syntactic and diacritics rules for automatic diacritization of arabic sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Chennoufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azzeddine</forename><surname>Mazroui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of King Saud University-Computer and Information Sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="156" to="163" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Arabic diacritization: Stats, rules, and hacks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Arabic Natural Language Processing Workshop</title>
		<editor>Kareem Darwish, Hamdy Mubarak, and Ahmed Abdelali</editor>
		<meeting>the Third Arabic Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Arabic text diacritization using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Tuffaha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Bara&amp;apos; Al-Jawarneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Ayyoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCAIS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shakkil: an automatic diacritization system for modern standard arabic texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amany</forename><surname>Fashwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Alansary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Arabic Natural Language Processing Workshop</title>
		<meeting>the Third Arabic Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="84" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluation of gated recurrent unit in arabic diacritization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajae</forename><surname>Moumen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raddouane</forename><surname>Chiheb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rdouan</forename><surname>Faizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdellatif</forename><forename type="middle">El</forename><surname>Afia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications (IJACSA)</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Highly effective arabic diacritization using sequence to sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younes</forename><surname>Samih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2390" to="2395" />
		</imprint>
		<respStmt>
			<orgName>Long and Short Papers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arfath</forename><surname>Pasha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Al-Badrashiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Eskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Pooleery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1094" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving arabic diacritization through syntactic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anas</forename><surname>Shahrour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salam</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1309" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lrec</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Blocknormalized gradient method: An empirical study for training deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<idno>ar- Xiv:1707.04822</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tashkeela: Novel corpus of arabic vocalized texts, data for autodiacritization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taha</forename><surname>Zerrouki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Balla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data in brief</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">147</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Arabic diacritic restoration approach based on maximum entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="276" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
