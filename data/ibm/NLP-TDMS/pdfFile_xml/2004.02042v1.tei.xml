<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ObjectNet Dataset: Reanalysis and Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
							<email>aliborji@gmail.com*</email>
						</author>
						<title level="a" type="main">ObjectNet Dataset: Reanalysis and Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Barbu et al. introduced a dataset called ObjectNet which includes objects in daily life situations. They showed a dramatic performance drop of the state of the art object recognition models on this dataset. Due to the importance and implications of their results regarding generalization ability of deep models, we take a second look at their findings. We highlight a major problem with their work which is applying object recognizers to the scenes containing multiple objects rather than isolated objects. The latter results in around 20-30% performance gain using our code. Compared with the results reported in the ObjectNet paper, we observe that around 10-15% of the performance loss can be recovered, without any test time data augmentation. In accordance with Barbu et al.'s conclusions, however, we also conclude that deep models suffer drastically on this dataset. Thus, we believe that ObjectNet remains a challenging dataset for testing the generalization power of models beyond datasets on which they have been trained. * Code and data is available at: https://github.com/aliborji/ObjectNetReanalysis.git 2 CNNs are not invariant to translation but are equivariant to it. 3 This dataset, however, has it own biases. It consists of indoor objects that are available to many people, are mobile, are not too large, too small, fragile or dangerous. 4 Unlike some benchmarks (e.g. ImageNet) that hide their test images, images in ObjectNet dataset will be publicly available. See http://objectnet.dev. 5 They mean object recognizers!</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object recognition is arguably the most important problem at the heart of computer vision. Application of an already known convolutional neural network architecture (CNN) known as LeNet <ref type="bibr" target="#b0">[1]</ref>, albeit with new tips and tricks <ref type="bibr" target="#b1">[2]</ref>, revolutionized not only computer vision but also several other areas including machine learning, natural language processing, time series prediction, etc. With the initial excitement gradually damping, researchers have started to study the shortcomings of deep learning models and question their generalization power. From prior research (e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>), we already know that CNNs: a) often fail when applied to transformed versions of the same object. In other words, they are not invariant to transformations such as translation 2 , in-plane and in-depth rotation, scale, lighting, and occlusion, b) lack out of distribution generalization. Even after being exposed to so many different instances of the same object category they are not good at learning that concept. In stark contrast, humans are able to generalize only from few examples, and c) are sensitive to small image perturbations (i.e. adversarial examples <ref type="bibr" target="#b4">[5]</ref>).</p><p>Several datasets have been proposed in the past to train and test deep models and to study their generalization ability (e.g. ImageNet <ref type="bibr" target="#b5">[6]</ref>, CIFAR <ref type="bibr" target="#b6">[7]</ref>, NORB <ref type="bibr" target="#b7">[8]</ref>, iLab20M <ref type="bibr" target="#b8">[9]</ref>). In a recent effort, Barbu et al. introduced a dataset called ObjectNet which has less bias than other datasets <ref type="bibr" target="#b2">3</ref> and is supposed to be used solely as a test set <ref type="bibr" target="#b3">4</ref> . Objects in this dataset are pictured by Mechanical Turk workers using a mobile app in a variety of backgrounds, rotations, and imaging viewpoints. It contains 50,000 images from 313 categories, out of which 113 are in common with the ImageNet, and comes with a licence that disallows the researchers to finetune models on it. <ref type="bibr">Barbu et al.</ref> find that the state of the art object detectors 5 perform drastically lower than their corresponding performance <ref type="figure">Figure 1</ref>: Sample images from the ObjectNet dataset from chairs, teapots and t-shirts categories, along with their corresponding object bounding boxes. The left panel shows objects in the ImageNet dataset. As it can be seen ImageNet scenes often contain a single isolated object. There are also many images in the ImageNet dataset with multiple objects, but we speculate that scenes in the ObjectNet dataset have more number of images on average <ref type="bibr" target="#b5">[6]</ref>. This figure is a modified version of the <ref type="figure" target="#fig_0">Figure 2</ref> in <ref type="bibr" target="#b9">[10]</ref>. on the ImageNet dataset (about 40-45% drop). Here, we revisit the Barbu et al.'s study and seek to answer how much performance of models drops on this dataset compared with their performance on ImageNet. Due to this, here we limit our analysis to the 113 overlapped categories.</p><p>Barbu et al.'s work is a great contribution to the field to answer how well object recognition models generalize to the real world situations. It, however, suffers from a major flaw which is making no distinction between "object recognition" and "object detection". They use the term "object detector" to refer to object recognition models. This bring along several concerns:</p><p>• Instead of applying object recognition models to individual object bounding boxes, they apply them to cluttered scenes containing multiple objects. Alternatively, they could have run object detection models on scenes and measured detection performance (i.e. mean average precision; mAP).</p><p>• Object detection and object recognition are two distinct, but related, tasks. Each has its own models, datasets, and evaluation measures. For example, as shown in <ref type="figure">Fig. 1</ref>, images in object recognition datasets (e.g. ImageNet <ref type="bibr" target="#b5">[6]</ref>) often contain a single object, usually from a closeup view, whereas scenes in object detection datasets (e.g. MS COCO <ref type="bibr" target="#b10">[11]</ref>) have multiple objects. Due to this, object characteristics in the two types of datasets might be different. For example, objects are often smaller in detection datasets compared to recognition datasets <ref type="bibr" target="#b11">[12]</ref>. • It is true that ImageNet also includes a fair amount of images that have more than one object, but it appears that ObjectNet images are more cluttered and have higher number of objects, although we have not quantified this since this dataset does not come with object-level labels. This discussion is also related to the difference between "scene understanding" and "object recognition". To understand a complex scene, we look around, fixate on individual objects to recognize them, and accumulate information over fixations to perform more complex tasks such as answering a question or describing an event. • One might argue that the top-5 metric somewhat takes care of scenes with multiple objects.</p><p>While this might be true to some degree, it certainly does not entail that it is better to train or test object recognition models on scenes with multiple objects. One problem is labeling such images. For example, a street scene with a car, a pedestrian, trees and buildings is given only one label which adds noise to the data. Another problem is invariance. Given that individual objects can undergo many transformations, feeding scenes with multiple objects to models exacerbates the problem and leads to combinatorially much more variations which are harder to learn. A better approach would be focusing on individual objects. As humans, we also move our eyes around and place our fovea on one object at a time to recognize it, using information from our peripheral vision. The analogue of this task still does not exist in computer vision. This task is reminiscent of object detection but there are subtle differences which will be elaborated on later in the discussion section.</p><p>Here, we first annotate the objects in the ObjectNet scenes and then apply a number of deep object recognition models to only object bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments and Results</head><p>We employ six deep models including AlexNet <ref type="bibr" target="#b1">[2]</ref>, VGG-19 <ref type="bibr" target="#b12">[13]</ref>, GoogLeNet <ref type="bibr" target="#b13">[14]</ref>, ResNet-152 <ref type="bibr" target="#b14">[15]</ref> Inception-v3 <ref type="bibr" target="#b15">[16]</ref>, and MNASNet <ref type="bibr" target="#b16">[17]</ref>. AlexNet, VGG-19, and GoogLeNet have also been employed in the ObjectNet paper <ref type="bibr" target="#b9">[10]</ref>. We use the pytorch implementation of these models <ref type="bibr" target="#b5">6</ref> . Notice that the code from the ObjectNet paper is still not available. Due to possible inconsistency in our code and the code in ObjectNet as well as different data processing methods, in addition to bounding boxes, we also run the models on the entire scenes. This allows us to study whether and how much performance varies across these two conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bounding box annotation</head><p>The 113 categories of the ObjectNet dataset, overlapped with the ImageNet, contain 18,574 images in total. On this subset, the average number of images per category is 164.4 (min = 55, max = 284). <ref type="figure" target="#fig_0">Fig. 2</ref> shows the distribution of number of images per category on this dataset. We drew 7 a bounding box around the object corresponding to the category label of each image. If there were multiple objects from that category we tried to included all of them in the bounding box (e.g. chairs around a table). Some example scenes and their corresponding bounding boxes are given in <ref type="figure">Fig. 1</ref>. <ref type="figure" target="#fig_1">Fig. 3</ref> shows an overlay of our results on the same figure from the ObjectNet paper. As it can be seen, applying models to boxes instead of the entire scene improves the performance about 10%, but sill much lower than the results over the ImageNet dataset. The gap, however, is narrower now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Results</head><p>Since the code of <ref type="bibr" target="#b9">[10]</ref> is not available, we could not run the exact pipeline used by them on the bounding boxes. It is possible that they might have performed a different data normalization or test  , VGG-19 <ref type="bibr" target="#b12">[13]</ref>, and ResNet-152 <ref type="bibr" target="#b14">[15]</ref> on bounding boxes are shown in blue. As it can be seen, applying models to boxes instead of the entire scene improves the performance about 10%, but sill much lower than results over the ImageNet dataset (overlap cases). Note: This is not the best possible score on this dataset since we are not performing test time data augmentation. Using the original code in the ObjectNet paper with bounding boxes will likely improve this results. This figure is a modified version of the <ref type="figure">Figure 1</ref> in <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20-30% performance gain</head><p>Recognizers <ref type="figure">Figure 4</ref>: Results of our analysis by running six deep models on bounding boxes and whole images over the ObjectNet dataset (Corresponding to the overlap case in <ref type="figure" target="#fig_1">Fig. 3</ref>). Models perform significantly better on boxes than full images.</p><p>time data augmentation 8 to achieve better results. To remedy this, we also applied models to the whole image to study how much performance varies. Results are shown in <ref type="figure">Fig. 4</ref>. We find that:</p><p>1. Focusing on a small image region containing only a single object increases the performance significantly by around 20-30% across all tested models. 2. Our results on the whole scene are lower than Barbu et al. <ref type="bibr">'</ref>s results (which are also on the whole scene). This entails that applying their code to bounding boxes will likely improve the performance even more than our results using boxes. Assuming 25% gain in performance on top of their best results, when using boxes, will still not close the gap in performance. Please see <ref type="figure" target="#fig_1">Fig. 3</ref>. We will discuss this further in the next section.</p><p>Break down of performance over 113 categories for each of the 6 tested models is shown in <ref type="figure">Fig. 5</ref> (over isolated objects) and <ref type="figure">Fig. 6</ref> (over full image). Interestingly, in both cases, almost all models (except the GoogLeNet on isolated objects and the AlexNet on full image) perform the best over the safety pin category. Inspecting the images from this class, we found that they have a single safety pin often hold by a person. The same story is true about the banana class which is the second easy category using the bounding boxes. This object becomes much harder to recognize when using the full image (26.88% vs. 70.3% using boxes) which highlights the benefit of applying models to isolated objects rather than scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion and Conclusion</head><p>Our investigation reveals that deep object recognition models perform significantly better when applied to isolated objects rather than scenes (around 20-30% increase in performance). The reason behind this is two fold. First, there is less variability in single objects compared to scenes containing those objects. Second, deep models used here have been trained on ImageNet images which are less cluttered compared to the ObjectNet images. We anticipate that training models from scratch on large scale datasets that contain isolated objects will likely result in even higher accuracy.</p><p>Assuming around 30% increase in performance (at best) over the <ref type="bibr">Barbu</ref>   means that ObjectNet is significantly much harder. It covers a wider range of variations than ImageNet including object instances, viewpoints, rotations, occlusions, etc which pushes the limits of object recognition in both humans and machines. Hence, despite its limitations and biases, ObjectNet is indeed a great resource to test models in realistic situations.</p><p>Throughout the annotation process of ObjectNet images, we came across the following observations:</p><p>• Some objects look very different when they are in motion (e.g. the fan in <ref type="figure">Fig. 8; row 4)</ref> • Some objects appear different under the shadow of other objects (e.g. the hammer in <ref type="figure">Fig. 8</ref>; row 4) • Some object instances look very different from the typical instances in the same class (e.g. the helmet in <ref type="figure">Fig. 8</ref>; row 5, the orange in <ref type="figure">Fig. 7</ref>; row 5) • Some objects can be recognized only by reading their labels (e.g. the pet food container in <ref type="figure">Fig. 7</ref>; row 2) • Some images have wrong labels (e.g. the pillow in <ref type="figure">Fig. 7</ref>; row 2, the skirt in <ref type="figure">Fig. 7</ref>; row 1, the tray in <ref type="figure">Fig. 8</ref>; row 2.) • Some objects are extremely difficult to be recognized by humans (e.g. the tennis racket in <ref type="figure">Fig. 8</ref>; row 4, the shovel in <ref type="figure">Fig. 7</ref>; row 4, and the tray in <ref type="figure">Fig. 7</ref>; row 1) • In many images, objects are occluded by hands holding them (e.g. the sock and the shovel in <ref type="figure">Fig. 7</ref>; row 4) • Some objects are hard to recognize in dim light (e.g. the printer in <ref type="figure">Fig. 7</ref>; row 2) • Some categories are often confused with other categories, for example: (bath towel, bed sheet, full sized towel, dishrag or hand towel), (sandal, dress shoe (men), running shoe), (t-shirt, dress, sweater, suit jacket, skirt), (ruler, spatula, pen, match), (padlock, combination lock), and (envelope, letter).</p><p>We foresee at least four directions for future work in this area:</p><p>1. First, it would be interesting to see how well state of the art object detectors (e.g. Faster RCNN <ref type="bibr" target="#b17">[18]</ref>) perform on this dataset (e.g. over classes overlapped with the MSCOCO dataset). We expect a big drop in detection performance since recognition is still the main bottleneck in object detection <ref type="bibr" target="#b18">[19]</ref>. 2. Second, measuring human performance on ObjectNet dataset will provide a baseline for gauging model performance. Barbu et al. report a human performance of around 95% on this dataset (via a pilot study) when subjects are asked to mention the objects that are present in the scene. This task, however, is different than recognizing isolated objects out of context similar to the regime that is considered here (i.e. similar to rapid scene categorization tasks). In addition, error patterns of models and humans (rather than just raw accuracy measures) will inform us about the mechanisms of object recognition in both humans and machines. It could be that models work in a completely different fashion than the human visual system. For instance, unlike CNNs, we are able to discern the foreground from the image background during recognition. This hints towards an interplay and feedback loop between recognition and segmentation that is currently missing in CNNs. Finally, we are invariant to image transformation only partially, therefore it may not make sense to desire models that are fully invariant (e.g. invariance to 360 degree in-plane rotation). 3. Third, and related to the second, is the role of context in object recognition. Context is a two-edge sword. On the one hand, using it may lead to relying on trivial correlations that may not always happen at the test time. For example, relying on the fact that a keyboard always appears next to a monitor, may lead to occasional failures when a model is tested on an isolated keyboard. A better example is adversarial patches <ref type="bibr" target="#b19">[20]</ref> where training a model on objects augmented with small random patches forces the model to rely on those features and hence get fooled when tested on other objects with the same patch. On the other hand, completely discarding the context also is a not wise for two reasons. First, the great success of CNNs for object recognition and scene segmentation is attributed to their ability to exploit visual context. Second, as humans we also heavily rely on surrounding visual context. Future research, should further investigate the role of context in object recognition and how best to exploit it. <ref type="bibr" target="#b3">4</ref>. Fourth, to operationalize the third item above, we propose a new task which is recognizing objects in cluttered scenes containing multiple objects (See <ref type="bibr" target="#b18">[19]</ref> for an example study). This task resembles object detection but it is actually different. First, here the ground truth object bounding boxes are given and the task is to correctly label them (i.e. no need to find objects). Second, the evaluation measure is accuracy which is easier to interpret than mean average precision. This task is also different than the current object recognition setup in which context is usually discarded. Existing object detection datasets, such as MS COCO <ref type="bibr" target="#b10">[11]</ref>, can be utilized to evaluate models built for solving this task (i.e. instead of detecting objects the aim is to recognize isolated objects confined in bounding boxes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Appendix</head><p>Here, we show the easiest and hardest objects for the ResNet-152 model over some categories.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Frequency of the images per category over the 113 categories of the ObjectNet dataset overlapped with the ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Performance of the state of the art object recognition models over the ObjectNet dataset. Results of our analysis by applying AlexNet<ref type="bibr" target="#b1">[2]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 Figure 5 : 6 Figure 6 :</head><label>5566</label><figDesc>et al.'s results using bounding boxes, still leaves a large gap of at least 15% between ImageNet and ObjectNet performances which 8 Such as rotation, scale, color jittering, cropping, etc. Performance of models on object bounding boxes Performance of models over the entire image (full image)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )Figure 9 : 12 (Figure 10 : 13 (Figure 11 :</head><label>a912101311</label><figDesc>Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Correctly classified and misclassified examples from the Alarm clock class by the ResNet model. a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Correctly classified and misclassified examples from the Banana class by the ResNet model. a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Correctly classified and misclassified examples from the Band-Aid class by the ResNet model. (a) Correctly classified; highest confidences. Only three benchs were correctly classified. (b) Misclassified; highest confidences (c) Misclassified; lowest confidences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 12 :</head><label>12</label><figDesc>Correctly classified and misclassified examples from the Bench class by the ResNet model. 15 (a) Correctly classified; highest confidences. Only two plates were correctly classified. (b) Misclassified; highest confidences (c) Misclassified; lowest confidences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 13 :Figure 14 :Figure 15 : 18 (Figure 16 :Figure 17 :Figure 18 :Figure 19 :Figure 20 :</head><label>131415181617181920</label><figDesc>Correctly classified and misclassified examples from the Plate class by the ResNet model. 16 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Correctly classified and misclassified examples from the Broom class by the ResNet model.(c) Misclassified; highest confidences (d) Misclassified; lowest confidences Correctly classified and misclassified examples from the Candle class by the ResNet model. a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Correctly classified and misclassified examples from the Fan class by the ResNet model. 19 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Correctly classified and misclassified examples from the Ruler class by the ResNet model. 20 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Correctly classified and misclassified examples from the Safety-pin class by the ResNet model. 21 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Correctly classified and misclassified examples from the Teapot class by the ResNet model. 22 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Correctly classified and misclassified examples from the TV class by the ResNet model.(c) Misclassified; highest confidences (d) Misclassified; lowest confidences Figure 21: Correctly classified and misclassified examples from the Sock class by the ResNet model. 24 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Figure 22: Correctly classified and misclassified examples from the Sunglasses class by the ResNet model. 25 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Figure 23: Correctly classified and misclassified examples from the Tie class by the ResNet model. 26 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Figure 24: Correctly classified and misclassified examples from the Mug class by the ResNet model. 27 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Figure 25: Correctly classified and misclassified examples from the Hammer class by the ResNet model. 28 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidencesFigure 26: Correctly classified and misclassified examples from the Bicycle class by the ResNet model. 29 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidencesFigure 27: Correctly classified and misclassified examples from the Cellphone class by the ResNet model. 30 (a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences (c) Misclassified; highest confidences (d) Misclassified; lowest confidences Figure 28: Correctly classified and misclassified examples from the Chair class by the ResNet model.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://pytorch.org/docs/stable/torchvision/models.html<ref type="bibr" target="#b6">7</ref> The annotation tools include: https://github.com/aliborji/objectAnnotation and https:// github.com/tzutalin/labelImg.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Correctly classified; highest confidences (b) Correctly classified; lowest confidences</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Why do deep convolutional networks generalize so poorly to small image transformations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azulay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">184</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet?</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ilab-20m: A large-scale controlled object dataset to investigate deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2221" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9448" to="9458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Empirical upper-bound in object detection and more</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Iranmanesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12451</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adversarial patch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
