<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhao</surname></persName>
							<email>yazhao@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Zhejiang Provincial Key Laboratory of Service Robots Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
							<email>ruixu@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Zhejiang Provincial Key Laboratory of Service Robots Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Zhejiang Provincial Key Laboratory of Service Robots Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3338533.3366579</idno>
					<note>ACM Reference Format: Ya Zhao, Rui Xu, and Mingli Song. 2019. A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading. In ACM Multimedia Asia (MMAsia &apos;19), December 15-18, 2019, Beijing, China. ACM, New York, NY, USA, 6 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Machine translation</term>
					<term>Computer vision</term>
					<term>Neural networks KEYWORDS lip reading, datasets, multimodal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lip reading aims at decoding texts from the movement of a speaker's mouth. In recent years, lip reading methods have made great progress for English, at both word-level and sentence-level. Unlike English, however, Chinese Mandarin is a tone-based language and relies on pitches to distinguish lexical or grammatical meaning, which significantly increases the ambiguity for the lip reading task. In this paper, we propose a Cascade Sequence-to-Sequence Model for Chinese Mandarin (CSSMCM) lip reading, which explicitly models tones when predicting sentence. Tones are modeled based on visual information and syntactic structure, and are used to predict sentence along with visual information and syntactic structure. In order to evaluate CSSMCM, a dataset called CMLR (Chinese Mandarin Lip Reading) is collected and released, consisting of over 100,000 natural sentences from China Network Television website. When trained on CMLR dataset, the proposed CSSMCM surpasses the performance of state-of-the-art lip reading frameworks, which confirms the effectiveness of explicit modeling of tones for Chinese Mandarin lip reading.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>speech recognition offers an alternative way to understand speech. Besides, lip reading has practical potential in improved hearing aids, security, and silent dictation in public spaces. Lip reading is essentially a difficult problem, as most lip reading actuations, besides the lips and sometimes tongue and teeth, are latent and ambiguous. Several seemingly identical lip movements can produce different words.</p><p>Thanks to the recent development of deep learning, Englishbased lip reading methods have made great progress, at both wordlevel <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> and sentence-level <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>. However, as the language of the most number of speakers, there is only a little work for Chinese Mandarin lip reading in the multimedia community. Yang et al. <ref type="bibr" target="#b13">[14]</ref> present a naturally-distributed large-scale benchmark for Chinese Mandarin lip-reading in the wild, named LRW-1000, which contains 1,000 classes with 718,018 samples from more than 2,000 individual speakers. Each class corresponds to the syllables of a Mandarin word composed of one or several Chinese characters. However, they perform only word classification for Chinese Mandarin lip reading but not at the complete sentence level. LipCH-Net <ref type="bibr" target="#b14">[15]</ref> is the first paper aiming for sentence-level Chinese Mandarin lip reading. LipCH-Net is a two-step end-to-end architecture, in which two deep neural network models are employed to perform the recognition of Picture-to-Pinyin (mouth motion pictures to pronunciations) and the recognition of Pinyin-to-Hanzi (pronunciations to texts) respectively. Then a joint optimization is performed to improve the overall performance.</p><p>Belong to two different language families, English and Chinese Mandarin have many differences. The most significant one might be that: Chinese Mandarin is a tone language, while English is not. The tone is the use of pitch in language to distinguish lexical or grammatical meaning -that is, to distinguish or to inflect words <ref type="bibr" target="#b0">1</ref> . Even two words look the same on the face when pronounced, they can have different tones, thus have different meanings. For example, even though "练习" (which means practice) and "联系" (which means contact) have different meanings, but they have the same mouth movement. This increases ambiguity when lip reading. So the tone is an important factor for Chinese Mandarin lip reading.</p><p>Based on the above considerations, in this paper, we present CSSMCM, a sentence-level Chinese Mandarin lip reading network, which contains three sub-networks. Same as <ref type="bibr" target="#b14">[15]</ref>, in the first subnetwork, pinyin sequence is predicted from the video. Different from <ref type="bibr" target="#b14">[15]</ref>, which predicts pinyin characters from video, pinyin is taken as a whole in CSSMCM, also known as syllables. As we know, Mandarin Chinese is a syllable-based language and syllables are their logical unit of pronunciation. Compared with pinyin characters, syllables are a longer linguistic unit, and can reduce the difficulty of syllable choices in the decoder by sequence-to-sequence attention-based models <ref type="bibr" target="#b16">[17]</ref>. Chen et al. <ref type="bibr" target="#b5">[6]</ref> find that there might be a relationship between the production of lexical tones and the visible movements of the neck, head, and mouth. Motivated by this observation, in the second sub-network, both video and pinyin sequence is used as input to predict tone. Then in the third subnetwork, video, pinyin, and tone sequence work together to predict the Chinese character sequence. At last, three sub-networks are jointly finetuned to improve overall performance.</p><p>As there is no public sentence-level Chinese Mandarin lip reading dataset, we collect a new Chinese Mandarin Lip Reading dataset called CMLR based on China Network Television broadcasts containing talking faces together with subtitles of what is said.</p><p>In summary, our major contributions are as follows.</p><p>• We argue that tone is an important factor for Chinese Mandarin lip reading, which increases the ambiguity compared with English lip reading. Based on this, a three-stage cascade network, CSSMCM, is proposed. The tone is inferred by video and syntactic structure, and are used to predict sentence along with visual information and syntactic structure. • We collect a 'Chinese Mandarin Lip Reading' (CMLR) dataset, consisting of over 100,000 natural sentences from national news program "News Broadcast". The dataset will be released as a resource for training and evaluation. • Detailed experiments on CMLR dataset show that explicitly modeling tone when predicting Chinese sentence performs a lower character error rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED METHOD</head><p>In this section, we present CSSMCM, a lip reading model for Chinese Mandarin. As mention in Section 1, pinyin and tone are both important for Chinese Mandarin lip reading. Pinyin represents how to pronounce a Chinese character and is related to mouth movement. Tone can alleviate the ambiguity of visemes (several speech sounds that look the same) to some extent and can be inferred from visible movements. Based on this, the lip reading task is defined as follow:</p><formula xml:id="formula_0">• = σ σ ,</formula><formula xml:id="formula_1">P(y|x) = p t P(y|p, t, x)P(t |p, x)P(p|x),<label>(1)</label></formula><p>The meaning of these symbols is given in <ref type="table" target="#tab_0">Table 1</ref>. As shown in Equation <ref type="formula" target="#formula_1">(1)</ref>, the whole problem is divided into three parts, which corresponds to pinyin prediction, tone prediction, and character prediction separately. Each part will be described in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pinyin Prediction Sub-network</head><p>The pinyin prediction sub-network transforms video sequence into pinyin sequence, which corresponds to P(p|x) in Equation <ref type="formula" target="#formula_1">(1)</ref>. This sub-network is based on the sequence-to-sequence architecture with attention mechanism <ref type="bibr" target="#b1">[2]</ref>. We name the encoder and decoder the video encoder and pinyin decoder, for the encoder process video sequence, and the decoder predicts pinyin sequence. The input video sequence is first fed into the VGG model <ref type="bibr" target="#b3">[4]</ref> to extract visual feature. The output of conv5 of VGG is appended with global average pooling <ref type="bibr" target="#b11">[12]</ref> to get the 512-dim feature vector. Then the 512-dim feature vector is fed into video encoder. The video encoder can be denoted as:</p><formula xml:id="formula_2">(h v e ) i = GRU v e ((h v e ) i−1 , VGG(x i )).<label>(2)</label></formula><p>When predicting pinyin sequence, at each timestep i, video encoder outputs are attended to calculate a context vector c v i :</p><formula xml:id="formula_3">(h p d ) i = GRU p d ((h p d ) i−1 , p i−1 ),<label>(3)</label></formula><formula xml:id="formula_4">c v i = h v e · Attention v p ((h p d ) i , h v e ),<label>(4)</label></formula><formula xml:id="formula_5">P(p i |p &lt;i , x) = softmax(MLP((h p d ) i , c v i )).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tone Prediction Sub-network</head><p>As shown in Equation <ref type="formula" target="#formula_1">(1)</ref>, tone prediction sub-network (P(t |p, x)) takes video and pinyin sequence as inputs and predict corresponding tone sequence. This problem is modeled as a sequence-tosequence learning problem too. The corresponding model architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In order to take both video and pinyin information into consideration when producing tone, a dual attention mechanism <ref type="bibr" target="#b7">[8]</ref> is employed. Two independent attention mechanisms are used for video and pinyin sequence. Video context vectors c v i and pinyin context vectors c p i are fused when predicting a tone character at each decoder step. The video encoder is the same as in Section 2.1 and the pinyin encoder is:</p><formula xml:id="formula_6">(h p e ) i = GRU p e ((h p e ) i−1 , p i−1 ). (6)</formula><p>The tone decoder takes both video encoder outputs and pinyin encoder outputs to calculate context vector, and then predicts tones:</p><formula xml:id="formula_7">(h t d ) i = GRU t d ((h t d ) i−1 , t i−1 ),<label>(7)</label></formula><p>c</p><formula xml:id="formula_8">v i = h v e · Attention v t ((h t d ) i , h v e ),<label>(8)</label></formula><formula xml:id="formula_9">c p i = h p e · Attention p t ((h t d ) i , h p e ),<label>(9)</label></formula><formula xml:id="formula_10">P(t i |t &lt;i , x, p) = softmax(MLP((h t d ) i , c v i , c p i )).<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Character Prediction Sub-network</head><p>The character prediction sub-network corresponds to P(y|p, t, x) in Equation <ref type="formula" target="#formula_1">(1)</ref>. It considers all the pinyin sequence, tone sequence and video sequence when predicting Chinese character. Similarly, we also use attention based sequence-to-sequence architecture to model this equation. Here the attention mechanism is modified into triplet attention mechanism:</p><formula xml:id="formula_11">(h c d ) i = GRU c d ((h c d ) i−1 , y i−1 ),<label>(11)</label></formula><formula xml:id="formula_12">c v i = h v e · Attention v c ((h c d ) i , h v e ),<label>(12)</label></formula><formula xml:id="formula_13">c p i = h p e · Attention p c ((h c d ) i , h p e ),<label>(13)</label></formula><formula xml:id="formula_14">c t i = h t e · Attention t c ((h c d ) i , h t e ),<label>(14)</label></formula><formula xml:id="formula_15">P(c i |c &lt;i , x, p, t) = softmax(MLP((h c d ) i , c v i , c p i , c t i )).<label>(15)</label></formula><p>For the following needs, the formula of tone encoder is also listed as follows:</p><formula xml:id="formula_16">(h t e ) i = GRU t e ((h t e ) i−1 , t i−1 ).<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">CSSMCM Architecture</head><p>The architecture of the proposed approach is demonstrated in <ref type="figure" target="#fig_2">Figure 3</ref>. For better display, the three attention mechanisms are not shown in the figure. During the training of CSSMCM, the outputs of pinyin decoder are fed into pinyin encoder, the outputs of tone decoder into tone encoder: We replace Equation <ref type="formula">(6)</ref> with Equation <ref type="formula" target="#formula_1">(17)</ref>, Equation <ref type="formula" target="#formula_1">(16)</ref> with Equation (18). Then, the three sub-networks are jointly trained and the overall loss function is defined as follows:</p><formula xml:id="formula_17">(h p e ) i = GRU p e ((h p e ) i−1 , MLP((h t d ) i , c v i , c p i )),<label>(17)</label></formula><formula xml:id="formula_18">(h t e ) i = GRU t e ((h p e ) i−1 , MLP((h c d ) i , c v i , c p i , c t i )).<label>(18)</label></formula><formula xml:id="formula_19">• = σ σ , , ,<label>( | )</label></formula><formula xml:id="formula_20">L = L p + L t + L c ,<label>(19)</label></formula><p>where L p , L t and L c stand for loss of pinyin prediction sub-network, tone prediction sub-network and character prediction sub-network respectively, as defined below.</p><formula xml:id="formula_21">L p = − i log P(p i |p &lt;i , x), L t = − i log P(t i |t &lt;i , x, p), L c = − i log P(c i |c &lt;i , x, p, t).</formula><p>(20)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training Strategy</head><p>To accelerate training and reduce overfitting, curriculum learning <ref type="bibr" target="#b7">[8]</ref> is employed. The sentences are grouped into subsets according to the length of less than 11, 12-17, 18-23, more than 24 Chinese characters. Scheduled sampling proposed by <ref type="bibr" target="#b2">[3]</ref> is used to eliminate the discrepancy between training and inference. At the training stage, the sampling rate from the previous output is selected from 0.7 to 1. Greedy decoder is used for fast decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATASET</head><p>In this section, a three-stage pipeline for generating the Chinese Mandarin Lip Reading (CMLR) dataset is described, which includes video pre-processing, text acquisition, and data generation. This three-stage pipeline is similar to the method mentioned in <ref type="bibr" target="#b7">[8]</ref>, but considering the characteristics of our Chinese Mandarin dataset, we have optimized some steps and parts to generate a better quality lip reading dataset. The three-stage pipeline is detailed below. Video Pre-processing. First, national news program "News Broadcast" recorded between June 2009 and June 2018 is obtained from China Network Television website. Then, the HOG-based face detection method is performed <ref type="bibr" target="#b10">[11]</ref>, followed by an open source platform for face recognition and alignment. The video clip set of eleven different hosts who broadcast the news is captured. During the face detection step, using frame skipping can improve efficiency while ensuring the program quality.</p><p>Text Acquisition. Since there is no subtitle or text annotation in the original "News Broadcast" program, FFmpeg tools 2 are used to extract the corresponding audio track from the video clip set. Then through the iFLYTEK 3 ASR, the corresponding text annotation of the video clip set is obtained. However, there is some noise in these text annotation. English letters, Arabic numerals, and rare punctuation are deleted to get a more pure Chinese Mandarin lip reading dataset. Data Generation. The text annotation acquired in the previous step also contains timestamp information. Therefore, video clip set is intercepted according to these timestamp information, and then the corresponding word, phrase, or sentence video segment of the text annotation are obtained. Since the text timestamp information may have a few uncertain errors, some adjustments are made to the start frame and the end frame when intercepting the video segment. It is worth noting that through experiments, we found that using OpenCV 4 can capture clearer video segment than the FFmpeg tools.</p><p>Through the three-stage pipeline mentioned above, we can obtain the Chinese Mandarin Lip Reading (CMLR) dataset containing more than 100,000 sentences, 25,000 phrases, 3,500 characters. The dataset is randomly divided into training set, validation set, and test set in a ratio of 7:1:2. Details are listed in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Further details of the dataset and the download links can be found on the web page: https://www.vipazoo.cn/CMLR.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Implementation Details</head><p>The input images are 64 × 128 in dimension. Lip frames are transformed into gray-scale, and the VGG network takes every 5 lip frames as an input, moving 2 frames at each timestep. For all subnetworks, a two-layer bi-direction GRU <ref type="bibr" target="#b6">[7]</ref> with a cell size of 256 is used for the encoder and a two-layer uni-direction GRU with a cell size of 512 for the decoder. For character and pinyin vocabulary, we keep characters and pinyin that appear more than 20 times.</p><p>[sos], [eos] and [pad] are also included in these three vocabularies. The final vocabulary size is 371 for pinyin prediction sub-network, 8 for tone prediction sub-network (four tones plus a neutral tone), and 1,779 for character prediction sub-network.</p><p>The initial learning rate was 0.0001 and decreased by 50% every time the training error did not improve for 4 epochs. CSSMCM is implemented using pytorch library and trained on a Quadro 64C P5000 with 16GB memory. The total end-to-end model was trained for around 12 days. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Methods and Evaluation Protocol</head><p>We list here the compared methods and the evaluation protocol. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WAS:</head><p>The architecture used in <ref type="bibr" target="#b7">[8]</ref> without the audio input. The decoder output Chinese character at each timestep. Others keep unchanged to the original implementation.</p><p>LipCH-Net-seq: For a fair comparison, we use sequence-tosequence with attention framework to replace the Connectionist temporal classification (CTC) loss <ref type="bibr" target="#b9">[10]</ref> used in LipCH-Net <ref type="bibr" target="#b14">[15]</ref> when converting picture to pinyin.</p><p>CSSMCM-w/o video: To evaluate the necessity of video information when predicting tone, the video stream is removed when predicting tone and Chinese characters. In other word, video is only used when predicting the pinyin sequence. The tone is predicted from the pinyin sequence. Tone information and pinyin information work together to predict Chinese character.</p><p>We tried to implement the Lipnet architecture <ref type="bibr" target="#b0">[1]</ref> to predict Chinese character at each timestep. However, the model did not converge. The possible reasons are due to the way CTC loss works and the difference between English and Chinese Mandarin. Compared to English, which only contains 26 characters, Chinese Mandarin contains thousands of Chinese characters. When CTC calculates loss, it first adds blank between every character in a sentence, that causes the number of the blank label is far more than any other Chinese character. Thus, when Lipnet starts training, it predicts only the blank label. After a certain epoch, "的" character will occasionally appear until the learning rate decays to close to zero.</p><p>For all experiments, Character Error Rate (CER) and Pinyin Error Rate (PER) are used as evaluation metrics. CER is defined as ErrorRate = (S + D + I )/N , where S is the number of substitutions, D is the number of deletions, I is the number of insertions to get from the reference to the hypothesis and N is the number of words in the reference. PER is calculated in the same way as CER. Tone  Error Rate (TER) is also included when analyzing CSSMCM, which is calculated in the same way as above. <ref type="table" target="#tab_2">Table 3</ref> shows a detailed comparison between various sub-network of different methods. Comparing P2T and VP2T, VP2T considers video information when predicting the pinyin sequence and achieves a lower error rate. This verifies the conjecture of <ref type="bibr" target="#b5">[6]</ref> that the generation of tones is related to the motion of the head. In terms of overall performance, CSSMCM exceeds all the other architecture on the CMLR dataset and achieves 32.48% character error rate. It is worth noting that CSSMCM-w/o video achieves the worst result (42.23% CER) even though its sub-networks perform well when trained separately. This may be due to the lack of visual information to support, and the accumulation of errors. CSSMCM using tone information performs better compared to LipCH-Net-seq, which does not use tone information. The comparison results show that tone is important when lip reading, and when predicting tone, visual information should be considered. <ref type="table" target="#tab_3">Table 4</ref> shows some generated sentences from different methods. CSSMCM-w/o video architecture is not included due to its relatively lower performance. These are sentences other methods fail to predict but CSSMCM succeeds. The phrase "实惠" (which means affordable) in the first example sentence, has a tone of 2, 4 and its corresponding pinyin are shi, hui. WAS predicts it as "事 会" (which means opportunity). Although the pinyin prediction is correct, the tone is wrong. LipCH-Net-seq predicts "实惠" as "吃贵" (not a word), which have the same finals "ui" and the corresponding mouth shapes are the same. It's the same in the second example. "前, 天, 年" have the same finals and mouth shapes, but the tone is different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>These show that when predicting characters with the same lip shape but different tones, other methods are often unable to predict correctly. However, CSSMCM can leverage the tone information to predict successfully.</p><p>Apart from the above results, <ref type="table" target="#tab_4">Table 5</ref> also lists some failure cases of CSSMCM. The characters that CSSMCM predicts wrong are usually homophones or characters with the same final as the ground truth. In the first example, "价" and "下" have the same final, ia, while "一" and "医" are homophones in the second example. Unlike English, if one character in an English word is predicted wrong, the understanding of the transcriptions has little effect. However, if there is a character predicted wrong in Chinese words, it will greatly affect the understandability of transcriptions. In the second example, CSSMCM mispredicts "医学" ( which means medical) to "一水" (which means all). Although their first characters are pronounced the same, the meaning of the sentence changed from Now with the progress of medical science and technology in our country to It is now with the footsteps of China's Yishui Technology.  <ref type="figure" target="#fig_4">Figure 4 (b)</ref>, the diagonal trend of the video attention map got by CSSMCM is more obvious. The video attention is more focused where WAS predicts wrong, i.e. the area corresponding to "还向". Although WAS mistakenly predicts the "媒体" as "么体", the "媒体" and the "么体" have the same mouth shape, so the attention concentrates on the correct frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Attention Visualisation</head><p>It's interesting to mention that in <ref type="figure">Figure 5</ref>, when predicting the i-th character, attention is concentrated on the i + 1-th tone. This may be because attention is applied to the outputs of the encoder, which actually includes all the information from the previous i + 1 timesteps. The attention to the tone of i + 1-th timestep serves as the language model, which reduces the options for generating the character at i-th timestep, making prediction more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SUMMARY AND EXTENSION</head><p>In this paper, we propose the CSSMCM, a Cascade Sequence-to-Sequence Model for Chinese Mandarin lip reading. CSSMCM is designed to predicting pinyin sequence, tone sequence, and Chinese character sequence one by one. When predicting tone sequence, a dual attention mechanism is used to consider video sequence and YLGHRIUDPHV SUHGLFWHG&amp;KLQHVHVHQWHQFH (a) YLGHRIUDPHV SUHGLFWHG&amp;KLQHVHVHQWHQFH (b) <ref type="figure" target="#fig_4">Figure 4</ref>: Video-to-text alignment using CSSMCM (a) and WAS (b). SUHGLFWHGWRQHVHQWHQFH SUHGLFWHG&amp;KLQHVHVHQWHQFH <ref type="figure">Figure 5</ref>: Aligenment between output characters and predicted tone sequences using CSSMCM. pinyin sequence at the same time. When predicting the Chinese character sequence, a triplet attention mechanism is proposed to take all the video sequence, pinyin sequence, and tone sequence information into consideration. CSSMCM consistently outperforms other lip reading architectures on the proposed CMLR dataset.</p><p>Lip reading and speech recognition are very similar. In Chinese Mandarin speech recognition, there have been kinds of different acoustic representations like syllable initial/final approach, syllable initial/final with tone approach, syllable approach, syllable with tone approach, preme/toneme approach <ref type="bibr" target="#b4">[5]</ref> and Chinese Character approach <ref type="bibr" target="#b15">[16]</ref>. In this paper, the Chinese character is chosen as the output unit. However, we find that the wrongly predicted characters severely affect the understandability of transcriptions. Using larger output units, like Chinese words, maybe can alleviate this problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The tone prediction sub-network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The character prediction sub-network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The overall of the CSSMCM network. The attention module is omitted for sake of simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3 https://www.xfyun.cn/ 4 http://docs.opencv.org/2.4.13/modules/refman.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 (</head><label>4</label><figDesc>a) and Figure 4 (b) visualise the alignment of video frames and Chinese characters predicted by CSSMCM and WAS respectively. The ground truth sequence is "同时他还向媒体表示". Comparing Figure 4 (a) with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Symbol Definition</figDesc><table><row><cell cols="2">Symbol</cell><cell>Definition</cell></row><row><cell cols="2">GRU v e GRU p e , GRU p d GRU t e , GRU t d GRU y d Attention v p</cell><cell>GRU unit in video encoder GRU unit in pinyin encoder and pinyin decoder GRU unit in tone encoder and tone decoder GRU unit in character decoder attention between pinyin decoder and video en-</cell></row><row><cell></cell><cell></cell><cell>coder. The superscript indicates the encoder and</cell></row><row><cell></cell><cell></cell><cell>the subscript indicates the decoder.</cell></row><row><cell cols="2">x, y, p, t</cell><cell>video, character, pinyin, and tone sequence</cell></row><row><cell>i</cell><cell></cell><cell>timestep</cell></row><row><cell>h v e , h</cell><cell>p e , h t e</cell><cell>video encoder output, pinyin encoder output, tone</cell></row><row><cell></cell><cell></cell><cell>encoder output</cell></row><row><cell cols="2">c v , c p , c t</cell><cell>video content, pinyin content, tone content</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The CMLR dataset. Division of training, validation and test data; and the number of sentences, phrases and characters of each partition.</figDesc><table><row><cell>Set</cell><cell># sentences</cell><cell># phrases</cell><cell># characters</cell></row><row><cell>Train</cell><cell>71,452</cell><cell>22,959</cell><cell>3,360</cell></row><row><cell>Validation</cell><cell>10,206</cell><cell>10,898</cell><cell>2,540</cell></row><row><cell>Test</cell><cell>20,418</cell><cell>14,478</cell><cell>2,834</cell></row><row><cell>All</cell><cell>102,076</cell><cell>25,633</cell><cell>3,517</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The detailed comparison between CSSMCM and other methods on the CMLR dataset. V, P, T, C stand for video, pinyin, tone and character. V2P stands for the transformation from video sequence to pinyin sequence. VP2T represents the input are video and pinyin sequence and the output is sequence of tone. OVERALL means to combine the sub-networks and make a joint optimization.</figDesc><table><row><cell>Models</cell><cell>sub-network</cell><cell>CER</cell><cell>PER</cell><cell>TER</cell></row><row><cell>WAS</cell><cell>-</cell><cell>38.93%</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>V2P</cell><cell>-</cell><cell>27.96%</cell><cell>-</cell></row><row><cell>LipCH-Net-seq</cell><cell>P2C</cell><cell>9.88%</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>OVERALL</cell><cell cols="2">34.07% 39.52%</cell><cell>-</cell></row><row><cell></cell><cell>V2P</cell><cell>-</cell><cell>27.96%</cell><cell>-</cell></row><row><cell>CSSMCM-w/o video</cell><cell>P2T PT2C</cell><cell>-4.70 %</cell><cell>--</cell><cell>6.99% -</cell></row><row><cell></cell><cell>OVERALL</cell><cell cols="3">42.23% 46.67% 13.14%</cell></row><row><cell></cell><cell>V2P</cell><cell>-</cell><cell>27.96%</cell><cell>-</cell></row><row><cell>CSSMCM</cell><cell>VP2T VPT2C</cell><cell>-3.90%</cell><cell>--</cell><cell>6.14% -</cell></row><row><cell></cell><cell>OVERALL</cell><cell cols="3">32.48% 36.22% 10.95%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Examples of sentences that CSSMCM correctly predicts while other methods do not. The pinyin and tone sequence corresponding to the Chinese character sentence are also displayed together. GT stands for ground truth.</figDesc><table><row><cell>Method</cell><cell>Chinese Character Sentence</cell><cell>Pinyin Sequence</cell><cell>Tone Sequence</cell></row><row><cell>GT</cell><cell>既让老百姓得实惠</cell><cell>ji rang lao bai xing de shi hui</cell><cell>4 4 3 3 4 2 2 4</cell></row><row><cell>WAS</cell><cell>介项老百姓姓事会</cell><cell>jie xiang lao bai xing xing shi hui</cell><cell>4 4 3 3 4 4 4 4</cell></row><row><cell>LipCH-Net-seq</cell><cell>既让老百姓的吃贵</cell><cell>ji rang lao bai xing de chi gui</cell><cell>4 4 3 3 4 0 1 4</cell></row><row><cell>CSSMCM</cell><cell>既让老百姓得实惠</cell><cell>ji rang lao bai xing de shi hui</cell><cell>4 4 3 3 4 2 2 4</cell></row><row><cell>GT</cell><cell>有效应对当前半岛局势</cell><cell cols="2">you xiao ying dui dang qian ban dao ju shi 3 4 4 4 1 2 4 3 2 4</cell></row><row><cell>WAS</cell><cell>有效应对当天半岛趋势</cell><cell cols="2">you xiao ying dui dang tian ban dao qu shi 3 4 4 4 1 1 4 3 1 4</cell></row><row><cell>LipCH-Net-seq</cell><cell>有效应对党年半岛局势</cell><cell cols="2">you xiao ying dui dang nian ban dao ju shi 3 4 4 4 3 2 4 3 2 4</cell></row><row><cell>CSSMCM</cell><cell>有效应对当前半岛局势</cell><cell cols="2">you xiao ying dui dang qian ban dao ju shi 3 4 4 4 1 2 4 3 2 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Failure cases of CSSMCM.</figDesc><table><row><cell>GT</cell><cell>向全球价值链中高端迈进 xiang quan qiu jia zhi lian zhong gao duan mai jin</cell></row><row><cell>CSSMCM</cell><cell>向全球下试联中高端迈进 xiang quan qiu xia shi lian zhong gao duan mai jin</cell></row><row><cell>GT</cell><cell>随着我国医学科技的进步 sui zhe wo guo yi xue ke ji de jin bu</cell></row><row><cell>CSSMCM</cell><cell>随着我国一水科技的信步 sui zhe wo guo yi shui ke ji de jin bu</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://ffmpeg.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Freitas</surname></persName>
		</author>
		<title level="m">Lipnet: Sentence-level lipreading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate. international conference on learning representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Scheduled sampling for sequence prediction with recurrent Neural networks. neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">New methods in continuous Mandarin speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><forename type="middle">A</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Monkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUROSPEECH</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Seeing pitch: Visual information for lexical tones of Mandarin-Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Massaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="2356" to="2366" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip Reading Sentences in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Network In Network. international conference on learning representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep complementary bottleneck features for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2304" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06990</idno>
		<title level="m">LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding Pictograph with Facial Features: End-to-End Sentence-level Lip Reading of Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haigang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xili</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2019 : Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Comparison of Modeling Units in Sequence-to-Sequence Speech Recognition with the Transformer on Mandarin Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
	<note>international conference on neural information processing 2018</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Syllable-Based Sequenceto-Sequence Speech Recognition with the Transformer in Mandarin Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Interspeech</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="791" to="795" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
