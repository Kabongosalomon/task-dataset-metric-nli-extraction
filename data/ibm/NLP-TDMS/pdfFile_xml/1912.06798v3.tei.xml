<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Batch Memory for Embedding Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
							<email>xunwang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
							<email>haozhang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
							<email>whuang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
							<email>mscott@malong.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Batch Memory for Embedding Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mining informative negative instances are of central importance to deep metric learning (DML), however this task is intrinsically limited by mini-batch training, where only a mini-batch of instances is accessible at each iteration. In this paper, we identify a "slow drift" phenomena by observing that the embedding features drift exceptionally slow even as the model parameters are updating throughout the training process. This suggests that the features of instances computed at preceding iterations can be used to considerably approximate their features extracted by the current model. We propose a cross-batch memory (XBM) mechanism that memorizes the embeddings of past iterations, allowing the model to collect sufficient hard negative pairs across multiple mini-batches -even over the whole dataset. Our XBM can be directly integrated into a general pairbased DML framework, where the XBM augmented DML can boost performance considerably. In particular, without bells and whistles, a simple contrastive loss with our XBM can have large R@1 improvements of 12%-22.5% on three large-scale image retrieval datasets, surpassing the most sophisticated state-of-the-art methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26,</ref> 2], by a large margin. Our XBM is conceptually simple, easy to implement -using several lines of codes, and is memory efficient -with a negligible 0.2 GB extra GPU memory. Code is available at: https://github.com/MalongTech/ research-xbm. arXiv:1912.06798v3 [cs.LG] 21 Apr 2020 16 32 64 128 256 batch-size (log-scale) 45 50 55 60 65 70 Recall@1(%) MS Triplet Contrastive 16 32 64 128 256 batch-size (log-scale)</p><p>40 50 60 70 80 90 Recall@1(%) SOP In-shop VehicleID 0.0 0.01 0.05 0.1 0.2 0.5 1.0 memory ratio 40 50 60 70 80 90 Recall@1(%) SOP In-shop VehicleID</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep metric learning (DML) aims to learn an embedding space where instances from the same class are encouraged to be closer than those from different classes. As a fundamental problem in computer vision, DML has been applied to various tasks, including image retrieval <ref type="bibr" target="#b38">[39,</ref><ref type="bibr">12,</ref><ref type="bibr">7]</ref>, face recognition <ref type="bibr" target="#b37">[38]</ref>, zero-shot learning <ref type="bibr" target="#b46">[47,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b15">16]</ref>, visual tracking <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref> and person re-identification <ref type="bibr" target="#b43">[44,</ref><ref type="bibr">13]</ref>. * Equal contribution â€  Corresponding author A family of DML approaches are known as pair-based, whose objectives can be defined in terms of pair-wise similarities within a mini-batch, such as contrastive loss <ref type="bibr">[3]</ref>, triplet loss <ref type="bibr" target="#b28">[29]</ref>, lifted-structure loss <ref type="bibr" target="#b21">[22]</ref>, n-pairs loss <ref type="bibr" target="#b29">[30]</ref>, multi-similarity (MS) loss <ref type="bibr" target="#b36">[37]</ref> and etc. Moreover, most existing pair-based DML methods can be unified as weighting schemes under a General Pair Weighting (GPW) framework <ref type="bibr" target="#b36">[37]</ref>. The performance of pair-based methods heavily rely on their capability of mining informative negative pairs. To collect sufficient informative negative pairs from each mini-batch, many efforts have been devoted to improving the sampling schemes, which can be categorized into two main directions: (1) sampling informative minibatches based on global data distribution <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">9]</ref>; <ref type="bibr">(2)</ref> weighting informative pairs within each individual minibatch <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Various sophisticated sampling schemes have been developed, but the hard mining ability is inherently limited by the size of a mini-batch, which the number of possible training pairs depends on. Therefore, to improve the sampling scheme, it is straightforward to enlarge the minibatch, which can boost the performance of pair-based DML methods immediately. We demonstrate by experiments that the performance of pair-based approaches, such as contrastive loss <ref type="bibr">[3]</ref> and recent MS loss <ref type="bibr" target="#b36">[37]</ref>, can be improved strikingly when the mini-batch grows larger on large-scale datasets <ref type="figure" target="#fig_0">(Figure 1</ref>, left and middle). It is not surprising because the number of negative pairs grows quadratically w.r.t. the mini-batch size. However, simply enlarging a mini-batch is not an ideal solution to solve the hard mining problem due to two limitations: (1) the mini-batch size is limited by the GPU memory and computational cost; (2) a large mini-batch (e.g. 1800 used in <ref type="bibr" target="#b28">[29]</ref>) often requires cross-device synchronization, which is a challenging engineering task. A naive solution to collect abundant informative pairs is to compute the features of instances in the whole training set at each training iteration, and then search for hard negative pairs from the whole dataset. Obviously, this solution is extremely time consuming, especially for a large-scale dataset, but it inspired us to break the limit of mining hard negatives within a single mini-batch. In this paper, we identify an interesting "slow drift" phenomena that the embedding of an instance actually drifts at a relatively slow rate throughout the training process. It suggests that the deep features of a mini-batch computed at past iterations can considerably approximate to those extracted by current model. Based on the "slow drift" phenomena, we propose a cross-batch memory (XBM) module to record and update the deep features of recent mini-batches, allowing for mining informative examples across multiple minibatches. Our XBM can provide plentiful hard negative pairs by directly connecting each anchor in the current mini-batch with the embeddings from recent mini-batches.</p><p>Our XBM is conceptually simple, easy to implement and memory efficient. The memory module can be updated using a simple enqueue-dequeue mechanism by leveraging the computation-free features computed at the past iterations, with only about a negligible 0.2 GB of extra GPU memory utilized. More importantly, our XBM can be directly integrated into most existing pair-based methods with just several lines of codes, and can boost performance considerably. We evaluate our XBM with various conventional pairbased DML techniques on three widely used large-scale image retrieval datasets: Stanford Online Products (SOP) <ref type="bibr" target="#b21">[22]</ref>, In-shop Clothes Retrieval (In-shop) <ref type="bibr" target="#b19">[20]</ref>, and PKU Vehi-cleID (VehicleID) <ref type="bibr" target="#b18">[19]</ref>. In <ref type="figure" target="#fig_0">Figure 1</ref> (middle and right), our approach demonstrates excellent robustness and brings consistent performance improvements across all settings; under the same configurations, our XBM obtains extraordinary R@1 improvements on all three datasets compared with the corresponding pair-based methods (e.g. over 20% for contrastive loss). Furthermore, with our XBM, a simple contrastive loss can easily outperform the most state-of-the-art sophisticated methods, such as <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr">2]</ref>, by a large margin.</p><p>In parallel to our work, He et al. <ref type="bibr">[10]</ref> built a dynamic dictionary as a queue of preceding mini-batches to provide a rich set of negative samples for unsupervised learning (with a contrastive loss). However, unlike <ref type="bibr">[10]</ref> which uses a specific encoding network to compute the features of current mini-batch, our features are computed more efficiently by taking them directly from the forward of the current model with no additional computational cost. More importantly, to solve the problem of feature drift, He et al. designed a momentum update that slowly progresses the key encoder to ensure the consistency between different iterations, while we identify the "slow drift" phenomena which suggests that the features can become stable by themselves when the early phase of training finishes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Pair-based DML. Pair-based DML methods can be optimized by computing the pair-wise similarities between instances in the embedding space <ref type="bibr">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37]</ref>. Contrastive loss <ref type="bibr">[8]</ref> is one of the classic pair-based DML methods, which learns a discriminative metric via Siamese networks. It encourages the deep features of positive pairs to be closer to each other, and those of negative pairs to be farther than a fixed threshold. Triplet loss <ref type="bibr" target="#b28">[29]</ref> requires the similarity of a positive pair to be higher than that of a negative pair (with the same anchor) by a given margin.</p><p>Inspired by contrastive loss and triplet loss, a number of pair-based DML algorithms have been developed, which attempted to weight all pairs in a mini-batch, such as upweighting informative pairs (e.g. N-pair loss <ref type="bibr" target="#b29">[30]</ref>, Multi-Similarity (MS) loss <ref type="bibr" target="#b36">[37]</ref>) through a log-exp formulation, or sampling negative pairs uniformly w.r.t. pair-wise distance <ref type="bibr" target="#b39">[40]</ref>. Generally, pair-based methods can be cast into a unified weighting formulation through General Pair Weighting (GPW) framework <ref type="bibr" target="#b36">[37]</ref>.</p><p>However, most deep models are trained with SGD where only a mini-batch of samples are accessible at each iteration, and the size of a mini-batch can be relatively small compared to the whole dataset, especially for a large-scale dataset. Moreover, a large fraction of the pairs is less informative as the model learns to embed most trivial pairs correctly. Thus the conventional pair-based DML techniques suffer from lacks of hard negative pairs, which are critical to promote model training.</p><p>To alleviate the aforementioned problems, a number of approaches have been developed to collect more potential information contained in a mini-batch, such as building a class-level hierarchical tree <ref type="bibr">[6]</ref>, updating class-level signatures to select hard negative instances <ref type="bibr" target="#b31">[32]</ref>, or obtaining samples from an individual cluster <ref type="bibr" target="#b27">[28]</ref>. Unlike these approaches which aim to enrich the information in a mini-batch, our XBM are designed to directly mine hard negative examples across multiple mini-batches.</p><p>Proxy-based DML. There is another branch of DML methods aiming to learn the embeddings by comparing each sample with proxies, including proxy NCA <ref type="bibr" target="#b20">[21]</ref>, NormSoftmax <ref type="bibr" target="#b45">[46]</ref> and SoftTriple <ref type="bibr" target="#b24">[25]</ref>. In fact, our XBM module can be regarded as the proxies to some extent. However, there are two main differences between the proxy-based methods and our XBM module: (1) proxies are often optimized along with the model weights, while the embeddings of our memory are directly taken from the past mini-batches; <ref type="bibr">(2)</ref> proxies are used to represent the class-level information, whereas the embedding of our memory computes the information for each individual instance. Both proxy-based methods and our XBM augmented pair-based methods are able to capture the global distribution of data over the whole dataset during training.</p><p>Feature Memory Module. Non-parametric memory module for embedding learning has shown power in various computer visual tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b17">18]</ref>. For examples, the external memory can be used to address the unaffordable computational demand of conventional NCA <ref type="bibr" target="#b40">[41]</ref> in large-scale recognition, and encourage instanceinvariance in domain adaptation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b41">42]</ref>. But only the positive pairs are optimized, while the negatives are ignored in <ref type="bibr" target="#b40">[41]</ref>. Our XBM is able to to provide a rich set of negative examples for the pair-based DML methods, which is more generalized and can make full use of the past embeddings.</p><p>The key distinction is that existing memory modules only store the embeddings of current mini-batch <ref type="bibr" target="#b35">[36]</ref>, or maintain the whole dataset <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48]</ref> with a moving average update, while our XBM is maintained as a dynamic queue of mini-batches, which is more flexible and applicable in extremely large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cross-Batch Memory Embedding Networks</head><p>In this section, we first analyze the limitation of existing pair-based DML methods. Then we introduce the "slow drift" phenomena, which provides the underlying evidence that supports our cross-batch mining approach. Finally, we describe our XBM module and integrate it into existing pairbased DML methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Delving into Pair-based DML</head><p>Let X = {x 1 , x 2 , . . . , x N } denotes a set of training instances, and y i is the corresponding label of x i . An embedding function, f (Â·; Î¸), projects a data point x i onto a D-dimensional unit hyper-sphere, v i = f (x i ; Î¸). We measure the similarity between two instances of a pair in the embedding space. During training, we denote an affinity matrix of all pairs within the current mini-batch as S, whose (i, j) element is the cosine similarity between the embeddings of the i-th sample and the j-th sample: v T i v j . To facilitate further analysis, we delve into the pair-based DML methods by using the GPW framework described in <ref type="bibr" target="#b36">[37]</ref>. With GPW, a pair-based function can be formulated in a unified pair-weighting form:</p><formula xml:id="formula_0">L = 1 m m i=1 ï£® ï£° m y j =y i wijSij âˆ’ m y j =y i wijSij ï£¹ ï£» ,<label>(1)</label></formula><p>where m is the mini-batch size, and w ij is the weight assigned to S ij . Eq. 1 shows that any pair-based method can be considered as a weighting scheme focusing on informative pairs. Here we list the weighting schemes of contrastive loss, triplet loss and MS loss.</p><p>-Contrastive loss. For each negative pair, w ij = 1 if S ij &gt; Î», otherwise w ij = 0. The weights of all positive pairs are set to 1.</p><p>-Triplet loss. For each negative pair, w ij = |P ij |, where P ij is the valid positive set sharing the anchor. Formally, P ij = {x i,k |y k = y i , and S ik &lt; S ij + Î·} and Î· is a predefined margin in triplet loss. Similarly, we can obtain the triplet weight for a positive pair.</p><p>-MS loss. Unlike contrastive loss and triplet loss which only assigns a weight with integer value, MS loss <ref type="bibr" target="#b36">[37]</ref> is able to weight the pairs more properly by jointly considering multiple similarities. The MS weight for a negative pair is computed as:</p><formula xml:id="formula_1">w ij = e Î²(Sij âˆ’Î») 1 + kâˆˆNi e Î²(S ik âˆ’Î») ,</formula><p>where Î² and Î» are hyper-parameters, and N i is the valid negative set for the anchor x i . The MS weight for a positive pair can be computed similarly.</p><p>In fact, the main path of developing pair-based DML is to design a better weighting mechanism for pairs within a mini-batch. Generally, with a small mini-batch (e.g. 16 or 32), the sophisticated weighting schemes can perform much better <ref type="figure" target="#fig_0">(Figure 1</ref>, left). However, beyond the weighting scheme, the mini-batch size is also of great importance to DML. <ref type="figure" target="#fig_0">Figure 1</ref> (left and middle) shows the R@1s of various pair-based methods are increased considerably by using a larger mini-batch on large-scale datasets. Intuitively, the number of negative pairs increase quadratically when the mini-batch size grows, which naturally provides more informative pairs. Instead of developing another sophisticated  <ref type="figure" target="#fig_12">Figure 2</ref>. Cross-Batch Memory (XBM) trains an embedding network by comparing each anchor with a memory bank using a pair-based loss. The memory bank is maintained as a queue with the current mini-batch enqueued and the oldest mini-batch dequeued. Our XBM enables a large amount of valid negatives for each anchor to benefit the model training with many pair-based methods.</p><p>but highly complicated algorithm to weight the informative pairs, our intuition is to simply collect sufficient informative negative pairs, where a simple weighting scheme, such as contrastive loss, can easily outperform the stage-of-theart weighting approaches. This provides a new path that is straightforward yet more efficient to solve the hard mining problem in DML. Naively, a straightforward solution to collect more informative negative pairs is to increase the mini-batch size. However, training deep networks with a large mini-batch is limited by GPU memory, and often requires massive data flow communication between multiple GPUs. To this end, we attempt to achieve the same goal by introducing an alternative approach using very low GPU memory and minimum computation burden. We propose a XBM module that allows the model to collect informative pairs over multiple past mini-batches, based on the "slow drift" phenomena as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Slow Drift Phenomena</head><p>The embeddings of past mini-batches are usually considered out-of-date because the model parameters are changing throughout the training process <ref type="bibr">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b24">25]</ref>. Such out-ofdate features are always discarded, but we learn that they can be an important resource, while being computationfree, by identifying the "slow drift" phenomena. We study the drifting speed of embeddings by measuring the difference of features for the same instance computed at different training iterations. Formally, the feature drift of an input x at t-th iteration with step âˆ†t is defined as:</p><formula xml:id="formula_2">D(x, t; âˆ†t) . . = ||f (x; Î¸ t ) âˆ’ f (x; Î¸ tâˆ’âˆ†t )|| 2 2 (2)</formula><p>We train GoogleNet <ref type="bibr" target="#b32">[33]</ref> from scratch with a contrastive loss, and compute the average feature drift for a set of randomly sampled instances at different steps: {10, 100, 1000} (in <ref type="figure" target="#fig_3">Figure 3</ref>). The feature drift is consistently small, within only e.g. 10 iterations. For the large steps, e.g. 100 and 1000, the features change drastically at the early phase, but  become relatively stable within about 3K iterations. Furthermore, when the learning rate decreases, the drift gets extremely slow. We define such phenomena as "slow drift", which suggests that with a certain number of training iterations, the embeddings of instances can drift very slowly, resulting in marginal differences between the features computed at different training iterations. Furthermore, we demonstrate that such "slow drift" phenomena can provide a strict upper bound for the error of gradients of a pair-based loss. For simplicity, we consider the contrastive loss of one single negative pair</p><formula xml:id="formula_3">L = v T i v j , where v i , v j are the embeddings of current model and v j is an approximation of v j . Lemma 1. Assume ||v j âˆ’ v j || 2 2 &lt; , L = v T</formula><p>i v j and f satisfies Lipschitz continuous condition, then the error of gradients related to v i is,</p><formula xml:id="formula_4">âˆ‚L âˆ‚Î¸ âˆ’ âˆ‚ L âˆ‚Î¸ 2 2 &lt; C ,<label>(3)</label></formula><p>where C is the Lipschitz constant.</p><p>Proof and discussion of Lemma 1 are provided in Supplementary Materials. Empirically, C is often less than 1 with the backbones used in our experiments. Lemma 1 suggests that the error of gradients is controlled by the error of embeddings under Lipschitz assumption. Thus, the "slow drift" phenomenon ensures that mining across mini-batches can provide negative pairs with valid information for pairbased methods.</p><p>In addition, we discover that the "slow drift" of embeddings is not a special phenomena in DML, and also exists in other conventional tasks, as shown in Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-Batch Memory Module</head><p>We first describe our cross-batch memory (XBM) module, with model initialization and updating mechanism. Then we show that our memory module is easy to implement, can be directly integrated into existing pair-based DML framework as a plug-and-play module, by simply using several lines of codes (in Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XBM.</head><p>As the feature drift is relatively large at the early epochs, we warm up the neural networks with 1k iterations, allowing the model to reach a certain local optimal field where the embeddings become more stable. Then we initialize the memory module M by computing the features of a set of randomly sampled training images with the warm-up model.</p><formula xml:id="formula_5">Formally, M = {( v 1 , y 1 ), ( v 2 , y 2 ), . . . , ( v m , y M )},</formula><p>where v i is initialized as the embedding of the i-th sample x i , and M is the memory size. We define a memory ratio as R M . . = M/N , the ratio of memory size to the training size.</p><p>We maintain and update our XBM module as a queue: at each iteration, we enqueue the embeddings and labels of the current mini-batch, and dequeue the entities of the earliest mini-batch. Thus our memory module is updated with embeddings of the current mini-batch directly, without any additional computation. Furthermore, the whole training set can be cached in the memory module, because very limited memory is required for storing the embedding features, e.g. 512-d float vectors. See the other update strategy in Supplementary Materials.</p><p>XBM augmented Pair-based DML. We perform hard negative mining with our XBM on the pair-based DML. For a pair-based loss, based on GPW in <ref type="bibr" target="#b36">[37]</ref>, it can be cast into a unified weighting formulation of pair-wise similarities within a mini-batch in Eqn. <ref type="bibr">(1)</ref>, where a similarity matrix is computed within a mini-batch, S. To perform our XBM mechanism, we simply compute a cross-batch similarity matrix S between the instances of current mini-batch and the memory bank.</p><p>Formally, the memory augmented pair-based DML can be formulated as below: </p><formula xml:id="formula_6">L = 1 m m i=1 Li = m i=1 ï£® ï£° M y j =y i wij Sij âˆ’ M y j =y i wij Sij ï£¹ ï£» , (4) where S ij = v T i v j .</formula><formula xml:id="formula_7">L i w.r.t. v i is, âˆ‚L i âˆ‚v i = M yj =yi w ij v j âˆ’ M yj =yi w ij v j<label>(5)</label></formula><p>and the gradients w.r.t. v i model parameters (Î¸) can be computed through a chain rule:</p><formula xml:id="formula_8">âˆ‚L i âˆ‚Î¸ = âˆ‚L i âˆ‚v i âˆ‚v i âˆ‚Î¸<label>(6)</label></formula><p>Finally, the model parameters Î¸ are optimized through stochastic gradient descent. Lemma 1 ensures that the gradient error raised by embedding drift can be strictly constrained with a bound, which minimizes the side effect to the model training.</p><p>Hard Mining Ability. We investigate the hard mining ability of our XBM mechanism. We study the amount of valid negative pairs produced by our memory module at each iteration. A negative pair with non-zero gradient is considered as valid. The statistical result is illustrated in <ref type="figure" target="#fig_7">Figure 4</ref>. Throughout the training procedure, our memory module steadily contributes about 1,000 hard negative pairs per iteration, whereas less than 10 valid pairs are generated by the original mini-batch mechanism. Qualitative hard mining results are shown in <ref type="figure" target="#fig_8">Figure 5</ref>. Given a bicycle image as an anchor, the mini-batch provides limited and different images, e.g. roof and sofa, as negatives. On the contrary, our XBM offers both semantically bicycle-related images and other samples, e.g. wheel and clothes. These results clearly demonstrate that the proposed   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We follow the standard settings in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr">14]</ref> for fair comparison. Specifically, we adopt GoogleNet <ref type="bibr" target="#b32">[33]</ref> as the default backbone network if not mentioned. The weights of the backbone were pre-trained on ILSVRC 2012-CLS dataset <ref type="bibr" target="#b26">[27]</ref>. A 512-d fully-connected layer with l 2 normalization is added after the global pooling layer. The default embedding dimension is set as 512. For all datasets, the input images are first resized to 256 Ã— 256, and then cropped to 224Ã—224. Random crops and random flips are utilized as data augmentation during training. For testing, we only use the single center crop to compute the embedding for each instance as <ref type="bibr" target="#b21">[22]</ref>. In all experiments, we use the Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with 5e âˆ’4 weight decay and the PK sampler (P categories, K samples/category) to construct mini-batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>Our methods are evaluated on three large-scale datasets for few-shot image retrieval. Recall@k is reported. The training and testing protocol follow the standard setups: Stanford Online Products (SOP) <ref type="bibr" target="#b21">[22]</ref> contains 120,053 online product images in 22,634 categories. There are only 2 to 10 images for each category. Following <ref type="bibr" target="#b21">[22]</ref>, we use 59,551 images (11,318 classes) for training, and 60,502 images (11,316 classes) for testing. In-shop Clothes Retrieval (In-shop) contains 72,712 clothing images of 7,986 classes. Following <ref type="bibr" target="#b19">[20]</ref>, we use 3,997 classes with 25,882 images as the training set. The test set is partitioned to a query set with 14,218 images of 3,985 classes, and a gallery set having 3,985 classes with 12,612 images. PKU VehicleID (VehicleID) <ref type="bibr" target="#b18">[19]</ref> contains 221,736 surveillance images of 26,267 vehicle categories, where 13,134 classes (110,178 images) are used for training. Following the test protocol described in <ref type="bibr" target="#b18">[19]</ref>, evaluation is conducted on a predefined small, medium and large test sets which contain 800 classes (7,332 images), 1600 classes (12,995 images) and 2400 classes (20,038 images) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We provide ablation study on SOP dataset with GoogleNet to verify the effectiveness of our XBM module. Memory Ratio. The search space of our cross-batch hard mining can be dynamically controlled by memory ratio R M . We illustrate the impact of memory ratio to XBM augmented contrastive loss on three benchmarks (in <ref type="figure" target="#fig_0">Figure 1</ref>, right). Firstly, our method significantly outperforms the baseline (with R M = 0), with over 20% improvements on all three datasets using various configurations of R M . Secondly, our method with mini-batch of 16 can achieve better performance than the non-memory counterpart using 256 mini-batch, e.g. with an improvement of 71.7%â†’78.2% on recall@1, while saving GPU memory considerably.</p><p>More importantly, our XBM can boost the contrastive loss largely with small R M (e.g. on In-shop, 52.0%â†’ 79.4% on recall@1 with R M = 0.01) and its performance is going to be saturated when the memory expands to a moderate size. It makes sense, since the memory with a small R M (e.g. 1%) already contains thousands of embeddings to generate sufficient valid negative instances on large-scale datasets, especially fine-grained ones, such as In-shop or VehicleID. Therefore, our memory scheme can have consistent and stable performance improvements with a wide range of memory ratios.  Mini-batch Size. Mini-batch size is critical to the performance of many pair-based approaches <ref type="figure" target="#fig_0">(Figure 1, left)</ref>. We further investigate its impact to our memory augmented pair-based methods (shown in <ref type="figure" target="#fig_9">Figure 6</ref>). Our method has a 3.2% performance gain by increasing its mini-batch size from 16 to 256, while the original contrastive method has a significantly larger improvement of 25.1%. Obviously, with the proposed memory module, the impact of mini-batch size is reduced significantly. This indicates that the effect of mini-batch size can be strongly compensated by our memory module, which provides a more principled solution to address the hard mining problem in DML.</p><p>With General Pair-based DML. Our memory module can be directly applied to the GPW framework. We evaluate it with contrastive loss, triplet loss and MS loss. As shown in  analysis see Supplementary Materials (SM). The results suggest that (1) both straightforward (e.g. contrastive loss) and carefully designed weighting scheme (e.g. MS loss) can be improved largely by our memory module, and (2) with our memory module, a simple pair-weighting method (e.g. contrastive loss) can easily outperform the most sophisticated, state-of-the-art methods such as MS loss <ref type="bibr" target="#b36">[37]</ref> by a large margin.</p><p>Memory and Computational Cost. We analyze the complexity of our XBM module on memory and computational cost. On memory cost, The XBM module M (O(DM )) and affinity matrix S (O(mM )) requires a negligible 0.2 GB GPU memory for caching the whole training set ( <ref type="table" target="#tab_2">Table 2)</ref>. On computational complexity, the cost of S (O(mDM )) increases linearly with memory size M . With a GPU implementation, it takes a reasonable 34% amount of extra training time w.r.t. the forward and backward procedure.</p><p>It is also worth noting that XBM does not act in the inference phase. It only requires 1 hour extra training time and 0.2GB memory, to achieve a surprising 13.5% performance gain by using a single GPU. Moreover, our method can be scalable to an extremely large-scale dataset, e.g. with 1 billion samples, since our XBM module can generate a rich set of valid negatives with a small-memory-ratio XBM, which requires acceptable cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Quantitative and Qualitative Results</head><p>In this section, we compare our XBM augmented contrastive loss with the state-of-the-art DML methods on three benchmarks on image retrieval. Even though our method can reach better performance with a larger mini-batch size <ref type="figure" target="#fig_9">(Figure 6</ref>), we only use 64 mini-batch which can be implemented on a single GPU with ResNet50 <ref type="bibr">[11]</ref>. Since the backbone architecture and embedding dimension can effect the recall metric, we list the results of our method with var-  ious configurations for fair comparison in <ref type="table" target="#tab_4">Table 3</ref>, 4 and 5.</p><p>See results on more datasets in SM.</p><p>The results demonstrate that our XBM module, with a contrastive loss, can surpass the state-of-the-art methods on all datasets by a large margin. On SOP, our method with R 128 outperforms the current state-of-the-art method: MIC <ref type="bibr" target="#b25">[26]</ref> by 77.2% â†’ 80.6%. On In-shop, our method with R 128 achieves even higher performance than FastAP <ref type="bibr">[2]</ref> with R 512 , and improves by 88.2%â†’91.3% compared to MIC. On VehicleID, our method outperforms existing approaches considerably. For example, on the large test dataset, by using a same G 512 , it improves the R@1 of recent A-BIER <ref type="bibr" target="#b23">[24]</ref> largely by 81.9%â†’92.5%. With R 128 , our method surpasses the best results by 87%â†’93%, which is obtained by FastAP <ref type="bibr">[2]</ref> using R 512 . <ref type="figure" target="#fig_10">Figure 7</ref> shows that our memory module promotes the learning of a more discriminative encoder. For example, at the first row, our model is aware of the deer under the lamp which is a specific character of the query product, and retrieves the correct images. In addition, we also present some bad cases in the bottom rows, where our retrieved results are visually closer to the query than that of baseline model. See more visualizations in SM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented a conceptually simple, easy to implement, and memory efficient cross-batch mining mechanism for pair-based DML. In this work, we identify the "slow drift" phenomena that the embeddings drift exceptionally slow during the training process. Then we propose a crossbatch memory (XBM) module to dynamically update the embeddings of instances of recent mini-batches, which allows us to collect sufficient hard negative pairs across multiple mini-batches, or even from the whole dataset. Without  <ref type="table" target="#tab_12">Table 5</ref>. Recall@K(%) performance on VehicleID. bells and whistles, the proposed XBM can be directly integrated into a general pair-based DML framework, and improve the performance of existing pair-based methods significantly on image retrieval. In particular, with our XBM, a contrastive loss can easily surpass state-of-the-art methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr">2]</ref> by a large margin on three large-scale datasets. This paves a new path in solving for hard negative mining which is a fundamental problem for various computer vision tasks. Furthermore, we hope that the dynamic memory mechanism can be extended to improve a wide variety of machine learning tasks because "slow drift" is a general phenomenon not only occurring in DML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Results on More Datasets</head><p>We further verify the effectiveness of our Cross-Batch Memory (XBM) on three more datasets. CUB-200-2011 (CUB) <ref type="bibr">[11]</ref> and Cars-196 (Car) <ref type="bibr">[5]</ref> are two widely used fine-grained datasets, which are relatively small. DeepFash-ion2 <ref type="bibr">[2]</ref> is a large-scale dataset just released recently. The details and evaluation protocols of the three datasets are described as below. CUB-200-2011 (CUB) contains 11,788 birds images of 200 classes. There are about 60 images/class. Following <ref type="bibr">[11]</ref>, we use 5,864 images of 100 classes for training and the remaining 5,924 images for testing. Cars-196 (Cars) contains 16,185 images in 196 model categories, Following <ref type="bibr">[5]</ref>, we use the first 98 models for training with the rest for testing. DeepFashion2 contains 216K clothes images with over 686K commercial-consumer pairs in the training set, whose size is nearly 7 times of In-shop. We use ground truth boxes in training and testing. We follow evaluation protocol described in <ref type="bibr">[2]</ref>, using 10,990 commercial images with 32,550 items as a query set, and 21,438 commercial images with 37,183 items as a gallery set. XBM meets Pair-based DML. We show that XBM consistently improves the performance of various pair-based methods on the three datasets. For instance, by applying our XBM to the conventional contrastive loss, we achieve significant Recall@1 improvements on CUB with +4.4% and Cars with +7.8%, as shown in <ref type="table" target="#tab_4">Table 3</ref>. On the large-scale DeepFashion2, XBM has a large improvement of +11.2% as shown in <ref type="table" target="#tab_12">Table 5</ref>. Comparison with the State-of-the-art. We compare our method with existing methods in <ref type="table" target="#tab_11">Table 4</ref>. On CUB, our XBM with a contractive loss achieves the best recall@1 of 65.8% without any tricks. On Cars, except ABE <ref type="bibr">[4]</ref> and A-Bier <ref type="bibr">[8]</ref>, our XBM augmented contrastive loss reaches the best performance using GoogleNet.</p><p>In fact, we observed that several tricks can improve the performance significantly on small-scale datasets. For example, freezing BN layer <ref type="bibr">[12,</ref><ref type="bibr">9]</ref> can increase recall@1 by more than 2% on CUB and Cars, or applying a 10 times smaller learning rate on the pretrained backbones <ref type="bibr">[7]</ref>. However, these tricks show no effect on large-scale datasets e.g. SOP, In-shop and VehicleID, which contains sufficient data   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Memory Update</head><p>We develop a simple enqueue-dequeue mechanism to update the memory bank of our XBM: enqueue the latest features, and at the same time dequeue the oldest ones. In this experiment, we evaluate two alternative memory update mechanisms: moving average <ref type="bibr">[13,</ref><ref type="bibr">14]</ref> and backpropagation <ref type="bibr">[6]</ref>, on SOP, In-shop, CUB and Cars datasets with GoogleNet as backbone. Furthermore, we also conduct a faster updated XBM to investigate the effect of feature drift. Moving Average Update. Update a memory embedding v i with its current feature v i as following:</p><formula xml:id="formula_9">v i = m v i + (1 âˆ’ m)v i v i = v i /|| v i || 2 ,</formula><p>where m is the momentum for the moving average update.</p><p>The embeddings in memory is updated slower when the momentum m becomes larger. We study the impact of momentum m to the performance in <ref type="table" target="#tab_0">Table 1</ref>. We observed that training with a large momentum can benefit a small dataset (e.g. CUB or Cars), but may impair the performance on a large-scale dataset (e.g. SOP or In-shop    </p><formula xml:id="formula_10">v i = v i + Î± âˆ‚ L âˆ‚ v i v i = v i /|| v i || 2 ,</formula><p>where Î± is the learning rate of memory features.</p><p>In BP update, the memory embeddings are optimized  along with the model, and serve as proxies in proxy-based DML methods <ref type="bibr">[6,</ref><ref type="bibr">9]</ref>. Obviously, BP requires much more memory and computational cost to compute and save the gradients compared to our XBM. However, it cannot obtain clear performance improvements in all datasets as shown in <ref type="table" target="#tab_2">Table 2</ref>. This suggests that the embeddings drift slowly, and the past mini-batches can largely represent the distribution of current embedding space. Faster Update. Generally, we update the XBM with one mini-batch at each iteration (Ã—1). To further evaluate the side effect of feature drift in our XBM, at each iteration, ten mini-batches (Ã—10) are enqueued into the XBM memory queue. This accelerates XBM update 10 times faster, which makes the feature drift smaller. As shown in <ref type="table" target="#tab_14">Table 6</ref>, the Ã—10 update cannot bring clear performance gain, which suggests that the natural feature drift is slow enough and cannot hinder the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature Drift on General Tasks</head><p>"Slow drift" phenomena not only exist in pair-based DML, but also happen in other machine learning tasks, e.g. image recognition. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates that the normalized embeddings at global pooling layer of ResNet50 drift at a slow rate when trained with cross entropy loss on ImageNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Hyperparameters</head><p>In <ref type="table">Table 7</ref>, we list all the key hyperparameters applied in our experiments. Our XBM achieves outstanding performance on large-scale datasets and comparable results on small-scale datasets without any training trick or large training iterations.  <ref type="table">Table 7</ref>: Hyperparameters used in training memory augmented models to compare with state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Proof of Lemma 1</head><p>Proof. The gradient of the accurate loss and the approximated loss can be computed as below:</p><formula xml:id="formula_11">âˆ‚L âˆ‚Î¸ = âˆ‚L âˆ‚v i âˆ‚v i âˆ‚Î¸ = v j âˆ‚v i âˆ‚Î¸ âˆ‚ L âˆ‚Î¸ = âˆ‚ L âˆ‚v i âˆ‚v i âˆ‚Î¸ = v j âˆ‚v i âˆ‚Î¸</formula><p>Then, the gradient error is:</p><formula xml:id="formula_12">âˆ‚L âˆ‚Î¸ âˆ’ âˆ‚ L âˆ‚Î¸ 2 2 = (v j âˆ’ v j ) âˆ‚v i âˆ‚Î¸ 2 2 â‰¤ v j âˆ’ v j 2 2 âˆ‚v i âˆ‚Î¸ 2 2 â‰¤ âˆ‚f (x i ; Î¸ t ) âˆ‚Î¸ 2 2 â‰¤ C .<label>(1)</label></formula><p>Empirically, we observed that âˆ‚f (xi;Î¸ t ) âˆ‚Î¸ 2 2 is usually less than 1 so that the gradient error can be strictly controlled by the small feature drift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Visualization.</head><p>We visualize a number of samples to investigate the performance of our XBM, including the mined negatives in training and the retrieved examples in testing. All examples are randomly selected from SOP, In-shop and VehicleID by following some rules. <ref type="figure" target="#fig_12">Figure 2</ref> demonstrates the hard negatives mined from memory with over 0.5 similarities. The negatives in each row are uniformly sampled from a sequence sorted by similarities. These results clearly demonstrate that the proposed XBM can provide diverse, visually related, and even finegrained samples to construct informative negative pairs. Furthermore, we select the anchors having a similarity with the hardest samples over 0.8, and show the top 10 negatives in <ref type="figure" target="#fig_3">Figure 3</ref>. Some of the presented negatives are extremely similar or even exactly the same items with corresponding anchor images.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>R@1 results with GoogleNet. Left: R@1 on SOP vs. mini-batch size with contrastive, triplet and MS approaches. Middle: R@1 vs. mini-batch size by varying datasets. Right R@1 vs. memory ratio at mini-batch size 16 with contrastive loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Feature drift with different steps on SOP. The embeddings of training instances drift within a relatively small distance even under a large interval, e.g. âˆ†t = 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>Pseudocode of XBM. train network f conventionally with K epochs initialize XBM as queue M for x, y in loader: # x: data, y: labels anchors = f.forward(x) # memory update enqueue(M, (anchors.detach(), y)) dequeue(M) # compare anchors with M sim = torch.matmul(anchors.transpose(), M.feats) loss = pair_based_loss(sim, y, M.labels) loss.backward() optimizer.step()</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The memory augmented pair-based loss in Eqn.(4) is in the same form as the normal pair-based loss in Eqn.(1), by computing a new similarity matrix S. Each instance in current mini-batch is compared with all the instances stored in the memory, enabling us to collect sufficient informative pairs for training. The gradient of the loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>The number of valid negative examples from mini-batch and that from memory per iteration. Model is trained on SOP with RM = 1, mini-batch size 64 and GoogleNet as the backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Given an anchor image (yellow), examples of positive (green) and negative from mini-batch (gray) and that from memory (purple). Current mini-batch can only bring few valid negatives with less information, while our XBM module can provide a wide variety of informative negative examples.XBM can provide diverse, related, and even fine-grained samples to construct negative pairs.Our results confirm that (1) existing pair-based approaches suffer from the problem of lacking informative negative pairs to learn a discriminative model, and (2) our XBM module can significantly strengthen the hard mining ability of pair-based DML in a very simple yet efficient manner. See more examples in Supplementary Materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Performance of contrastive loss by training with different mini-batch sizes. Unlike conventional pair-based methods, XBM augmented contrastive loss is equally effective under random shuffle mini-batch sampler (denoted with superscript *).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Top 4 retrieved images w/o and w/ memory module. Correct results are highlighted with green, while incorrect purple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>10 Figure 1 :</head><label>101</label><figDesc>Feature drift with different steps of ResNet50 on Im-ageNet trained from scratch. The embeddings drift within a relatively small distance even under a large interval, e.g. âˆ†t = 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 2 :</head><label>2</label><figDesc>Given an anchor image (yellow), we present examples of a positive (green), and multiple negatives mined from memory (purple) uniformly sorted from hard to simple. The demonstrated anchors are randomly chosen from training datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 3 :Figure 4 :Figure 5 :</head><label>345</label><figDesc>Given an anchor image (yellow), we present examples of a positive (green), and top 10 negatives mined from memory (purple). The examples are randomly chosen from anchors whose top 1 negative has over 0.8 similarity. Top 5 retrieved images w/o and w/ memory module. We randomly select the examples with the correct top 1 predictions given by our XBM but incorrect by the baseline model. Correct results are highlighted with green, while incorrect purple. Top 5 retrieved images w/o and w/ memory module. We randomly select the examples with the wrong top 1 predictions by both the baseline model and our XBM. Correct results are highlighted with green, while incorrect purple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>93.0 95.2 96.1 96.8 97.1 79.5 91.6 76.2 89.3 70.0 86.0 Contrastive w/ M 77.8 89.8 95.4 98.5 89.1 97.3 98.1 98.4 98.7 98.8 94.1 96.2 93.1 95.5 92.5 95.5 Triplet 61.6 80.2 91.6 97.7 79.8 94.8 96.5 97.4 97.8 98.2 86.9 94.8 84.8 93.4 79.7 91.4 Retrieval results of memory augmented ('w/ M') pair-based methods compared with their respective baselines on three datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>SOP</cell><cell></cell><cell></cell><cell></cell><cell cols="2">In-shop</cell><cell></cell><cell></cell><cell>Small</cell><cell></cell><cell cols="2">VehicleID Medium</cell><cell>Large</cell><cell></cell></row><row><cell>Recall@K (%)</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>1000</cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>5</cell></row><row><cell cols="17">Contrastive 77.1 Triplet w/ M 64.0 81.4 92.1 97.8 74.2 87.4 94.2 98.0 82.9 95.7 96.9 97.4 97.8 98.0 93.3 95.8 92.0 95.0 91.3 94.8</cell></row><row><cell>MS</cell><cell cols="3">69.7 84.2 93.1</cell><cell>97.9</cell><cell cols="12">85.1 96.7 97.8 98.3 98.7 98.8 91.0 96.1 89.4 94.8 86.7 93.8</cell></row><row><cell>MS w/ M</cell><cell cols="3">76.2 89.3 95.4</cell><cell>98.6</cell><cell cols="12">87.1 97.1 98.0 98.4 98.7 98.9 94.1 96.7 93.0 95.8 92.1 95.6</cell></row><row><cell>16</cell><cell>32</cell><cell cols="2">64 batch-size (log-scale)</cell><cell>128</cell><cell></cell><cell>256</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 ,</head><label>1</label><figDesc>our memory module can improve the original DML approaches significantly and consistently on all benchmarks. Specifically, the memory module remarkably boosts the performance of contrastive loss by 64.0%â†’77.8% and MS loss by 69.7%â†’76.2%. Furthermore, with sophisticated sampling and weighting approach, MS loss has 16.7% recall@1 performance improvement over contrastive loss on VehicleID Large test set. Such a large gap can be simply filled by our memory module, with a further 5.8% im-</figDesc><table><row><cell>Method</cell><cell>Time</cell><cell>GPU Mem.</cell><cell>R@1</cell><cell>Gain</cell></row><row><cell>Cont. bs. 64</cell><cell>2.10 h.</cell><cell>5.12 GB</cell><cell>63.9</cell><cell>-</cell></row><row><cell>Cont. bs. 256</cell><cell>4.32 h.</cell><cell>+15.7 GB</cell><cell>71.7</cell><cell>+7.8</cell></row><row><cell>Cont. w/ 1% R M</cell><cell>2.48 h.</cell><cell>+0.01 GB</cell><cell>69.8</cell><cell>+5.9</cell></row><row><cell>Cont. w/ 100% R M</cell><cell>3.19 h.</cell><cell>+0.20 GB</cell><cell>77.4</cell><cell>+13.5</cell></row></table><note>provement. MS loss has a smaller improvement because it weights extremely hard negatives heavily which might be outliers, while such a harmful influence is weakened by the equally weighting scheme of contrastive loss. For a detailed</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Training time and GPU memory cost on 64, 256 minibatch size and 1%, 100% memory ratio with 64 mini-batch size.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Recall@K(%) performance on SOP. 'G', 'B' and 'R'</cell></row><row><cell>denotes applying GoogleNet, InceptionBN and ResNet50 as back-</cell></row><row><cell>bone respectively, and the superscript is embedding size.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="6">: Recall@1(%) performance of moving average up-date mechanism with different momentum m.</cell></row><row><cell>learning rate Î±</cell><cell>0</cell><cell>0.01</cell><cell>0.1</cell><cell>0.5</cell><cell>0.9</cell></row><row><cell>SOP</cell><cell>78.2</cell><cell>77.2</cell><cell>77.6</cell><cell>78.2</cell><cell>78.2</cell></row><row><cell>In-shop</cell><cell>89.3</cell><cell>88.7</cell><cell>89.0</cell><cell>87.8</cell><cell>88.4</cell></row><row><cell>CUB</cell><cell>60.3</cell><cell>60.2</cell><cell>60.7</cell><cell>60.0</cell><cell>59.4</cell></row><row><cell>Cars</cell><cell>78.8</cell><cell>78.8</cell><cell>79.3</cell><cell>77.9</cell><cell>77.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Recall@1(%) performance of BP update mechanism with different memory learning rates Î± to mitigate overfitting. Note that to demonstrate the actual effectiveness of our XBM module, the performance of our XBM reported here was trained without such bells and whistles.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 :</head><label>3</label><figDesc>Retrieval results of XBM augmented ('w/ M') pair-based DML methods and baseline methods with GoogleNet on CUB and Cars.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cars</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall@K (%)</cell><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>Smart Mining [3] HDC [10] A-BIER [8] ABE [4]</cell><cell>G 64 G 384 G 512 G 512</cell><cell>49.8 53.6 57.5 60.6</cell><cell>62.3 65.7 68.7 71.5</cell><cell>74.1 77.0 78.3 79.8</cell><cell>83.3 85.6 86.2 87.4</cell><cell>-91.5 91.9 -</cell><cell>-95.5 95.5 -</cell><cell>64.7 73.7 82.0 85.2</cell><cell>76.2 83.2 89.0 90.5</cell><cell>84.2 89.5 93.2 94.0</cell><cell>90.2 93.8 96.1 96.1</cell><cell>-96.7 97.8 -</cell><cell>-98.4 98.7 -</cell></row><row><cell>Clustering [10] ProxyNCA [6] HTL [1] MS [12] SoftTriple [9] Contrative w/ M Contrative w/ M</cell><cell>B 64 B 64 B 512 B 512 B 512 G 512 B 512</cell><cell>48.2 49.2 57.1 65.7 65.4 61.9 65.8</cell><cell>61.4 61.9 68.8 77.0 76.4 72.9 75.9</cell><cell>71.8 67.9 78.7 86.3 84.5 81.2 84.0</cell><cell>81.9 72.4 86.5 91.2 90.4 88.6 89.9</cell><cell>--92.5 95.0 -93.5 94.3</cell><cell>--95.5 97.3 -96.5 97.0</cell><cell>58.1 73.2 81.4 84.1 84.5 80.3 82.0</cell><cell>70.6 82.4 88.0 90.4 90.7 87.1 88.7</cell><cell>80.3 86.4 92.7 94.0 94.5 91.9 93.1</cell><cell>87.8 87.8 95.7 96.5 96.9 95.1 96.1</cell><cell>--97.4 98.0 -97.3 97.6</cell><cell>--99.0 98.9 -98.2 98.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Recall@K(%) performance on CUB and Cars.</figDesc><table><row><cell>Recall@K(%)</cell><cell></cell><cell>1</cell><cell>10</cell><cell>20</cell></row><row><cell>Match RCNN [2]</cell><cell></cell><cell>26.8</cell><cell>57.4</cell><cell>66.5</cell></row><row><cell>Contrative Contrative w/ M Contrative w/ M Contrative w/ M</cell><cell>G 512 G 512 B 512 R 128</cell><cell>29.3 40.5 40.9 41.9</cell><cell>51.9 63.2 63.3 64.6</cell><cell>60.3 69.4 69.6 70.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Recall@K(%) performance on DeepFashion2.because on a small-scale dataset, the training epoch is short (e.g. 100 iters.), which enables a small feature drift between adjacent epochs, while a large-scale dataset has a longer epoch, and the features computed at the past epochs are highly possible to be out-of-date, and thus a large momentum may hinder the training process. Moreover, the moving average update may benefit the training by enhancing the embeddings in the memory by aggregating its embeddings of an instance with different augmentations when the feature drift is small.</figDesc><table><row><cell>Back-Propagation (BP) Update. Besides substituting the memory features of instances sampled into current mini-batch, BP method updates the memory features of each in-stance based on its gradients computed at back-propagation (BP):</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Recall@k(%) performance on SOP with update Ã—1 and Ã—10 configurations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>init. lr. lr.Ã—0.1 total iter. R M (%) Î±</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>m</cell></row><row><cell>SOP</cell><cell>3e-4</cell><cell>24k</cell><cell>34k</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>In-shop</cell><cell>3e-4</cell><cell>24k</cell><cell>34k</cell><cell>0.2</cell><cell>0</cell><cell>0</cell></row><row><cell>VehicleID</cell><cell>1e-4</cell><cell>30k</cell><cell>50k</cell><cell>0.5</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">DeepFashion2 3e-4</cell><cell>20k</cell><cell>36k</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>CUB</cell><cell>3e-5</cell><cell>-</cell><cell>1.4k</cell><cell>-</cell><cell cols="2">0.2 0.9</cell></row><row><cell>Cars</cell><cell>1e-4</cell><cell>1.4k</cell><cell>2k</cell><cell>-</cell><cell cols="2">0.1 0.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving semantic embedding consistency by metric learning for zero-shot classiffication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">StÃ©phane</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">FrÃ©dÃ©ric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep metric learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Cakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vehicle re-identification with viewpointaware metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Incorporating intra-class variance to finegrained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Em</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Gag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<editor>ICME. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d pose estimation and 3d model retrieval for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Triplet-center loss for multi-view 3d object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737v4</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunjoo</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A zero-shot framework for sketch based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sasi Kiran Yelamarthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Shiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning by tracking: Siamese cnn for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-TaixÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memory-based neighbourhood embedding for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suichan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep relative distance learning: Tell the difference between similar vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bier -boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep metric learning with bier: Boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Softtriple loss: Deep metric learning without triplet sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mic: Mining interclass characteristics for improved metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Divide and conquer the embedding space for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Tschernezki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stochastic class-based hard example mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hard-aware point-to-set deep metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Classification is a strong baseline for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Us San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francisco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunjoo</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<title level="m">Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly. PAMI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Softtriple loss: Deep metric learning without triplet sampling. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset. Master&apos;s thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
