<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-19">November 19, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sainte</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fare</forename><surname>Garnot</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LASTIG-STRUDEL</orgName>
								<orgName type="institution" key="instit1">Univ. Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">IGN-ENSG</orgName>
								<address>
									<postCode>F-94160</postCode>
									<settlement>Saint-Mandé</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LASTIG-STRUDEL</orgName>
								<orgName type="institution" key="instit1">Univ. Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">IGN-ENSG</orgName>
								<address>
									<postCode>F-94160</postCode>
									<settlement>Saint-Mandé</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giordano</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LASTIG-STRUDEL</orgName>
								<orgName type="institution" key="instit1">Univ. Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">IGN-ENSG</orgName>
								<address>
									<postCode>F-94160</postCode>
									<settlement>Saint-Mandé</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chehata</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LASTIG-STRUDEL</orgName>
								<orgName type="institution" key="instit1">Univ. Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">IGN-ENSG</orgName>
								<address>
									<postCode>F-94160</postCode>
									<settlement>Saint-Mandé</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">EA G&amp;E Bordeaux INP</orgName>
								<orgName type="institution" key="instit2">Université Bordeaux Montaigne</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-19">November 19, 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Satellite image time series, bolstered by their growing availability, are at the forefront of an extensive effort towards automated Earth monitoring by international institutions. In particular, large-scale control of agricultural parcels is an issue of major political and economic importance. In this regard, hybrid convolutional-recurrent neural architectures have shown promising results for the automated classification of satellite image time series.We propose an alternative approach in which the convolutional layers are advantageously replaced with encoders operating on unordered sets of pixels to exploit the typically coarse resolution of publicly available satellite images. We also propose to extract temporal features using a bespoke neural architecture based on self-attention instead of recurrent networks. We demonstrate experimentally that our method not only outperforms previous state-of-the-art approaches in terms of precision, but also significantly decreases processing time and memory requirements. Lastly, we release a large openaccess annotated dataset as a benchmark for future work on satellite image time series.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rising availability of high quality satellite data by both state <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b9">10]</ref> and private actors <ref type="bibr" target="#b36">[37]</ref> opens up numerous high-impact applications for machine learning methods. Among these, crop type classification is a major challenge for agricultural and environmental policy makers. In the European Union (EU), yearly crop maps are needed to grant the Common Agricultural Policy subsidies, an endowment of over 50 billion euros each year <ref type="bibr" target="#b0">[1]</ref>. Currently, European farmers declare the cultivated species manually on a yearly basis. The EU's Joint Research Center has thus called for the development of efficient tools to achieve automated monitoring <ref type="bibr" target="#b1">[2]</ref>. This push to automation is motivated in part by the launch of the Sentinel-2 satellite-which became fully operational in mid-2017-by the European Space Agency <ref type="bibr" target="#b9">[10]</ref>, and whose settings are particularly valuable for crop classification. Indeed, its high spectral resolution (13 bands) and short revisit time of 5 days are well-suited to analysing crop phenology, i.e. the cyclical evolution of vegetation <ref type="bibr" target="#b39">[40]</ref>. Additionally, the farmers' yearly manual declarations provide a considerable amount of annotated data (10 million parcels labelled each year in France alone) to train learning algorithms. Such models would have a wide array of applications beyond crop monitoring, for both public and private entities.</p><p>Practitioners mainly rely on traditional methods such as Random Forest (RF) and Support Vector Machine (SVM), which operate on handcrafted features for automated crop classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">47]</ref>. Recently, the gradual adoption of deep learning methods such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) for learning spatial and temporal attributes has brought significant improvements in classification performance. More specifically, hybrid neural architectures combining convolutions and recurrent units in a single architecture are the current state-of-the-art for crop type classification <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>In this paper, we argue that such hybrid recurrentconvolutional architectures fail to adapt to some key characteristics of the problem under consideration.</p><p>Spatial Encoding of Parcels: Sensors typically used for crop classification, such as the Sentinel-2 satellites, have a coarser spatial resolution (10m per pixel) than the typical agricultural textural information such as furrows or tree rows. However, CNNs rely heavily on texture to extract spatial features <ref type="bibr" target="#b11">[12]</ref>. Given this limitation, we propose to view medium-resolution images of agricultural parcels as unordered sets of pixels. Indeed, recent advances in 3D point cloud processing have spurred the development of powerful encoders for data comprised of sets of unordered elements <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45]</ref>. We show in this paper that set-based encoders can successfully extract learned statistics of the distribution of spectra across the spatial extent of the parcels. Furthermore, <ref type="figure">Figure 1</ref>: Example of Sentinel-2 time series (shown: RGB bands, 10m per pixel) for two parcels of the Winter cereal and Spring cereal classes. The dots on the horizontal axis represent the unevenly distributed acquisition dates over the period of interest. Note the importance of the temporal evolution of the parcels to discriminate between the classes.</p><p>we show that this approach handles the highly-variable size of parcels in a more efficient way than CNNs.</p><p>Temporal Encoding of Satellite Time Series: Earlier work in crop classification has shown the importance of the temporal dimension when classifying crop types <ref type="bibr" target="#b33">[34]</ref>. While RNNs have been widely used to analyse temporal sequences, recent work in Natural Language Processing (NLP) has introduced a promising new approach based on attention mechanisms <ref type="bibr" target="#b38">[39]</ref>. The improved parallelism brought by this approach is particularly valuable for automated crop monitoring, as its typical scale spans entire continents: one year of Sentinel-2 observations amounts to 25Tb of data for agricultural areas in the EU. Therefore, we propose an adapted attention-based approach for the classification of time series.</p><p>The key contributions of this paper are as follows:</p><p>• Inspired by Qi et al. <ref type="bibr" target="#b28">[29]</ref>, we introduce the pixel-set encoder as an efficient alternative to convolutional neural networks for medium-resolution satellite images.</p><p>• We adapted the work of Vaswani et al. <ref type="bibr" target="#b38">[39]</ref> to an endto-end, sequence-to-embedding setting for time series.</p><p>• We establish a new state-of-the-art for the task of largescale agricultural parcel classification. Moreover, our method not only improves the classification precision by a significant margin, but simultaneously boasts a acceleration of over 4 times and a memory imprint reduced by over 70% compared to the best-performing approaches in the literature.</p><p>• We release the first open-access dataset of Sentinel-2 images for crop classification with ground truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The problem of satellite image time series classification can be addressed at pixel level or object level. Pixel-based approaches do not require a priori knowledge of the borders of parcels, but cannot leverage the spatial homogeneity of class labels within the object's extent. Conversely, in the case of crop classification, object-based approaches can leverage the parcels' shape to extract helpful spatial information for achieving better classifications <ref type="bibr" target="#b8">[9]</ref>.</p><p>Traditional Machine Learning: Until recently, the common approach for crop classification has been to use traditional discriminative models with handcrafted features <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b41">42]</ref>. For instance, the Normalized Difference Vegetation Index (NDVI) combining the red and near-infrared spectral bands has been widely used as it relates to crop photosynthetic activity <ref type="bibr" target="#b37">[38]</ref>. Certain work also includes phenological features derived from the study of the NDVI as well as external meteorological information <ref type="bibr" target="#b47">[48]</ref>. Although robust and easily interpretable, such handcrafted indices do not compare favorably to end-to-end learned features. In such work, the prevalent approach to represent temporal evolution is to concatenate each date's spatial and spectral features. This is not well-suited to application over large geographical areas, in which the acquisition dates vary depending on the satellite orbit, and in which cloud cover and meteorological condition can be heterogeneous, resulting in sequences of variable length and temporal sampling. Consequently, other work oriented their efforts towards a better modeling of time using Hidden Markov Models <ref type="bibr" target="#b35">[36]</ref>, Conditional Random Fields <ref type="bibr" target="#b2">[3]</ref>, or Dynamic Time Warping <ref type="bibr" target="#b3">[4]</ref>.</p><p>Convolutional and Recurrent Approaches: More recently, the successful advances in the deep learning literature have provided efficient tools for both spatial and tem-poral feature extraction. Although some work only uses these tools as feature extractors <ref type="bibr" target="#b25">[26]</ref>, or combine them with feature engineering <ref type="bibr" target="#b45">[46]</ref>, most current work follows the deep learning paradigm of end-to-end trainable architectures. More specifically, Kussul et al. <ref type="bibr" target="#b19">[20]</ref> proposed to use a Multi Layer Perceptron (MLP) on raw observation data instead of traditional RF of SVM. Further work sets out to leverage the spatial and temporal structures of time series of satellite images. CNNs <ref type="bibr" target="#b20">[21]</ref> appeared to be a natural choice to address the spatial dimensions of the data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref>. Similarly, Long-Short Term Memory (LSTM) networks <ref type="bibr" target="#b12">[13]</ref> were successfully applied to model the temporal dimension of the data <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b24">25]</ref>, outperforming RF and SVM <ref type="bibr" target="#b13">[14]</ref>.</p><p>Furthermore, Rußwurm et al. <ref type="bibr" target="#b30">[31]</ref> first proposed to use hybrid recurrent convolutional approach by applying the ConvLSTM architecture <ref type="bibr" target="#b43">[44]</ref> to parcel classification. This work yielded state-of-the-art results and also showed that ConvLSTMs are able to learn to detect and ignore cloud obstruction. A similar approach was successfully used for automated change detection from Sentinel-2 data as well <ref type="bibr" target="#b26">[27]</ref>. Finally, Garnot et al. showed in <ref type="bibr" target="#b33">[34]</ref> that higher classification performance can be obtained by implementing such a hybrid model but with two dedicated modules for spatial and temporal feature extraction respectively: the series of images is first embedded by a shared CNN and the resulting embeddings sequence is fed to a Gate Recurrent Unit (GRU) <ref type="bibr" target="#b7">[8]</ref>. The use of a GRU is motivated by the smaller number of parameters required to achieve similar performance as LSTM, as corroborated in <ref type="bibr" target="#b31">[32]</ref>. Additionally, Garnot et al. show that the relatively low spatial resolution of multi-temporal satellite images may question the relevance of CNNs since handcrafted descriptors of spectral distribution performed nearly as well as trainable spatial encoders when used in combination to the recurrent units. This is one of the issues we propose to address in the present study.</p><p>Attention-Based Approach: Following the adoption of self-attention in the NLP literature as an efficient alternative to RNNs, Rußwurm et al. proposed in <ref type="bibr" target="#b32">[33]</ref> to apply the Transformer architecture [39]-a self-attention based network-to pixel-based classification. Their extensive experiments show that the Transformer yields classification performance that is on par with RNN-based models and present the same robustness to cloud-obstructed observations. Likewise, we propose to extend self-attention mechanisms to end-to-end sequence-to-embedding learning on images for object-level classification.</p><p>Purely Convolutional Approach: Multiple papers propose to address the temporal dimension with convolutions. Ji et al. present in <ref type="bibr" target="#b16">[17]</ref> a spatio-temporal 3D-CNN for parcel-based classification, and spectro-temporal convolutions are found to outperform LSTMs for pixel-based segmentation on temporal profiles in <ref type="bibr" target="#b27">[28]</ref>, and outperform an MLP in <ref type="bibr" target="#b18">[19]</ref>. Similar results are found in <ref type="bibr" target="#b48">[49]</ref>, where temporal convolutions yield better results than an LSTM network for classification based on NDVI temporal profiles. In addition, temporal convolutions have significantly lower processing times than RNNs. Yet, the ability to account for long-term dependencies requires deeper architectures. Furthermore, the fixed architecture of temporal CNN prevents the same network from being used on sequences of different lengths or with different acquisition dates.</p><p>Lastly, 2D and 3D convolutions have been extensively used in video analysis for object segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref> or action recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>. However, specificities of satellite time series such as their different time-scale and resolution prevents the direct application of such networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we present the different components of our proposed architecture for encoding time series of mediumresolution multi-spectral images. We denote the observations of a given parcel by a spatio-spectro-temporal tensor</p><formula xml:id="formula_0">[x (0) , · · · , x (T ) ] T t=1 of size T × C × H × W ,</formula><p>with T the number of temporal observations, C the number of spectral channels, and H and W the dimension in pixels of a tight bounding box containing the spatial extent of the parcel. All values are set to 0 outside the parcel's borders, as shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatial Encoder</head><p>In recent years, CNNs have become the established approach to extract spatial features from images. However, our analysis suggests that convolutions may not be wellsuited for the analysis of medium-resolution satellite images of agricultural parcels. Indeed, as mentioned above, the typical spatial resolution of satellites with high revisit frequency struggles to capture textural information. Second, efficiently training CNNs requires organizing the data into batches of images of identical dimensions. The irregular size of the parcels makes this process very memory intensive. Indeed, to limit textural information loss for large parcels, this amounts to oversampling most smaller parcels several times over.</p><p>To circumvent both these issues, we propose an alternative architecture called Pixel-Set Encoder (PSE) and inspired by the point-set encoder PointNet <ref type="bibr" target="#b28">[29]</ref> and the Deep-Set architecture <ref type="bibr" target="#b44">[45]</ref> commonly used for 3D point cloud processing. The motivation behind this design is that, instead of textural information, the network computes learned statistical descriptors of the spectral distribution of the parcel's observations.</p><p>The network proceeds as follows to embed an input observation x (t) : i) A set S ⊂ [1, · · · , N ] of S pixels is randomly drawn from the N pixels within the parcel, as described in Equation 1. When the total number of pixels in the image is smaller than S, an arbitrary pixel is repeated to match this fixed size. The same set S is used for sampling all T acquisitions of a given parcel.</p><p>ii) Each sampled pixel s is processed by a shared multilayer perceptron MLP 1 , as seen in Equation 2, composed of a succession of fully-connected layers, batchnorms <ref type="bibr" target="#b15">[16]</ref>, and Rectified Linear Units <ref type="bibr" target="#b23">[24]</ref>.</p><p>iii) The resulting set of values is pooled along the pixel axis-of dimension S-to obtain a vector capturing the statistics of the whole parcel and which is invariant by permutation of the pixels' indices. We concatenate to this learned feature a vector of pre-computed geometric features f : perimeter, pixel count N , cover ratio (N divided by the number of pixels in the bounding box) and the ratio between perimeter and surface of the parcel.</p><p>iv) This vector is processed by another perceptron MLP 2 , as shown in Equation 3, to yield e (t) the parcel's spatio-spectral embedding at time t.</p><p>The PSE architecture is represented in <ref type="figure" target="#fig_0">Figure 2</ref>, and can be summarized by the following equations:</p><formula xml:id="formula_1">S = sample (S, N ) (1) e (t) s = MLP 1 x (t) s , ∀s ∈ S (2) e (t) = MLP 2 pooling {ê (t) s } s∈S , f .<label>(3)</label></formula><p>Among possible pooling operations, we had the best results for the concatenation of the mean and the standard deviation across the sampled pixel dimension S. For parcels smaller than S, repeated pixels should be removed before pooling to obtain unbiased estimates.</p><p>Although only a limited amount of information per parcel is used by this encoder, the sampling being different at each training step ensures the learning of robust embeddings exploiting all the available information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Temporal Attention Encoder</head><p>RNNs have proven efficient for encoding sequential information <ref type="bibr" target="#b22">[23]</ref>. However, since RNNs process the elements of the sequence successively, they prevent parallelization and incur long training times. In <ref type="bibr" target="#b38">[39]</ref>, Vaswani et al. introduce the Transformer architecture, an attention-based network achieving equal or better performance than RNNs on text translation tasks, while being completely parallelizable and thus faster. We propose to adapt their ideas to the the encoding of satellite image time series.</p><p>Transformer Network: In the original Transformer model a query-key-value triplet q (t) , k (t) , v (t) is computed simultaneously for each element of the input sequence by three fully-connected layers. For a given element of a sequence, the key k (t) conveys information about the nature of its content, while the value v (t) encodes the content itself. The output of a given element is defined as the sum of the values of previous elements weighted by an attention mask. This mask is defined as the compatibility (dot product) of the keys of the previous elements with the query q (t) , re-scaled through a modified softmax layer. In other words, each element indicates which kind of information it needs through its query, and what sort of information it contains through its key.</p><p>Since the computation of the triplets q (t) , k (t) , v (t) and their multiplications can be performed in parallel, the Transformer takes full advantage of modern GPU architecture and boasts a significant speed increase compared to recurrent architectures. This procedure can be computed several times in parallel with different set of independent parameters, or heads. This approach, called multi-head attention, allows for the specialization of different set of query-key compatibility.</p><p>Positional Encoding: In their paper on text translation, Vaswani et al. add order information to elements of the input sequence by adding a positional encoding tensor to each element. Equation 4 describes this positional encoding of the observation t, with d e the dimension of the input, and i the coordinates of the positional encoding. Since our considered sequences are typically shorter than the ones considered in NLP, we chose τ = 1 000-instead of 10 000. Additionally, day(t) is the number of days since the first observation for observation t instead of its index. This helps to account for inconsistent temporal sampling (see <ref type="figure">Figure 1</ref>).</p><formula xml:id="formula_2">[p (t) ] de i=1 = sin day(t)\τ 2i de + π 2 mod(i, 2)<label>(4)</label></formula><p>End-to-End Encoding: The original Transformer network takes pretrained word embeddings as inputs. In our setting however, the parameters of the network producing the inputs are learnt simultaneously to the attention parameters. Therefore, we propose that each head only computes key-query pairs from the spatial embeddings (5) since these embeddings can directly serve as values: v (t) = e (t) + p (t) . This removes needless computations, and avoids a potential information bottleneck when computing the values.</p><p>Sequence-to-Embedding Attention: While the original Transformer produces an output for each element of a sequence, our goal is to encode an entire time series into a single embedding. Consequently, we only retain the encoder part of the Transformer and define a single master queryq h for each head h. Such a query, in combination with the keys of the elements of the sequence, determines which dates contain the most useful information. A first approach would be to select the query of a given date, such as the last one. However, the selected element of the sequence may not contain enough information to produce a meaningful query. Instead, we propose to construct the master query as a temporal average of the queries of all dates and processed by a single fully-connected layer <ref type="bibr" target="#b5">(6)</ref>. As shown in Equation 7, this query is then multiplied with the keys of all elements of the sequence to determine a single attention mask a (h) ∈ [0, 1] T , in turn weighting the input sequence of embeddings <ref type="bibr" target="#b7">(8)</ref>.</p><p>Multi-Head Self-Attention: We concatenate the output o h of each head h for the H different heads and process the resulting tensor with MLP 3 , to obtain the final output o of the Temporal Attention Encoder (TAE), as shown in <ref type="bibr">Equation 9</ref>. Note that unlike the Transformer network, we directly useô as the spatio-temporal embedding instead of using residual connections.</p><p>Temporal Attention Encoder For each head h, we denote by FC the fully-connected layer yielding the master query, and d k the shared dimensions of keys and queries. Our attention mechanism can be summarized by the following equations for all t ∈ [1, . . . , T ] and h ∈ [1, · · · , H]:</p><formula xml:id="formula_3">k (t) h , q (t) h = FC (h) 1 e (t) + p (t) (5) q h = FC (h) 2 mean {q (t) h } T t=1 (6) a h = softmax 1 √ d k q h · k (t) h T t=1 (7) o h = T t=1 a h [t] e (t) + p (t) (8) o = MLP 3 ([o 1 , · · · , o H ]) .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spatio-Temporal Classifier</head><p>Our spatio-temporal classifier architecture combines the two components presented in the previous sections: all input images of the time series are embedded in parallel by a shared PSE, and the resulting sequence of embeddings is processed by the temporal encoder, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Finally, the resulting embedding is processed by an MLP decoder MLP 4 to produce class logits y:</p><formula xml:id="formula_4">y = MLP 4 (ô) .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation details</head><p>All the architectures presented here are implemented in PyTorch, and released on GitHub upon publication. <ref type="bibr" target="#b0">1</ref> We trained all models on a machine with a single GPU (Nvidia 1080Ti) and an 8-core Intel i7 CPU for data loading from an SSD hard drive. We chose the hyperparameters of each architecture presented in the numerical experiments such that they all have approximately 150k trainable parameters. The exact configuration of our network is displayed in Table 1. We use the Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with its default values (lr = 10 −3 , β = (0.9, 0.999)) and a batch size of 128 parcels. We train the models with focal loss <ref type="bibr" target="#b21">[22]</ref> (γ = 1) and implement a 5-fold cross-validation scheme: for each fold the dataset is split into train, validation, and test set with a 3:1:1 ratio. The networks are trained for 100 epochs, which is sufficient for all models to achieve convergence. We use the validation step to select the bestperforming epoch, and evaluate it on the test set. For augmentation purpose, we add a random Gaussian noise to x (t) with standard deviation 10 −2 and clipped to 5.10 −2 on the values of the pixels, normalized channel-wise and for each date individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Numerical Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluate our models using Sentinel-2 multi-spectral image sequences in top-of-canopy reflectance. We leave out the atmospheric bands (bands 1, 9, and 10), keeping C = 10 spectral bands. The six 20m-resolution bands are resampled to the maximum spatial resolution of 10m. The area of interest (AOI) corresponds to a single tile of the Sentinel-2 tiling grid (T31TFM) in southern France. This tile provides a challenging use case with a high diversity of crop type and different terrain conditions. The AOI spans a surface of 12 100 km 2 and contains 191 703 individual parcels, all observed on 24 dates from January to October 2017. The values of cloudy pixels are linearly interpolated from the first previous and next available pixel using Orfeo Toolbox <ref type="bibr" target="#b6">[7]</ref>.</p><p>We retrieve the geo-referenced polygon and class label of each parcel from the French Land Parcel Identification System records. <ref type="bibr" target="#b1">2</ref> We crop the satellite images using this polygon to constitute the image time series.</p><p>Data Preparation: In order to evaluate both ours and convolution-based methods, we organize the parcels into two different formats: patches and pixel sets.</p><p>In the patch format, we resize each parcel into a tensor of size T × C × 32 × 32 by interpolating each spectral channel and temporal acquisition independently into patches of fixed size 32 × 32. We use nearest neighbor interpolation, and both the horizontal and vertical axes are rescaled so that the overall shape of the parcel may be altered. We use zero-padding outside the extent of the parcel (see <ref type="figure">Figure 1</ref>). This same size of 32 pixels was used in <ref type="bibr" target="#b33">[34]</ref>, while a larger 48 × 48 patch size was used in <ref type="bibr" target="#b30">[31]</ref>, albeit for a pixel-wise classification task.</p><p>For the pixel-set format, the pixels of each parcels are stored in arbitrary order into a tensor of size T × C × N , with N the total number of pixels in a given parcel. Note that this format neither lose nor create information, regardless of parcel size. Hence, this setup saves up to 70% disk space compared to the patch format (28.6Gb vs. 98.1Gb). Note that the geometric features f must be computed and saved before preparing the dataset, as all spatial structure is henceforth lost.</p><p>The classification labels are defined with respect to a 20 class nomenclature designed by the subsidy allocation authority of France. We show the class break-down on the AOI in <ref type="figure" target="#fig_2">Figure 3</ref>. The dataset is highly imbalanced as is often the case in such real word applications and this motivated the use of the focal loss to train our models.</p><p>Both datasets will be released upon publication. <ref type="bibr" target="#b2">3</ref> To the best of our knowledge, no benchmark dataset currently exists for object-based agricultural parcel classification. Our datasets are a first step towards more reproducible and comparable methodological work in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-Art</head><p>Competing Methods: We compare our approach to recent algorithms operating on similar dataset, which we have re-implemented. The different hyperparameters chosen for each model are shown in the appendix. All share the same decoding layer configuration MLP 4 .</p><p>CNN+GRU In <ref type="bibr" target="#b33">[34]</ref>, Garnot et al. propose a similar approach to ours, but with CNNs instead of PSE, and GRUs instead of our proposed temporal encoder. The last hidden state of the recurrent unit is used as input to MLP 4 for classification.</p><p>CNN+TempCNN In <ref type="bibr" target="#b27">[28]</ref>, Pelletier et al. propose to use one-dimensional temporal convolution to address the sequential nature of the observations. While their approach is applied on a per-pixel classification task and therefore not comparable, we have implemented a variation of CNN+GRU in which the GRUs are replaced with one-dimensional convolutions as the closest translation of their ideas.</p><p>Transformer In <ref type="bibr" target="#b32">[33]</ref>, Rußwurm et al. perform objectbased classification with the encoder part of the Transformer network. They do not use a spatial encoder and compute average values of the different spectral bands over each parcel. Furthermore they produce a single embedding for the whole sequence with a global maximum pooling through the temporal dimension of the output sequence. We re-implemented the same pipeline and simply modified the hyperparameters to match the 150k parameters constraint.</p><p>ConvLSTM In <ref type="bibr" target="#b30">[31]</ref>, Rußwurm et al. process the time series of patch images with a ConvLSTM network <ref type="bibr" target="#b43">[44]</ref> for pixel-based classification. We adapt the architecture to the parcel-based setting by using the spatiallyaveraged last hidden state of the ConvLSTM cell to be processed by MLP 4 .</p><p>Random Forest Lastly, we use a Random Forest classifier with 100 trees as a non-deep learning baseline. The classifier operates on handcrafted features comprised of the mean and standard deviation of each band within the parcel, and concatenated along the temporal axis, as described by <ref type="bibr" target="#b2">[3]</ref>.</p><p>We present the results of our experiments in <ref type="table" target="#tab_2">Table 2</ref>. Our proposed architecture outperforms the other deep learning models in Overall Accuracy (OA) by 0.4 points, and mean per-class Intersect over Union (mIoU) by 3 to 9 points. It also provides a four-fold speed up over convolution-based methods, and a decrease in disk usage of over 70% for training, and close to 90% when considering the inference task alone (i.e. when only S pixels per parcels are kept). This speed-up is due both to improved loading time as the pixel set dataset is smaller, but also inference and backpropagation time, as detailed in <ref type="table" target="#tab_2">Table 2</ref> of the appendix. While the temporal convolutions of TempCNN are faster to train, they yield worse performance and suffer from the limitations discussed in section 2. The Transformer method, which processes pre-computed parcel means, is also faster to train, but only achieves a 46.3 mIoU score.</p><p>Beyond its poor precision, the RF classifier has a significant speed and memory advantage. This can explain its persisting popularity among practitioners. However, our approach bridges in part this performance gap and provides much higher classification rates, making it a compelling strategy for large-scale object-based crop type mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>In order to independently assess the contribution of the spatial and temporal components of our proposed architecture, we present in <ref type="table" target="#tab_3">Table 3</ref> the results obtained when alternatively replacing the PSE by a CNN (CNN+TAE) or the TAE by a GRU (PSE+GRU). <ref type="table" target="#tab_3">Table 3</ref>, the PSE accounts for an increase of 1.7 points of mIoU compared to the CNN-based model (CNN+TAE). This supports both the hypothesis that CNNs are only partly relevant on mediumresolution images, and that considering the image as an unordered set of pixels is a valid alternative. Not only does this approach yield better classification performance, but it also circumvents the problem of image batching, which leads to faster data loading (see <ref type="table" target="#tab_2">Table 2</ref> in the appendix). Additionally, we train a TAE on pre-computed means and standard deviations of the spectral channels over the parcels (MS+TAE), which achieves a 48.9 mIoU score. We can thus conclude that the PSE learns statistical descriptors of the acquisitions' spectra which are more meaningful than simple means and variances or convolutional features. <ref type="table" target="#tab_3">Table 3</ref>, the performance of our architecture without geometric features f . The resulting 0.9 point decrease in mIoU confirms that geometric information plays a role in the classification process. We note that, even without such features, our proposed approach outperforms the convolution-based model (CNN+TAE ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution of the PSE: As seen in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design of the PSE: We show in</head><p>We have tried replacing the handcrafted geometric features f with a CNN operating on the binary mask of the parcel. However, the gains were minimal, and we removed this extra step for simplicity's sake.</p><p>Lastly, we tried training our architecture with a reduced number of sampled pixels (S = 16, and S = 32   Contribution of the TAE: Replacing the temporal attention encoder with a GRU (PSE+GRU) decreases the performance by 3.6 points of mIoU ( <ref type="table" target="#tab_3">Table 3</ref>). The TAE not only produces a better classification but also trains faster thanks to parallelization. Unlike the comparison between Transformer and RNNs architectures in <ref type="bibr" target="#b32">[33]</ref>, our modified self-attention mechanism extracts more expressive features than the RNN-based approach.</p><p>We also evaluate the influence of the positional encoding p of the Transformer by adding p to the input tensors of the GRU unit (PSE+GRU+p). This reduces the gap with our method to 2.2 points of mIoU. This shows that the improvement brought by the TAE is due to both its structure and the use of a positional encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design of the TAE:</head><p>In order to evaluate the benefits of our different contributions over the Transformer, we adapted the architecture presented in <ref type="bibr" target="#b32">[33]</ref> to use a PSE network instead of spectral means for embedding parcels (PSE+Transformer), for a performance 4.3 points below our TAE. By replacing the proposed temporal max-pooling by our our master query forming scheme (PSE+Transformer+q), we observed an increase of 2.9 points of mIoU. The remaining 1.4 mIoU points between this implementation and ours can thus be attributed to our direct use of inputs to compute the TAE's output instead of a smaller intermediary value tensor.</p><p>Finally, we compare our mean-pooling strategy with max-pooling (q = max t q (t) ) and computing the master query from the last element of the sequence (q = q (T ) ). While the mean query approach yields the best performance, the last element of the sequence in our dataset produces a meaningful query as well. However, this may not be the case for other regions or acquisition years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we considered the problem of object-based classification from time series of satellite images. We proposed to view such images as unordered sets of pixels to reflect the typical coarseness of their spatial resolution, and introduced a fitting encoder. To exploit the temporal dimension of such series, we adapted the Transformer architecture <ref type="bibr" target="#b38">[39]</ref> for embedding time-sequences. We introduced a master query forming strategy, and exploited that our network learns end-to-end to simplify some operations.</p><p>Evaluated on our new open-access annotated benchmark of agricultural parcels, our method produces a better classification than all other re-implemented methods. Furthermore, our network is several times faster and more parsimonious in memory than other state-of-the-art methods such as convolutional-recurrent hybrid networks. We hope that by mitigating some of the limitations of deep learning methods such as processing time and memory requirement, our approach would accelerate their adoption in real-life, largescale Earth observation applications.</p><p>Our results suggest that attention-based models are an interesting venue to explore for analysing the temporal profiles of satellite time series, as well as other analogous vision tasks such as action recognition in videos. Likewise, set-based encoders are a promising and overlooked paradigm for working with the coarser resolutions of remote sensing applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We show the hyperparameters of the different competing methods in <ref type="table" target="#tab_0">Table 1</ref>. We also provide a breakdown of the processing times during training for the different architectures in <ref type="table" target="#tab_2">Table 2</ref>. The average time per batch is decomposed into data loading time, forward pass and gradient backpropagation.  <ref type="table" target="#tab_2">Table 2</ref>: Comparison of processing time for different methods for batches of 128 parcels. We can see that the processing time is dominated by the loading time except for the Transformer which processes pre-computed means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of parameters</head><p>Lastly, for a more qualitative evaluation of our PSE+TAE architecture, we provide its confusion matrix on the test set on <ref type="figure">Figure 1</ref> as well as a visual representation of its predictions on <ref type="figure">Figure</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Schematic view of our spatio-temporal encoder. Variables in bold are tensors concatenated along the temporal dimension, e.g. e = [e (0) , · · · , e (T ) ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Class repartition in the AOI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 .Figure 1 :Figure 2 :</head><label>212</label><figDesc>M e a d o w W i n t e r D u r u m W h e a t S p r i n g C e r e a l S u m m e r C e r e a l W i n t e r C e r e a l S o r g h o / M i l l e t / M o h a C e r e a l L e g u m i n o u s F o d d e r O t h e r F o d d e r W i n t e r R a p e s e e d S u n f l o w e r S o y P r o t e i n C r o p s P o t a t o F r u i t s / L e g u m e s / F l o w e r s L i g n e o u s O r c h a r d G r a p e v i n e N o n -A g r i c u l t u r a l W o o d la n Confusion matrix for our PSE+TAE architecture on the AOI. The color represents the number of parcels, expressed relatively to the total population of the class they belong to. We note many of the errors are misclassification as Meadows, the most represented class in our dataset. Additionally, the model struggles to discriminate between Winter Durum Wheat and Winter Cereal, likely due to their similar phenology. The left picture shows the true labels of a set of parcels, drawn from the test set. The right hand figure shows the parcels for which our PSE+TAE architecture produced a correct prediction in green and a false prediction in red. On both figures, the background corresponds to a Sentinel-2 observation (May 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Modules</cell><cell>Hyperparameters</cell><cell>Number of parameters</cell></row><row><cell>PSE</cell><cell></cell><cell>19 936</cell></row><row><cell>S</cell><cell>64</cell><cell></cell></row><row><cell>MLP 1</cell><cell>10 → 32 → 64</cell><cell></cell></row><row><cell>MLP 2</cell><cell>68 → 128</cell><cell></cell></row><row><cell>TAE</cell><cell></cell><cell>116 480</cell></row><row><cell cols="2">d e , d k , H 128, 32, 4</cell><cell></cell></row><row><cell>FC 1</cell><cell>128 → (32 × 2)</cell><cell></cell></row><row><cell>FC 2</cell><cell>32 → 32</cell><cell></cell></row><row><cell>MLP 3</cell><cell>512 → 128 → 128</cell><cell></cell></row><row><cell>Decoder</cell><cell></cell><cell>11 180</cell></row><row><cell>MLP 4</cell><cell>128 → 64 → 32 → 20</cell><cell></cell></row><row><cell>Total</cell><cell></cell><cell>147 604</cell></row></table><note>Configuration of our model chosen for the numer- ical experiments. The dimension of each successive feature space is given for MLPs and fully connected layers. We show the corresponding number of trainable parameters on the last column.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Classification metrics and time benchmark of the different architectures. The inter-fold standard deviation of the OA and mIoU is given in smaller font. Additionally, the total time for one epoch of training, and for inference on the complete dataset are given on the third and fourth columns. 1 disk space required for training and pure inference, 2 time for the entire training step, 3 preprocessing and inference time, 4 dataset before and after preprocessing. ±0.1 50.1 ±0.<ref type="bibr" target="#b4">5</ref> No geometric features 93.9 ±0.1 50.0 ±0.7</figDesc><table><row><cell cols="3">model maintains a good performance with an mIoU over</cell></row><row><cell cols="3">50 points. This indicates that the decrease in processing</cell></row><row><cell cols="3">time and memory could be further improved at the cost of a</cell></row><row><cell>minor drop in precision.</cell><cell></cell></row><row><cell></cell><cell>O.A.</cell><cell>mIoU</cell></row><row><cell>PSE+TAE (ours)</cell><cell cols="2">94.2 ±0.1 50.9 ±0.8</cell></row><row><cell>q = q (T )</cell><cell cols="2">94.2 ±0.1 50.7 ±0.5</cell></row><row><cell>S = 16</cell><cell cols="2">94.3 ±0.2 50.5 ±0.8</cell></row><row><cell>q = max t q (t)</cell><cell cols="2">94.2 ±0.2 50.3 ±0.7</cell></row><row><cell cols="3">S = 32 94.2 PSE+Transformer+q 94.1 ±0.2 49.5 ±0.7</cell></row><row><cell>CNN+TAE</cell><cell cols="2">94.0 ±0.1 49.2 ±1.1</cell></row><row><cell>MS+TAE</cell><cell cols="2">93.7 ±0.1 48.9 ±0.9</cell></row><row><cell>PSE+GRU+p</cell><cell cols="2">93.6 ±0.2 48.7 ±0.3</cell></row><row><cell>PSE+GRU</cell><cell cols="2">93.6 ±0.2 47.3 ±0.3</cell></row><row><cell>PSE+Transformer</cell><cell cols="2">93.4 ±0.2 46.6 ±0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of our different design choices, sorted by decreasing mIoU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>= 32, d v = 64, d model = 128, d inner = 256 • n head = 4, n layer = 1 Hyperparameters of the competing architectures. For all models we use the same values for the decoder MLP 3 .</figDesc><table><row><cell>CNN+GRU</cell><cell></cell><cell></cell><cell></cell><cell>144 204</cell></row><row><cell cols="3">• 3 × 3 convolutions: 32, 32, 64 kernels</cell><cell></cell><cell></cell></row><row><cell cols="2">• Global average pooling</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">• Fully connected layer: 128 neurons</cell><cell></cell><cell></cell></row><row><cell cols="2">• Hidden state size: 130</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNN+TempCNN</cell><cell></cell><cell></cell><cell></cell><cell>156 788</cell></row><row><cell cols="3">• 3 × 3 convolutions: 32, 32, 64 kernels</cell><cell></cell><cell></cell></row><row><cell cols="2">• Global average pooling</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">• Fully connected layer: 64 neurons</cell><cell></cell><cell></cell></row><row><cell cols="2">• Temporal convolutions:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">32, 32, 64 kernels of size 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>• Flatten layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell></cell><cell></cell><cell></cell><cell>178 504</cell></row><row><cell>• d k ConvLSTM</cell><cell></cell><cell></cell><cell></cell><cell>178 356</cell></row><row><cell cols="2">• Hidden feature maps: 64</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">• Number of trees: 100</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Time in ms/batch</cell><cell cols="4">Total Loading Forward Backward</cell></row><row><cell>PSE+TAE (ours)</cell><cell>107</cell><cell>85</cell><cell>11</cell><cell>11</cell></row><row><cell>CNN+TempCNN</cell><cell>381</cell><cell>365</cell><cell>4</cell><cell>12</cell></row><row><cell>CNN+GRU</cell><cell>437</cell><cell>365</cell><cell>14</cell><cell>58</cell></row><row><cell>Transformer</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>5</cell></row><row><cell>ConvLSTM</cell><cell>530</cell><cell>365</cell><cell>61</cell><cell>104</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">github.com/VSainteuf/psetae</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://professionnels.ign.fr/rpg 3 github.com/VSainteuf/psetae</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The common agricultural policy at a glance, accessed nov</title>
		<ptr target="https://ec.europa.eu/info/food-farming-fisheries/key-policies/common-agricultural-policy/cap-glance_en" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="https://www.copernicus.eu/sites/default/files/2018-10/AGRI_Conceptnote.pdf" />
		<title level="m">Concept note: Towards future copernicus service componentes in support to agriculture, accessed nov</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Crop-rotation structured classification using multi-source Sentinel images and lpis for crop type mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nesrine</forename><surname>Chehata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IGARSS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sentinel-2 cropland mapping using pixel-based and object-based time-weighted dynamic time warping analysis. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Belgiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ovidiu</forename><surname>Csillik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Quo vadis, action recognition? A new model and the kinetics dataset. ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Orfeo toolbox: a complete solution for mapping from high resolution satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Christophe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Inglada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Giros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Support vector machine classification of object-based data for crop mapping, using multi-temporal landsat imagery. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Devadas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pringle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sentinel-2: Esa&apos;s optical high-resolution mission for gmes operational services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Drusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><forename type="middle">Del</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Carlier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Colin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Gascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Hoersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Laberinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Martimort</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Remote Sensing of Environment</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Land cover classification via multitemporal spatial data by deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dino</forename><surname>Ienco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Gaetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Dupaquier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Maurel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IEEE Geoscience and Remote Sensing Letters</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Assessment of an operational system for crop type map production using high temporal and spatial resolution satellite optical imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Inglada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Hagolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Dedieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guadalupe</forename><surname>Sepulcre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Bontemps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Defourny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for crop classification with multi-temporal remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunping</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep learning classification of land cover and crop types using remote sensing data. IEEE Geoscience and Remote Sensing Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataliia</forename><surname>Kussul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Lavreniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergii</forename><surname>Skakun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrii</forename><surname>Shelestov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Parcel based classification for agricultural mapping and monitoring using multitemporal satellite image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataliia</forename><surname>Kussul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergii</forename><surname>Skakun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Lavreniuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IGARSS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A critical review of recurrent neural networks for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elkan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00019</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Rectified linear units improve restricted Boltzmann machines. ICML</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep recurrent neural network for agricultural classification using multitemporal sar Sentinel-1 for camargue, france</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Ndikumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Ho Tong Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Courault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Hossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A deep learning hybrid CNN framework approach for vegetation cover mapping using deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Nijhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshita</forename><surname>Sahni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashita</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting urban changes with recurrent neural networks from multitemporal sentinel-2 data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Papadomanolaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Vakalopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Karantzalos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IGARSS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Temporal convolutional neural network for the classification of satellite image time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Petitjean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Temporal vegetation modelling using long short-term memory networks for crop identification from medium-resolution multi-spectral satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Rußwurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Körner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional LSTMs for cloud-robust segmentation of remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Rußwurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Körner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-temporal land cover classification with sequential recurrent encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Rußwurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Körner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IS-PRS International Journal of Geo-Information</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self-attention for raw optical satellite time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Rußwurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Körner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10536</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Time-space tradeoff in deep learning models for crop classification on satellite multispectral image time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Sainte Fare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Garnot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nesrine</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chehata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IGARSS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Seunghak Shin, and In So Kweon. Pixel-level matching for video object segmentation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A hidden markov models approach for crop classification: Linking crop phenology to time series of multi-sensor remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofia</forename><surname>Siachalou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Mallinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Tsakiri-Strati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Helping facebook connect the world with deep learning</title>
		<ptr target="http://blog.digitalglobe.com/news/helping-facebook-connect-the-world-with-deep-learning/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Maxar Technologies</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Red and photographic infrared linear combinations for monitoring vegetation. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Compton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. NIPS</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Vegetation phenology from Sentinel-2 and field cameras for a dutch barrier island</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Vrieling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Meroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshanak</forename><surname>Darvishzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Skidmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Zurita-Milla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oosterbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paganini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Remote Sensing of Environment</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How much does multi-temporal Sentinel-2 data improve crop type classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Vuolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Neuwirth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Immitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Atzberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Tim</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Earth Observation and Geoinformation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Large-area crop mapping using time-series modis 250m NDV data: An assessment for the us central great plains. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wardlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Egbert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Photogrammetric Engineering &amp; Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Darrel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Goward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arvidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Landsat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
		<title level="m">Convolutional LSTM network: A machine learning approach for precipitation nowcasting. NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
		<title level="m">Deep sets. NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Mapping paddy rice using a convolutional neural network (CNN) with landsat 8 datasets in the dongting lake area, china. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A support vector machine to identify irrigated crop types using time-series landsat NDVI data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baojuan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Soe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Myint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rimjhim M</forename><surname>Thenkabail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Earth Observation and Geoinformation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Efficient corn and soybean mapping with temporal extendability: A multi-year experiment using landsat imagery. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Biging</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep learning based multi-temporal crop classification. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
