<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Match Them Up: Visually Explainable Few-shot Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Wang</surname></persName>
							<email>3bowen.wang@is.ids.osaka-u.ac.jp4ryo.kawasaki@ophthal.med.osaka-u.ac.jp5li</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Datability Science (IDS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Datability Science (IDS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manisha</forename><surname>Verma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Datability Science (IDS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Datability Science (IDS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Kawasaki</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Medicine</orgName>
								<orgName type="institution">Osaka University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Nagahara</surname></persName>
							<email>nagahara@ids.osaka-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Datability Science (IDS)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Match Them Up: Visually Explainable Few-shot Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot learning (FSL) approaches are usually based on an assumption that the pre-trained knowledge can be obtained from base (seen) categories and can be well transferred to novel (unseen) categories. However, there is no guarantee, especially for the latter part. This issue leads to the unknown nature of the inference process in most FSL methods, which hampers its application in some risksensitive areas. In this paper, we reveal a new way to perform FSL for image classification, using visual representations from the backbone model and weights generated by a newly-emerged explainable classifier. The weighted representations only include a minimum number of distinguishable features and the visualized weights can serve as an informative hint for the FSL process. Finally, a discriminator will compare the representations of each pair of the images in the support set and the query set. Pairs with the highest scores will decide the classification results. Experimental results prove that the proposed method can achieve both good accuracy and satisfactory explainability on three mainstream datasets. Code is available 6 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Few-shot learning (FSL) is of great significance at least for the following two scenarios <ref type="bibr" target="#b44">[43]</ref>: Firstly, FSL can relieve the heavy needs for data gathering and labeling, which can boost the ubiquitous use of deep learning techniques, especially for users without enough resources. Secondly, FSL is an important solution for applications in which rare cases matter or image acquisition is costly because of high operation difficulty or ethical issues. Typical examples of such applications include computer assisted diagnosis with medical imaging, classification of endangered species, etc.</p><p>There have been lots of FSL methods, most of which  are based on the assumption that knowledge can be well extracted from base (seen) classes and transferred to novel (unseen) classes. However, this is not always the case. The knowledge in a pre-trained backbone convolutional neural network (CNN), which computes features of an input image, may sometimes be useless when novel categories have significant visual differences from images of the base categories <ref type="bibr" target="#b47">[46]</ref>. What makes matter worse is that we even have no way to see if the visual differences between the base and novel categories are significant for an FSL model. This raised one essential question: Is there any way to see what is actually transferred? Actually, in the FSL task, most works only treat the convolutional layer as the image embedding tool, and do not pay attention to the reasons for the extracted features. In this paper, we redesign the mechanism of knowledge transfer for FSL tasks, which offers an answer to the above question. Our approach is inspired by what human beings actually do when trying to recognize a rarely seen object. That is, we usually try to find some patterns in the object and match them in a small number of seen examples in our memory. We adopt a recently-emerged explainable classifier, called SCOUTER <ref type="bibr" target="#b19">[20]</ref>, and propose a new FSL method, named match-them-up network (MTUNet) consisted of pattern extractor (PE) and pairwise matching (PM).</p><p>PE is designed for finding discriminative and consistent patterns for image representation. The knowledge transferred from the base categories to the novel categories is the learned patterns. Owing to the explainability of SCOUTER, the extracted patterns themselves can be easily visualized by exemplifying them in the images as shown in <ref type="figure" target="#fig_1">Figure  1</ref>(a). This directly means that we have a way to see what is actually transferred in our FSL pipeline. The patterns extracted in each of the support and query images are aggregated to form discriminative image representation (overall attention), which is used for matching. As shown in <ref type="figure" target="#fig_1">Figure  1</ref>(b), the visualization of aggregated patterns collectively shows a consistent and meaningful clue for the images of the same category. For example, PE shows strong attention on the neck of the goose in the second column, which is consistent in both support and query images. Image representation based on the patterns from base categories makes matching between a pair of images much easier by incorporating only a small number of regions to pay attention to.</p><p>On top of the PE, PM is adopted to judge whether image pairs belong to the same category or not. Each pair consists of one image from a support set and one image from a query set. The category of the support image that gives the highest score is regarded as the query image's category. Together with PE, MTUNet can provide a matching score matrix to further relate the visualization and the model decision.</p><p>The main contributions of our work include: </p><formula xml:id="formula_0">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Few-shot Learning</head><p>Recently, deep neural networks have achieved outstanding performance in various classification tasks, thanks to the availability of a sufficient number of images for each category. Such large datasets usually require lots of effort for their creation, and some tasks, such as medical tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7]</ref>, may not inherently have enough supervising signals. For these tasks, we need a new paradigm that allows learning a model with a small number of labeled images. The popular FSL tasks <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b22">23]</ref> serve as a testbed for some certain aspects of such small tasks. Recent efforts toward FSL are summarized as follows.</p><p>Image Embedding and Metric Learning. Many works focus on how to transform images into vectors in embedding space, in which the distance between a pair of vectors represents the conceptual dissimilarity. An early approach uses Siamese networks <ref type="bibr" target="#b17">[18]</ref> as a shared feature extractor to produce image embeddings for both support and query images. The weighted 1 distance is used for the classification criterion. Some use a multi-layer perceptron (MLP) to parameterize and learn a classifier <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">37]</ref>. Metric learning can offer a better way to train the mapping into the embedding space <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b36">35]</ref>. Some works try to improve the discriminatory power of image embeddings. Simple Shot <ref type="bibr" target="#b42">[41]</ref> applies 2 normalization and Central method to make the distance calculation easier.</p><p>Meta-learning. Another major approach to FSL is to optimize models so that they can rapidly adapt to new tasks. It is a good thing that adapting feature extractor to new tasks at the novel test time. Fine-tuning transfer-learned networks <ref type="bibr" target="#b46">[45]</ref> fine-tune the feature extractor using the task-specific support images. MAML <ref type="bibr" target="#b7">[8]</ref> and its extensions <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25]</ref> train a set of initialization parameters, and through one or more steps of gradient adjustment on the basis of the initial parameters, they can be easily adapted to a new task with only a small amount of data. Besides training a good parameter initialization, Meta-SGD <ref type="bibr" target="#b20">[21]</ref> also trains the parameter update direction and step size.</p><p>Data Augmentation. Data augmentation aims at introducing immutability to models to capture in both image and feature levels <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b3">4]</ref>. Some works try to use samples which are weakly labeled or unlabeled <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>. ICI <ref type="bibr" target="#b43">[42]</ref> introduces a judgment mechanism to increase training samples. It is always worth increasing the training set by utilizing the unlabeled data with confidently predicted labels. In general, solving an FSL problem by augmenting D train is straightforward and easy to understand.</p><p>Transductive or Semi-supervised Paradigm. Transductive or semi-supervised approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5]</ref> have gained popularity, which makes great progress in the past few years. They use the statistics of query examples or statis-  tics across the few-shot tasks, which assumes that all novel images are ready beforehand. We only employ the original inductive paradigm to explore explainable feature extraction, but our idea can be easily adopted to the transductive paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Explainable AI</head><p>Deep neural networks are considered as a black-box technology, and explainable artificial intelligence (XAI) is a series of attempts to unveil them. Most XAI methods for classification tasks are based on back-propagation <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b2">3]</ref> or perturbation <ref type="bibr" target="#b32">[32]</ref>. All these methods are post-hoc, which can not be added to the model structure during training.</p><p>A few works <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref> have tried XAI for FSL tasks. Geng et al. <ref type="bibr" target="#b10">[11]</ref> uses a knowledge graph to make an explanation for zero-shot tasks. Sun et al. <ref type="bibr" target="#b37">[36]</ref> adopts layerwise relevance propagation (LRP) <ref type="bibr" target="#b0">[1]</ref> to explain the output of a classifier. StarNet <ref type="bibr" target="#b15">[16]</ref> realize visualization through heat maps derived from back-project. Recently, a new type of XAI, coined SCOUTER <ref type="bibr" target="#b19">[20]</ref>, has been proposed, which applies the self-attention mechanism <ref type="bibr" target="#b39">[38]</ref> to the classifier. This method can further extract the discriminant attentions for each category during training, which makes classification results explainable. We apply this technique to FSL tasks in order to explore a new explainable FSL paradigm. PE provides insights on why it classified an input image into a certain novel category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>This paper addresses an inductive FSL task (c.f ., transductive one <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>), in which we are given two disjoint sets D base and D novel of samples. The former is the base set that includes categories (C base ) with many labeled images. The latter is the novel set and include categories (C novel ) with a few labeled images. C base and C novel are disjoint. The FSL task is to find a mapping from a novel image x into the corresponding category y.</p><p>The literature typically uses the K-way N -shot episodic paradigm for training/evaluating FSL models. For each episode, we sample two subsets of D base for training, namely, support set S = {(x i , y i )|i = 1, . . . , K × N } and query set Q = {(x q i , y q i )|i = 1, . . . , K × M }. These images are of the same K categories in C base , and we sampled the same numbers of images (N images for the support set and M images for the query set). An FSL model is trained so that it can find a match between images in Q (with abuse of notation) and S. The image in Q is classified as the category of the matched image in S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overview</head><p>The overall process is illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>. In each episode, we extract feature maps F = f θ (x) ∈ R c×h×w from image x in both S and Q using backbone convolutional neural network f θ , where θ is the set of learnable parameters. F is then fed into the pattern extractor (PE) module, f φ , with learnable parameter set φ. This module gives attention A = f φ (F ) ∈ R z×l over F . Our pair matching (PM) module uses an MLP to compute the score of query image x q ∈ Q belonging to the category of x's in S.</p><p>PE plays a major role in the FSL task. PE is designed to learn a transferable attention mechanism. This ends up in finding common patterns that are shared among different episodes sampled from D base . Consequently the patterns are shared also among D novel given that D base and D novel are from similar domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pattern Extractor</head><p>SCOUTER is originally designed as an explainable classifier, of which decision is based directly on the existence of certain learned patterns in an image <ref type="bibr" target="#b19">[20]</ref>. It is built upon the self-attention mechanism <ref type="bibr" target="#b39">[38]</ref> to efficiently find common patterns in images of a certain category. In the context of FSL, we extend this idea to find common patterns to efficiently differentiate given sets of categories even across different episodes. The presence of a certain combination of these learned patterns gives a strong clue on the category of the image, facilitating classification even of novel categories. We implement our PE on top of SCOUTER.</p><p>The basic idea of PE is to find common patterns through the self-attention mechanism. Input feature maps F is firstly fed into a 1 × 1 convolution layer followed ReLU nonlinearity to squeeze the dimensionality of F from c to d. We flatten the spatial dimensions of the squeezed features to form F ∈ R d×l , where l = hw. To maintain the spatial information, position embedding P <ref type="bibr" target="#b45">[44,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref> is added to the features, i.e.,F = F + P .</p><p>The self-attention mechanism gives the attention over F for the spatial dimension by the dot-product similarity between a set of z learned patterns W ∈ R z×d (z is the number of the patterns) andF after nonlinear transformations g Q and g K . PE repeats this process with updating the pattern with a gated recurrent unit (GRU) to refine the attention. That is, given</p><formula xml:id="formula_1">g Q (W (t) ) ∈ R z×d , g K (F ) ∈ R d×l ,<label>(1)</label></formula><p>for the t-th repetition, the attention is given using certain normalization function ξ bȳ</p><formula xml:id="formula_2">A (t) = g Q (W (t) )g K (F ) ∈ (0, 1) z×l (2) A (t) = ξ(Ā (t) ).<label>(3)</label></formula><p>Patterns W (t) is updated T times (i.e., t = 1, . . . , T ) by</p><formula xml:id="formula_3">U (t) = A (t) F<label>(4)</label></formula><formula xml:id="formula_4">W (t+1) = GRU (U (t) , W (t) ).<label>(5)</label></formula><p>PE adopts a different normalization strategy from the original SCOUTER. Let Softmax R (X) and σ(X) be softmax over respective row vectors of matrix X and sigmoid. SCOUTER normalizes the attention map only over the flattened spatial dimensions, i.e.,</p><formula xml:id="formula_5">A (t) = σ(Ā (t) ).<label>(6)</label></formula><p>This allows finding multiple patterns in a single image. MTUNet further modulates this map by</p><formula xml:id="formula_6">A (t) = σ(Ā (t) ) Softmax R (Ā (t) ),<label>(7)</label></formula><p>which suppresses weak attention over different patterns at the same spatial location, where is the Hadamard product. This enforces the network to find more specific yet discriminative patterns with fewer correlations among them and thus ends up with more pinpoint attentions. The learned patterns can be more responsive in different images with this modulation as an attention map only responds to a single pattern that does not include its peripheral region. The input image is finally described by the overall attention corresponding to the extracted patterns, given by</p><formula xml:id="formula_7">V = 1 z A (T ) F 1 z ,<label>(8)</label></formula><p>where 1 z is the row vector with all z elements being 1. A (T ) is reshaped the l into the same spatial structure as F . V will then undergo an average pooling among spatial dimension and only keep the channel dimension c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pairwise Matching</head><p>An FSL classification can be solved by finding the membership of the query to one of the given support images. Some FSL methods use metric learning <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b40">39]</ref> to find matches between the query and the supports. The cosine similarity or the 2 distance are the typical choices <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">41]</ref>. Learnable distances are another popular choice for the metric learning-based FSL methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">37]</ref>. We use a learnable distance with an MLP.</p><p>Let V q be features of query image x q ∈ Q and V kn of support image x kn ∈ S, where the subscripts k and n stand for the n-th image of category k. For n &gt; 1, the average over the n images are taken to generate representative fea-tureV k ; otherwise (i.e., n = 1),V k = V k1 . For computing the membership score s of query image x q to category k, we use MLP f γ with learnable parameters γ:</p><formula xml:id="formula_8">s(x q , S k ) = σ(f γ ([V q ,V k ])),<label>(9)</label></formula><p>where [·, ·] is concatenation of two vectors for the one-toone pair and S k ⊂ S contains images of category k. x q is classified into the category with maximum s over S k for k = 1, 2, . . . , K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>After pre-training of the backbone CNN f θ , we first train the PE module in the same way as <ref type="bibr" target="#b19">[20]</ref>. The area loss facilitates to find compact patterns. For this training, we further split D base into two subsets D base,T and D base,V . The former contains 90% of images of each category and the latter contains the rest. We sample z categories from D base and use images of these categories in D base,T for training. The images of the same sampled categories in D base,V is used for validation. With these sampled categories, the training is trying to find discriminative patterns together with our attention map modulation in Eq. <ref type="bibr" target="#b6">(7)</ref>.</p><p>We then train MTUNet with the backbone and the PE module fixed. For Q and S sampled from D base for each episode, we train f γ with the binary cross-entropy loss:</p><formula xml:id="formula_9">L = − (x q ,y q )∈Q y q log(s(x q , S)),<label>(10)</label></formula><p>where s(x q , S) = (s 1 (x q , S 1 ), . . . , s K (x q , S K )) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datesets</head><p>We evaluate our approach on three commonly-used datasets, mini-ImageNet <ref type="bibr" target="#b40">[39]</ref>, tiered-ImageNet <ref type="bibr" target="#b29">[30]</ref>, and CIFAR-FS <ref type="bibr" target="#b1">[2]</ref>. Mini-ImageNet consists of 100 categories sampled from ImageNet with 600 images per class. These images are divided into the base D base , novel validation D val , and novel test D test sets with 64, 16, and 20 categories, respectively, where both D val and D test corresponded to D novel in Section 3.1. The images in miniImageNet are of size 84 × 84. Tiered-ImageNet consists of ImageNet 608 classes divided into 351 base classes, 97 novel validation classes, and 160 novel test classes. There are 779,165 images with size 84 × 84. CIFAR-FS is a dataset with images from CIFAR-100 <ref type="bibr" target="#b18">[19]</ref>. This dataset contains 100 categories with 600 images each. We follow the split given in <ref type="bibr" target="#b1">[2]</ref>, which are 64, 16, and 20 categories for the base, novel validation, and novel test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>Following the majority of the literature, we evaluate MTUNet on 10,000 episodes of 5-way classification created by first randomly sampling 5 categories from D base and then sampling support and query images of these categories with N = 1 or 5 and M = 15 per category. We report the average accuracy over K × M = 75 queries in the 10,000 episodes and the 95% confidence interval.</p><p>We employ two CNN architectures as our backbone f θ , which are often used for FSL tasks, namely WRN-28-10 <ref type="bibr" target="#b48">[47]</ref> and ResNet-18 <ref type="bibr" target="#b13">[14]</ref>. For ResNet-18, we remove the first two down-sampling layers and change the kernel of the first 7 × 7 convolutional layer to 3 × 3. We use the hidden vector of the last convolutional layer after ReLU as feature maps F , where the numbers of feature maps are 512 and 640 for ResNet-18 and WRN-28-10 respectively.</p><p>As noted in <ref type="bibr" target="#b19">[20]</ref>, pre-training of the backbone CNNs is important for our PE module. We adopted a distance-based strategy, which is similar to SimpleShot <ref type="bibr" target="#b42">[41]</ref>. We train the backbone CNNs with all images in D base . The performance of a simple nearest-neighbor-based method is then evaluated over D val with 2,000 episodes of 5-way FSL task, and † Results are reported in <ref type="bibr" target="#b42">[41]</ref> ‡ Results are reported in <ref type="bibr" target="#b1">[2]</ref>  <ref type="table">Table 1</ref>. Average accuracy of 10000 sampling 5-ways task on test set of mini-ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>One shot Five shots</p><p>MetaLSTM <ref type="bibr" target="#b28">[29]</ref> 43.44±0.77 60.60±0.71 MatchingNet <ref type="bibr" target="#b40">[39]</ref> 43.56±0.84 55.31±0.73 MAML <ref type="bibr" target="#b7">[8]</ref> 48.70±1.84 63.11±0.92 LLAMA <ref type="bibr" target="#b12">[13]</ref> 49.40±1.83 -ProtoNet <ref type="bibr" target="#b36">[35]</ref> 49.42±0.78 68.20±0.66 PLATIPUS <ref type="bibr" target="#b8">[9]</ref> 50.13±1.86 -GNN <ref type="bibr" target="#b9">[10]</ref> 50.33±0.36 66.41±0.63 RelationNet <ref type="bibr" target="#b38">[37]</ref> 50.44±0.82 65.32±0.70 Meta SGD <ref type="bibr" target="#b20">[21]</ref> 50.47±1.87 64.03±0.94 R2-D2 <ref type="bibr" target="#b1">[2]</ref> 51.20±0.60 68.20±0.60 RelationNet <ref type="bibr" target="#b38">[37]</ref> 52.48±0.86 69.83±0.68 Gidaris <ref type="bibr" target="#b11">[12]</ref> 55.45±0.89 70.13±0.68 SNAIL <ref type="bibr" target="#b22">[23]</ref> 55.71±0.99 68.88±0.92 adaCNN <ref type="bibr" target="#b23">[24]</ref> 56.88±0.62 71.94±0.57 SimpleShot(UN) <ref type="bibr" target="#b42">[41]</ref>    has total of 50 epochs.</p><p>As for the PE module, we set d to 64. We set the maximum update T as 3. For the number z of the patterns is empirically set to 1/10 of the number of the base set categories, which are 7, 36, and 7 for mini-ImageNet, tiered-ImageNet, and CIFAR-FS, respectively. The importance of this choice is discussed in Section 4.5. SCOUTER's loss has two hyper-parameters e and λ, which controls over positive/negative explanation and over the preference to smaller attention areas, respectively. We set e = 1 (i.e., positive explanation) and λ = 1 following <ref type="bibr" target="#b19">[20]</ref>. Both g Q and g K have three FC layers with ReLU nonlinearities between them. All the parameters in the backbone f θ are fixed. We adopt the training strategy described in 3.5. The learning rate for training starts with 10 −4 and is divided by 10 every 40 epochs. It is trained for total of 60 epochs.</p><p>For training the whole MTUNet, the learnable parameters in backbone CNNs and PE are frozen. In a single epoch of training, we sample 1,000 episodes of 5-way tasks. The model is trained for 20 epochs with an initial learning rate 10 −3 , which is divided by 10 at the 10-th epoch. We use the model with the best performance with 2,000 episodes sampled from D val .</p><p>Our model is implemented with PyTorch. AdaBelief <ref type="bibr" target="#b49">[48]</ref> is adapted as optimizer. Input images are resized into 80×80, going through data augmentation including random flip and affine transformation following <ref type="bibr" target="#b42">[41]</ref>. A GPU workstation with two NVIDIA Quadro GV100 (32GB memory) GPUs are used for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>MTUNet is compared with state-of-the-art (SoTA) FSL methods. We exclude ones in semi-supervised and transductive paradigms, which use the statistics of novel set across different FSL tasks. We also do not adopt any postprocessing methods like 2 normalization in <ref type="bibr" target="#b42">[41]</ref>.</p><p>We report our best model by randomly sampling 10,000 1-shot and 5-shot tasks over D test in <ref type="table" target="#tab_2">Tables 1-3</ref> for the three datasets. We select the category for PE pre-training by sampling every 10 categories from the base set category list. The results demonstrate that MTUNet outperforms or is comparable with SoTA methods.</p><p>The different architectures of the backbone CNNs affect the performance. The variants with WRN always give a better performance than those with ResNet-18. Asides from the difference in the network architecture, the size of feature maps may be one of the factors. For mini-ImageNet, the WRN variants has 20 × 20 feature maps, while the ResNet-18 variants has 10 × 10. Such larger feature maps not only provides more information to the PM module but also give a better basis of patterns as higher resolutions may help find more specific patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Explainability</head><p>In addition to the classification performance, MTUNet is designed to be explainable in two different aspects. Firstly, MTUNet's decision is based on certain combinations of learned patterns. These patterns are localized in both query and support images through A (T ) , which can be easily visualized. This visualization offers intuition on the learned patterns and how much these patterns are shared among the query and support images. Secondly, thanks to the one-toone matching strategy formulated as a binary classification problem in Eq. (9), the distributions (or appearances) of learned patterns in query and support images give a strong clue on MTUNet's matching score s.</p><p>Pattern-based visual explanation. MTUNet's decision is based on the learned patterns, i.e., it is solely based on how much shared patterns (or features) appear in both a query and a support. This design in turn means that, by pinpointing each pattern in the images, we can obtain an intuition behind the decision made by the model. This can be done by merely visualizing A (T ) <ref type="figure" target="#fig_4">.  Figures 3 (a)</ref> and (b) respectively show a pair of support and query images in a 5-way task in mini-ImageNet. The pairs (a) and (b) are of categories lock and horizontal bar, respectively. The second column shows the visualization of averaged attention, given by</p><formula xml:id="formula_10">A = 1 z A (T ) 1 z .<label>(11)</label></formula><p>The third to ninth columns are the visualization of the regions corresponding to the learned patterns in A (T ) (i.e., the i-th row vectors of A (T ) represents the appearance of the i-th learned pattern at the respective spatial location). For (a) with category lock, the support image is a small golden combination lock used for storage cabinets or post boxes. Among all 7 patterns, only pattern 5 shows a strong response, whereas the others are not observed. We can see that pattern 5 pays attention to the discs of the lock. It also gives a strong response at the words on the left which shows similar morphological characteristics. The query image of (a) is a black combination lock often used for bicycles. The attention maps show almost the same distributions as the support: Only pattern 5 has a response on the discs. From these visualizations, we can infer that pattern 5 represents periodical changes in colors. Although these two locks have different functions, MTUNet finds a shared pattern among them.</p><p>For (b), the support image is the gymnast wearing red. Multiple patterns are observed in the image. We can see that the visualization of pattern 1 identifies the part of the human body (head), and pattern 3 appears around the hands grabbing the horizontal bar. The query image is the gymnast in blue. Patterns 1 and 3 respond in a similar way to the support image. Patters 4 and 5 appear in the background and around other parts of the body, however their responses are relatively weak compared to patterns 1 and 3. Patterns 1 and 3 may responsible for human heads and hands grabbing the horizontal bar, lead to the successful classification of novel categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ferret Beetle</head><p>Carton Goose Cata. Visualization of pairwise matching scores <ref type="figure" target="#fig_5">Figure 4</ref> shows the visualization of the pairwise matching score of a 5-way 1-shot task over mini-ImageNet, compiled in a matrix. Through the pairwise matching module, an FSL task is cast into a binary classification problem. The output for each pair is a value between 0 to 1 due to the sigmoid function, whereas the scores are shown in percentage in the figure. The first row and the first column are the visualization of overall attention for the support and query images of each category. Among all pairwise combinations, the combination of the support and query images of catamaran makes the full score (100%). The visualization of overall attention covers the hulls, especially the masts, in both images, which are the main characteristics of this category, explaining the high score. Category goose gets a low matching score. The query is a close-up of a goose on the ground from its front side, which captures the goose's blackhead or beak. The support image is an overall view of a goose about to fly from its backside. The visualization of overall attention captures the leg. With this combination, finding a shared pattern may not be easy, although these two extracted patterns are both representative parts of a bird. This problem stems from the difference in viewing angles, which can be relieved in 5-shot tasks, giving more supports from different viewing angles. Surprisingly, the query image for goose gets 81% for the support image for beetle. This may suggest that one of the patterns responds to black regions and this pattern is solely used as the clue of goose. This is a negative result for FSL tasks but clearly demonstrates MTUNet's explainability on the relationship between visual patterns and the pairwise matching scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussion</head><p>The number z of patterns. The number of patterns can be another crucial factor for MTUNet. Intuitively, a larger z makes the model more discriminative. To show the impact of z, we uniformly sample categories in C base (i.e., default as sampling every I categories from the category list, where I = 10, 8, 7, 5, 4, 3, 2, and 1); thus, I = 1 ends up with using all categories in C base .</p><p>The test accuracies are shown in <ref type="figure">Figure 5</ref> for 5-way 1shot and 5-way 5-shot tasks in 10,000 sampled episodes over D test of the three datasets. The horizontal axis represents the number of patterns and the vertical axis represents the average accuracy. Interestingly, the results show no clear tendency with respect to z. We would say that the performance is slightly decreased in mini-ImageNet with a larger z, whereas slightly increased in CIFAR-FS. For tiered-ImageNet, when setting I as 1 and use all 351 base categories for patterns, the PE module can not be trained successfully. This situation is also reported in <ref type="bibr" target="#b19">[20]</ref>. Other settings also show no obvious differences. In general, tuning over z may help gain performance, but its impact is not significant.</p><p>Selection of categories for training PE. Our PE module is supposed to learn common visual patterns. We use images of a certain subset of categories in C base to learn such patterns in our experiments. The selection of this subset thus affects the performance of downstream FSL tasks. To clarify the impact of the choice of the subset, we randomly sample seven categories in C base of mini-ImageNet for 50 times and use the corresponding images for training PE on top of ResNet-18. The trained PE is used for training MTUNet, which is evaluated over 2,000 episodes of FSL tasks with both the validation and test sets.</p><p>The mean and the 95% confidence interval over the 50 test accuracies are 54.63% and 0.16%, respectively. This implies that our model benefits from a better choice of categories for training PE. For this choice, we only have access to the validation set; however, since the validation set and the test set have disjoint categories, the best choice for the validation set is not necessarily the best choice for the test set. <ref type="figure">Figure 6</ref> shows the scatter plot of the validation accuracies and corresponding test accuracies, over 50 different random samples of seven categories. The plot empirically shows that the validation and test accuracies are highly correlated to each other, with Pearson's correlation coefficient of 0.64. This leads to the conclusion that, at least for mini-ImageNet, we can use the validation set to find the better choice. The green square in the plot is the choice that we adopted in our experiments, which shows that it is a better choice but not the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose MTUNet designed for explainable FSL. We achieved comparable performance on three benchmark datasets and qualitatively demonstrated its strong explainability through patterns in images. The approach taken in our model might be analogous to human beings as we usually try to find shared patterns when making a match between images of an object that one has never seen before. This can be advantageous as the explanation given We further provide the randomly sampled seven categories in C base of cifar-FS and tiered-ImageNet for 50 times and 20 times respectively. ResNet-18 is used as backbone. MTUNet with the trained PE is evaluated over 2,000 episodes of FSL tasks with both the validation and test sets.</p><p>The results are shown in <ref type="figure" target="#fig_1">Figures 1 and 2</ref>. For cifar-FS, the mean and the 95% confidence interval over the 50 test accuracies are 69.29% and 0.14%, respectively. Pearson's correlation coefficient is 0.53. For tiered-ImageNet, the mean and the 95% confidence interval over the 20 test accuracies are 61.58% and 0.20%, respectively. Pearson's correlation coefficient is 0.81. Through the result we can find that the performances over the validation and test sets show a strong correlation. These results imply that we can use the validation set to find a better choice not only for miniImageNet but also the other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Explainability</head><p>We provide visualization of patterns for 4 randomly sampled 5-way 1-shot tasks with a single query image over mini-ImageNet. The pattern-based visualization <ref type="figure" target="#fig_4">(Figures  3-6</ref>) and the pairwise matching scores <ref type="figure" target="#fig_1">(Figures 7-10</ref>) are shown for sample 1-4, respectively. We shall provide some discussion on the respective samples. <ref type="figure">Figure 7</ref>, we find there are two confusing categories of lock and carton. They all get a high score for each other category. The visualization in <ref type="figure" target="#fig_4">Figure 3</ref> shows that pattern 5 is responsible for both the letters (or a face of the character) on the carton and the discs of the lock. We would say that the letters and the discs share some similar structures, which cause the confusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample 1 By observing the matching matrix in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample 2</head><p>The pairwise matching scores in this sample find proper matches except for poncho. In <ref type="figure" target="#fig_11">Figure 8</ref>, the poncho image in support is a baby girl wearing a poncho, while the image in query is just the poncho with black color in the white background. The query image for poncho yields high scores for the support images of poncho, skirt, and beetle. The highest score of beetle may be due to the black color. Interestingly, the support and query images for skirt shows the attention over the door behind the person but not over the skirt itself. This is a good example of the importance of explanation for FSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample 3</head><p>In <ref type="figure">Figure 5</ref>, we find both the query and support give attention on the body part of the goose, but the differences in the perspective and the number of objects may make matching difficult. As a result, the query goose gets low scores for all support images. This also happens to carton in this sample.</p><p>Sample 4 In <ref type="figure" target="#fig_1">Figure 10</ref>, the query hound shows high matching scores to hound, goose, catamaran, and skirt. Through <ref type="figure">Figure 6</ref>, we find that the query hound contains a hound as well as people, which is also in the supports skirt and catamaran. This means that some learned patterns cover people, and the people in the query hound lead to high matching scores against skirt and catamaran. Inclusion of different objects often causes prediction failure.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reef</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>6 https://github.com/wbw520/MTUNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Few-shot learning by pair-matching with the pattern extractor (PE). Images are from mini-ImageNet dataset<ref type="bibr" target="#b40">[39]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Overall structure of MTUNet. One query is processed by CNN backbone and pattern extractor (PE) to provide exclusive patterns and then turned into an overall attention. Query will be concatenated to each support to make a pair for final discrimination through pairwise matching (PM). The dotted line represent each support image undergo the same calculation as query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of each pattern and the average features for a sampled task in mini-ImageNet. a is the class of lock and c is the horizontal bar. Overall is the overall attention among all patterns. The third to ninth columns are the visualization of the re-gions corresponding to the learned patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Matching point matrix of one sampled task in mini-ImageNet. Row and column are consisted with the overall attention visualization for support and query of each category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Results of patterns number settings for mini-ImageNet, tiered-ImageNet, and CIFAR-FS. The horizontal axis represents the number of patterns and the vertical axis represents the average accuracy. We report all the results with 10,000 sampled 5-way episodes in the novel test set. Performance of 50 turns random categories sampling for PE pre-training with 7 patterns. All the experiments are implemented in mini-ImageNet using ResNet-18 as the backbone. Result of the sample for our experiments is marked in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>PE training experiments implemented in cifar-FS. PE training experiments implemented tiered-ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 .</head><label>3</label><figDesc>Pattern-based visualization of sample 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 .Figure 5 .Figure 6 .Figure 7 .</head><label>4567</label><figDesc>Pattern-based visualization of sample 2. Pattern-based visualization of sample 3. Pattern-based visualization of sample 4. Pairwise matching of sample 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Pairwise matching of sample 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Pairwise matching of sample 3. Pairwise matching of sample 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>57.81±0.21 80.43±0.15</figDesc><table><row><cell>Qiao [28]</cell><cell cols="2">59.60±0.41 73.74±0.19</cell></row><row><cell>LEO [31]</cell><cell cols="2">61.76±0.08 77.59±0.12</cell></row><row><cell cols="3">MTUNet+ResNet-18 55.03±0.49 70.22±0.35</cell></row><row><cell>MTUNet+WRN</cell><cell cols="2">56.12±0.43 71.93±0.40</cell></row><row><cell>Approach</cell><cell>One shot</cell><cell>Five shots</cell></row><row><cell>Reptile [25]  †</cell><cell cols="2">48.97±0.21 66.47±0.21</cell></row><row><cell>MAML [8]</cell><cell cols="2">51.67±1.81 70.30±0.08</cell></row><row><cell>ProtoNet [35]  †</cell><cell cols="2">53.31±0.20 72.69±0.74</cell></row><row><cell>RelationNet [37]</cell><cell cols="2">54.48±0.93 71.32±0.78</cell></row><row><cell>Meta SGD [21]  †</cell><cell cols="2">62.95±0.03 79.34±0.06</cell></row><row><cell cols="3">SimpleShot(UN) [41] 64.35±0.23 85.69±0.15</cell></row><row><cell>LEO [31]</cell><cell cols="2">66.33±0.05 81.44±0.09</cell></row><row><cell cols="3">MTUNet+ResNet-18 61.27±0.50 77.82±0.41</cell></row><row><cell>MTUNet+WRN</cell><cell cols="2">62.42±0.51 80.05±0.46</cell></row></table><note>Table 2. Average accuracy of 10000 sampling 5-ways task on test set of tiered-ImageNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Average accuracy of 10000 sampling 5-ways task on test set of CIFAR-FS.</figDesc><table><row><cell>Approach</cell><cell>One shot</cell><cell>Five shots</cell></row><row><cell>RelationNet [37]  ‡</cell><cell cols="2">55.00±1.00 69.30±0.80</cell></row><row><cell>ProtoNet [35]  ‡</cell><cell cols="2">55.50±0.70 72.00±0.60</cell></row><row><cell>MAML [8]  ‡</cell><cell cols="2">58.90±1.90 71.50±1.00</cell></row><row><cell>GNN [10]  ‡</cell><cell>61.90</cell><cell>75.30</cell></row><row><cell>R2-D2 [2]</cell><cell cols="2">65.30±0.20 78.30±0.20</cell></row><row><cell cols="3">MTUNet+ResNet-18 66.31±0.50 80.16±0.39</cell></row><row><cell>MTUNet+WRN</cell><cell cols="2">68.34±0.49 82.93±0.37</cell></row></table><note>the best model is adopted. The learning rate for training starts with 10 −3 and is divided by 10 every 20 epochs. It</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>by MTUNet can provide an intuitive interpretation of what the model actually does. Our future work includes testing our model in a practical application scenario of FSL, such as computer-assisted diagnoses.</figDesc><table><row><cell>Match Them Up: Visually Explainable Few-shot Image Classification</cell></row><row><cell>(Supplementary Material)</cell></row><row><cell>1. Selection of Categories for Training PE.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This work was supported by Council for Science, Technology and Innovation (CSTI), cross-ministerial Strategic Innovation Promotion Program (SIP), "Innovative AI Hospital System" (Funding Agency: National Institute of Biomedical Innovation, Health and Nutrition (NIBIOHN)). This work was also supported by JSPS KAKENHI Grant  Number 19K10662 and 20K23343.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Meta-learning with differentiable closed-form solvers. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grad-CAM++: Generalized gradient-based visual explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Chattopadhay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prantik</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8680" to="8689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Low-shot learning with large-scale diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3349" to="3358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised fewshot learning for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Abdur R Feyjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Kauffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08462</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9516" to="9527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explainable zero-shot learning via attentive graph convolutional network and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxia</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiquan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SWJ</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Recasting gradient-based meta-learning as hierarchical bayes. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Leveraging the feature distribution in transfer-based few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03806</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">StarNet: towards weakly supervised few-shot detection and explainable few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Alfassy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Lichtenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06798</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">SCOUTER: Slot attention-based classifier for explainable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manisha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Kawasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Nagahara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06138</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-SGD: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Objectcentric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05076</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Metalearning with hebbian fast weights. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain-adaptive discriminative one-shot learning of gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="814" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Few-shot learning for dermatological disease diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhu</forename><surname>Viraj Uday</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Georgia Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fewshot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<imprint>
			<pubPlace>Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sygnowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Restricting the flow: Information bottlenecks for attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Sixt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Landgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Cam</forename><surname>Grad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Explain and improve: Cross-domain few-shot-learning using explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08790</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Score-CAM: Score-weighted visual explanations for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mardziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="24" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearestneighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Instance credibility inference for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12836" to="12845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalizing from a few examples: A survey on few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13000</idno>
		<title level="m">Interventional few-shot learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adabelief optimizer: Adapting stepsizes by the belief in observed gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommy</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sekhar</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicha</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenophon</forename><surname>Papademetris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
