<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
							<email>minfengzhu@</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
							<email>pingbo.pan@student</email>
							<affiliation key="aff2">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<email>chenwei@cadzju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Centre for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we focus on generating realistic images from text descriptions. Current methods first generate an initial image with rough shape and color, and then refine the initial image to a high-resolution one. Most existing text-to-image synthesis methods have two main problems.</p><p>(1) These methods depend heavily on the quality of the initial images. If the initial image is not well initialized, the following processes can hardly refine the image to a satisfactory quality. (2) Each word contributes a different level of importance when depicting different image contents, however, unchanged text representation is used in existing image refinement processes. In this paper, we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to generate high-quality images. The proposed method introduces a dynamic memory module to refine fuzzy image contents, when the initial images are not well generated. A memory writing gate is designed to select the important text information based on the initial image content, which enables our method to accurately generate images from the text description. We also utilize a response gate to adaptively fuse the information read from the memories and the image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200 dataset and the Microsoft Common Objects in Context dataset. Experimental results demonstrate that our DM-GAN model performs favorably against the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The last few years have seen remarkable growth in the use of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b4">[4]</ref> for image and video generation. Recently, GANs have been * This work was done when Minfeng Zhu was visiting the University of Technology Sydney. † Part of this work was done when Yi Yang was visiting Baidu Research during his Professional Experience Program.</p><p>This small bird has a yellow crown and a white belly.</p><p>This bird has a blue crown with white throat and brown secondaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthesized images</head><p>People at the park flying kites and walking.</p><p>The bathroom with the white tile has been cleaned. widely used to generate photo-realistic images according to text descriptions (see <ref type="figure" target="#fig_0">Figure 1</ref>). Fully understanding the relationship between visual contents and natural languages is an essential step towards artificial intelligence, e.g., image search and video understanding <ref type="bibr" target="#b33">[33]</ref>. Multi-stage methods <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref> first generate low-resolution initial images and then refine the initial images to high-resolution ones.</p><p>Although these multi-stage methods achieve remarkable progress, there remain two problems. First, the generation result depends heavily on the quality of initial images. The image refinement process cannot generate high-quality images, if the initial images are badly generated. Second, each word in an input sentence has a different level of information depicting the image content. Current models utilize the same word representations in different image refinement processes, which makes the refinement process ineffective. The image information should be taken into account to determine the importance of every word for refinement.</p><p>In this paper, we introduce a novel Dynamic Memory Generative Adversarial Network (DM-GAN) to address the aforementioned issues. For the first issue, we propose to add a memory mechanism to cope with badly-generated initial images. Recent work <ref type="bibr" target="#b27">[27]</ref> has shown the memory network's ability to encode knowledge sources. Inspired by this work, we propose to add the key-value memory structure <ref type="bibr" target="#b13">[13]</ref> to the GAN framework. The fuzzy image features of initial images are treated as queries to read features from the memory module. The reads of the memory are used to refine the initial images. To solve the second issue, we introduce a memory writing gate to dynamically select the words that are relevant to the generated image. This makes our generated image well conditioned on the text description. Therefore, the memory component is written and read dynamically at each image refinement process according to the initial image and text information. In addition, instead of directly concatenating image and memory, a response gate is used to adaptively receive information from image and memory.</p><p>We conducted experiments to evaluate the DM-GAN model on the Caltech-UCSD Birds 200 (CUB) dataset and the Microsoft Common Objects in Context (COCO) dataset. The quality of generated images is measured using the Inception Score (IS), the Fréchet Inception Distance (FID) and the R-precision. The experiments illustrate that our DM-GAN model outperforms the previous text-to-image synthesis methods, quantitatively and qualitatively. Our model improves the IS from 4.36 to 4.75 and decreases the FID from 23.98 to 16.09 on the CUB dataset. The Rprecision is improved by 4.49% and 3.09% on the above two datasets. The qualitative evaluation proves that our model generates more photo-realistic images.</p><p>This paper makes the following key contributions:</p><p>• We propose a novel GAN model combined with a dynamic memory component to generate high-quality images even if the initial image is not well generated.</p><p>• We introduce a memory writing gate that is capable of selecting relevant word according to the initial image.</p><p>• A response gate is proposed to adaptively fuse information from image and memory.</p><p>• The experimental results demonstrate that the DM-GAN outperforms the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generative Adversarial Networks.</head><p>With the recent successes of Variational Autoencoders (VAEs) <ref type="bibr" target="#b9">[9]</ref> and GANs <ref type="bibr" target="#b4">[4]</ref>, a large number of methods have been proposed to handle generation <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b0">1]</ref> and domain adaptation task <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b32">32]</ref>. Recently, generating images based on the text descriptions gains interest in the research community nowadays.</p><p>Single-stage. The text-to-image synthesis problem is decomposed by Reed et al. <ref type="bibr" target="#b20">[20]</ref> into two sub-problems: first, the joint embedding is learned to capture the relations between natural language and real-world images; second, a deep convolutional generative adversarial network <ref type="bibr" target="#b19">[19]</ref> is trained to synthesize a compelling image. Dong et al. <ref type="bibr" target="#b3">[3]</ref> adopted the pair-wise ranking loss <ref type="bibr" target="#b10">[10]</ref> to project both images and natural languages into a joint embedding space. Since previous generative models failed to add the location information, Reed et al. proposed GAWWN <ref type="bibr" target="#b21">[21]</ref> to encode localization constraints. To diversify the generated images, the discriminator of TAC-GAN <ref type="bibr" target="#b1">[2]</ref> not only distinguishes real images from synthetic images, but also classifies synthetic images into true classes. Similar to TAC-GAN, PPGN <ref type="bibr" target="#b16">[16]</ref> includes a conditional network to synthesize images conditioned on a caption.</p><p>Multi-stage. StackGAN <ref type="bibr" target="#b30">[30]</ref> and StackGAN++ <ref type="bibr" target="#b31">[31]</ref> generate photo-realistic high-resolution images with two stages. Yuan et al. <ref type="bibr" target="#b29">[29]</ref> employed symmetrical distillation networks to minimize the multi-level difference between real and synthetic images. DA-GAN <ref type="bibr" target="#b12">[12]</ref> translates each word into a sub-region of an image. Our method considers the interaction between each word and the whole generated image. Conditioning on the global sentence vector may result in low-quality images, AttnGAN <ref type="bibr" target="#b28">[28]</ref> refines the images to high-resolution ones by leveraging the attention mechanism. Each word in an input sentence has a different level of information depicting the image content. However, AttnGAN takes all the words equally, it employs an attention module to use the same word representation. Our proposed memory module is able to uncover such difference for image generation, as it dynamically selects the important word information based on the initial image content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Memory Networks.</head><p>Recently, memory network <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b27">27]</ref> provides a new architecture to reason answers from memories more effectively using explicit storage and a notion of attention. Memory network first writes information into an external memory and then reads contents from memory slots according to a relevance probability. Weston et al. <ref type="bibr" target="#b27">[27]</ref> introduced the memory network to produce the output by searching supporting memories one by one. End-to-end memory network <ref type="bibr" target="#b23">[23]</ref> is a continues form of memory network, where each memory slot is weighted according to the inner product between the memory and the query. To understand the unstructured documents, the Key-Value Memory Network (KV-MemNN) <ref type="bibr" target="#b13">[13]</ref> performs reasoning by utilizing different encodings for key memory and value memory. The key memory is used to infer the weight of the corresponding value memory when predicting the final answer. Inspired by the recent success of the memory network, we introduce DM-GAN, a novel network architecture to generate highquality images via nontrivial transforms between key and value memories.   <ref type="figure">Figure 2</ref>. The DM-GAN architecture for text-to-image synthesis. Our DM-GAN model first generates an initial image, and then refines the initial image to generate a high-quality one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial Image Generation</head><formula xml:id="formula_0">initial image x 0 g w 1-g w Mw(w) × R w + Mr(R) m g r g r 1-g r O O + R new Ф V (m) Ф K (m</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DM-GAN</head><p>As shown in <ref type="figure">Figure 2</ref>, the architecture of our DM-GAN model is composed of two stages: initial image generation and dynamic memory based image refinement.</p><p>At the initial image generation stage, firstly, the input text description is transformed into some internal representation (a sentence feature s and several word features W ) by a text encoder. Then, a deep conventional generator predicts an initial image x 0 with a rough shape and few details according to the sentence feature and a random noise vector z: x 0 , R 0 = G 0 (z, s), where R 0 is the image feature. The noise vector is sampled from a normal distribution.</p><p>At the dynamic memory based image refinement stage, more fine-grained visual contents are added to the fuzzy initial images to generate a photo-realistic image x i :</p><formula xml:id="formula_1">x i = G i (R i−1 , W ), where R i−1</formula><p>is the image feature from the last stage. The refinement stage can be repeated multiple times to retrieve more pertinent information and generate a high-resolution image with more fine-grained details.</p><p>The dynamic memory based image refinement stage consists of four components: Memory Writing, Key Addressing, Value Reading, and Response (Section 3.1). The Memory Writing operation stores the text information into a keyvalue structured memory for further retrieval. Then, Key Addressing and Value Reading operations are employed to read features from the memory module to refine the visual features of the low-quality images. At last, the Response operation is adopted to control the fusion of the image features and the reads of the memory. We propose a memory writing gate to highlight important word information according to the image content in memory writing step (Section 3.2).</p><p>We also utilize a response gate to adaptively fuse the information read from the memory and the image features (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dynamic Memory</head><p>We start with the given input word representations W , image x and image features R i :</p><formula xml:id="formula_2">W = {w 1 , w 2 , ..., w T }, w i ∈ R Nw ,<label>(1)</label></formula><formula xml:id="formula_3">R i = {r 1 , r 2 , ..., r N }, r i ∈ R Nr ,<label>(2)</label></formula><p>where T is the number of words, N w is the dimension of word features, N is the number of image pixels and image pixel feature is a N r dimensional vector. We are intended to learn a model to refine the image using a more effective way to fuse text and image information via nontrivial transforms between key and value memory. The refinement stage includes the following four steps. Memory Writing: Encoding prior knowledge is an important part of the dynamic memory, which enables recovering high-quality images from text. A naive way to write the memory is considering only partial text information.</p><formula xml:id="formula_4">m i = M (w i ), m i ∈ R Nm<label>(3)</label></formula><p>where M (·) denotes the 1×1 convolution operation which embeds word features into the memory feature space with N m dimensions. Key Addressing: In this step, we retrieve relevant memories using key memory. We compute a weight of each memory slot as a similarity probability between a memory slot m i and an image feature r j :</p><formula xml:id="formula_5">α i,j = exp(φ K (m i ) T r j ) T l=1 exp(φ K (m l ) T r j ) ,<label>(4)</label></formula><p>where α i,j is the similarity probability between the i-th memory and the j-th image feature and φ K () is the key memory access process which maps memory features into dimension N r . φ K () is implemented as a 1×1 convolution.</p><p>Value Reading: The output memory representation is defined as the weighted summation of value memories according to the similarity probability:</p><formula xml:id="formula_6">o j = T i=1 α i,j φ V (m i ),<label>(5)</label></formula><p>where φ V () is the value memory access process which maps memory features into dimension N r . φ V () is implemented as a 1×1 convolution. Response: After receiving the output memory, we combine the current image and the output representation to provide a new image feature. A naive approach will be simply concatenating the image features and the output representation. The new image features are obtained by:</p><formula xml:id="formula_7">r new i = [o i , r i ],<label>(6)</label></formula><p>where [·, ·] denotes concatenation operation. Then, we are able to utilize an upsampling block and several residual blocks <ref type="bibr" target="#b6">[6]</ref> to upscale the new image features into a highresolution image. The upsampling block consists of a nearest neighbor upsampling layer and a 3×3 convolution. Finally, the refined image x is obtained from the new image features using a 3×3 convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gated Memory Writing</head><p>Instead of considering only partial text information using Eq.3, the memory writing gate allows the DM-GAN model to select the relevant word to refine the initial images. The memory writing gate g w i combines image features R i from the last stage with word features W to calculate the importance of a word:</p><formula xml:id="formula_8">g w i (R, w i ) = σ(A * w i + B * 1 N N i=1 r i ),<label>(7)</label></formula><p>where σ is the sigmoid function, A is a 1 × N w matrix, and B is a 1 × N r matrix. Then, the memory slot m i ∈ R Nm is written by combining the image and word features.</p><formula xml:id="formula_9">m i = M w (w i ) * g w i + M r ( 1 N N i=1 r i ) * (1 − g w i ),<label>(8)</label></formula><p>where M w (·) and M r (·) denote the 1x1 convolution operation. M w (·) and M r (·) embed image and word features into the same feature space with N m dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Gated Response</head><p>We utilize the adaptive gating mechanism to dynamically control the information flow and update image features:</p><formula xml:id="formula_10">g r i = σ(W [o i , r i ] + b), r new i = o i * g r i + r i * (1 − g r i ),<label>(9)</label></formula><p>where g r i is the response gate for information fusion, σ is the sigmoid function, W and b are the parameter matrix and bias term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Function</head><p>The objective function of the generator network is defined as:</p><formula xml:id="formula_11">L = i L Gi + λ 1 L CA + λ 2 L DAM SM ,<label>(10)</label></formula><p>in which λ 1 and λ 2 are the corresponding weights of conditioning augmentation loss and DAMSM loss. G 0 denotes the generator of the initial generation stage. G i denotes the generator of the i-th iteration of the image refinement stage. Adversarial Loss: The adversarial loss for G i is defined as follows:</p><formula xml:id="formula_12">L Gi = − 1 2 [E x∼p G i logD i (x) + E x∼p G i logD i (x, s)],<label>(11)</label></formula><p>where the first term is the unconditional loss which makes the generated image real as much as possible and the second term is the conditional loss which makes the image match the input sentence. Alternatively, the adversarial loss for each discriminator D i is defined as:</p><formula xml:id="formula_13">L Di = − 1 2 [E x∼p data logD i (x)+E x∼p G i log(1−D i (x)) unconditional loss , +E x∼p data logD i (x, s)+E x∼p G i log(1−D i (x, s))] conditional loss ,<label>(12)</label></formula><p>where the unconditional loss is designed to distinguish the generated image from real images and the conditional loss determines whether the image and the input sentence match. Conditioning Augmentation Loss: The Conditioning Augmentation (CA) technique <ref type="bibr" target="#b30">[30]</ref> is proposed to augment training data and avoid overfitting by resampling the input sentence vector from an independent Gaussian distribution. Thus, the CA loss is defined as the Kullback-Leibler divergence between the standard Gaussian distribution and the Gaussian distribution of training data.</p><p>L CA = D KL (N (µ(s), Σ(s))||N (0, I)),</p><p>where µ(s) and Σ(s) are mean and diagonal covariance matrix of the sentence feature. µ(s) and Σ(s) are computed by fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAMSM Loss:</head><p>We utilize the DAMSM loss <ref type="bibr" target="#b28">[28]</ref> to measure the matching degree between images and text descriptions. The DAMSM loss makes generated images better conditioned on text descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>For text embedding, we employ a pre-trained bidirectional LSTM text encoder by Xu et al. <ref type="bibr" target="#b28">[28]</ref> and fix their parameters during training. Each word feature corresponds to the hidden states of two directions. The sentence feature is generated by concatenating the last hidden states of two directions. The initial image generation stage first synthesizes images with 64x64 resolution. Then, the dynamic memory based image refinement stage refines images to 128x128 and 256x256 resolution. We only repeat the refinement process with dynamic memory module two times due to GPU memory limitation. Introducing dynamic memory to lowresolution images (i.e. 16x16, 32x32) can not further improve the performance. Because low-resolution images are not well generated and their features are more like random vectors. For all discriminator networks, we apply spectral normalization <ref type="bibr" target="#b15">[15]</ref> after every convolution to avoid unusual gradients to improve text-to-image synthesis performance. By default, we set N w = 256, N r = 64 and N m = 128 to be the dimension of text, image and memory feature vectors respectively. We set the hyperparameter λ 1 = 1 and λ 2 = 5 for the CUB dataset and λ 1 = 1 and λ 2 = 50 for the COCO dataset. All networks are trained using ADAM optimizer <ref type="bibr" target="#b8">[8]</ref> with batch size 10, β 1 = 0.5 and β 2 = 0.999. The learning rate is set to be 0.0002. We train the DM-GAN model with 600 epochs on the CUB dataset and 120 epochs on the COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the DM-GAN model quantitatively and qualitatively. We implemented the DM-GAN model using the open-source Python library PyTorch <ref type="bibr" target="#b18">[18]</ref>.</p><p>Datasets. To demonstrate the capability of our proposed method for text-to-image synthesis, we conducted experiments on the CUB <ref type="bibr" target="#b26">[26]</ref>  Evaluation Metric. We quantify the performance of the DM-GAN in terms of Inception Score (IS), Fréchet Inception Distance (FID), and R-precision. Each model generated 30,000 images conditioning on the text descriptions from the unseen test set for evaluation.</p><p>The IS <ref type="bibr" target="#b22">[22]</ref> uses a pre-trained Inception v3 network <ref type="bibr" target="#b24">[24]</ref> to compute the KL-divergence between the conditional class distribution and the marginal class distribution. A large IS means that the generated model outputs a high diversity of images for all classes and each image clearly belongs to a specific class.</p><p>The FID <ref type="bibr" target="#b7">[7]</ref> computes the Fréchet distance between synthetic and real-world images based on the extracted features from a pre-trained Inception v3 network. A lower FID implies a closer distance between generated image distribution and real-world image distribution.</p><p>Following Xu et al. <ref type="bibr" target="#b28">[28]</ref>, we use the R-precision to evaluate whether a generated image is well conditioned on the given text description. The R-precision is measured by retrieving relevant text given an image query. We compute the cosine distance between a global image vector and 100 candidate sentence vectors. The candidate text descriptions include R ground truth and 100-R randomly selected mismatching descriptions. For each query, if r results in the top R ranked retrieval descriptions are relevant, then the Rprecision is r/R. In practice, we compute the R-precision with R=1. We divide the generated images into ten folds for retrieval and then take the mean and standard deviation of the resulting scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Text-to-Image Quality</head><p>We compare our DM-GAN model with the state-of-theart models on the CUB and COCO test datasets. The performance results are reported in <ref type="table" target="#tab_3">Table 1</ref> and 2.</p><p>As shown in <ref type="table" target="#tab_3">Table 1</ref>, our DM-GAN model achieves 4.75 IS on the CUB dataset, which outperforms other methods by a large margin. Compared with AttnGAN, DM-GAN improves the IS from 4.36 to 4.75 on the CUB dataset (8.94% improvement) and from 25.89 to 30.49 on the COCO dataset (17.77% improvement). The experimental results indicate that our DM-GAN model generates images with higher quality than other approaches. <ref type="table">Table 2</ref> compares the performance between AttnGAN and DM-GAN with respect to the FID on the CUB and COCO datasets. We measure the FID of AttnGAN from the officially pre-trained model. Our DM-GAN decreases the FID from 23.98 to 16.09 on the CUB dataset and from 35.49 to 32.64 on the COCO dataset, which demonstrates that DM-GAN learns a better data distribution.</p><p>As shown in <ref type="table">Table 2</ref>, the DM-GAN improves the Rprecision by 4.49% on the CUB dataset and 3.09% on the COCO dataset. Higher R-precision indicates that the generated images by the DM-GAN are better conditioned on the given text description, which further demonstrates the effectiveness of the employed dynamic memory.</p><p>In summary, the experimental results indicate that our DM-GAN is superior to the state-of-the-art models. <ref type="bibr" target="#b20">[20]</ref> GAWWN <ref type="bibr" target="#b21">[21]</ref> StackGAN <ref type="bibr" target="#b30">[30]</ref> PPGN <ref type="bibr" target="#b16">[16]</ref> AttnGAN <ref type="bibr">[</ref>  <ref type="table">Table 2</ref>. Performance of FID and R-precision for AttnGAN <ref type="bibr" target="#b28">[28]</ref> and our DM-GAN on the CUB and COCO datasets. The FID of AttnGAN is calculated from officially released weights. Lower is better for FID and higher is better for R-precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN-INT-CLS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Visual Quality</head><p>For qualitative evaluation, <ref type="figure" target="#fig_2">Figure 3</ref> shows text-to-image synthesis examples generated by our DM-GAN and the state-of-the-art models. In general, our DM-GAN approach generates images with more vivid details as well as more clear backgrounds in most cases, comparing to the At-tnGAN <ref type="bibr" target="#b28">[28]</ref>, GAN-INT-CLS <ref type="bibr" target="#b20">[20]</ref> and StackGAN <ref type="bibr" target="#b30">[30]</ref>, because it employs a dynamic memory model using varied weighted word information to improve image quality.</p><p>Our DM-GAN method has the capacity to better understand the logic of the text description and present a more clear structure of the images. Observing the samples generated on the CUB dataset in <ref type="figure" target="#fig_2">Figure 3(a)</ref>, with a single character, although DM-GAN and AttnGAN both perform well in accurately capture and present the character's feature, our DM-GAN model better highlights the main subject of the image, the bird, differentiating from its background. It demonstrates that, with the dynamic memory module, our DM-GAN model is able to bridge the gap between visual contents and natural languages. In terms of multi-subjectsimage generation, for example, the COCO dataset in <ref type="figure" target="#fig_2">Figure 3(b)</ref>, it is more challenging to generate photo-realistic images when the text description is more complicated and contains more than one subject. DM-GAN precisely captures the major scene based on the most important subject and arrange the rest descriptive contents logically, which improves the global structure of the image. For instance, DM-GAN is the only successful method clearly identifies the bathroom with required components in the column 3 in <ref type="figure" target="#fig_2">Figure 3</ref>(b). The visual results show that our DM-GAN is more effective to capture important subjects using a memory writing gate to dynamically select important words. <ref type="figure">Figure 4</ref> indicates that our DM-GAN model is able to refine badly initialized images and generate more photo-  realistic high-resolution images. So the image quality is obviously well-improved, with clear backgrounds and convincing details. In most cases, the initial stage generates a blurry image with rough shape and color, so that the background is fine-tuned to be more realistic with fine-grained textures, while the refined image will be better conditioned on the input text and provide more photo-realistic highresolution images. In the fourth column of <ref type="figure">Figure 4</ref>, no white streaks can be found on the bird's body from the initial image with 64×64 resolution. The refinement process helps to encode "white streaks" information from text description and add back missing features based on the text description and image content. In order word, our DM-GAN model is able to refine the image to match the input text description.</p><p>To evaluate the diversity of our DM-GAN model, we generate several images using the same text description, and multiple noise vectors. <ref type="figure" target="#fig_3">Figure 5</ref> shows text descriptions and synthetic images with different shapes and backgrounds. Images are similar but not identical to each other, which means our DM-GAN generates images with high diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In order to verify the effectiveness of our proposed components, we evaluate the DM-GAN architecture and its variants on the CUB dataset. The control components between architectures include the key-value memory (M), the writing gate (WG) and the response gate (RG). We define a baseline model which removes M, WG and RG from DM-GAN. The memory is written according to partial text information (Eq.3). The response operation simply concatenates the image features and the memory output (Eq.6). The performance of the DM-GAN architecture and its variants is reported in <ref type="table" target="#tab_5">Table 3</ref>. Our baseline model produces slightly better performance than AttnGAN. By integrating these com-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DM-GAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AttnGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StackGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN-INT-CLS</head><p>This bird has wings that are grey and has a white belly.</p><p>A silhouette of a man surfing over waves.</p><p>This bird has wings that are black and has a white belly.</p><p>Room with wood floors and a stone fire place. This is a grey bird with a brown wing and a small orange beak.</p><p>The bathroom with the white tile has been cleaned. This bird has a short brown bill, a white eyering, and a medium brown crown.</p><p>A fruit stand that has bananas, papaya, and plantains.</p><p>This particular bird has a belly that is yellow and brown.</p><p>A train accident where some cars when into a river. This bird is a lime green with greyish wings and long legs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A bunch of various vegetables on a table.</head><p>This yellow bird has a thin beak and jet black eyes and thin feet.</p><p>A plane parked at an airport near a terminal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DM-GAN</head><p>This bird has a white throat and a dark yellow bill and grey wings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN-INT-CLS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StackGAN AttnGAN</head><p>A stop sign that is sitting in the grass.  This bird has a blue crown with white throat and brown secondaries.</p><p>This small bird has a yellow crown and a white belly.</p><p>A primarily black bird with streaks of white and yellow and a medium sized beak.</p><p>This bird has a red head, throat and chest, with a white belly.</p><p>People at the park flying kites and walking.</p><p>The bathroom with the white tile has been cleaned.</p><p>Multiple people are standing on the beach at the edge of the water.</p><p>A clock that is on the side of a tower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>64×64</head><p>128×128 256×256 <ref type="figure">Figure 4</ref>. The results of different stages of our DM-GAN model, including the initial images, the images after one refinement process and the images after two refinement processes.</p><p>This bird has wings that are grey and has a white belly.</p><p>A group of people standing on a beach next to the ocean. ponents, our model can achieve further improvement which demonstrates the effectiveness of every component.</p><p>Further, we visualize the most relevant words selected by the AttnGAN <ref type="bibr" target="#b28">[28]</ref> and our DM-GAN. We notice that the attention mechanism cannot accurately select relevant words when the initial images are not well generated. We propose the dynamic memory module to select the most relevant words based on the global image feature. As <ref type="figure" target="#fig_4">Fig.  6 (a)</ref> shows, although a bird with incorrect red breast is generated, dynamic memory module selects the word, i.e., "white" to correct the image. The DM-GAN selects and combines word information with image features in two steps (see <ref type="figure" target="#fig_4">Fig. 6 (b)</ref>). The gated memory writing step first roughly selects words relevant to the image and writes them into the memory. Then the key addressing step further reads more relevant words from the memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have proposed a new architecture called DM-GAN for text-to-image synthesis task. We employ a dynamic memory component to refine the initial generated image, a memory writing gate to highlight important (a) This bird is red in color with a black and white breast and a black eyering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>64×64 128×128</head><p>Attention Dynamic memory text information and a repose gate to fuse image and memory representation. Experiment results on two real-world datasets show that DM-GAN outperforms the state-of-theart by both qualitative and quantitative measures. Our DA-GAN refines initial images with wrong color and rough shapes. However, the final results still rely heavily on the layout of multi-subjects in initial images. In the future, we will try to design a more powerful model to generate initial images with better organizations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of text-to-image synthesis by our DM-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) The CUB dataset(b) The COCO dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Example results for text-to-image synthesis by DM-GAN and AttnGAN. (a) Generated bird images by conditioning on text from CUB test set. (b) Generated images by conditioning on text from COCO test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Generated images using the same text description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>(a) Comparison between the top 5 relevant words selected by attention module and dynamic memory module. (b) The top 5 relevant words selected by memory writing step and key addressing step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Dynamic Memory based Image Refinement</head><label></label><figDesc></figDesc><table><row><cell cols="2">text description This small bird has a yellow crown and a white belly.</cell><cell>text encoder</cell><cell>sent feat s</cell><cell>FC</cell><cell>UpBlock</cell><cell>UpBlock</cell><cell>UpBlock</cell><cell>UpBlock</cell><cell>img feat R0</cell><cell>3 x 3 Conv</cell><cell>D0</cell></row><row><cell>word feat</cell><cell>g w</cell><cell></cell><cell>dynamic memory</cell><cell></cell><cell></cell><cell>UpBlock</cell><cell>ResBlock</cell><cell>ResBlock</cell><cell>img feat R k</cell><cell>3 x 3 Conv</cell><cell>Di</cell></row><row><cell>img feat R k-1</cell><cell></cell><cell></cell><cell>k=1,2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and the COCO [11] datasets. The CUB dataset contains 200 bird categories with 11,788 images, where 150 categories with 8,855 images are employed for training while the remaining 50 categories with 2,933 images for testing. There are ten captions for each image in CUB dataset. The COCO dataset includes a training set with 80k images and a test set with 40k images. Each image in the COCO dataset has five text descriptions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>The inception scores (higher is better) of GAN-INT-CLS<ref type="bibr" target="#b20">[20]</ref>, GAWWN<ref type="bibr" target="#b21">[21]</ref>, StackGAN<ref type="bibr" target="#b30">[30]</ref>, PPGN<ref type="bibr" target="#b16">[16]</ref>, AttnGAN<ref type="bibr" target="#b28">[28]</ref> and our DM-GAN on the CUB and COCO datasets. The best results are in bold.</figDesc><table><row><cell>28]</cell><cell>DM-GAN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>The performance of different architectures of our DM-GAN on the CUB datasets. M, WG and RG denote dynamic memory, memory writing gate and response gate respectively.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research has been supported by National Key Research and Development Program (2018YFB0904503) and National Natural Science Foundation of China (U1866602, 61772456).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Adversarial learning with local coordinate coding. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayushman</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Cristian Borges</forename><surname>Gamboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheraz</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad Zeshan</forename><surname>Afzal</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<idno type="arXiv">arXiv:1703.06412</idno>
		<title level="m">Tac-gan-text conditioned auxiliary classifier generative adversarial network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic image synthesis via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICCV</title>
		<meeting>the IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5706" to="5714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative Adversarial Nets. In NIPS</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic neural turing machine with continuous and discrete addressing schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="857" to="884" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Da-gan: Instance-level image translation by deep attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5657" to="5666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conditional Generative Adversarial Nets. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Alexey Dosovitskiy, and Jason Yosinski. Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4467" to="4477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Scott E Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Text-to-image synthesis via symmetrical distillation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06801</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5908" to="5916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Stackgan++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sim-real joint reinforcement transfer for 3d indoor navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bidirectional multirate reconstruction for temporal modeling in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
