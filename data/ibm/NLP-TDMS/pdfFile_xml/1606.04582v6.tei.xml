<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2017 QUERY-REDUCTION NETWORKS FOR QUESTION ANSWERING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
							<email>minjoon@cs.washington.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<email>hannaneh@cs.washington.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington 1</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2017 QUERY-REDUCTION NETWORKS FOR QUESTION ANSWERING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In this paper, we address the problem of question answering (QA) when reasoning over multiple facts is required. For example, consider we know that Frogs eat insects and Flies are insects. Then answering Do frogs eat flies? requires reasoning over both of the above facts. Question answering, more specifically context-based QA, has been extensively studied in machine comprehension tasks <ref type="bibr" target="#b17">(Richardson et al., 2013;</ref><ref type="bibr" target="#b8">Hermann et al., 2015;</ref><ref type="bibr" target="#b9">Hill et al., 2016;</ref><ref type="bibr" target="#b15">Rajpurkar et al., 2016)</ref>. However, most of the datasets are primarily focused on lexical and syntactic understanding, and hardly concentrate on inference over multiple facts. Recently, several datasets aimed for testing multi-hop reasoning have emerged; among them are story-based QA  and the dialog task .</p><p>Recurrent Neural Network (RNN) and its variants, such as Long Short-Term Memory (LSTM) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref> and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>, are popular choices for modeling natural language. However, when used for multi-hop reasoning in question answering, purely RNN-based models have shown to perform poorly . This is largely due to the fact that RNN's internal memory is inherently unstable over a long term. For this reason, most recent approaches in the literature have mainly relied on global attention mechanism and shared external memory <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b13">Peng et al., 2015;</ref><ref type="bibr" target="#b21">Xiong et al., 2016;</ref><ref type="bibr" target="#b6">Graves et al., 2016)</ref>. The attention mechanism allows these models to focus on a single sentence in each layer. They can sequentially read multiple relevant sentences from the memory with multiple layers to perform multi-hop reasoning. However, one major drawback of these standard attention mechanisms is that they are insensitive to the time step (memory address) of the sentences when accessing them.</p><p>Our proposed model, Query-Reduction Network 1 (QRN), is a single recurrent unit that addresses the long-term dependency problem of most RNN-based models by simplifying the recurrent update, while taking the advantage of RNN's capability to model sequential data <ref type="figure" target="#fig_1">(Figure 1)</ref>. QRN considers the context sentences as a sequence of state-changing triggers, and transforms (reduces) the original query to a more informed query as it observes each trigger through time. For instance in <ref type="figure" target="#fig_1">Figure 1b</ref>, the original question, Where is the apple?, cannot be directly answered by any single sentence from the story. After observing the first sentence, Sandra got the apple there, QRN transforms the original question to a reduced query Where is Sandra?, which is presumably Where is the apple? x, q,ŷ are the story, question and predicted answer in natural language, respectively. x = x1, . . . , xT , q,ŷ are their corresponding vector representations (upright font). α and ρ are update gate and reduce functions, respectively.ŷ is assigned to be h 2 5 , the local query at the last time step in the last layer. Also, red-colored text is the inferred meanings of the vectors (see <ref type="bibr">'Interpretations' of Section 5.3)</ref>. easier to answer than the original question given the context provided by the first sentence. 2 Unlike RNN-based models, QRN's candidate state (h t in <ref type="figure" target="#fig_1">Figure 1a</ref>) does not depend on the previous hidden state (h t−1 ). Compared to memory-based approaches <ref type="bibr" target="#b18">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b13">Peng et al., 2015;</ref><ref type="bibr" target="#b11">Kumar et al., 2016;</ref><ref type="bibr" target="#b21">Xiong et al., 2016)</ref>, QRN can better encodes locality information because it does not use a global memory access controller (circle nodes in <ref type="figure">Figure 2</ref>), and the query updates are performed locally.</p><p>In short, the main contribution of QRN is threefold. First, QRN is a simple variant of RNN that reduces the query given the context sentences in a differentiable manner. Second, QRN is situated between the attention mechanism and RNN, effectively handling time dependency and long-term dependency problems of each technique, respectively. Hence it is well-suited for sequential data with both local and global interactions (note that QRN is not the replacement of RNN, which is arguably better for modeling complex local interactions). Third, unlike most RNN-based models, QRN can be parallelized over time by computing candidate reduced queries (h t ) directly from local input queries (q t ) and context sentence vectors (x t ). In fact, the parallelizability of QRN implies that QRN does not suffer from the vanishing gradient problem of RNN, hence effectively addressing the long-term dependency. We experimentally demonstrate these contributions by achieving the state-of-the-art results on story-based QA and interactive dialog datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL</head><p>In story-based QA (or dialog dataset), the input is the context as a sequence of sentences (story or past conversations) and a question in natural language (equivalent to the user's last utterance in the dialog). The output is the predicted answer to the question in natural language (the system's next utterance in the dialog). The only supervision provided during training is the answer to the question.</p><p>In this paper we particularly focus on end-to-end solutions, i.e., the only supervision comes from questions and answers, and we restrain from using manually defined rules or external language resources, such as lexicon or dependency parser. Let x 1 , . . . , x T denote the sequence of sentences, where T is the number of sentences in the story, and let q denote the question. Letŷ denote the predicted answer, and y denote the true answer. Our proposed system for end-to-end QA task is divided into three modules ( <ref type="figure" target="#fig_1">Figure 1c</ref>): input module, QRN layers, and output module.</p><p>Input module. Input module maps each sentence x t and the question q to d-dimensional vector space, x t ∈ R d and q t ∈ R d . We adopt a previous solution for the input module (details in Section 5).</p><p>QRN layers. QRN layers use the sentence vectors and the question vector from the input module to obtain the predicted answer in vector space,ŷ ∈ R d . A QRN layer refers to the recurrent application of a QRN unit, which can be considered as a variant of RNN with two inputs, two outputs, and a hidden state (reduced query), all of which operate in vector space. The details of the QRN module is explained throughout this section (2.1, 2.2).</p><p>Output module. Output module mapsŷ obtained from QRN to a natural language answerŷ. Similar to the input module, we adopt a standard solution for the output module (details in Section 5).</p><p>We first formally define the base model of a QRN unit, and then we explain how we connect the input and output modules to it (Section 2.1). We also present a few extensions to the network that can improve QRN's performance (Section 2.2). Finally, we show that QRN can be parallelized over time, giving computational advantage over most RNN-based models by one order of magnitude (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">QRN UNIT</head><p>As an RNN-based model, QRN is a single recurrent unit that updates its hidden state (reduced query) through time and layers. <ref type="figure" target="#fig_1">Figure 1a</ref> depicts the schematic structure of a QRN unit, and <ref type="figure" target="#fig_1">Figure 1b</ref> demonstrates how layers are stacked. A QRN unit accepts two inputs (local query vector q t ∈ R d and sentence vector x t ∈ R d ), and two outputs (reduced query vector h t ∈ R d , which is similar to the hidden state in RNN, and the sentence vector x t from the input without modification). The local query vector is not necessarily identical to the original query (question) vector q. In order to compute the outputs, we use update gate function α :</p><formula xml:id="formula_0">R d × R d → [0, 1] and reduce function ρ : R d × R d → R d .</formula><p>Intuitively, the update gate function measures the relevance between the sentence and the local query and is used to update the hidden state. The reduce function transforms the local query input to a candidate state which is a new reduced (easier) query given the sentence. The outputs are calculated with the following equations:</p><formula xml:id="formula_1">z t = α(x t , q t ) = σ(W (z) (x t • q t ) + b (z) ) (1) h t = ρ(x t , q t ) = tanh(W (h) [x t ; q t ] + b (h) )<label>(2)</label></formula><formula xml:id="formula_2">h t = z tht + (1 − z t )h t−1<label>(3)</label></formula><p>where z t is the scalar update gate,h t is the candidate reduced query, and h t is the final reduced query at time step t, σ(·) is sigmoid activation, tanh(·) is hyperboolic tangent activation (applied element-wise),</p><formula xml:id="formula_3">W (z) ∈ R 1×d , W (h) ∈ R d×2d are weight matrices, b (z) ∈ R, b (h) ∈ R d are bias terms,</formula><p>• is element-wise vector multiplication, and [; ] is vector concatenation along the row. As a base case, h 0 = 0. Here we have explicitly defined α and ρ, but they can be any reasonable differentiable functions.</p><p>The update gate is similar to the global attention mechanism <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b21">Xiong et al., 2016)</ref> in that it measures the similarity between the sentence (a memory slot) and the query. However, a significant difference is that the update gate is computed using sigmoid (σ) function on the current memory slot only (hence internally embedded within the unit), whereas the global attention is computed using softmax function over the entire memory (hence globally defined). The update gate can be rather considered as local sigmoid attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stacking layers</head><p>We just showed the single-layer case of QRN, but QRN with multiple layers is able to perform reasoning over multiple facts more effectively, as shown in the example of <ref type="figure" target="#fig_1">Figure 1b</ref>. In order to stack several layers of QRN, the outputs of the current layer are used as the inputs to the next layer. That is, using superscript k to denote the current layer's index (assuming 1-based indexing), we let q k+1 t = h k t . Note that x t is passed to the next layer without any modification, so we do not put a layer index on it.</p><p>Bi-direction. So far we have assumed that QRN only needs to look at past sentences, whereas often times, query answers can depend on future sentences. For instance, consider a sentence "John dropped the football." at time t. Then, even if there is no mention about the "football" in the past (at time i &lt; t), it can be implied that "John" has the "football" at the current time t. In order to incorporate the future dependency, we obtain − → h t and ← − h t in both forward and backward directions, respectively, using Equation 3. We then add them together to get q t for the next layer. That is, <ref type="bibr">b (h)</ref> are shared between the two directions.</p><formula xml:id="formula_4">q k+1 t = − → h k t + ← − h k t (4) for layer indices 1 ≤ k ≤ K − 1. Note that the variables W (z) , b (z) , W (h) ,</formula><p>Connecting input and output modules. <ref type="figure" target="#fig_1">Figure 1c</ref> depicts how QRN is connected with the input and output modules. In the first layer of QRN, q 1 t = q for all t, where q is obtained from the input module by processing the natural language question input q. x t is also obtained from x t by the same input module. The output at the last time step in the last layer is passed to the output module. That is, y = h K t where K represent the number of layers in the network. Then the output module gives the predicted answerŷ in natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">EXTENSIONS</head><p>Here we introduce a few extensions of QRN, and later in our experiments, we test QRN's performance with and without each of these extensions.</p><p>Reset gate. Inspired by GRU <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>, we found that it is useful to allow the QRN unit to reset (nullify) the candidate reduced query (i.e.,h t ) when necessary. For this we use a reset gate function β :</p><formula xml:id="formula_5">R d × R d → [0, 1]</formula><p>, which can be defined similarly to the update gate function:</p><formula xml:id="formula_6">r t = β(x t , q t ) = σ(W (r) (x t • q t ) + b (r) ) (5) where W (r) ∈ R 1×d is a weight matrix, and b (r) ∈ R is a bias term. Equation 3 is rewritten as h t = z t r tht + (1 − z t )h t−1 .<label>(6)</label></formula><p>Note that we do not use the reset gate in the last layer.</p><p>Vector gates. As in LSTM and GRU, update and reset gates can be vectors instead of scalar values for fine-controlled gating. For vector gates, we modify the row dimension of weights and biases in Equation 1 and 5 from 1 to d. Then we obtain z t , r t ∈ R d (instead of z t , r t ∈ R), and these can be element-wise multiplied (•) instead of being broadcasted in Equation 3 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PARALLELIZATION</head><p>An important advantage of QRN is that the recurrent updates in Equation 3 and 5 can be computed in parallel across time. This is in contrast with most RNN-based models that cannot be parallelized, where computing the candidate hidden state at time t explicitly requires the previous hidden state.</p><p>In QRN, the final reduced queries (h t ) can be decomposed into computing over candidate reduced queries (h t ), without looking at the previous reduced query. Here we primarily show that the query update in Equation 3 can be parallelized by rewriting the equation with matrix operations. The extension to Equation 5 is straightforward. The proof for QRN with vector gates is shown in Appendix B. The recursive definition of Equation 3 can be explicitly written as</p><formula xml:id="formula_7">h t = t i=1   t j=i+1 1 − z j   z ihi = t i=1 exp    t j=i+1 log (1 − z j )    z ihi .<label>(7)</label></formula><p>Let b i = log(1 − z i ) for brevity. Then we can rewrite Equation 7 as the following equation: <ref type="bibr" target="#b21">(Xiong et al., 2016)</ref> Figure 2: The schematics of QRN and the two state-of-the-art models, End-to-End Memory Networks <ref type="formula" target="#formula_1">(N2N)</ref> and Improved Dynamic Memory Networks (DMN+), simplified to emphasize the differences among the models. AGRU is a variant of GRU where the update gate is replaced with soft attention, proposed by <ref type="bibr" target="#b11">Kumar et al. (2016)</ref>. For QRN and DMN+, only forward direction arrows are shown.</p><formula xml:id="formula_8">       h 1 h 2 h 3 . . . h T        =        exp                     0 −∞ −∞ . . . −∞ b 2 0 −∞ . . . −∞ b 2 + b 3 b 3 0 . . . −∞ . . . . . . . . . . . . . . . T j=2 b j T j=3 b j T j=4 b j . . . 0                                   z 1h 1 z 2h 2 z 3h 3 . . . z Th T        (8) " # $ q ⋯ " # $ QRN ⋯ $ # = ) " " # " $ " " # " " # " # # QRN QRN QRN QRN QRN (a) QRN " # $ = ( " # $ ⋯ " + = # Σ Σ ⋯ (b) N2N (Sukhbaatar et al., 2015) " # $ = ( AGRU AGRU AGRU GRU ⋯ " # $ AGRU AGRU AGRU GRU ⋯ " + = # (c) DMN+</formula><p>Let H = [h 1 ; . . . ; h T ] be a T -by-d matrix where the transposes ( ) of the column vectors h t are concatenated across row. We similarly defineH fromh t . Also, let z = [z 1 ; . . . ; z T ] and b = [0; b 2 ; . . . ; b T ] be column vectors (note that we use 0 instead of b 1 ). Then Equation 8 is: </p><formula xml:id="formula_9">H = [L • exp (L [B • L ])] Z •H (9) where L, L ∈ R T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>QRN is inspired by RNN-based models with gating mechanism, such as LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref> and GRU <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>. While GRU and LSTM use the previous hidden state and the current input to obtain the candidate hidden state, QRN only uses the current two inputs to obtain the candidate reduced query (equivalent to candidate hidden state). We conjecture that this not only gives computational advantage via parallelization, but also makes training easier, i.e., avoiding vanishing gradient (which is critical for long-term dependency), overfitting (by simplifying the model), and converging to local minima.</p><p>The idea of structurally simplifying (constraining) RNNs for learning longer-term patterns has been explored in recent previous work, such as Structurally Constrained Recurrent Network <ref type="bibr" target="#b12">(Mikolov et al., 2015)</ref> and Strongly-Typed Recurrent Neural Network (STRNN) <ref type="bibr" target="#b1">(Balduzzi and Ghifary, 2016)</ref>. QRN is similar to STRNN in that both architectures use gating mechanism, and the gates and the candidate hidden states do not depend on the previous hidden states, which simplifies the recurrent relation. However, QRN can be distinguished from STRNN in three ways. First, QRN's update gate simulates attention mechanism, measuring the relevance between the input sentence and query. On the other hand, the gates in STRNN can be considered as the simplification of LSTM/GRU by removing their dependency on previous hidden state. Second, QRN is an RNN that is natively compatible with context-based QA tasks, where the QRN unit accepts two inputs, i.e. each context sentence and query. This is distinct from STRNN which has only one input. Third, we show that QRN is timewise-parallelizable on GPUs. Our parallelization algorithm is also applicable to STRNN.</p><p>End-to-end Memory Network (N2N) <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015)</ref> uses external memory with multi-layer attention mechanism to focus on sentences that are relevant to the question. There are two key differences between N2N and our QRN. First, N2N summarizes the entire memory in each layer to control the attention in the next layer (circle nodes in <ref type="figure">Figure 2b</ref>). Instead, QRN does not have any controller node <ref type="figure">(Figure 2a</ref>) and is able to focus on relevant sentences through the update gate that is internally embodied within its unit. Second, N2N adds time-dependent trainable weights to the sentence representations to model the time dependency of the sentences (as discussed in Section 1). QRN does not need such additional weights as its inherent RNN architecture allows QRN to effectively model the time dependency. Neural Reasoner <ref type="bibr" target="#b13">(Peng et al., 2015)</ref> and Gated End-toend Memory Network <ref type="bibr" target="#b14">(Perez and Liu, 2016)</ref>) are variants of MemN2N that share its fundamental characteristics.</p><p>Improved Dynamic Memory Network (DMN+) <ref type="bibr" target="#b21">(Xiong et al., 2016)</ref> uses the hybrid of the attention mechanism and the RNN architecture to model the sequence of sentences. It consists of two distinct GRUs, one for the time axis (rectangle nodes in <ref type="figure">Figure 2c</ref>) and one for the layer axis (circle nodes in <ref type="figure">Figure 2c</ref>). Note that the update gate of the GRU for the time axis is replaced with external softmax attention weights. DMN+ uses the time-axis GRU to summarizes the entire memory in each layer, and then the layer-axis GRU controls the attention weights in each layer. In contrast, QRN is simply a single recurrent unit without any controller node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>5.1 DATA bAbI story-based QA dataset bAbI story-based QA dataset  is composed of 20 different tasks (Appendix A), each of which has 1,000 (1k) synthetically-generated story-question pair. A story can be as short as two sentences and as long as 200+ sentences. A system is evaluated on the accuracy of getting the correct answers to the questions. The answers are single words or lists (e.g. "football, apple"). Answering questions in each task requires selecting a set of relevant sentences and applying different kinds of logical reasoning over them. The dataset also includes 10k training data (for each task), which allows training more complex models. Note that DMN+ <ref type="bibr" target="#b21">(Xiong et al., 2016)</ref> only reports on the 10k dataset.</p><p>bAbI dialog dataset bAbI dialog dataset  consists of 5 different tasks <ref type="table" target="#tab_4">(Table 3)</ref>, each of which has 1k synthetically-generated goal-oriented dialogs between a user and the system in the domain of restaurant reservation. Each dialog is as long as 96 utterances and comes with external knowledge base (KB) providing information of each restaurant. The authors also provide Out-Of-Vocabulary (OOV) version of the dataset, where many of the words and KB keywords in test data are not seen during training. A system is evaluated on the accuracy of its response to each utterance of the user, choosing from up to 2500 possible candidate responses. A system is required not only to understand the user's request but also refer to previous conversations in order to obtain the context information of the current conversation.</p><p>DSTC2 (Task 6) dialog dataset  transformed the Second Dialog State Tracking Challenge (DSTC2) dataset <ref type="bibr" target="#b7">(Henderson et al., 2014)</ref> into the same format as the bAbI dialog dataset, for the measurement of performance on a real dataset. Each dialog can be as long as 800+ utterances, and a system needs to choose from 2407 possible candidate responses for each utterance of the user. Note that the evaluation metric of the original DSTC2 is different from that of the transformed DSTC2, so previous work on the original DSTC2 should not be directly compared to our work. We will refer to this transformed DSTC2 dataset by "Task 6" of dialog dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MODEL DETAILS</head><p>Input Module. In the input module, we are given sentences (previous conversations in dialog) x t and a question (most recent user utterance) q, and we want to obtain their vector representations, x t , q ∈ R d . We use a trainable embedding matrix A ∈ R d×V to encode the one-hot vector of each word x tj in each sentence x t into a d-dimensional vector x tj ∈ R d . Then the sentence representation x t is obtained by Position Encoder . The same encoder with the same embedding matrix is also used to obtain the question vector q from q.</p><p>Output Module for story-based QA. In the output module, we are given the vector representation of the predicted answerŷ and we want to obtain the natural language form of the answer,ŷ. We use a V -way single-layer softmax classifier to mapŷ to a V -dimensional sparse vector,v = softmax W (y)ŷ ∈ R V , where W (y) ∈ R V ×d is a weight matrix. Then the final answerŷ is simply the argmax word inv. To handle questions with multiple-word answers, we consider each of them as a single word that contains punctuations such as space and comma, and put it in the vocabulary.</p><p>Output Module for dialog. We use a fixed number single-layer softmax classifiers, each of which is similar to that of the sotry-based QA model, to sequentially output each word of the system's response. While it is similar in spirit to the RNN decoder <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>, our output module does not have a recurrent hidden state or gating mechanism. Instead, it solely uses the final ouptut of the QRN,ŷ, and the current word output to influence the prediction of the next word among possible candidates.</p><p>Training. We withhold 10% of the training for development. We use the hidden state size of 50 by deafult. Batch sizes of 32 for bAbI story-based QA 1k, bAbI dialog and DSTC2 dialog, and 128 for bAbI QA 10k are used. The weights in the input and output modules are initialized with zero mean and the standard deviation of 1/ √ d. Weights in the QRN unit are initialized using techniques by <ref type="bibr" target="#b5">Glorot and Bengio (2010)</ref>, and are tied across the layers. Forget bias of 2.5 is used for update gates (no bias for reset gates). L2 weight decay of 0.001 (0.0005 for QA 10k) is used for all weights. The loss function is the cross entropy betweenv and the one-hot vector of the true answer. The loss is minimized by stochastic gradient descent for maximally 500 epochs, but training is early stopped if the loss on the development data does not decrease for 50 epochs. The learning rate is controlled by AdaGrad <ref type="bibr" target="#b4">(Duchi et al., 2011)</ref> with the initial learning rate of 0.5 (0.1 for QA 10k). Since the model is sensitive to the weight initialization, we repeat each training procedure 10 times (50 times for 10k) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RESULTS.</head><p>We compare our model with baselines and previous state-of-the-art models on story-based and dialog tasks <ref type="table">(Table 1)</ref>. These include LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref>, End-to-end Memory Networks (N2N) <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015)</ref>, Dynamic Memory Networks (DMN+) <ref type="bibr" target="#b21">(Xiong et al., 2016)</ref>, Gated End-to-end Memory Networks (GMemN2N) <ref type="bibr" target="#b14">(Perez and Liu, 2016)</ref>, and Differentiable Neural Computer (DNC) <ref type="bibr" target="#b6">(Graves et al., 2016)</ref>.</p><p>Story-based QA. Table 1(top) reports the summary of results of our model (QRN) and previous work on bAbI QA (task-wise results are shown in <ref type="table" target="#tab_2">Table 2</ref> in Appendix). In 1k data, QRN's '2r' (2 layers + reset gate + d = 50) outperforms all other models by a large margin (2.8+%). In 10k dataset, the average accuracy of QRN's '6r200' (6 layers + reset gate + d = 200) model outperforms all previous models by a large margin (2.5+%), achieving a nearly perfect score of 99.7%.</p><p>Dialog. Table 1(bottom) reports the summary of the results of our model (QRN) and previous work on bAbI dialog and Task 6 dialog (task-wise results are shown in <ref type="table" target="#tab_4">Table 3</ref> in Appendix). As done in previous work <ref type="bibr" target="#b14">Perez and Liu, 2016)</ref>, we also report results when we use 'Match' for dialogs. 'Match' is the extension to the model which additionally takes as input whether each answer candidate matches with context (more details on Appendix). QRN outperforms previous work by a large margin (2.0+%) in every comparison.</p><p>Ablations. We test four types of ablations (also discussed in Section 2.2): number of layers (1, 2, 3, or 6), reset gate (r), and gate vectorization (v) and the dimension of the hidden vector (50, 100). We show a subset of combinations of the ablations for bAbI QA in <ref type="table" target="#tab_2">Table 1 and Table 2</ref>; other combinations performed poorly and/or did not give interesting observations. According to the ablation results, we infer that: (a) When the number of layers is only one, the model lacks reasoning capability. In the case of 1k dataset, when there are too many layers (6), it seems correctly training the model becomes increasingly difficult. In the case of 10k dataset, many layers (6) and hidden dimensions (200) helps reasoning, most notably in difficult task such as task 16. (b) Adding the reset gate helps. (c) Including vector gates hurts in 1k datasets, as the model either overfits to the training data or converges to local minima. On the other hand, vector gates in bAbI story-based QA 10k dataset sometimes help. (d) Increasing the dimension of the hidden state to 100 in the dialog's Task 6 (DSTC2) helps, while there is not much improvement in the dialog's Task 1-5. It can be hypothesized that a larger hidden state is required for real data.  Parallelization. We implement QRN with and without parallelization in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016</ref>) on a single Titan X GPU to qunaitify the computational gain of the parallelization. For QRN without parallelization, we use the RNN library provided by TensorFlow. QRN with parallelization gives 6.2 times faster training and inference than QRN without parallelization on average. We expect that the speedup can be even higher for datasets with larger context.</p><p>Interpretations. An advantage of QRN is that the intermediate query updates are interpretable. <ref type="figure" target="#fig_1">Figure 1</ref> shows intermediate local queries (q k t ) interpreted in natural language, such as "Where is Sandra?". In order to obtain these, we place a decoder on the input question embedding q and add its loss for recovering the question to the classification loss (similarly to <ref type="bibr" target="#b13">Peng et al. (2015)</ref>). We then use the same decoder to decode the intermediate queries. This helps us understand the flow of information in the networks. In <ref type="figure" target="#fig_1">Figure 1</ref>, the question Where is apple? is transformed into Where is Sandra? at t = 1. At t = 2, as Sandra dropped the apple, the apple is no more relevant to Sandra. We obtain Where is Daniel? at time t = 3, and it is propagated until t = 5, where we observe a sentence (fact) that can be used to answer the query.</p><p>Visualization. <ref type="figure" target="#fig_3">Figure 3</ref> shows vizualization of the (scalar) magnitudes of update and reset gates on story sentences and dialog utterances. More visualizations are shown in Appendices: <ref type="figure">Figure 4</ref> and <ref type="figure">Figure 5</ref>. In <ref type="figure" target="#fig_3">Figure 3</ref>, we observe high values on facts that provide information to answer question (the system's next utterance for dialog). In QA Task 2 example (top left), we observe high update gate values in the first layer on facts that state who has the apple, and in the second layer, the high update gate values are on those that inform where that person went to. We also observe that the forward reset gate at t = 2 in the first layer ( − → r 1 2 ) is low, which is signifying that apple no more belongs to Sandra. In dialog Task 3 (bottom left), the model is able to infer that three restaurants are already recommended so that it can recommend another one. In dialog Task 6 (bottom), the model focuses on the sentences containing Spanish, and does not concentrate much on other facts such as I don't care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we introduce Query-Reduction Network (QRN) to answer context-based questions and carry out conversations with users that require multi-hop reasoning. We show the state-of-theart results in the three datasets of story-based QA and dialog. We model a story or a dialog as a sequence of state-changing triggers and compute the final answer to the question or the system's next utterance by recurrently updating (or reducing) the query. QRN is situated between the attention mechanism and RNN, effectively handling time dependency and long-term dependency problems of each technique, respectively. It addresses the long-term dependency problem of most RNNs by simplifying the recurrent update, in which the candidate hidden state (reduced query) does not depend on the previous state. Moreover, QRN can be parallelized and can address the well-known problem of RNN's vanishing gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TASK-WISE RESULTS</head><p>Here we provide detailed per-task breakdown of our results in QA <ref type="table" target="#tab_2">(Table 2</ref>) and dialog datasets <ref type="table" target="#tab_4">(Table 3)</ref>.     error rates (%) of QRN and previous work: LSTM , End-to-end Memory Networks (N2N) <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015)</ref>, Dynamic Memory Networks (DMN+) <ref type="bibr" target="#b21">(Xiong et al., 2016)</ref>, Gated End-to-end Memory Networks(GMemN2N) <ref type="bibr" target="#b14">(Perez and Liu, 2016)</ref>. Results within each task of Differentiable Neural Computer(DNC) were not provided in its paper <ref type="bibr" target="#b6">Graves et al. (2016)</ref>   ) and Gated End-to-end Memory Networks(GMemN2N <ref type="bibr" target="#b14">(Perez and Liu, 2016)</ref>). For QRN, a number in the front (1, 2, 3, 6) indicates the number of layers and a number in the back (100) indicates the dimension of hidden vector, while the default value is 50. 'r' indicates that the reset gate is used, 'v' indicates that the gates were vectorized, and '+' indicates that 'match' was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B VECTOR GATE PARALLELIZATION</head><p>For vector gates, we have z t ∈ R d instead of z t ∈ R. Therefore the following equation replaces Equation <ref type="formula" target="#formula_7">7</ref>:</p><formula xml:id="formula_10">h t = t i=1 exp                  t j=i+1 log 1 − z j 1 t j=i+1 log 1 − z j 2 . . . t j=i+1 log 1 − z j d                  • z i •h i<label>(10)</label></formula><p>where z j k is the k-th column vector of z j . Let b ij = log(1 − z i j ) for brevity. Then, we can rewrite Equation 8 as following: </p><formula xml:id="formula_11">       h 1 h 2 h 3 . . . h T        j =       exp                  0 −∞ −∞ . . . −∞ b 2j 0 −∞ . . . −∞ b 2j + b 3j b 3j 0 . . . −∞ . . . . . . . . . . . . . . . T k=2 b kj T k=3 b kj T k=4 b kj . . . 0                               z 1 •h 1 z 2 •h 2 z 3 •h 3 . . . z T •h T        j<label>(11)</label></formula><formula xml:id="formula_12">H =          [L • exp (L [B 1 • L ])] Z •H 1 [L • exp (L [B 2 • L ])] Z •H 2 . . . [L • exp (L [B d • L ])] Z •H d         <label>(12)</label></formula><p>where L, L ∈ R T ×T are lower and strictly lower triangular matrices of 1's are tiled across the column. Z = [z 1 , . . . , z d ] ∈ R T ×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MODEL DETAILS</head><p>Match. While similar in spirit, our 'Match' model is slightly different from previous work (Bordes and <ref type="bibr" target="#b14">Perez and Liu, 2016)</ref>. We use answer candidate embedding matrix, and add 2 dimension of 0-1 matrix which expresses whether the answer candidate matches with any word in the paragraph and the question. In other words, the softmax is computed bŷ</p><formula xml:id="formula_13">v = softmax W[W (y) ; M (y) ]ŷ ∈ R V , where W ∈ R d×d and W (y) ∈ R V ×(d−2)</formula><p>are trainable weight matrices, and M (y) ∈ R V ×2 is the 0-1 match matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D VISUALIZATIONS</head><p>Visualization of Story-based QA. <ref type="figure">Figure 4</ref> shows visualization of models for story-based QA tasks.</p><p>In the task 3 (left), the model focuses on the facts that contain 'football' in the first layer, and found out where Mary journeyed to before the bathroom in the second layer. In task 7 (right), the model focuses on the facts that provide information about the location of Sandra. 0.00 0.87 1.00 1.00 I'm on it.</p><p>0.73 0.97 0.38 0.00 How many people would you in your party.</p><p>1.00 1.00 0.00 0.41 For four people please.</p><p>Which price range are you looking for. Layer 1 Layer 2 Task 1 Issuing API calls z 1 − → r 1 ← − r 1 z 2 Can you make a restaurant reservation for eight in a cheap price range in madrid 0.00 1.00 0.93 1.00 I'm on it.</p><p>0.00 1.00 0.74 0.00 Any preference on a type of cuisine. 0.00 0.11 1.00 0.01 I love british food. 0.00 0.99 0.99 0.57 Okay let me look into some options for you.</p><p>1.00 0.00 0.00 0.02 &lt;SILENCE&gt; API CALL british madrid eight cheap Layer 1 Layer 2 Task 4 Providing extra-information z 1 − → r 1 ← − r 1 z 2 resto-paris-expen-spanish-8stars R-phone resto-paris-expen-spanish-8stars-phone 0.71 0.84 0.99 0.36 resto-paris-expen-spanish-8stars R-address resto-paris-expen-spanish-8stars-address 1.00 0.99 1.00 1.00 resto-paris-expen-spanish-8stars R-location paris 0.05 0.01 1.00 0.00 resto-paris-expen-spanish-8stars R-number four 0.02 0.95 0.97 0.00 resto-paris-expen-spanish-8stars R-price expensive 0.00 0.05 0.92 0.00 resto-paris-expen-spanish-8stars R-rating 8 0.38 0.91 1.00 0.10 What do you think of this option: resto-paris-expen-spanish-8stars 0.90 0.93 0.99 1.00 Let's do it. 0.00 0.00 1.00 0.00 Great let me do the reservation. 0.98 0.99 0.97 0.00 Do you have its address.</p><p>Here it is: resto-paris-expen-spanish-8stars-address <ref type="figure">Figure 5</ref>: Visualization of update and reset gates in QRN '2r' model for on several tasks of bAbI dialog and DSTC2 dialog <ref type="table" target="#tab_4">(Table 3)</ref>. We do not put reset gate in the last layer. Note that we only show some of recent sentences here, even the dialog has more sentences.</p><p>Visualization of Dialog. <ref type="figure">Figure 5</ref> shows visualization of models for dialog tasks.</p><p>In the first dialog of task 1, the model focuses on the user utterance that mentions the user's desired cuisine and location, and the current query (user's last utterance) informs the system of the number of people, so the system is able to learn that it now needs to ask the user about the desired price range.</p><p>In the second dialog of task 1, the model focuses on the facts that provide information about the requests of the user. In task 4 (third), the model focuses on what restaurant a user is talking about and the information about the restaurant.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(1a) QRN unit, (1b) 2-layer QRN on 5-sentence story, and (1c) entire QA system (QRN and input / output modules).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>×T are lower and strictly lower triangular matrices of 1's, respectively, • is elementwise multiplication, and B is a matrix where T b's are tiled across the column, i.e. B = [b, . . . , b] ∈ R T ×T , and similarly Z = [z, . . . , z] ∈ R T ×d . All implicit operations are matrix multiplications. With reasonable N (batch size), d and T (e.g. N, d, T = 100), matrix operations in Equation 9 can be comfortably computed in most modern GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(top) bAbI QA dataset visualization of update and reset gates in QRN '2r' model (bottom two) bAbI dialog and DSTC2 dialog dataset visualization of update and reset gates in QRN '2r' model. Note that the stories can have as many as 800+ sentences; we only show part of them here. More visualizations are shown inFigure 4(bAbI QA) andFigure 5(dialog datasets).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1: (top) bAbI QA dataset: number of failed tasks and average error rates (%). † is obtained from github.com/therne/dmn-tensorflow.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10k</cell></row><row><cell>Task</cell><cell></cell><cell></cell><cell cols="4">Previous works</cell><cell></cell><cell cols="2">QRN</cell><cell></cell><cell cols="2">Previous works</cell><cell>QRN</cell></row><row><cell cols="9">LSTM N2N DMN+  † GMemN2N 2r</cell><cell>3r</cell><cell cols="3">N2N DMN+ GMemN2N DNC 6r200</cell></row><row><cell># Failed</cell><cell>20</cell><cell></cell><cell>10</cell><cell></cell><cell>16</cell><cell cols="2">10</cell><cell>7</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>3</cell><cell>2</cell><cell>0</cell></row><row><cell>Average error rates</cell><cell>51.3</cell><cell cols="2">15.2</cell><cell></cell><cell>33.2</cell><cell cols="2">12.7</cell><cell cols="2">9.9 11.3</cell><cell>4.2</cell><cell>2.8</cell><cell>3.7</cell><cell>3.8</cell><cell>0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Plain</cell><cell></cell><cell></cell><cell></cell><cell>With Match</cell></row><row><cell>Task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Previous works</cell><cell cols="2">QRN</cell><cell cols="2">Previous works</cell><cell>QRN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">N2N GMemN2N</cell><cell>2r</cell><cell cols="3">2r100 N2N+ GMemN2N+</cell><cell>2r+</cell></row><row><cell cols="4">bAbI dialog Average error rates</cell><cell></cell><cell></cell><cell>13.9</cell><cell>14.3</cell><cell></cell><cell>5.5</cell><cell>5.5</cell><cell>6.7</cell><cell>5.4</cell><cell>1.5</cell></row><row><cell cols="6">bAbI dialog (OOV) Average error rates</cell><cell>30.3</cell><cell>27.9</cell><cell></cell><cell>11.1</cell><cell>11.1</cell><cell>11.2</cell><cell>10.3</cell><cell>2.3</cell></row><row><cell cols="4">DSTC2 dialog Average error rates</cell><cell></cell><cell></cell><cell>58.9</cell><cell>52.6</cell><cell></cell><cell>49.5</cell><cell>48.9</cell><cell>59.0</cell><cell>51.3</cell><cell>49.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(bottom) bAbI dialog and DSTC2 dialog</cell></row><row><cell cols="13">dataset (Bordes and Weston, 2016) average error rates (%) of QRN and previous work (LSTM, N2N, DMN+,</cell></row><row><cell cols="13">GMemN2N, and DNC). For QRN, the first number (1, 2, 3) indicates the number of layers, 'r' means the reset</cell></row><row><cell cols="13">gate is used, and the last number (100, 200), if exists, indicates the dimension of the hidden state, where the</cell></row><row><cell cols="13">default value is 50. '+' indicates that 'match' (See Appendix for details) is used. The task-wise results are shown</cell></row><row><cell cols="13">in Appendices: Table 2 (bAbI QA) and Table 3 (dialog datasets). See Section 5.3 for details.</cell></row><row><cell cols="2">Task 2: Two Supporting Facts</cell><cell></cell><cell>z 1</cell><cell cols="2">Layer 1 − → r 1</cell><cell>← − r 1</cell><cell>Layer 2 z 2</cell><cell cols="3">Task 15: Deduction</cell><cell></cell><cell>z 1</cell><cell>Layer 1 − → r 1</cell><cell>← − r 1</cell><cell>Layer 2 z 2</cell></row><row><cell cols="7">Sandra picked up the apple there. 0.95 0.89 0.98</cell><cell>0.00</cell><cell cols="4">Mice are afraid of wolves.</cell><cell>0.11 0.99 0.13</cell><cell>0.78</cell></row><row><cell cols="2">Sandra dropped the apple.</cell><cell></cell><cell>0.83</cell><cell></cell><cell cols="2">0.05 0.92</cell><cell>0.01</cell><cell cols="3">Gertrude is a mouse.</cell><cell></cell><cell>0.77 0.99 0.96</cell><cell>0.00</cell></row><row><cell cols="2">Daniel grabbed the apple there.</cell><cell></cell><cell cols="4">0.88 0.93 0.98</cell><cell>0.00</cell><cell cols="4">Cats are afraid of sheep.</cell><cell>0.01 0.99 0.07</cell><cell>0.03</cell></row><row><cell cols="7">Sandra travelled to the bathroom. 0.01 0.18 0.63</cell><cell>0.02</cell><cell cols="3">Winona is a mouse.</cell><cell></cell><cell>0.14 0.85 0.77</cell><cell>0.05</cell></row><row><cell cols="2">Daniel went to the hallway.</cell><cell></cell><cell cols="4">0.01 0.24 0.62</cell><cell>0.83</cell><cell cols="5">Sheep are afraid of wolves. 0.02 0.98 0.27</cell><cell>0.05</cell></row><row><cell>Where is the apple?</cell><cell></cell><cell></cell><cell cols="3">hallway</cell><cell></cell><cell></cell><cell cols="4">What is Gertrude afraid of?</cell><cell>wolf</cell></row><row><cell>Task 3: Displaying options</cell><cell></cell><cell>z 1</cell><cell cols="2">Layer 1 − → r 1</cell><cell>← − r 1</cell><cell>Layer 2 z 2</cell><cell cols="3">Task 6: DSTC2 dialog</cell><cell></cell><cell></cell><cell>z 1</cell><cell>Layer 1 − → r 1</cell><cell>← − r 1</cell><cell>Layer 2 z 2</cell></row><row><cell cols="2">resto-paris-expen-frech-8stars?</cell><cell cols="4">0.00 1.00 0.96</cell><cell>0.91</cell><cell cols="2">Spanish food.</cell><cell></cell><cell></cell><cell></cell><cell>0.84 0.07 0.00</cell><cell>0.82</cell></row><row><cell cols="2">Do you have something else?</cell><cell cols="4">0.41 0.99 0.00</cell><cell>0.00</cell><cell cols="6">You are lookng for a spanish restaurant right? 0.98 0.02 0.49</cell><cell>0.75</cell></row><row><cell cols="2">Sure let me find another option.</cell><cell cols="4">1.00 0.00 0.00</cell><cell>0.12</cell><cell>Yes.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.01 1.00 0.33</cell><cell>0.13</cell></row><row><cell cols="2">resto-paris-expen-frech-5stars?</cell><cell cols="4">0.00 1.00 0.96</cell><cell>0.91</cell><cell cols="5">What part of town do you have in mind?</cell><cell>0.20 0.73 0.41</cell><cell>0.11</cell></row><row><cell cols="2">No this does not work for me.</cell><cell cols="4">0.00 0.00 0.14</cell><cell>0.00</cell><cell cols="2">I don't care.</cell><cell></cell><cell></cell><cell></cell><cell>0.00 1.00 0.02</cell><cell>0.00</cell></row><row><cell cols="6">Sure let me find an other option. 1.00 0.00 0.00</cell><cell>0.12</cell><cell cols="4">What price range would you like?</cell><cell></cell><cell>0.72 0.46 0.52</cell><cell>0.72</cell></row><row><cell cols="7">What do you think of this? resto-paris-expen-french-4stars</cell><cell cols="2">I don't care.</cell><cell></cell><cell cols="3">API CALL spanish R-location R-price</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>bAbI QA dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>bAbI dialog and DSTC2 dialog dataset average error rates (%) of QRN and previous work: End-to-end Memory Networks(N2N</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Let H = [h 1 ; . . . ; h T ] be a T -by-d matrix where the transposes ( ) of the column vectors h t are concatenated across row. We similarly defineH fromh t . Also, let z = [z 1 ; . . . ; z T ], and B d be a T -by-T matrix where T [0; b 2d ; . . . ; b T d ]'s are tiled across the column.</figDesc><table><row><cell>Then Equation 11 is:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Mary got the football there. 0.82 1.00 0.0 0.06 John went back to the bedroom. 0.01 0.00 0.72 0.57 Mary journeyed to the office. 0.01 0.04 0.06 0.88 Mary journeyed to the bathroom. 0.44 0.00 0.89 0.05 Mary dropped the football. 0.62 0.01 0.00 0.03 Where was the football before the bathroom? office Mary journeyed to the garden. 0.67 0.08 0.58 0.12 Mary journeyed to the office. 0.91 0.44 0.11 0.21 Sandra grabbed the apple there. 0.02 0.34 0.92 0.89 Sandra discarded the apple. 0.26 0.61 0.95 0.97 Daniel went to the bedroom. 0.70 0.44 0.99 0.03 How many objects is Sandra carrying? noneFigure 4: Visualization of update and reset gates in QRN '2r' model for on several tasks of bAbI QA(Table 2). We do not put reset gate in the last layer. Note that we only show some of recent sentences here, though the stories can have as many as 200+ sentences.</figDesc><table><row><cell>Task 3: Three Supporting Facts</cell><cell>z 1</cell><cell>Layer 1 − → r 1</cell><cell>← − r 1</cell><cell>Layer 2 z 2</cell><cell>Task 7: Counting</cell><cell>z 1</cell><cell cols="2">Layer 1 − → r 1</cell><cell>← − r 1</cell><cell>Layer 2 z 2</cell></row><row><cell></cell><cell cols="4">Task 1 Issuing API calls</cell><cell>z 1</cell><cell cols="2">Layer 1 − → r 1</cell><cell cols="2">← − r 1</cell><cell>Layer 2 z 2</cell></row><row><cell></cell><cell cols="3">Good morning.</cell><cell></cell><cell cols="5">0.12 0.34 0.98</cell><cell>0.20</cell></row><row><cell cols="5">Hello what can i help you with today.</cell><cell cols="5">0.97 0.97 0.12</cell><cell>0.12</cell></row><row><cell cols="5">Can you book a table in rome with italian cuisine.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This mechanism is akin to logic regression in situation calculus<ref type="bibr" target="#b16">(Reiter, 2001)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by the NSF (IIS 1616112), Allen Institute for AI (66-9175), Allen Distinguished Investigator Award, Google Research Faculty Award, and Samsung GRO Award. We thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martın</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Strongly-typed recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGdial</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning longer memory in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015 Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05508</idno>
		<title level="m">Towards neural network-based reasoning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gated end-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Knowledge in Action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Reiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
