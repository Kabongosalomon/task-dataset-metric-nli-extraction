<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Compare: Relation Network for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Yongxin</forename><surname>Flood</surname></persName>
							<email>floodsung@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>t.xiang@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@ed.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Compare: Relation Network for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning models have achieved great success in visual recognition tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>. However, these supervised learning models need large amounts of labelled data and many iterations to train their large number of parameters. This severely limits their scalability to new classes due to annotation cost, but more fundamentally limits their applicability to newly emerging (eg. new consumer devices) or rare (eg. rare animals) categories where numerous annotated images may simply never exist. In contrast, humans are very good at recognising objects with very little direct supervision, or none at all i.e., few-shot <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref> or zero-shot <ref type="bibr" target="#b23">[24]</ref> learning. For example, children have no problem generalising the concept of "zebra" from a single picture in a book, or hearing its description as looking like a stripy horse. Motivated by the failure of conventional deep learning methods to work well on one or few examples per class, and inspired by the few-and zero-shot learning ability of humans, there has been a recent resurgence of interest in machine one/few-shot <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b28">29]</ref> and zero-shot <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref> learning.</p><p>Few-shot learning aims to recognise novel visual categories from very few labelled examples. The availability of only one or very few examples challenges the standard 'fine-tuning' practice in deep learning <ref type="bibr" target="#b9">[10]</ref>. Data augmentation and regularisation techniques can alleviate overfitting in such a limited-data regime, but they do not solve it. Therefore contemporary approaches to few-shot learning often decompose training into an auxiliary meta learning phase where transferrable knowledge is learned in the form of good initial conditions <ref type="bibr" target="#b9">[10]</ref>, embeddings <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39]</ref> or optimisation strategies <ref type="bibr" target="#b28">[29]</ref>. The target few-shot learning problem is then learned by fine-tuning <ref type="bibr" target="#b9">[10]</ref> with the learned optimisation strategy <ref type="bibr" target="#b28">[29]</ref> or computed in a feed-forward pass <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32]</ref> without updating network weights. Zero-shot learning also suffers from a related challenge. Recognisers are trained by a single example in the form of a class description (c.f., single exemplar image in one-shot), making data insufficiency for gradient-based learning a challenge.</p><p>While promising, most existing few-shot learning approaches either require complex inference mechanisms <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref>, complex recurrent neural network (RNN) architectures <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b31">32]</ref>, or fine-tuning the target problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref>. Our approach is most related to others that aim to train an effective metric for one-shot learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20]</ref>. Where they focus on the learning of the transferrable embedding and pre-define a fixed metric (e.g., as Euclidean <ref type="bibr" target="#b35">[36]</ref>), we further aim to learn a transferrable deep metric for comparing the relation between images (few-shot learning), or between images and class descriptions (zero-shot learning). By expressing the inductive bias of a deeper solution (multiple non-linear learned stages at both embedding and relation modules), we make it easier to learn a generalisable solution to the problem.</p><p>Specifically, we propose a two-branch Relation Network (RN) that performs few-shot recognition by learning to compare query images against few-shot labeled sample images. First an embedding module generates representations of the query and training images. Then these embeddings are compared by a relation module that determines if they arXiv:1711.06025v2 [cs.CV] 27 Mar 2018 are from matching categories or not. Defining an episodebased strategy inspired by <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36]</ref>, the embedding and relation modules are meta-learned end-to-end to support fewshot learning. This can be seen as extending the strategy of <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36]</ref> to include a learnable non-linear comparator, instead of a fixed linear comparator. Our approach outperforms prior approaches, while being simpler (no RNNs <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29]</ref>) and faster (no fine-tuning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref>). Our proposed strategy also directly generalises to zero-shot learning. In this case the sample branch embeds a single-shot category description rather than a single exemplar training image, and the relation module learns to compare query image and category description embeddings.</p><p>Overall our contribution is to provide a clean framework that elegantly encompasses both few and zero-shot learning. Our evaluation on four benchmarks show that it provides compelling performance across the board while being simpler and faster than the alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The study of one or few-shot object recognition has been of interest for some time <ref type="bibr" target="#b8">[9]</ref>. Earlier work on few-shot learning tended to involve generative models with complex iterative inference strategies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>. With the success of discriminative deep learning-based approaches in the datarich many-shot setting <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>, there has been a surge of interest in generalising such deep learning approaches to the few-shot learning setting. Many of these approaches use a meta-learning or learning-to-learn strategy in the sense that they extract some transferrable knowledge from a set of auxiliary tasks (meta-learning, learning-to-learn), which then helps them to learn the target few-shot problem well without suffering from the overfitting that might be expected when applying deep models to sparse data problems.</p><p>Learning to Fine-Tune The successful MAML approach <ref type="bibr" target="#b9">[10]</ref> aimed to meta-learn an initial condition (set of neural network weights) that is good for fine-tuning on few-shot problems. The strategy here is to search for the weight configuration of a given neural network such that it can be effectively fine-tuned on a sparse data problem within a few gradient-descent update steps. Many distinct target problems are sampled from a multiple task training set; the base neural network model is then fine-tuned to solve each of them, and the success at each target problem after finetuning drives updates in the base model -thus driving the production of an easy to fine-tune initial condition. The few-shot optimisation approach <ref type="bibr" target="#b28">[29]</ref> goes further in metalearning not only a good initial condition but an LSTMbased optimizer that is trained to be specifically effective for fine-tuning. However both of these approaches suffer from the need to fine-tune on the target problem. In contrast, our approach solves target problems in an entirely feed-forward manner with no model updates required, making it more convenient for low-latency or low-power applications.</p><p>RNN Memory Based Another category of approaches leverage recurrent neural networks with memories <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32]</ref>. Here the idea is typically that an RNN iterates over an examples of given problem and accumulates the knowledge required to solve that problem in its hidden activations, or external memory. New examples can be classified, for example by comparing them to historic information stored in the memory. So 'learning' a single target problem can occur in unrolling the RNN, while learning-to-learn means training the weights of the RNN by learning many distinct problems. While appealing, these architectures face issues in ensuring that they reliably store all the, potentially long term, historical information of relevance without forgetting. In our approach we avoid the complexity of recurrent networks, and the issues involved in ensuring the adequacy of their memory. Instead our learning-to-learn approach is defined entirely with simple and fast feed forward CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding and Metric Learning Approaches</head><p>The prior approaches entail some complexity when learning the target few-shot problem. Another category of approach aims to learn a set of projection functions that take query and sample images from the target problem and classify them in a feed forward manner <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4]</ref>. One approach is to parameterise the weights of a feed-forward classifier in terms of the sample set <ref type="bibr" target="#b3">[4]</ref>. The meta-learning here is to train the auxiliary parameterisation net that learns how to paramaterise a given feed-forward classification problem in terms of a few-shot sample set. Metric-learning based approaches aim to learn a set of projection functions such that when represented in this embedding, images are easy to recognise using simple nearest neighbour or linear classifiers <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20]</ref>. In this case the meta-learned transferrable knowledge are the projection functions and the target problem is a simple feed-forward computation.</p><p>The most related methodologies to ours are the prototypical networks of <ref type="bibr" target="#b35">[36]</ref> and the siamese networks of <ref type="bibr" target="#b19">[20]</ref>. These approaches focus on learning embeddings that transform the data such that it can be recognised with a fixed nearest-neighbour <ref type="bibr" target="#b35">[36]</ref> or linear <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref> classifier. In contrast, our framework further defines a relation classifier CNN, in the style of <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b13">14]</ref> (While <ref type="bibr" target="#b32">[33]</ref> focuses on reasoning about relation between two objects in a same image which is to address a different problem.). Compared to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>, this can be seen as providing a learnable rather than fixed metric, or non-linear rather than linear classifier. Compared to <ref type="bibr" target="#b19">[20]</ref> we benefit from an episodic training strategy with an end-to-end manner from scratch, and compared to <ref type="bibr" target="#b31">[32]</ref> we avoid the complexity of set-to-set RNN embedding of the sample-set, and simply rely on pooling <ref type="bibr" target="#b32">[33]</ref>.</p><p>Zero-Shot Learning Our approach is designed for few-shot learning, but elegantly spans the space into zero-shot learning (ZSL) by modifying the sample branch to input a single category description rather than single training image. When applied to ZSL our architecture is related to methods that learn to align images and category embeddings and perform recognition by predicting if an image and category embedding pair match <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref>. Similarly to the case with the prior metric-based few-shot approaches, most of these apply a fixed manually defined similarity metric or linear classifier after combining the image and category embedding. In contrast, we again benefit from a deeper end-to-end architecture including a learned nonlinear metric in the form of our learned convolutional relation network; as well as from an episode-based training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>We consider the task of few-shot classifier learning. Formally, we have three datasets: a training set, a support set, and a testing set. The support set and testing set share the same label space, but the training set has its own label space that is disjoint with support/testing set. If the support set contains K labelled examples for each of C unique classes, the target few-shot problem is called C-way K-shot.</p><p>With the support set only, we can in principle train a classifier to assign a class labelŷ to each samplex in the test set. However, due to the lack of labelled samples in the support set, the performance of such a classifier is usually not satisfactory. Therefore we aim to perform meta-learning on the training set, in order to extract transferrable knowledge that will allow us to perform better few-shot learning on the support set and thus classify the test set more successfully.</p><p>An effective way to exploit the training set is to mimic the few-shot learning setting via episode based training, as proposed in <ref type="bibr" target="#b38">[39]</ref>. In each training iteration, an episode is formed by randomly selecting C classes from the training set with K labelled samples from each of the C classes to act as the sample set</p><formula xml:id="formula_0">S = {(x i , y i )} m i=1 (m = K × C)</formula><p>, as well as a fraction of the remainder of those C classes' samples to serve as the query set Q = {(x j , y j )} n j=1 . This sample/query set split is designed to simulate the support/test set that will be encountered at test time. A model trained from sample/query set can be further fine-tuned using the support set, if desired. In this work we adopt such an episode-based training strategy. In our few-shot experiments (see Section 4.1) we consider one-shot (K = 1, <ref type="figure" target="#fig_0">Figure 1</ref>) and five-shot (K = 5) settings. We also address the K = 0 zero-shot learning case as explained in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model</head><p>One-Shot Our Relation Network (RN) consists of two modules: an embedding module f ϕ and a relation module g φ , as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Samples x j in the query set Q, and samples x i in the sample set S are fed through the embedding module f ϕ , which produces feature maps f ϕ (</p><formula xml:id="formula_1">x i ) and f ϕ (x j ). The feature maps f ϕ (x i ) and f ϕ (x j ) are com- bined with operator C(f ϕ (x i ), f ϕ (x j ))</formula><p>. In this work we assume C(·, ·) to be concatenation of feature maps in depth, although other choices are possible.</p><p>The combined feature map of the sample and query are fed into the relation module g φ , which eventually produces a scalar in range of 0 to 1 representing the similarity between x i and x j , which is called relation score. Thus, in the C-way one-shot setting, we generate C relation scores r i,j for the relation between one query input x j and training sample set examples x i ,</p><formula xml:id="formula_2">r i,j = g φ (C(f ϕ (x i ), f ϕ (x j ))), i = 1, 2, . . . , C<label>(1)</label></formula><p>K-shot For K-shot where K &gt; 1, we element-wise sum over the embedding module outputs of all samples from each training class to form this class' feature map. This pooled class-level feature map is combined with the query image feature map as above. Thus, the number of relation scores for one query is always C in both one-shot or fewshot setting.</p><p>Objective function We use mean square error (MSE) loss (Eq. <ref type="formula">(2)</ref>) to train our model, regressing the relation score r i,j to the ground truth: matched pairs have similarity 1 and the mismatched pair have similarity 0.</p><formula xml:id="formula_3">ϕ, φ ← argmin ϕ,φ m i=1 n j=1 (r i,j − 1(y i == y j )) 2 (2)</formula><p>The choice of MSE is somewhat non-standard. Our problem may seem to be a classification problem with a label space {0, 1}. However conceptually we are predicting relation scores, which can be considered a regression problem despite that for ground-truth we can only automatically generate {0, 1} targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Zero-shot Learning</head><p>Zero-shot learning is analogous to one-shot learning in that one datum is given to define each class to recognise. However instead of being given a support set with one-shot image for each of C training classes, it contains a semantic class embedding vector v c for each. Modifying our framework to deal with the zero-shot case is straightforward: as a different modality of semantic vectors is used for the support set (e.g. attribute vectors instead of images), we use a second heterogeneous embedding module f ϕ2 besides the embedding module f ϕ1 used for the image query set. Then the relation net g φ is applied as before. Therefore, the relation score for each query input x j will be:</p><formula xml:id="formula_4">r i,j = g φ (C(f ϕ1 (v c ), f ϕ2 (x j ))), i = 1, 2, . . . , C (3)</formula><p>The objective function for zero-shot learning is the same as that for few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture</head><p>As most few-shot learning models utilise four convolutional blocks for embedding module <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36]</ref>, we follow the same architecture setting for fair comparison, see <ref type="figure" target="#fig_1">Figure 2</ref>. More concretely, each convolutional block contains a 64filter 3 × 3 convolution, a batch normalisation and a ReLU nonlinearity layer respectively. The first two blocks also contain a 2 × 2 max-pooling layer while the latter two do not. We do so because we need the output feature maps for further convolutional layers in the relation module. The relation module consists of two convolutional blocks and two fully-connected layers. Each of convolutional block is a 3 × 3 convolution with 64 filters followed by batch normalisation, ReLU non-linearity and 2 × 2 max-pooling. The output size of last max pooling layer is H = 64 and H = 64 * 3 * 3 = 576 for Omniglot and miniImageNet respectively. The two fully-connected layers are 8 and 1 dimensional, respectively. All fully-connected layers are ReLU except the output layer is Sigmoid in order to generate relation scores in a reasonable range for all versions of our network architecture.</p><p>The zero-shot learning architecture is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. In this architecture, the DNN subnet is an existing network (e.g., Inception or ResNet) pretrained on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our approach on two related tasks: few-shot classification on Omniglot and miniImagenet, and zeroshot classification on Animals with Attributes (AwA) and Caltech-UCSD Birds-200-2011 (CUB). All the experiments are implemented based on PyTorch [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Few-shot Recognition Settings</head><p>Few-shot learning in all experiments uses Adam <ref type="bibr" target="#b18">[19]</ref> with initial learning rate 10 −3 , annealed by half for every 100,000 episodes. All our models are end-to-end trained from scratch with no additional dataset. Baselines We compare against various state of the art baselines for few-shot recognition, including neural statistician <ref type="bibr" target="#b7">[8]</ref>, Matching Nets with and without fine-tuning <ref type="bibr" target="#b38">[39]</ref>, MANN <ref type="bibr" target="#b31">[32]</ref>, Siamese Nets with Memory <ref type="bibr" target="#b17">[18]</ref>, Convolutional Siamese Nets <ref type="bibr" target="#b19">[20]</ref>, MAML <ref type="bibr" target="#b9">[10]</ref>, Meta Nets <ref type="bibr" target="#b26">[27]</ref>, Prototypical Nets <ref type="bibr" target="#b35">[36]</ref> and Meta-Learner LSTM <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Omniglot</head><p>Dataset Omniglot <ref type="bibr" target="#b22">[23]</ref> contains 1623 characters (classes) from 50 different alphabets. Each class contains 20 samples drawn by different people. Following <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36]</ref>, we augment new classes through 90 • , 180 • and 270 • rotations of existing data and use 1200 original classes plus rotations for training and remaining 423 classes plus rotations for testing. All input images are resized to 28 × 28.</p><p>Training Besides the K sample images, the 5-way 1shot contains 19 query images, the 5-way 5-shot has 15 query images, the 20-way 1-shot has 10 query images and the 20-way 5-shot has 5 query images for each of the C sampled classes in each training episode. This means for  example that there are 19 × 5 + 1 × 5 = 100 images in one training episode/mini-batch for the 5-way 1-shot experiments.</p><p>Results Following <ref type="bibr" target="#b35">[36]</ref>, we computed few-shot classification accuracies on Omniglot by averaging over 1000 randomly generated episodes from the testing set. For the 1shot and 5-shot experiments, we batch one and five query images per class respectively for evaluation during testing. The results are shown in <ref type="table" target="#tab_2">Table 1</ref>. We achieved state-of-theart performance under all experiments setting with higher averaged accuracies and lower standard deviations, except 5-way 5-shot where our model is 0.1% lower in accuracy than <ref type="bibr" target="#b9">[10]</ref>. This is despite that many alternatives have significantly more complicated machinery <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8]</ref>, or fine-tune on the target problem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39]</ref>, while we do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">miniImageNet</head><p>Dataset The miniImagenet dataset, originally proposed by <ref type="bibr" target="#b38">[39]</ref>, consists of 60,000 colour images with 100 classes, each having 600 examples. We followed the split introduced by <ref type="bibr" target="#b28">[29]</ref>, with 64, 16, and 20 classes for training, validation and testing, respectively. The 16 validation classes is used for monitoring generalisation performance only. Training Following the standard setting adopted by most existing few-shot learning work, we conducted 5 way 1-shot and 5-shot classification. Beside the K sample images, the 5-way 1-shot contains 15 query images, and the 5-way 5shot has 10 query images for each of the C sampled classes in each training episode. This means for example that there are 15×5+1×5 = 80 images in one training episode/minibatch for 5-way 1-shot experiments. We resize input images to 84 × 84. Our model is trained end-to-end from scratch, with random initialisation, and no additional training set.</p><p>Results Following <ref type="bibr" target="#b35">[36]</ref>, we batch 15 query images per class in each episode for evaluation in both 1-shot and 5shot scenarios and the few-shot classification accuracies are computed by averaging over 600 randomly generated episodes from the test set.</p><p>From <ref type="table">Table 2</ref>, we can see that our model achieved stateof-the-art performance on 5-way 1-shot settings and competitive results on 5-way 5-shot. However, the 1-shot result reported by prototypical networks <ref type="bibr" target="#b35">[36]</ref> reqired to be trained on 30-way 15 queries per training episode, and 5-shot result was trained on 20-way 15 queries per training episode. When trained with 5-way 15 query per training episode, <ref type="bibr" target="#b35">[36]</ref> only got 46.14 ± 0.77% for 1-shot evaluation, clearly weaker than ours. In contrast, all our models are trained on 5-way, 1 query for 1-shot and 5 queries for 5-shot per training episode, with much less training queries than <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Zero-shot Recognition</head><p>Datasets and settings We follow two ZSL settings: the old setting and the new GBU setting provided by <ref type="bibr" target="#b41">[42]</ref> for training/test splits. Under the old setting, adopted by most existing ZSL works before <ref type="bibr" target="#b41">[42]</ref>, some of the test classes also appear in the ImageNet 1K classes, which have been used to pretrain the image embedding network, thus violating the zero-shot assumption. In contrast, the new GBU setting ensures that none of the test classes of the datasets appear in the ImageNet 1K classes. Under both settings, the    <ref type="table">Table 2</ref>: Few-shot classification accuracies on miniImagenet. All accuracy results are averaged over 600 test episodes and are reported with 95% confidence intervals, same as <ref type="bibr" target="#b35">[36]</ref>. For each task, the best-performing method is highlighted, along with any others whose confidence intervals overlap. '-': not reported.</p><p>test set can comprise only the unseen class samples (conventional test set setting) or a mixture of seen and unseen class samples. The latter, termed generalised zero-shot learning (GZSL), is more realistic in practice. Two widely used ZSL benchmarks are selected for the old setting: AwA (Animals with Attributes) <ref type="bibr" target="#b23">[24]</ref> consists of 30,745 images of 50 classes of animals. It has a fixed split for evaluation with 40 training classes and 10 test classes. CUB (Caltech-UCSD Birds-200-2011) <ref type="bibr" target="#b39">[40]</ref> contains 11,788 images of 200 bird species with 150 seen classes and 50 disjoint unseen classes. Three datasets <ref type="bibr" target="#b41">[42]</ref> are selected for GBU setting: AwA1, AwA2 and CUB. The newly released AwA2 <ref type="bibr" target="#b41">[42]</ref> consists of 37,322 images of 50 classes which is an extension of AwA while AwA1 is same as AwA but under the GBU setting.</p><p>Semantic representation For AwA, we use the continuous 85-dimension class-level attribute vector from <ref type="bibr" target="#b23">[24]</ref>, which has been used by all recent works. For CUB, a continuous 312-dimension class-level attribute vector is used.</p><p>Implementation details Two different embedding modules are used for the two input modalities in zero-shot learning. Unless otherwise specified, we use Inception-V2 <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b16">17]</ref> as the query image embedding DNN in the old and conventional setting and ResNet101 <ref type="bibr" target="#b15">[16]</ref> for the GBU and generalised setting, taking the top pooling units as image embedding with dimension D = 1024 and 2048 respectively. This DNN is pre-trained on ILSVRC 2012 1K classification without fine-tuning, as in recent deep ZSL works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45]</ref>. A MLP network is used for embedding semantic attribute vectors. The size of hidden layer FC1 <ref type="figure" target="#fig_2">(Figure 3</ref>) is set to 1024 and 1200 for AwA and CUB respectively, and the output size FC2 is set to the same dimension as the image embedding for both datasets. For the relation module, the image and semantic embeddings are concatenated before being fed into MLPs with hidden layer FC3 size 400 and 1200 for AwA and CUB, respectively.</p><p>We add weight decay (L2 regularisation) in FC1 &amp; 2 as there is a hubness problem <ref type="bibr" target="#b44">[45]</ref> in cross-modal mapping for ZSL which can be best solved by mapping the semantic feature vector to the visual feature space with regularisation. After that, FC3 &amp; 4 (relation module) are used to compute the relation between the semantic representation (in the visual feature space) and the visual representation. Since the hubness problem does not existing in this step, no L2 regularisation/weight decay is needed. All the ZSL models are trained with weight decay 10 −5 in the embedding network. The learning rate is initialised to 10 −5 with Adam <ref type="bibr" target="#b18">[19]</ref> and then annealed by half every 200,000 iterations.</p><p>Results under the old setting The conventional evaluation for ZSL followed by the majority of prior work is to assume that the test data all comes from unseen classes. We evaluate this setting first. We compare 15 alternative approaches in <ref type="table" target="#tab_4">Table 3</ref>. With only the attribute vector used as the sample class embedding, our model achieves competitive result on AwA and state-of-the-art performance on the more challenging CUB dataset, outperforming the most related alternative prototypical networks <ref type="bibr" target="#b35">[36]</ref> by a big margin. Note that only inductive methods are considered. Some re-   <ref type="bibr" target="#b33">[34]</ref> is used; F G for GoogLeNet <ref type="bibr" target="#b37">[38]</ref>; and F V for VGG net <ref type="bibr" target="#b34">[35]</ref>. For neural network based methods, all use Inception-V2 (GoogLeNet with batch normalisation) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b16">17]</ref> as the DNN image imbedding subnet, indicated as N G . cent methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> are tranductive in that they use all test data at once for model training, which gives them a big advantage at the cost of making a very strong assumption that may not be met in practical applications, so we do not compare with them here. Results under the GBU setting We follow the evaluation setting of <ref type="bibr" target="#b41">[42]</ref>. We compare our model with 11 alternative ZSL models in <ref type="table" target="#tab_6">Table 4</ref>. The 10 shallow models results are from <ref type="bibr" target="#b41">[42]</ref> and the result of the state-of-the-art method DEM <ref type="bibr" target="#b44">[45]</ref> is from the authors' GitHub page <ref type="bibr" target="#b0">1</ref> . We can see that on AwA2 and CUB, Our model is particularly strong under the more realistic GZSL setting measured using the harmonic mean (H) metric. While on AwA1, our method is only outperformed by DEM <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Why does Relation Network Work?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Relationship to existing models</head><p>Related prior few-shot work uses fixed pre-specified distance metrics such as Euclidean or cosine distance to perform classification <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36]</ref>. These studies can be seen as distance metric learning, but where all the learning occurs in the feature embedding, and a fixed metric is used given the learned embedding. Also related are conventional metric learning approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7]</ref> that focus on learning a shallow (linear) Mahalanobis metric for a fixed feature representa- tion. In contrast to prior work's fixed metric or fixed features and shallow learned metric, Relation Network can be seen as both learning a deep embedding and learning a deep non-linear metric (similarity function) 2 . These are mutually tuned end-to-end to support each other in few short learning. Why might this be particularly useful? By using a flexible function approximator to learn similarity, we learn a good metric in a data driven way and do not have to manually choose the right metric (Euclidean, cosine, Mahalanobis). Fixed metrics like <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36]</ref> assume that features are solely compared element-wise, and the most related <ref type="bibr" target="#b35">[36]</ref> assumes linear separability after the embedding. These are thus critically dependent on the efficacy of the learned embedding network, and hence limited by the extent to which the embedding networks generate inadequately discriminative representations. In contrast, by deep learning a nonlinear similarity metric jointly with the embedding, Relation Network can better identify matching/mismatching pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Visualisation</head><p>To illustrate the previous point about adequacy of learned input embeddings, we show a synthetic example where existing approaches definitely fail and our Relation Network can succeed due to using a deep relation module. Assuming 2D query and sample input embeddings to a relation module, <ref type="figure" target="#fig_4">Fig. 4(a)</ref> shows the space of 2D sample inputs for a fixed 2D query input. Each sample input (pixel) is colored according to whether it matches the fixed query or not. This   represents a case where the output of the embedding modules is not discriminative enough for trivial (Euclidean NN) comparison between query and sample set. In <ref type="figure" target="#fig_4">Fig. 4</ref>(c) we attempt to learn matching via a Mahalanobis metric learning relation module, and we can see the result is inadequate. In <ref type="figure" target="#fig_4">Fig. 4(d)</ref> we learn a further 2-hidden layer MLP embedding of query and sample inputs as well as the subsequent Mahalanobis metric, which is also not adequate. Only by learning the full deep relation module for similarity can we solve this problem in <ref type="figure" target="#fig_4">Fig. 4(b)</ref>. In a real problem the difficulty of comparing embeddings may not be this extreme, but it can still be challenging. We qualitatively illustrate the challenge of matching two example Omniglot query images (embeddings projected to 2D, <ref type="figure" target="#fig_6">Figure 5</ref>(left)) by showing an analogous plot of real sample images colored by match (cyan) or mismatch (magenta) to two example queries (yellow). Under standard assumptions <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7]</ref> the cyan matching samples should be nearest neighbours to the yellow query image with some metric (Euclidean, Cosine, Mahalanobis). But we can see that the match relation is more complex than this. In <ref type="figure" target="#fig_6">Figure 5</ref>(right), we instead plot the same two example queries in terms of a 2D PCA representation of each query-sample pair, as represented by the relation module's penultimate layer. We can see that the relation network has mapped the data into a space where the (mis)matched pairs are linearly separable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a simple method called the Relation Network for few-shot and zero-shot learning. Relation network learns an embedding and a deep non-linear distance metric for comparing query and sample items. Training the network end-to-end with episodic training tunes the embedding and distance metric for effective few-shot learning. This ap- proach is far simpler and more efficient than recent few-shot meta-learning approaches, and produces state-of-the-art results. It further proves effective at both conventional and generalised zero-shot settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Relation Network architecture for a 5-way 1-shot problem with one query example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Relation Network architecture for few-shot learning (b) which is composed of elements including convolutional block (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Relation Network architecture for zero-shot learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1Figure 4 :</head><label>4</label><figDesc>https://github.com/lzrobots/ DeepEmbeddingModel_ZSL An example relation learnable by Relation Network and not by non-linear embedding + metric learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>AwA1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Example Omniglot few-shot problem visualisations. Left: Matched (cyan) and mismatched (magenta) sample embeddings for a given query (yellow) are not straightforward to differentiate. Right: Matched (yellow) and mismatched (magenta) relation module pair representations are linearly separable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Omniglot few-shot classification. Results are accuracies averaged over 1000 test episodes and with 95% confidence intervals where reported. The best-performing method is highlighted, along with others whose confidence intervals overlap. '-': not reported.</figDesc><table><row><cell>Model</cell><cell>FT</cell><cell>5-way Acc.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>MATCHING NETS [39]</cell><cell>N</cell><cell cols="2">43.56 ± 0.84% 55.31 ± 0.73%</cell></row><row><cell>META NETS [27]</cell><cell>N</cell><cell>49.21 ± 0.96%</cell><cell>-</cell></row><row><cell>META-LEARN LSTM [29]</cell><cell>N</cell><cell cols="2">43.44 ± 0.77% 60.60 ± 0.71%</cell></row><row><cell>MAML [10]</cell><cell>Y</cell><cell cols="2">48.70 ± 1.84% 63.11 ± 0.92%</cell></row><row><cell cols="2">PROTOTYPICAL NETS [36] N</cell><cell cols="2">49.42 ± 0.78% 68.20 ± 0.66%</cell></row><row><cell>RELATION NET</cell><cell></cell><cell></cell><cell></cell></row></table><note>N 50.44 ± 0.82% 65.32 ± 0.70%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Zero-shot classification accuracy (%) comparison on AwA and</cell></row><row><cell>CUB (hit@1 accuracy over all samples) under the old and conventional</cell></row><row><cell>setting. SS: semantic space; A: attribute space; W: semantic word vector</cell></row><row><cell>space; D: sentence description (only available for CUB). F: how the vi-</cell></row><row><cell>sual feature space is computed; For non-deep models: F</cell></row></table><note>O if overfeat</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparative results under the GBU setting. Under the conventional ZSL setting, the performance is evaluated using per-class average Top-1 (T1) accuracy (%), and under GZSL, it is measured using u = T1 on unseen classes, s = T1 on seen classes, and H = harmonic mean.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our architecture does not guarantee the self-similarity and symmetry properties of a formal similarity function. But empirically we find these properties hold numerically for a trained Relation Network.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1, EPSRC/MURI grant EP/N019474/1, EPSRC grant EP/R026173/1, and the European Union's Horizon 2020 research and innovation program (grant agreement no. 640891). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU and the ES-PRC funded Tier 2 facility, JADE used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://github.com/pytorch/pytorch.4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Labelembedding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving semantic embedding consistency by metric learning for zero-shot classiffication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards a neural statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">One-shot learning of object categories. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised vocabulary-informed learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning to remember rare events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic autoencoder for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CogSci</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attributebased classification for zero-shot visual object categorization. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiclass recognition and part localization with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Zeroshot learning-a comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00600</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A unified perspective on multi-domain and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zero-shot recognition via structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
