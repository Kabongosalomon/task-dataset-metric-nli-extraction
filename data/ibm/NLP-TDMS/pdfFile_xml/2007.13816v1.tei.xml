<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Corner Proposal Network for Anchor-free, Two-stage Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
							<email>kaiwenduan@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<email>qmhuang@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Corner Proposal Network for Anchor-free, Two-stage Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object Detection</term>
					<term>Anchor-Free Detector</term>
					<term>Two-stage Detec- tor</term>
					<term>Corner Keypoints</term>
					<term>Object Proposals</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of object detection is to determine the class and location of objects in an image. This paper proposes a novel anchor-free, two-stage framework which first extracts a number of object proposals by finding potential corner keypoint combinations and then assigns a class label to each proposal by a standalone classification stage. We demonstrate that these two stages are effective solutions for improving recall and precision, respectively, and they can be integrated into an end-to-end network. Our approach, dubbed Corner Proposal Network (CPN), enjoys the ability to detect objects of various scales and also avoids being confused by a large number of false-positive proposals. On the MS-COCO dataset, CPN achieves an AP of 49.2% which is competitive among stateof-the-art object detection methods. CPN also fits the scenario of computational efficiency, which achieves an AP of 41.6%/39.7% at 26.2/43.3 FPS, surpassing most competitors with the same inference speed. Code is available at https://github.com/Duankaiwen/CPNDet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Top: an anchor-based method (e.g., Faster R-CNN <ref type="bibr" target="#b33">[34]</ref>) may have difficulty in finding objects with a peculiar shape (e.g., with a very large size or an extreme aspect ratio). Bottom: an anchor-free method (e.g., CornerNet <ref type="bibr" target="#b18">[19]</ref>) may mistakenly group irrelevant keypoints into an object. Green, blue and red bounding-boxes indicate true positives, false positives and false negatives, respectively.</p><p>anchor-free <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b38">39]</ref> methods which suggested depicting each object with one or few keypoints and the geometry. These potential objects are named proposals, and for each of them, the class label is either inherited from a previous output or verified by an individual classifier trained for this purpose. This brings a debate between the so-called two-stage and one-stage approaches, in which people tend to believe that the former works slower but produces higher detection accuracy.</p><p>This paper provides an alternative opinion on the design of object detection approaches. There are two arguments. First, the recall of a detection approach is determined by its ability to locate objects of different geometries, especially those with rare shapes, and anchor-free methods (in particular, the methods based on locating the border of objects) are potentially better in this task. Second, anchor-free methods often incur a large number of false positives, and thus an individual classifier is strongly required to improve the precision of detection, see <ref type="figure" target="#fig_0">Fig. 1</ref>. Therefore, we inherit the merits of both anchor-free and two-stage object detectors and design an efficient, end-to-end implementation.</p><p>Our approach is named Corner Proposal Network (CPN). It detects an object by locating the top-left and bottom-right corners of it and then assigning a class label to it. We make use of the keypoint detection method of CornerNet <ref type="bibr" target="#b18">[19]</ref> but, instead of grouping the keypoints with keypoint feature embedding, enumerate all valid corner combinations as potential objects. This leads to a large number of proposals, most of which are false positives. We then train a classifier to discriminate real objects from incorrectly paired keypoints based on the corresponding regional features. There are two steps for classification, with the first one, a binary classifier, filtering out a large part of proposals (i.e., that do not correspond to objects), and the second one, with stronger features, re-ranking the survived objects with multi-class classification scores.</p><p>The effectiveness of CPN is verified on the MS-COCO dataset <ref type="bibr" target="#b24">[25]</ref>, one of the most challenging object detection benchmarks. Using a 104-layer stacked Hourglass network <ref type="bibr" target="#b29">[30]</ref> as the backbone, CPN reports an AP of 49.2%, which outperforms the previously best anchor-free detectors, CenterNet <ref type="bibr" target="#b7">[8]</ref>, by a significant margin of 2.2%. In particular, CPN enjoys even larger accuracy gain in detecting objects with peculiar shapes (e.g., very large or small areas or extreme aspect ratios), demonstrating the advantage of using anchor-free methods for proposal extraction. Last but not least, CPN can also fit scenarios that desire for network efficiency. Working on a lighter backbone DLA-34 <ref type="bibr" target="#b42">[43]</ref> and switching off image flip in inference, CPN achieves 41.6% at 26.2 FPS or 39.7% at 43.3 FPS, surpassing most competitors with the same inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Object detection is an important yet challenging problem in computer vision. It aims to obtain a tight bounding-box as well as a class label for each object in an image. In recent years, with the rapid development of deep learning, most powerful object detection methods are based on training deep neural networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>. According to the way of determining the geometry and class of an object, existing detection approaches can be roughly categorized into anchor-based and anchor-free methods.</p><p>An anchor-based approach starts with placing a large number of anchors, which are regional proposals with different but fixed scales and shapes, and are uniformly distributed on the image plane. These anchors are then considered as object proposals and an individual classifier is trained to determine the objectness as well as the class of each proposal <ref type="bibr" target="#b33">[34]</ref>. Beyond this framework, researchers made efforts in two aspects, namely, improving the basic quality of regional features extracted from the proposal, and arriving at a better alignment between the proposals and features. For the first type of efforts, typical examples include using more powerful network backbones <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b13">14]</ref> and using hierarchical features to represent a region <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b26">27]</ref>. Regarding the second type, there exist methods to align anchors to features <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, align features to anchors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref>, and adjust the anchors after classification has been done <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Alternatively, an anchor-free approach does not assume the objects to come from uniformly distributed anchors. Early efforts including DenseBox <ref type="bibr" target="#b14">[15]</ref> and UnitBox <ref type="bibr" target="#b43">[44]</ref> proved that the detectors can achieve the detection task without anchors. Recently, anchor-free approaches have been greatly promoted by the development of keypoint detection <ref type="bibr" target="#b28">[29]</ref> and the assist of the focal loss <ref type="bibr" target="#b23">[24]</ref>. The fundamental of anchor-free approaches is usually one or few keypoints. Depending on how keypoints are used for object depiction, anchor-free approaches can be roughly categorized into point-grouping detectors and point-vector detectors. Point-grouping detectors, including CornerNet <ref type="bibr" target="#b18">[19]</ref>, CenterNet <ref type="bibr" target="#b7">[8]</ref>, Ex-tremeNet <ref type="bibr" target="#b48">[49]</ref>, etc, group more than one keypoints into an object, while the pointvector detectors such as FCOS <ref type="bibr" target="#b38">[39]</ref>, CenterNet <ref type="bibr" target="#b47">[48]</ref>, FoveaBox <ref type="bibr" target="#b16">[17]</ref>, SAPD <ref type="bibr" target="#b49">[50]</ref>, etc., use a keypoint and a vector of object geometry (e.g., the width, height, or its distance to the borders) to determine the shape of objects.</p><p>Based on the object proposals, it remains to determine whether each proposal is an object and what class of object it is. There is also discussion on using two-stage and one-stage detectors for object detection. A two-stage detector <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22]</ref> refers to an individual classifier is trained for this purpose, while a one-stage detector <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b4">5]</ref> mostly uses classification cues from the previous stage. Two-stage detectors are often more accurate but slower, compared to one-stage detectors. To accelerate it, an efficient method is to partition classification into two steps <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31]</ref>, with the first step filtering out most easy false positives, and the second step using heavier computation to assign each survived proposal a class label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Object detection starts with an image I denoted by raw pixels, on which a few rectangles, often referred to as bounding-boxes, that tightly covers the objects are labeled with a class label. Denote a ground-truth bounding-box as b n , n = 1, 2, . . . , N , the corresponding class label as c n , and I n represents the image region within the corresponding bounding-box. The goal is to locate a few bounding-boxes, b m , m = 1, 2, . . . , M , and assign each one with a class label, c m , so that the sets of {b n , c n } 3.1 Anchor-based or Anchor-free? One-stage or Two-stage?</p><p>We focus on two important choices of object detection, namely, whether to use anchor-based or anchor-free methods for proposal extraction, and whether to use one-stage or two-stage methods for determining the class of proposals. Based on these discussions, we present a novel framework in the next subsection.</p><p>We first investigate anchor-based vs. anchor-free methods. Anchor-based methods first place a number of anchors on the image as object proposals and then use an individual classifier to judge the objectness and class of each proposal. Most often, each anchor is associated with a specific position on the image and its size is fixed, although the following process named bounding-box regression can slightly change its geometry. Anchor-free methods do not assume the objects to come from anchors of relatively fixed geometry, and instead, locate one or few keypoints of an object and determine its geometry and/or class afterward.</p><p>Our core opinion is that anchor-free methods have better flexibility of locating objects with arbitrary geometry, and thus a higher recall. This is mainly due to the design nature of anchors, which is mostly empirical (e.g., to reduce the number of anchors and improve efficiency, only common object sizes and shapes are considered), the detection algorithm is potential of lower flexibility and objects with a peculiar shape can be missing. Typical examples are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, and a quantitative study is provided in <ref type="table">Table 1</ref>. We evaluate four object proposal extraction methods as well as our work on the MS-COCO validation dataset and show that anchor-free methods often have a higher overall recall, which is mainly due to their advantages in two scenarios. First, when the object is very large, e.g., larger than 400 2 pixels, Faster R-CNN, an anchor-based <ref type="table">Table 1</ref>. Comparison among the average recall (AR) of anchor-based and anchor-free detection methods. Here, the average recall is recorded for targets of different aspect ratios and different sizes. To explore the limit of the average recall for each method, we exclude the impacts of bounding-box categories and sorts on recall, and compute it by allowing at most 1000 object proposals. AR1+, AR2+, AR3+ and AR4+ denote box area in the ranges of 96 2 , 200 2 , 200 2 , 300 2 , 300 2 , 400 2 , and 400 2 , +∞ , respectively. 'X' and 'HG' stand for ResNeXt and Hourglass, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone AR AR1+ AR2+ AR3+ AR4+ AR5:1 AR6:1 AR7: approach, does not report a much higher recall. This is not expected because large objects should be easier to detect, as the other three anchor-free methods suggest. Second, Faster R-CNN suffers a very low recall when the aspect ratio of the object becomes peculiar, e.g., 5 : 1 and 8 : 1, in which cases the recalls are significantly lower than CornerNet <ref type="bibr" target="#b18">[19]</ref> and CenterNet <ref type="bibr" target="#b7">[8]</ref>, because no predefined anchors (also used in other variants <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>) can fit these objects. A similar phenomenon is also observed in FCOS, an anchor-free approach which represents an object by a keypoint and the distance to the border, because it is difficult to predict an accurate distance when the border is far from the center. Since CornerNet and CenterNet group corner (and center) keypoints into an object, they somewhat get rid of this trouble. Therefore, we choose anchor-free methods, in particular, point-grouping methods (CornerNet and CenterNet), to improve the recall of object detection. Moreover, we report the corresponding results of CPN, the method proposed in this paper, which demonstrates that CPN inherits the merits of CenterNet and CornerNet and has better flexibility of locating objects, especially with peculiar shapes. However, anchor-free methods free the constraints of finding object proposals, it encounters a major difficulty of building a close relationship between keypoints and objects, since the latter often requires richer semantic information. As shown <ref type="table">Table 2</ref>. Anchor-free detection methods such as CornerNet and CenterNet suffer a large number of false positives and can benefit from incorporating richer semantics for judgment. Here, AP original , AP refined , and APcorrect indicate the AP of the original output, after non-object proposals are removed, and after the correct label is assigned to each survived proposal. Both AP refined and APcorrect require ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone  in <ref type="figure" target="#fig_0">Fig. 1</ref>, lacking semantics can incur a large number of false positives and thus harms the precision of detection. We take CornerNet <ref type="bibr" target="#b18">[19]</ref> and CenterNet <ref type="bibr" target="#b7">[8]</ref> with potentially high recalls as examples. As shown in <ref type="table">Table 2</ref>, the CornerNets with 52-layer and 104-layer Hourglass networks achieved APs of 37.6% and 41.0% on the MS-COCO validation dataset, while many of the detected 'objects' are false positives. Either when we remove the non-object proposals or assign each preserved proposal with a correct label, the detection accuracy goes up significantly. This observation also holds on CenterNet <ref type="bibr" target="#b7">[8]</ref>, which added a center point to filter out false positives but obviously did not remove them all. To further alleviate this problem, we need to inherit the merits of two-stage methods, which extract the features within proposals and train a classifier to filter out false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Framework of Corner Proposal Network</head><p>Motivated by the above analysis, the goal of our approach is to integrate the advantages of anchor-free methods and alleviate their drawbacks by leveraging the mechanism of discrimination from two-stage methods. We present a new framework named Corner-Proposal-Network (CPN). It uses an anchor-free method to extract object proposals followed by efficient regional feature computation and classification to filter out false positives. <ref type="figure" target="#fig_2">Fig. 2</ref> shows the overall pipeline which contains two stages, and details of the two stages are elaborated as follows.</p><p>• Stage 1: Anchor-free Proposals with Corner Keypoints The first stage is an anchor-free proposal extraction process, in which we assume that each object is located by two keypoints determining its top-left and bottom-right corners. We follow CornerNet <ref type="bibr" target="#b18">[19]</ref> to locate an object with a pair of keypoints located in its top-left and bottom-right corners, respectively. For each class, we compute two heatmaps (i.e., the top-left heatmap and the bottomright heatmap, each value on a heatmap indicates the probability that a corner keypoint occurs in the corresponding position) with a 4×-reduced resolution compared to the original image. The heatmaps are equipped with two loss terms, namely, a focal loss L corner det to locate the keypoint on the heatmap and a offset loss L corner offset to learn its offset to the accurate corner position. After heatmaps are computed, a fixed number of keypoints (K top-left and K bottom-right) are extracted from all heatmaps. Each corner keypoint is equipped with a class label.</p><p>Next, each valid pair of keypoints defines an object proposal. Here by valid we mean that two keypoints belong to the same class (i.e., extracted from the top-left heatmap and the bottom-right heatmap of the same class), and the x and y coordinates of the top-left point are smaller than that of the bottom-right point, respectively. This leads to a large number of false positives (incorrectly paired corner keypoints) on each image, and we leave the task of discriminating and classifying these proposals to the second stage.</p><p>As a side comment, we emphasize that although we extract object proposals based on CornerNet, the follow-up mechanism of determining objectness and class is quite different. CornerNet generates objects by projecting the keypoints to a one-dimensional space, and grouping keypoints with closely embedded numbers into the same instance. We argue that the embedding process, while necessary under the assumption that no additional computation can be used, can incur significant errors in pairing keypoints. In particular, there is no guarantee that the embedding function (assigning a number to each object) is learnable, and more importantly, the loss function only works in each training image to force the embedded numbers of different objects to be separated, but this mechanism often fails to generalize to unseen scenarios, e.g., even when multiple training images are simply concatenated together, the embedding function that works well on separate images can fail dramatically. Differently, our method determines object instances using an individual classifier, which makes full use of the internal features to improve accuracy. Please refer to <ref type="table">Table 6</ref> for the advantage of an individual classifier over instance embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Stage 2: Two-step Classification for Filtering Proposals</head><p>Thanks to the high resolution of the keypoint heatmap and a flexible mechanism of grouping keypoints, the detected objects can be of an arbitrary shape, and the upper-bound of recall is largely improved. However, this strategy increases the number of proposals and thus brings two problems: a considerable amount of false positives as well as computational costs to filter out them. To solve this issue, an efficient, two-step classification method is designed for the second stage, which first removes 80% of proposals with a light-weighted binary classifier, and then applies a fine-level classifier to determine the class label of each survived proposal.</p><p>Let M be the total number of object proposals generated by K top-left and K bottom-right keypoints. We follow CenterNet <ref type="bibr" target="#b7">[8]</ref> to set K to be 70, which results in an average of 2,500 object proposals on each image. Individually validating and classifying each of them is computationally expensive, so we design a two-step discrimination process, with the first light-weighted classifier identifying most proposals to be 'non-object', and the second classifier spending more resource on each survived object and assigning a class label as well as a confidence score to it.</p><p>The first step involves training a binary classifier to determine whether each proposal is an object. To this end, we first adopt RoIAlign <ref type="bibr" target="#b11">[12]</ref> with a kernel size of 7 × 7 to extract the features for each proposal on the box feature map (see <ref type="figure" target="#fig_2">Fig. 2</ref>). Then a 32×7×7 convolution layer is followed to obtain the classification score for each proposal. A binary classifier is built, with the loss function being:</p><formula xml:id="formula_0">L prop = − 1 N M m=1 (1 − p m ) α log (p m ) , if IoU m τ p α m log (1 − p m ) , otherwise ,<label>(1)</label></formula><p>where N denotes the number of positive samples, p m denotes the objectness score for the m-th proposal, p m ∈ [0, 1], and IoU m denotes the maximum IoU value between the m-th proposal and all the ground-truth bounding-boxes. τ is the IoU threshold, set to be 0.7 throughout this paper. This is to sample a few positive examples to avoid training data imbalance <ref type="bibr" target="#b23">[24]</ref>. α = 2 is a hyperparameter that smoothes the loss function. According to <ref type="bibr" target="#b23">[24]</ref>, we use π = 0.1, so the value of biases is set to −2.19.</p><p>The second step follows to assign a class label for each survived proposal. This step is very important, since the class labels associated to the corner keypoints are not always reliable. Although, we rely on the corner classes to reject invalid corner pairs, the consensus between them may be incorrect due to the lack of information from the ROI region, so we need a more powerful classifier that incorporates the ROI features to make the final decision. To this end, we train another classifier with C outputs where C is the number of classes in the dataset. This classifier is also built upon the RoIAlign-features extracted in the first step, but instead extract the features from the category feature map (see <ref type="figure" target="#fig_2">Fig. 2</ref>) to preserve more information and a C dimensional vector is obtained using a 256 × 7 × 7 convolution layer, for each of the survived proposals. Then C-way classifier is built. A similar loss function considering the class label is used:</p><formula xml:id="formula_1">L class = − 1 NM m=1 C c=1 (1 − q m,c ) β log (q m,c ) , if IoU m,c τ q β m,c log (1 − q m,c ) , otherwise ,<label>(2)</label></formula><p>whereM andN denote the number of survived proposals and the number of positive samples within them, respectively. IoU m,c denotes the maximum IoU value between the m-th proposal and all the ground-truth bounding-boxes of the c-th class, and the IoU threshold, τ , remains unchanged. q m,c is the classification score for the c-th class of the m-th object, and β plays a similar role as α, and we also fix it to be 2 in this paper.</p><p>Here we emphasize the differences between DeNet <ref type="bibr" target="#b39">[40]</ref> and our method, although they are similar in the idea level. First, we equip each corner with a multi-class label rather than a binary label, thus we can rely on the class labels to reject the unnecessary invalid corner pairs to save the computational costs of the overall framework. Second, we use an extra lightweight binary classification network to first reduce the number of proposals to be processed by the classification network, while DeNet only relies on one classification network. This helps our method be more efficient. Finally, we design a novel variant of the focal loss for the two classifiers, which is different from the maximum likelihood function in DeNet. This is mainly to solve the significant imbalance between the positive and negative proposals during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Inference Process</head><p>The inference process simply repeats the training process but uses thresholds to filter out clearly low-quality proposals. Note that even with augmented positive training data, the predicted scores, p m and q m,c , are biased towards 0. So, in the inference stage, we use a relatively low threshold (0.2 in this paper) in the first step to allow more proposals to survive. For each proposal, provided the RoIAlign-features, the computational cost of the first classifier is about 10% of that of the second one. Under the threshold of 0.2, the average fraction of survived proposals is around 20%, making the overheads of these two stages comparable.</p><p>For each proposal survived to the second step, we assign it with up to 2 class labels, corresponding to the dominant class of the corner keypoints and that of the proposal (two classes may be identical, if not, the proposal becomes two proposals with potentially different scores). For each candidate class, we denote s 1 as the corner classification score (the average of two corner keypoints, in the range of (0, 1)), and s 2 as the regional classification score (the probability of the proposal class label, predicted by the multi-class classifier, also in the range of (0, 1)). We assume that both scores contribute to the final score, and a positive evidence should be added if either score is larger than 0.5. Therefore, we compute the score by s c = (s 1 + 0.5) (s 2 + 0.5), then we will apply normalization to rescale this score into the [0, 1]. We finally preserve 100 proposals with highest scores into evaluation. In <ref type="table" target="#tab_4">Table 4</ref>, we will show that two classifiers provide complementary information and boost the detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>We evaluate our approach on the MS-COCO dataset <ref type="bibr" target="#b24">[25]</ref>, one of the most popular object detection benchmarks. It contains a total of 120K images with more than 1.5 million bounding boxes covering 80 object categories, making it a very challenging dataset. Following the common practice <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>, we train our model using the 'trainval35k', which is the union set of 80K training images and 35K (a subset of) validation images. We report evaluation results on the standard test-dev set, which has no public annotations, by uploading the results to the evaluation server. For the ablation studies and visualization experiments, we use the 5K validation images remained in the validation set.</p><p>We use the average precision (AP) metric defined in MS-COCO to characterize the performance of our approach as well as other competitors. AP computes the average precision over ten IoU thresholds (i.e., 0.5 : 0.05 : 0.95), for all categories. Meanwhile, we follow the convention to report some other important metrics, e.g., AP 50 and AP 75 are computed at single IoU thresholds of 0.50 and 0.75 <ref type="bibr" target="#b8">[9]</ref>, and AP small , AP medium , and AP large are computed under different object scales (i.e., small for area &lt; 32 2 , medium for 32 2 &lt; area &lt; 96 2 , and large for area &gt; 96 2 ), respectively. All metrics are computed by allowing at most 100 proposals preserved on each testing image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement our method using Pytorch <ref type="bibr" target="#b31">[32]</ref>, and refer to some codes from CornerNet <ref type="bibr" target="#b18">[19]</ref>, mmdetection <ref type="bibr" target="#b3">[4]</ref> and CenterNet <ref type="bibr" target="#b47">[48]</ref>. We use CornerNet <ref type="bibr" target="#b18">[19]</ref> and CenterNet <ref type="bibr" target="#b7">[8]</ref> as our baselines. The stacked Hourglass networks <ref type="bibr" target="#b29">[30]</ref> with 52 and 104 layers are trained for keypoint extraction, with all modifications made by CornerNet preserved. In addition, we experiment another backbone named DLA-34 <ref type="bibr" target="#b42">[43]</ref>. We follow the modifications made by CenterNet <ref type="bibr" target="#b47">[48]</ref>, but replace the deformable convolutional layers with normal layers. Training. All networks are trained from scratch, except the DLA-34, which is initialized with ImageNet pretrain. Cascade corner pooling <ref type="bibr" target="#b7">[8]</ref> is used to help the network better detect corners. The input image is resized into 511 × 511, and the output resolutions for the four feature maps (the top-left and bottomright heatmaps, the proposal and class feature maps) are all 128 × 128. The data augmentation strategy presented in <ref type="bibr" target="#b18">[19]</ref> is used. The overall loss function is</p><formula xml:id="formula_2">L = L corner det + L corner offset + L prop + L class ,<label>(3)</label></formula><p>which we use an Adam <ref type="bibr" target="#b15">[16]</ref> optimizer to train our model. On eight NVIDIA Tesla-V100 (32GB) GPUs, we use a batch size of 48 <ref type="bibr">(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with State-of-the-Art Detectors</head><p>We report the inference accuracy of CPN on the MS-COCO test-dev set and compare with the state-of-the-art detectors, as shown in <ref type="table" target="#tab_3">Table 3</ref>. CPN obtains a significant improvement compared to CornerNet <ref type="bibr" target="#b18">[19]</ref> and CenterNet <ref type="bibr" target="#b7">[8]</ref>, two direct baselines. Specifically, CPN-52 (indicating that the backbone is Hourglass-52) reports a single-scale testing AP of 43.9% and a multi-scale testing AP of 45.8%, which surpasses CornerNet-104, with a deeper backbone, by 3.4% and 3.7%, respectively. Equipped with a deeper backbone (i.e., Hourglass-104), CPN demonstrate a large advantage over CenterNet, the previous best anchor- free detector, with margins of 2.1% and 2.2% under single-scale and multi-scale settings, respectively. CPN also takes the lead in other metrics. For example, AP 50 and AP 75 reflect the accuracy of proposal localization and class prediction. Compared to CenterNet, CPN reports higher AP scores especially for AP 75 (e.g., CPN-104 reports a single-scale testing AP 75 of 51.0%, claiming an improvement of 2.9% over CenterNet). This suggests that some inaccurate bounding boxes with IoU value between 0.5 and 0.7 are difficult for CenterNet to filter out with merely center information incorporated. AP S , AP M and AP L reflect the detection accuracy for objects with different scales. CPN improves more for AP M and AP L than AP S (e.g., CPN-104 reports single-scale testing AP S , AP M and AP L of 26.5%, 50.2% and 60.7%, which improves by 0.9%, 2.8% and 3.3% from Cen-terNet, respectively). This is because medium and large objects require richer semantic information to be extracted from the proposal, which is not likely to be handled well with a center keypoint.</p><p>When comparing with other anchor-free approaches, CPN-52 reports a singlescale testing AP of 43.9%, which is already better than some of anchor-free detector with deeper backbones (e.g., FoveaBox <ref type="bibr" target="#b16">[17]</ref> and ExtremeNet <ref type="bibr" target="#b48">[49]</ref> †). The best performance of CPN reaches an AP of 49.2%, surpassing all published anchor-free approaches to the best of our knowledge. Meanwhile, CPN is also competitive among anchor-based detectors, e.g., CPN-52 reports a single-scale testing AP of 43.9% which is comparable to 44.1% of AlignDet <ref type="bibr" target="#b4">[5]</ref>, and CPN-104 reports a single-scale testing AP of 47.0% which is comparable 47.4% of PANet <ref type="bibr" target="#b25">[26]</ref>. FreeAnchor <ref type="bibr" target="#b46">[47]</ref> used a larger resolution of input images (e.g., ∼ 800 × 1300) to improve the AP of small objects, while CPN-104 with multi-scale testing outperforms it on AP S by a margin of 0.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Classification Improves Precision</head><p>We investigate the improvement of precision brought by the classification stage. Note that there are two classifiers, with the first one (binary) determines the objectness of each proposal, and the second one (multi-class) providing complementary information of the class label. We perform ablation study in <ref type="table" target="#tab_4">Table 4</ref> to analyze the contribution of individual classifiers -in the scenarios that the multiclass classifier is missing, we directly use the class label of the corner keypoints as the final prediction. On the one hand, both classifiers can improve the AP of the basic model (one-stage corner keypoint grouping) significantly (3.4% and 3.8% absolute gains). On the other hand, two classifiers provide complementary <ref type="table">Table 5</ref>. We report the average false discovery rates (%, lower is better) for CornerNet, CenterNet and CPN on the MS-COCO validation dataset. The results show that our approach generates fewer false positives. Under the same corner keypoint extractor, this is the key to outperform the baselines in the AP metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone AF AF5 AF25 AF50 AFS AFM AFL information, so that combining them can further improve the AP by more than 1%. This indicates that the semantic information required for determining the objectness and class label of a proposal is slightly different. We have discussed the importance of the incorrect bounding box suppression for the improvement of detection accuracy and recall in Section 3.1. For a more intuitive analysis of false positives, we adopt the average false discovery rate (AF) metric 1 <ref type="bibr" target="#b7">[8]</ref> to quantify the fraction of incorrectly grouped proposals for different detectors. Results are shown in <ref type="table">Table 5</ref>. CPN-52 and CPN-104 report AF of 33.4% and 30.6%, respectively, which are lower than the direct baselines, CornerNet and CenterNet. <ref type="table">Table 6</ref>. The detection performance (%) of using different ways (instance embedding and binary classification) to determine the validity of a proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Objectness AP AP50 AP75 APS APM APL AR100 Note that these three methods share a similar way of extracting corner keypoints, but CornerNet suffers large AF values due to the lack of validation beyond the proposals. CenterNet, by forcing a center keypoint to be detected, was believed effective in filtering out false positives, and our approach, by reevaluating the proposal based on regional features, performs better than CenterNet. More importantly, by inserting an individual classification stage, CPN alleviates the false positives caused by the instance embedding mechanism, as shown in <ref type="table">Table 6</ref>.  <ref type="bibr" target="#b18">[19]</ref> and CenterNet <ref type="bibr" target="#b7">[8]</ref>. In each group, from top to bottom are the detection results of CornerNet, CenterNet and CPN, respectively. The green and red bounding-boxes denote true positives and false positives, respectively.</p><p>Last but not least, we provide some visualization results in <ref type="figure">Figure 3</ref> to show that CPN indeed enjoys a strong ability in improving the precision of detection. We compare against three keypoint-grouping-based detectors. CornerNet, merely relying on instance embedding, found many seemingly strange 'objects'. CenterNet, by enforcing a center keypoint to be found in the central region, eliminated a part of them but some of the false positives can still survive. Using an individual classifier, as CPN does, is more powerful in filtering out false positives. Yet, as we shall see in the next part, CPN enjoys faster inference speed than both CornerNet and CenterNet by efficiently sharing computation between two stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Inference Speed</head><p>To show that CPN can generate high-quality bounding boxes with small computational costs, we report the inference speed for CPN on the MS-COCO validation dataset under different settings and compare the results to state-of-the-art efficient detectors, as shown in <ref type="table">Table 7</ref>. For fair comparison, we test the inference speed for all detectors on an NVIDIA Tesla-V100 GPU. CPN-104 reports an FPS/AP of 7.3/46.8%, which is both faster and better than CenterNet-104 (5.1/44.8%) under the same setting. With a lighter backbone of Hourglass-52, CPN-52 reports an FPS/AP of 9.9/43.8%, which outperforms both CornerNet-52 and CenterNet-52. This indicates that two-stage detectors are not necessarily slow -our solution, by sharing a large amount of computation between the first (for keypoint extraction) and the second (for feature extraction) stage, achieves a good trade-off between inference speed and accuracy.</p><p>In the scenarios that require faster inference speed, CPN can be further accelerated by replacing with a lighter backbone and not using flip augmentation at the inference stage. In this configuration, CPN-34 (indicating that the back- <ref type="table">Table 7</ref>. Inference speed of CPN under different conditions vs. other detectors on the MS-COCO validation dataset. FPS is measured on the on an NVIDIA Tesla-V100 GPU. CPN achieves a good trade-off between accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Input Size Flip AP FPS FCOS <ref type="bibr" target="#b38">[39]</ref> X-101-64x4d 800 × 42. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we present an anchor-free, two-stage object detection framework. It starts with extracting corner keypoints and composing them into object proposals, and applies two-step classification to filter out false positives. With the above two stages, the recall and precision of detection are significantly improved, and the final result ranks among the top of existing object detection methods. The most important take-away is that anchor-free methods are more flexible in proposal extraction, while an individual discrimination stage is required to improve precision. When implemented properly, such a two-stage framework can be efficient in evaluation. Therefore, the debate on using one-stage or two-stage detectors seems not critical.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Typical errors by existing object detection approaches (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>N</head><label></label><figDesc>n=1 and {b m , c m } M m=1 are as close as possible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The overall pipeline of Corner Proposal Network (CPN). It first predicts corners to compose a number of object proposals, and then applies two-step classification to filter out false positives and assign a class label for each survived proposal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>6 samples on each card) and train the model for 200K iterations with a base learning rate of 2.5 × 10 −4 followed by another 50K iterations with a reduced learning rate of 2.5×10 −5 . The training lasts about 9 days, 5 days and 3 days for Hourglass-104, Hourglass-52 and DLA-34, respectively. Inference. Following<ref type="bibr" target="#b18">[19]</ref>, both single-scale and multi-scale detection processes are performed. For single-scale testing, we feed the image with the original resolution into the network, while for multi-scale testing, the image is resized into</figDesc><table /><note>different resolutions (0.6×, 1×, 1.2×, 1.5×, and 1.8×) and then fed into the network. Flip argumentation is added to both single-scale or multi-scale evalua- tion. For the multi-scale evaluation, the predictions for all scales (including the flipped variants) are fused into the final result. we use soft-NMS [2] to suppress the redundant bounding-boxes, and preserve 100 top-scored bounding-boxes for final evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Inference accuracy (%) of CPN and state-of-the-art detectors on the COCO test-dev set. CPN ranks among the top of state-of-the-art detectors. 'R', 'X', 'HG', 'DCN' and ' †' denote ResNet, ResNeXt, Hourglass, Deformable Convolution Network<ref type="bibr" target="#b6">[7]</ref>, and multi-scale training or testing, respectively.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="4">Input Size AP AP50 AP75 APS APM APL</cell></row><row><cell>Anchor-based:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faster R-CNN [23]</cell><cell>R-101</cell><cell>600</cell><cell cols="3">36.2 59.1 39.0 18.2 39.0 48.2</cell></row><row><cell>RetinaNet [24]</cell><cell>R-101</cell><cell>800</cell><cell cols="3">39.1 59.1 42.3 21.8 42.7 50.2</cell></row><row><cell>Cascade R-CNN [3]</cell><cell>R-101</cell><cell>800</cell><cell cols="3">42.8 62.1 46.3 23.7 45.5 55.2</cell></row><row><cell>Libra R-CNN [31]</cell><cell>X-101-64x4d</cell><cell>800</cell><cell cols="3">43.0 64.0 47.0 25.3 45.6 54.6</cell></row><row><cell>Grid R-CNN [28]</cell><cell>X-101-64x4d</cell><cell>800</cell><cell cols="3">43.2 63.0 46.6 25.1 46.5 55.2</cell></row><row><cell>YOLOv4 [1]</cell><cell>CSPDarknet-53</cell><cell>608</cell><cell cols="3">43.5 65.7 47.3 26.7 46.7 53.3</cell></row><row><cell>AlignDet [5]</cell><cell>X-101-32x8d</cell><cell>800</cell><cell cols="3">44.1 64.7 48.9 26.9 47.0 54.7</cell></row><row><cell>AB+FSAF [51]  †</cell><cell>X-101-64x4d</cell><cell>800</cell><cell cols="3">44.6 65.2 48.6 29.7 47.1 54.6</cell></row><row><cell>FreeAnchor [47]  †</cell><cell>X-101-32x8d</cell><cell>≤1280</cell><cell cols="3">47.3 66.3 51.5 30.6 50.4 59.0</cell></row><row><cell>PANet [26]  †</cell><cell>X-101-64x4d</cell><cell>840</cell><cell cols="3">47.4 67.2 51.8 30.1 51.7 60.0</cell></row><row><cell>TridentNet [22]  †</cell><cell>R-101-DCN</cell><cell>800</cell><cell cols="3">48.4 69.7 53.5 31.8 51.3 60.3</cell></row><row><cell>ATSS [45]  †</cell><cell>X-101-64x4d-DCN</cell><cell>800</cell><cell cols="3">50.7 68.9 56.3 33.2 52.9 62.4</cell></row><row><cell>EfficientDet [38]</cell><cell>EfficientNet [37]</cell><cell>1536</cell><cell>53.7 72.4 58.4 -</cell><cell>-</cell><cell>-</cell></row><row><cell>Anchor-free:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GA-Faster-RCNN [41]</cell><cell>R-50</cell><cell>800</cell><cell cols="3">39.8 59.2 43.5 21.8 42.6 50.7</cell></row><row><cell>FoveaBox [17]</cell><cell>R-101</cell><cell>800</cell><cell cols="3">42.1 61.9 45.2 24.9 46.8 55.6</cell></row><row><cell>ExtremeNet [49]  †</cell><cell>HG-104</cell><cell>≤1.5×</cell><cell cols="3">43.2 59.8 46.4 24.1 46.0 57.1</cell></row><row><cell>FCOS [39] w/ imprv</cell><cell>X-101-64x4d</cell><cell>800</cell><cell cols="3">44.7 64.1 48.4 27.6 47.5 55.6</cell></row><row><cell>CenterNet [48]  †</cell><cell>HG-104</cell><cell>≤1.5×</cell><cell cols="3">45.1 63.9 49.3 26.6 47.1 57.7</cell></row><row><cell>RPDet [42]  †</cell><cell>R-101-DCN</cell><cell>800</cell><cell cols="3">46.5 67.4 50.9 30.3 49.7 57.1</cell></row><row><cell>SAPD [50]</cell><cell>X-101-64x4d</cell><cell>800</cell><cell cols="3">45.4 65.6 48.9 27.3 48.7 56.8</cell></row><row><cell>SAPD [50]</cell><cell>X-101-64x4d-DCN</cell><cell>800</cell><cell cols="3">47.4 67.4 51.1 28.1 50.3 61.5</cell></row><row><cell>CornerNet [19]</cell><cell>HG-104</cell><cell>ori.</cell><cell cols="3">40.5 56.5 43.1 19.4 42.7 53.9</cell></row><row><cell>CornerNet [19]  †</cell><cell>HG-104</cell><cell>≤1.5×</cell><cell cols="3">42.1 57.8 45.3 20.8 44.8 56.7</cell></row><row><cell>CenterNet [8]</cell><cell>HG-104</cell><cell>ori.</cell><cell cols="3">44.9 62.4 48.1 25.6 47.4 57.4</cell></row><row><cell>CenterNet [8]  †</cell><cell>HG-104</cell><cell>≤1.8×</cell><cell cols="3">47.0 64.5 50.7 28.9 49.9 58.9</cell></row><row><cell>CPN</cell><cell>DLA-34</cell><cell>ori.</cell><cell cols="3">41.7 58.9 44.9 20.2 44.1 56.4</cell></row><row><cell>CPN</cell><cell>HG-52</cell><cell>ori.</cell><cell cols="3">43.9 61.6 47.5 23.9 46.3 57.1</cell></row><row><cell>CPN</cell><cell>HG-104</cell><cell>ori.</cell><cell cols="3">47.0 65.0 51.0 26.5 50.2 60.7</cell></row><row><cell>CPN  †</cell><cell>DLA-34</cell><cell>≤1.8×</cell><cell cols="3">44.5 62.3 48.3 25.2 46.7 58.2</cell></row><row><cell>CPN  †</cell><cell>HG-52</cell><cell>≤1.8×</cell><cell cols="3">45.8 63.9 49.7 26.8 48.4 59.4</cell></row><row><cell>CPN  †</cell><cell>HG-104</cell><cell cols="4">≤1.8× 49.2 67.4 53.7 31.0 51.9 62.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The detection performance (%) of different classification options on CPN. Backbone B-Classifier M-Classifier AP AP50 AP75 APS APM APL AR100 HG-52 38.6 54.5 41.4 19.0 40.6 54.4 57.4 42.0 59.7 45.1 24.2 45.0 56.6 60.5 42.4 60.2 45.7 23.3 44.7 58.6 59.0 43.8 61.4 47.4 24.7 46.5 60.0 60.9</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Typical detection results showing how CPN filters out false positives detected by CornerNet</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ SHUVRQ SHUVRQ</cell><cell>SHUVRQ SHUVRQ SHUVRQ</cell><cell></cell><cell></cell><cell>SHUVRQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ</cell><cell cols="2">SHUVRQ</cell><cell></cell><cell>SHUVRQ</cell><cell>SHUVRQ</cell><cell>SHUVRQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ SHUVRQ</cell><cell>SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EHDU</cell><cell>EHDU</cell><cell>EHDU</cell></row><row><cell>NLWH NLWH</cell><cell></cell><cell></cell><cell cols="2">SHUVRQ NLWH</cell><cell>NLWH NLWH NLWH NLWH</cell><cell></cell><cell></cell><cell cols="5">SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ VSRUWVEDOO SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell>SHUVRQ SHUVRQ</cell><cell>SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell>SHUVRQ</cell><cell cols="2">WHQQLVUDFNHW SHUVRQ VSRUWVEDOO VSRUWVEDOO SHUVRQ WHQQLVUDFNHW WHQQLVUDFNHW WHQQLVUDFNHW</cell><cell>SHUVRQ</cell><cell>SHUVRQ SHUVRQ SHUVRQ</cell><cell>SHUVRQ</cell><cell>SHUVRQ</cell><cell>SHUVRQ VXUIERDUG</cell><cell>SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell>EHDU</cell><cell>EHDU EHDU</cell><cell>EHDU EHDU EHDU</cell><cell>SHUVRQ</cell><cell>SHUVRQ VNLV</cell></row><row><cell>SHUVRQ SHUVRQ SHUVRQ SHUVRQ NLWH NLWH SHUVRQ</cell><cell cols="2">SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell cols="2">SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell></cell><cell></cell><cell>SHUVRQ SHUVRQ SHUVRQ EDFNSDFN SHUVRQ SHUVRQ</cell><cell></cell><cell></cell><cell>VSRUWVEDOO</cell><cell cols="2">SHUVRQ</cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ</cell><cell></cell><cell>SHUVRQ</cell><cell>SHUVRQ VXUIERDUG</cell><cell>VXUIERDUG VXUIERDUG VXUIERDUG</cell><cell>VXUIERDUG VXUIERDUG SHUVRQ</cell><cell>VNLV</cell><cell>SHUVRQ VNLV VNLV VNLV VNLV VNLV VNLV VNLV VNLV</cell><cell>VNLV</cell><cell>VNLV VNLV VNLV VNLV</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ</cell><cell cols="2">SHUVRQ</cell><cell>EHDU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VSRUWVEDOO</cell><cell>SHUVRQ</cell><cell></cell><cell></cell><cell>SHUVRQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ SHUVRQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>EHDU</cell><cell>SHUVRQ</cell><cell>SHUVRQ</cell><cell>SHUVRQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ</cell><cell>EHDU</cell><cell>EHDU EHDU EHDU</cell></row><row><cell>SHUVRQ SHUVRQ SHUVRQ</cell><cell>SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell></cell><cell>SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell>NLWH</cell><cell>SHUVRQ NLWH NLWH NLWH</cell><cell>NLWH</cell><cell>SHUVRQ SHUVRQ</cell><cell cols="4">SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ KDQGEDJ SHUVRQ EDVHEDOOJORYH ERWWOH VSRUWVEDOO VSRUWVEDOO</cell><cell>SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ FXS SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell cols="2">SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell></cell><cell cols="2">WHQQLVUDFNHW WHQQLVUDFNHW WHQQLVUDFNHW VSRUWVEDOO VSRUWVEDOO</cell><cell></cell><cell>VXUIERDUG</cell><cell>VXUIERDUG SHUVRQ VXUIERDUG VXUIERDUG VXUIERDUG SHUVRQ SHUVRQ VXUIERDUG VXUIERDUG</cell><cell>EHDU EHDU EHDU</cell><cell>VNLV</cell><cell>VNLV VNLV VNLV VNLV VNLV</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">VSRUWVEDOO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ SHUVRQ SHUVRQ</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ SHUVRQ SHUVRQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NLWH NLWH NLWH</cell><cell></cell><cell></cell><cell cols="2">SHUVRQ SHUVRQ SHUVRQ EDFNSDFN KDQGEDJ IULVEHH SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell cols="3">SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ IULVEHH NLWH EDVHEDOOEDW EDVHEDOOEDW EDVHEDOOJORYH SHUVRQ SHUVRQ WHQQLVUDFNHW WHQQLVUDFNHW SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell>SHUVRQ</cell><cell>SHUVRQ SHUVRQ SHUVRQ SHUVRQ VNDWHERDUG SHUVRQ EHQFK EHQFK VSRUWVEDOO WHQQLVUDFNHW SHUVRQ KDQGEDJ VSRUWVEDOO SHUVRQ SHUVRQ SHUVRQ SHUVRQ SHUVRQ</cell><cell></cell><cell cols="2">WHQQLVUDFNHW IULVEHH WHQQLVUDFNHW WHQQLVUDFNHW WHQQLVUDFNHW</cell><cell></cell><cell>SHUVRQ SHUVRQ SHUVRQ</cell><cell>EHDU EHDU</cell><cell>EHDU</cell><cell>EHDU EHDU EHDU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VSRUWVEDOO VSRUWVEDOO VSRUWVEDOO</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>VNLV</cell><cell>VNLV VNLV VNLV</cell></row><row><cell>NLWH NLWH</cell><cell>SHUVRQ SHUVRQ</cell><cell cols="2">SHUVRQ SHUVRQ SHUVRQ</cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ SHUVRQ</cell><cell></cell><cell></cell><cell cols="2">VSRUWVEDOO VSRUWVEDOO VSRUWVEDOO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SHUVRQ VXUIERDUG VXUIERDUG VXUIERDUG VXUIERDUG</cell></row><row><cell cols="6">Fig. 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>bone is DLA-34) reports FPS/AP of 43.3/39.7% and 26.2/41.6%, respectively, surpassing other competitors with similar computational complexity.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>6 8.1</cell></row><row><cell cols="2">Faster R-CNN [34] X-101-64x4d</cell><cell>800</cell><cell>× 41.1 8.2</cell></row><row><cell cols="2">CornerNet-lite [20] HG-Squeeze</cell><cell>ori.</cell><cell>36.5 22.0</cell></row><row><cell>CornerNet [19]</cell><cell>HG-104</cell><cell>ori.</cell><cell>41.0 5.8</cell></row><row><cell>CenterNet [8]</cell><cell>HG-104</cell><cell>ori.</cell><cell>44.8 5.1</cell></row><row><cell>CPN</cell><cell>HG-104</cell><cell>ori.</cell><cell>46.8 7.3</cell></row><row><cell>CPN</cell><cell>HG-52</cell><cell>ori.</cell><cell>43.8 9.9</cell></row><row><cell>CPN</cell><cell>HG-104</cell><cell>0.7×ori.</cell><cell>× 40.5 17.9</cell></row><row><cell>CPN</cell><cell>DLA-34</cell><cell>ori.</cell><cell>41.6 26.2</cell></row><row><cell>CPN</cell><cell>DLA-34</cell><cell>ori.</cell><cell>× 39.7 43.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">AF = 1 − AP, where AP is computed over IoU thresholds of [0.05 : 0.05 : 0.5] for all categories. AFτ = 1 − APτ , where APτ is computed at the IoU threshold of τ %, AF scale = 1 − AP scale , where scale = {small, medium, large}, indicating the scale of object.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y M</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Revisiting feature alignment for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01570</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer science</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchorbased object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cornernet-lite: Efficient keypoint based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08900</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>3, 4, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grid r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Denet: Scalable real-time object detection with directed sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tychsen-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Region proposal by guided anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12448</idno>
		<title level="m">Soft anchor-point object detection</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
