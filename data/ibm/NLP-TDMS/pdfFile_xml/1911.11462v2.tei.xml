<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">G-TAD: Sub-Graph Localization for Temporal Action Detection https://www.deepgcns.org/app/g-tad</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
							<email>mengmeng.xu@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
							<email>chen.zhao@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
							<email>ali.thabet@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">G-TAD: Sub-Graph Localization for Temporal Action Detection https://www.deepgcns.org/app/g-tad</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action detection is a fundamental yet challenging task in video understanding. Video context is a critical cue to effectively detect actions, but current works mainly focus on temporal context, while neglecting semantic context as well as other important context properties. In this work, we propose a graph convolutional network (GCN) model to adaptively incorporate multi-level semantic context into video features and cast temporal action detection as a sub-graph localization problem. Specifically, we formulate video snippets as graph nodes, snippet-snippet correlations as edges, and actions associated with context as target sub-graphs. With graph convolution as the basic operation, we design a GCN block called GCNeXt, which learns the features of each node by aggregating its context and dynamically updates the edges in the graph. To localize each sub-graph, we also design an SGAlign layer to embed each sub-graph into the Euclidean space. Extensive experiments show that G-TAD is capable of finding effective video context without extra supervision and achieves stateof-the-art performance on two detection benchmarks. On ActivityNet-1.3, it obtains an average mAP of 34.09%; on THUMOS14, it reaches 51.6% at IoU@0.5 when combined with a proposal processing method. G-TAD code is publicly available at https://github.com/frostinassiky/gtad.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video understanding has gained much attention from both academia and industry over recent years, given the rapid growth of videos published in online platforms. Temporal action detection is one of the interesting but challenging tasks in this area. It involves detecting the start and end frames of action instances, as well as predicting their class labels. This is onerous especially in long untrimmed videos.</p><p>Video context is an important cue to effectively detect actions. Here, we refer to context as frames that do not belong to the target action but carry its valuable indicative information. Using video context to infer potential actions is  <ref type="figure">Figure 1</ref>. Graph formulation of a video. Nodes: video snippets (a video snippet is defined as consecutive frames within a short time period). Edges: snippet-snippet correlations. Sub-graphs: actions associated with context. There are 4 types of nodes: action, start, end, and background, shown as colored dots. There are 2 types of edges: <ref type="bibr" target="#b0">(1)</ref> temporal edges, which are pre-defined according to the snippets' temporal order; <ref type="bibr" target="#b1">(2)</ref> semantic edges, which are learned from node features. natural for human beings. In fact, empirical evidence shows that humans can reliably guess or predict the occurrence of a certain type of action by only looking at short video snippets where the action does not happen <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Therefore, incorporating context into temporal action detection has become an important strategy to boost detection accuracy in the recent literature <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b29">30]</ref>. Researchers have proposed various ways to take advantage of video context, such as extending temporal action boundaries by a predefined ratio <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b29">30]</ref>, using dilated convolution to encode context into features <ref type="bibr" target="#b8">[9]</ref>, and aggregating context features implicitly by way of a Gaussian curve <ref type="bibr" target="#b32">[33]</ref>. All these methods only utilize temporal context, which precedes or follows an action instance in its immediate temporal neighborhood. However, real-world videos vary dramatically in temporal extent, action content, and even editing preferences. Temporal context does not fully exploit the rich merits of video context, and it may even impair detection accuracy if not properly designed for underlying videos.</p><p>So, what properties characterize desirable video context for the purpose of accurate action detection? First, context should be semantically correlated to the target action other than merely temporally located in its vicinity. Imagine the case where we manually stitch an action clip into some irrelevant frames, the abrupt scene change surrounding the action would definitely not benefit the action detection. On the other hand, snippets located at a distance from an action but containing similar semantic content might provide indicative hints for detecting the action. Second, context should be content-adaptive rather than manually pre-defined. Considering the vast variation of videos, context that helps to detect different action instances could be different in lengths and locations based on the video content. Third, context should be based on multiple semantic levels, since using only one form/level of context is unlikely to generalize well.</p><p>We endow video context with all the above properties by casting action detection as a sub-graph localization problem based on a graph convolutional network (GCN) <ref type="bibr" target="#b23">[24]</ref>. We represent each video sequence as a graph, each snippet as a node, each snippet-snippet correlation as an edge, and target action associated with context as sub-graph, as shown in <ref type="figure">Fig. 1</ref>. The context of a snippet is considered to be all snippets connected to it by edges in a video graph. We define two types of edges -temporal edges and semantic edges, corresponding to temporal context and semantic context, respectively. Temporal edges exist between each pair of adjacent snippets, whereas semantic edges are dynamically learned from the video features at each layer. Hence, multi-level context of each snippet is gradually aggregated into the features of the snippet throughout the entire GCN.</p><p>The pipeline of our proposed Graph-Temporal Action Detection method, dubbed G-TAD, is analogous to faster R-CNN <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref> in object detection. There are two critical designs in G-TAD. First, the GCN-based feature extraction block GCNext, which is inspired by ResNeXt <ref type="bibr" target="#b48">[49]</ref>, generates context-enriched features. It corresponds to the CNN blocks of the backbone network in faster R-CNN. Second, to mimic region of interest (RoI) alignment <ref type="bibr" target="#b18">[19]</ref>, we design a sub-graph of interest alignment layer SGAlign to generate a fixed-size representation for each sub-graph and embed all sub-graphs into the same Euclidean space. Finally, we apply a classifier on the features of each sub-graph to obtain detection. We summarize our contributions as follows.</p><p>(1) We present a novel GCN-based video model to fully exploit video context for effective temporal action detection. Using this video GCN representation, we are able to adaptively incorporate multi-level semantic context into the features of each snippet.</p><p>(2) We propose G-TAD, a new sub-graph detection framework to localize actions in video graphs. G-TAD includes two main modules: GCNeXt and SGAlign. GCNeXt performs graph convolutions on video graphs, leveraging both temporal and semantic context. SGAlign re-arranges subgraph features in an embedded space suitable for detection.</p><p>(3) G-TAD achieves state-of-the-art performance on two popular action detection benchmarks. On ActivityNet-1.3, it achieves an average mAP of 34.09%. On THUMOS14 it reaches 51.6% at IoU@0.5 when combined with a proposal processing method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Representation</head><p>Action Recognition. Many CNN based methods have been proposed to address the action recognition task. Twostream networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43]</ref> use 2D CNNs to extract frame features from RGB and optical flow sequences. These 2D CNNs can be designed from scratch <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref> or pretrained on image recognition tasks <ref type="bibr" target="#b11">[12]</ref>. Other methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b51">52]</ref> use 3D CNNs to encode spatio-temporal information from the original video. In our work, we use the pre-trained action recognition model in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b44">45]</ref> to extract video snippet features as G-TAD input. Action Detection. Temporal action detection is to predict the boundaries and categories of action instances in untrimmed videos. A common practice is to first generate temporal proposals and then classify each proposal into one of the action categories <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref>. For proposal generation, they either use fixed handcrafted anchors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37]</ref> , or adaptively form proposal candidates by connecting potential start and end frames <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b29">30]</ref>. G-TAD uses anchors to define sub-graphs, but also incorporates start/end prediction to regularize the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">GCN in Videos</head><p>Graphs in Video Understanding. Graphs have been widely used for data/feature representation in various video understanding tasks, such as action recognition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b9">10]</ref>, and action localization <ref type="bibr" target="#b54">[55]</ref>. In action recognition, Liu et al. <ref type="bibr" target="#b30">[31]</ref> view a video as a 3D point cloud in the spatialtemporal space. Wang et al. <ref type="bibr" target="#b46">[47]</ref> represent a video as a space-time region graph, in which the graph nodes are defined by object region proposals. In action detection, Zeng et al. <ref type="bibr" target="#b54">[55]</ref> consider temporal action proposals as nodes in a graph, and refine their boundaries and classification scores based on the established proposal-proposal dependencies. Differently from previous works, G-TAD takes video snippets as nodes in a graph and form edges between them based on both their temporal ordering and semantic similarity. Graph Convolutions Networks. Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b23">[24]</ref> are widely used for non-Euclidean structures. In these years, its successful application has been seen in computer vision tasks due to their versatility and effectiveness, such as 3D object detection <ref type="bibr" target="#b17">[18]</ref> and point cloud segmentation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50]</ref>. Meanwhile, various GCN architectures are proposed for more effective and flexible modelling. One representative work is the edge convolution method by Wang et al. <ref type="bibr" target="#b47">[48]</ref>  putes graph edges (represented as node adjacency) at each graph layer based on the feature distance between nodes, and enriches the node feature by aggregating the features over the neighbourhood as node output. Recently, Li et al. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> propose DeepGCNs to enable GCNs to go as deep as 100 layers using residual/dense graph connections and dilated graph convolutions, and explore ways to automatically design GCNs <ref type="bibr" target="#b26">[27]</ref>. G-TAD uses a DeepGCN-like structure to apply graph convolutions on a dynamic semantic graph as well as a fixed temporal graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>The input to our pipeline is a video sequence of l v frames. Following recent video action proposal generation methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref>, we construct our G-TAD model using feature sequences extracted from raw video frames. We average the features of every σ consecutive frames and refer to each set of the σ frames as a snippet. In this way, our input visual feature sequence is represented by X (0) ∈ R C×L , where C is the feature dimension of each snippet, and L is the number of snippets. Each video sequence has a set of N annotations Ψ = {ψ n = (t s,n , t e,n , c n )} N n=1 , where ψ n represents an action instance, and t s,n , t e,n , and c n are its start time, end time, and action class, respectively. The temporal action detection task is to predict M possible actions Φ = φ m = t s,m ,t e,m ,ĉ m , p m M m=1 from V . Here, (t s,m ,t e,m ) represents the predicted temporal boundaries for the m th predicted action;ĉ m and p m are its pre-dicted action class and confidence score, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">G-TAD Architecture</head><p>Our action detection framework is illustrated in <ref type="figure">Fig. 2</ref>. We feed the snippet features X (0) , into a stack of b GCNeXt blocks, which is designed inspired by ResNeXt <ref type="bibr" target="#b48">[49]</ref> to obtain context-aware features. Each GCNeXt contains two graph convolution streams. One stream operates on fixed temporal neighbors, and the other adaptively aggregates semantic context into snippet features. Both streams follow the split-transform-merge strategy with multiple convolution paths <ref type="bibr" target="#b48">[49]</ref> (the number of paths is defined as cardinality) to generate updated graphs, which are aggregated into one graph as the block output. At the end of all b GC-NeXt blocks, we extract a set of sub-graphs based on the pre-defined temporal anchors (see Section 4.2).</p><p>Then we have the sub-graph of interest alignment layer SGAlign to represent each sub-graph using a feature vector. In the end, we use multiple fully connected layers to predict the intersection over union (IoU) of the feature vector representing every sub-graph and the ground truth action instance. We provide a detailed description of both GCNeXt and SGAlign in Sections 3.3 and 3.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">GCNeXt for Context Feature Encoding</head><p>Our basic graph convolution block, GCNeXt, operates on a graph representation of the video sequence. It encodes snippets using their temporal and semantic neighbors.   <ref type="figure">Figure 3</ref>. GCNeXt block. The input feature is processed by temporal and semantic streams with the same cardinality. Black and purple boxes represent operations in the temporal and semantic streams, respectively, darker colors referring to graph convolutions and lighter ones 1-by-1 convolutions. The numbers in each box refer to input and output channels. Both streams follow a splittransform-merge strategy with 32 paths each to increase the diversity of transformations. The module output is the summation of both streams and the input. and E = E t ∪ E s denote the node and edge sets, respectively. In this case, each node represents a snippet and each edge shows a dependency between a pair of snippets. We define two types of edges -temporal edges E t and semantic edges E s . Accordingly we have the temporal stream and the semantic stream. We describe each type of edge as well as the graph convolution process in the following. Temporal Edges (E t ) encode the temporal order of video snippets. Each node v i ∈ V has one unique forward edge to node v i+1 , and one backward edge to node v i−1 . In this case, we have</p><formula xml:id="formula_0">E t = E f t ∪ E b t ,</formula><p>where E f t and E b t are forward and backward temporal edge sets defined as follows:</p><formula xml:id="formula_1">E f t = {(v i , v i+1 )| i ∈ {1, 2, . . . , L − 1}},<label>(1)</label></formula><formula xml:id="formula_2">E b t = {(v i , v i−1 )| i ∈ {2, . . . , L − 1, L}},<label>(2)</label></formula><p>where L is the number of snippets in the video. Semantic Edges (E s ) are defined from the notion of dynamic edge convolutions <ref type="bibr" target="#b47">[48]</ref>, which dynamically constructs edges between graph nodes according to their feature distances. The goal of our semantic edges is to collect information from semantically correlated snippets. we define the semantic edge set E s for each node v i in G as follows</p><formula xml:id="formula_3">E s = {(v i , v ni(k) )|i ∈ {1, 2, . . . , L}; k ∈ {1, 2, . . . K}}.</formula><p>Here, n i (k) refers to the node index of the k th nearest neighbor of node v i . It is determined dynamically at every GC-NeXt block in the node feature space, enabling us to update the nodes that intrinsically carry semantic context information throughout the network. Therefore, E s adaptively changes to represent new levels of semantic context. Graph Convolution. We use X = [x 1 , x 2 , . . . , x L ] ∈ R C×L to represent the features for all the nodes in the graph On the bottom, the dots represent graph nodes, grey lines are semantic edges, and the orange highlighted zones are sub-graphs. Note that since the semantic feature of each node is computed using its neighbors, each entry in the the sub-graph feature essentially corresponds to multiple semantically correlated nodes in the graph. and transform it using the graph convolution operation F. There are several choices for F in the literature. For simplicity, we use a single-layer edge convolution <ref type="bibr" target="#b47">[48]</ref> as our graph convolution operation:</p><formula xml:id="formula_4">F(X, A, W ) = ([X T , AX T − X T ]W ) T .<label>(3)</label></formula><p>Here, W ∈ R 2C×C is trainable weight; A ∈ R L×L is the adjacency matrix without self-loops (i.e. edges between a node and itself); [·, ·] represents the matrix concatenation of columns. We formulate the (i, j) th element in A as</p><formula xml:id="formula_5">A (i,j) = 1{(v i , v j ) ∈ E}, where 1{·} is the indicator function.</formula><p>Either stream in GCNeXt leverages a split-transformmerge strategy <ref type="bibr" target="#b48">[49]</ref> with 32 paths to increase the diversity of transformations. Each path contains one graph convolution as in Eq. 3 and two 1-by-1 convolutions, their composition denoted as F . Stream Aggregation. The GCNeXt output is the aggregation of semantic and temporal steams as well as the input, which can be formulated as:</p><formula xml:id="formula_6">H(X, A, W ) = ReLU (F (X, A f t , W f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Sub-Graph Alignment and Localization</head><p>Sub-Graph of Interest Alignment (SGAlign). The GC-NeXt blocks generate the features of all snippets {x l } L l=1 (dubbed as GCNeXt features), which contains aggregated information from their temporal and semantic context. Using {x l } L l=1 , we obtain an updated graph {V, E}. In</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Interpolation and Rescaling in SGAlign</head><p>Input: Features of all nodes in the entire graph {x l } L l=1 ; sub-graphs {G aj } J j=1 , where J is the total number of sub-graphs, a j = (t s,j , t e,j ); alignment quantity τ ; <ref type="bibr">1:</ref> for each sub-graph G aj do <ref type="bibr">2:</ref> Arrange all nodes in G aj in their temporal order; <ref type="bibr">3:</ref> Compute sub-graph size d = t s,j − t e,j , sampling interval s = d/τ , interpolation quantity T = τ s; <ref type="bibr">4:</ref> Sample T points based on linear interpolation using the two neighbors of each point l = [t s + kd/T for k in range(T )] <ref type="bibr" target="#b4">5</ref>:</p><formula xml:id="formula_7">X in = [( i − i)x i + (i − i )x i for i in l] 6: z aj = [mean(X in [ks:(k + 1)s]) for k in range(τ )] 7: end for Output: Z = {z aj } J j=1 .</formula><p>SGAlign, we further exploit semantic context by averaging the neighbor features of each node, formulated as y l = 1 K K k=1 x n l (k) , and dub y l as semantic features. SGAlign uses pre-defined anchors to extract sub-graphs from {V, E}. Given each action anchor a = (t s , t e ), a sub-graph G a is defined as a subset of G such that</p><formula xml:id="formula_8">G a = {V a , E a }, where V a = {v l ∈ V}|t s ≤ l ≤ t e } and E a = {(v i , v j ) ∈ E s |v i ∈ V a }.</formula><p>For the sub-graph G a , we sample τ points (τ : alignment quantity) via interpolation and rescaling as described in Alg. 1, and generate the sub-graph feature y a ∈ R τ C , where C is the feature dimension.</p><p>We run Alg. 1 independently using the GCNeXt features {x l } L l=1 and the semantic features {y l } L l=1 as input. For the former, we sample τ 1 points and obtain the sub-graph features z 1a ∈ R τ1C ; and for the latter, we sample τ 2 points and obtain z 2a ∈ R τ2C , respectively. We concatenate z 1a and z 2a as the output of the SGAlign layer. <ref type="figure" target="#fig_2">Fig. 4</ref> illustrates the idea of SGAlign using the two features.</p><p>By explicitly using the semantic feature y l , SGAlign adaptively aggregates semantic context information when computing the features of each sub-graph. This is essentially different from the methods that manually extend the anchor boundaries for incorporating temporal context <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b55">56]</ref> and leads to superior performance.</p><p>It is worth mentioning that the sampling interval s is based on the sub-graph size d and alignment quantity τ , to ensure that the output z aj is the weighted sum of all the nodes in the sub-graph. In Sec. 4.4, we show that this sampling strategy gives us experimental improvements. Sub-Graph Localization. For each sub-graph G a , we calculate its Intersection-over-Union (IoU) with all groundtruth actions ψ in Ψ, and denote the maximum IoU g c as the training target. We apply three fully connected (FC) layers on top of the SGAlign layer for each sub-graph feature. The last FC layer has two output scores p cls and p reg , which are trained to match g c using classification and re-gression losses, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">G-TAD Training</head><p>We train G-TAD with the sub-graph localization loss L g and the node classification loss L n , as well as an L 2 -norm regularization loss L r for all trainable parameters Θ:</p><formula xml:id="formula_9">L = L g + L n + λ 2 · L r ,<label>(5)</label></formula><p>where we set λ 2 = 10 −4 . The loss L g is used to determine the confidence scores of sub-graphs, and the loss L n , classifying each node based on its location relative to an action, can drastically improve the network convergence. Sub-Graph Localization Loss. The sub-graph loss L g is defined as follows:</p><formula xml:id="formula_10">L g = L wce (p cls , 1{g c &gt; 0.5}) + λ 1 · L mse (p reg , g c ),<label>(6)</label></formula><p>where L mse is the mean square error loss and L wce is the weighted cross entropy loss. The weight is computed to balance the positive and negative training samples. In our experiments, we take the trade-off coefficient λ 1 = 10, since the second loss term tends to be smaller than the first. Node Classification Loss. Along with the sub-graph localization loss L g , we use the loss L n to classify each node in the whole graph based on whether they are start or end points of an action. We add FC layers after the first GCNeXt block to produce the start/end probabilities (p s , p e ) (these layers are ignored at test time). We use (g ns , g ne ) to denote the corresponding training targets for each node. We use the weighted cross entropy loss to compute the discrepancy between the predictions and the targets, and hence have L n formulated as L n = L wce (p s , g ns ) + L wce (p e , g ne ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">G-TAD Inference</head><p>At inference time, G-TAD predicts classification and regression scores for each sub-graph G a .</p><p>From the J sub-graphs, we construct predicted actions Φ = φ j = (t s,j ,t e,j ,ĉ j , p j ) J j=1 , where (t s,j ,t e,j ) refer to the predicted action boundaries,ĉ j is the predicted action class, and p j is the fused confidence score of this prediction, computed as p j = p α cls · p 1−α reg . In our experiments, we search for the optimal α in each setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>ActivityNet-1.3 <ref type="bibr" target="#b6">[7]</ref> is a large-scale action understanding dataset for action recognition, temporal detection, proposal generation and dense captioning tasks. It contains 19,994 temporally annotated untrimmed videos with 200 action categories, which are divided into training, validation and testing sets by the ratio of 2:1:1. <ref type="bibr" target="#b22">[23]</ref> dataset contains 413 temporally annotated untrimmed videos with 20 action categories. We use the 200 videos in the validation set for training and evaluate on the 213 videos in the testing set. Detection Metric. We take mean Average Precision (mAP) at certain IoU thresholds as the main evaluation metric. Following the official evaluation API, the IoU thresholds {0.5, 0.75, 0.95} and {0.3, 0.4, 0.5, 0.6, 0.7} are used for ActivityNet-1.3 and THUMOS-14, respectively. On ActivityNet-1.3, we also report average mAP over 10 different IoU thresholds [0.5 : 0.05 : 0.95].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THUMOS-14</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Features and Anchors. We use pre-extracted features for both datasets. For ActivityNet-1.3, we adopt the pre-trained two-stream network by Xiong et. al. <ref type="bibr" target="#b50">[51]</ref>, with downsampling ratio σ = 16. Each video feature sequence is rescaled to L = 100 snippets using linear interpolation. For THUMOS-14, the video features are extracted using TSN model <ref type="bibr" target="#b43">[44]</ref> pre-trained on Kinetics <ref type="bibr" target="#b56">[57]</ref> with σ = 5. We crop each video feature sequence with overlapped windows of size L = 256 and stride 128. In training, we do not use any crops void of actions.</p><p>For ActivityNet-1.3 and THUMOS-14, we enumerate all possible combinations of start and end as anchors, e.g.</p><formula xml:id="formula_11">{(t s , t e )| 0 &lt; t s &lt; t e &lt; L; t s , t e ∈ N ; t e − t s &lt; D},</formula><p>where D is 100 for ActivityNet-1.3 and 64 for THUMOS-14. In SGAlign, we use τ 1 = 32, τ 2 = 4 for ActivityNet-1.3 and τ 1 = τ 2 = 16 for THUMOS-14.</p><p>Training and Inference. We implement and compile our framework using PyTorch 1.1, Python 3.7, and CUDA 10.0. We use b = 3 GCNeXt blocks and train our model end-toend, with batch size of 16. The learning rate is 4 × 10 −3 on ActivityNet-1.3 and 6×10 −6 on THUMOS-14 for the first 5 epochs, and is reduced by 10 for the following 5 epochs. In inference, following <ref type="bibr" target="#b29">[30]</ref> to leverage the global video context, we take the video classification scores from <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b50">[51]</ref>, and multiply them by the fused confidence score p j for evaluation. For post-processing, we apply Soft-NMS <ref type="bibr" target="#b2">[3]</ref>, where the threshld is 0.84 and select the top-M prediction for final evaluation, where M is 100 for ActivityNet-1.3 and 200 for THUMOS-14. More details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art</head><p>ActivityNet-1.3: Tab. 1 compares G-TAD with state-of-theart detectors. We report mAP at different tIoU thresholds, as well as average mAP. G-TAD reports the highest average mAP results on this large-scale and diverse dataset. Notably, G-TAD reaches an mAP of 9.02% at IoU 0.95, indicating that the localization is more accurate than others. THUMOS-14: Tab. 2 compares the action localization results of G-TAD and various state-of-the-art methods on the THUMOS14 dataset. At IoU 0.7, G-TAD reaches an mAP of 23.4%, obviously higher than the current best 20.8% of TALNet. At IoU 0.5, G-TAD outperforms all methods except TALNet. Besides, when combined with a proposal post-processing method P-GCN <ref type="bibr" target="#b54">[55]</ref>, G-TAD performs even better, especially at IoUs ≤ 0.5. Now G-TAD reaches 51.6% at IoU 0.5, outperforming all the other methods. In addition, we also report the results of BSN with P-GCN (directly taken from <ref type="bibr" target="#b54">[55]</ref>), which are not as good as G-TAD + P-GCN, albeit showing improvement from BSN. This signifies the advantage of G-TAD proposals regardless of post-processing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>GCNeXt Module: We ablate the three main components of GCNeXt, mainly GCN on temporal edges, GCN on semantic edges, and cardinality increase. Tab. 3 reports the performance on ActivityNet-1.3, where each component is separately enabled/disabled. We see how each of these components contributes to the performance of the final G-TAD model. We highlight the gains from the semantic graph, showing the benefit of integrating adaptive context from semantic neighbors. It also shows cardinality 32 mostly outperforms cardinality 1.</p><p>SGAlign Module: The incorporation of semantic features in SGAlign aggregates more semantic context into each sub-graph, which benefits the subsequent localization compared to merely using the GCNeXt features. The sampling interval s in Alg. 1 is adaptively computed for each subgraph, leading to better performance than a fixed value (e.g. s = 1). Tab. 4 shows the effect of semantic features and the sampling strategy from both temporal and semantic graphs on ActivityNet-1.3. While sampling densely gives us minor improvements, we obtain a larger gain by including context information from the semantic graph.</p><p>Sensitivity to Video Length: We report the results of the sensitivity of G-TAD to different window sizes in THUMOS-14 in Tab. 5. G-TAD benefits more from larger window sizes (L = 256 vs. 128). Larger windows mean that G-TAD can aggregate more context information from the semantic graph. Performance degrades at L = 512, where GPU memory limits the batch size and network training is influenced. t <ref type="figure">Figure 5</ref>. Semantic graphs and Context. Given two videos (left and right), we combine action frames of one video with background frames of another to create a synthetic video with no action context (middle). As expected, the semantic graph of the synthetic video contains no edges between action and background snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussion of Action Context</head><p>In the ablation study, graph convolutions on the semantic graph improve G-TAD performance in both the GCNeXt block and in the SGAlign layer. Semantic edges connecting background to action snippets can adaptively pass the action context information to each possible action. In this section, we define 2 extra experiments to show how semantic edges encode meaningful context information. Zero-Context Video. How zero context between action and background leads to semantic graphs with no actionbackground edges is visually shown by comparing semantic graphs resulting from natural videos and synthetically compiled ones. In <ref type="figure">Fig. 5 (left and right)</ref>, we present two natural videos that include actions "wrestling" and "playing darts", respectively. Semantic edges in their resultant graphs do exist, connecting action with background snippets, thus exemplifying the usage of context in the detection process. Then, we compile a synthetic video that stacks action frames from the wrestling video and background frames from the darts video, feed it to G-TAD and again visualize the semantic graph (middle). As expected, the semantic graph does not include any action-background semantic edges. Correlation to Context Amount. We show the correlation between context edges and context as defined by human annotators. We define the video context amount as the av-  <ref type="figure">Figure 7</ref>. Action-Background Semantic Edge Ratio vs. Context Amount. In the scatter plot, each purple dot corresponds to a different video graph. Strong positive correlation is observed between context amount and action-background semantic edge ratio, which means we predict on average more semantic edges in the presence of larger video context.</p><p>Layer Epoch t <ref type="figure">Figure 8</ref>. Semantic graph evolution during G-TAD training.</p><p>We visualize the semantic graphs at first, middle, and last layers during training epoch 0, 3, 6, and 9. The semantic edges at the first layer are always the same, while the semantic graphs at the middle and last layers evolve to incorporate more context. erage number of background snippets which can be used to predict the foreground class. Following DETAD <ref type="bibr" target="#b0">[1]</ref>, we collect context amount for all videos in ActivityNet validation set from Amazon Mechanical Turk. The scatters in <ref type="figure">Fig. 7</ref> shows the relation between context amount and the ratio of action-background semantic edges over all the semantic edges. From the plot, we observe that if a video has a higher amount of context, it is more likely to have more action-background semantic edges in its semantic graph.</p><p>We further average the ratios in five context amount ranges, and plot them in green. The strong positive correlation between context amount and action-background semantic edge ratio indicates that our G-TAD model can effectively find related context snippets in the semantic graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization</head><p>We show a few qualitative detection results in <ref type="figure">Fig. 6</ref> on both ActivityNet-1.3 and THUMOS-14. In <ref type="figure">Fig. 8</ref>, we visualize the evolution of semantic graphs during the training process across GCNeXt layers. Specifically, we feed a video into G-TAD and visualize the semantic graphs emerging at the first, middle, and last layers at epochs 0, 3, 6, and 9 of training. The semantic graphs at the first layer are the same, since they are built on the same input features. As we progress to different layers and epochs, semantic graphs adaptively update their edges. Interestingly, we observe the presence of more context edges as training advances. This indicates that G-TAD progressively learns to incorporate multiple levels of context in the detection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we cast the temporal action detection task as a sub-graph localization problem by formulating videos as graphs. We take video snippets as graph nodes, snippetsnippet correlations as edges, and apply graph convolution as the basic operation. We propose a new architecture G-TAD to localize sub-graphs. G-TAD includes GC-NeXt blocks to aggregate context features from semantically correlated snippets and an SGAlign layer to transform sub-graph features into vector representations. G-TAD can learn enriched multi-level semantic context in an adaptive way using stacked dynamic graph convolutions. Extensive experiments show that G-TAD can find global video context without extra supervision and achieve the state-of-theart performance on both THUMOS-14 and ActivityNet-1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary</head><p>A. Derivation and Efficient Implementation of Eq. 4</p><p>In this section, we provide the derivation of Eq. 4 in the paper (listed here in the following). We also show that Eq. 4 can be efficiently implemented by zero-padded 1D/edge convolutions.</p><formula xml:id="formula_12">H(X, E, W ) = ReLU [W α X + W β XA f t + W γ XA b t + φ XA s + X].<label>(4)</label></formula><p>A.1. Derivation of Eq. 4 a) Temporal Graph Convolution. We first provide the derivation for temporal graph convolution. The temporal forward edges E f t and backward edges E b t are formulated as</p><formula xml:id="formula_13">E f t = {(v i , v i+1 )| i ∈ {1, 2, . . . , L − 1}}, E b t = {(v i , v i−1 )| i ∈ {2, . . . , L − 1, L},<label>(7)</label></formula><p>The corresponding adjacency matrices A f t , A b t can be present by L vectors, respectively, shown in Eq. 9. We use e k ∈ R L to present the vector in which the k-th element is one but the others are zeros. </p><formula xml:id="formula_14">A f t =          0 0 0 . . . 0 0 1 0 0 0 0 0 1 0 0 0 . . . . . . . . . . . . 0 0 0 . . . 0 0 0 0 0 . . . 1 0          = [e 2 ,</formula><formula xml:id="formula_15">⇒A f t = [A b t ] T<label>(8)</label></formula><p>Given the input X ∈ R C×L , after temporal graph convolution, the output, X t , becomes</p><formula xml:id="formula_16">X t = f agg (X, A f t , W f ) + f agg (X, A b t , W b ) = W f,0 X + W f,1 XA f t + W b,0 X + W b,1 XA b t = (W f,0 + W b,0 )X + W f,1 XA f t + W b,1 XA b t .<label>(9)</label></formula><p>Here W * is the trainable weights in the neural network. b) Semantic Graph Convolution. It is straightforward to obtain Eq. 4 for semantic graph convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Efficient Implementation of Eq. 4</head><p>In implementation of Eq. 4, we use an efficient zero-padded 1D convolution and edge convolution for temporal graph convolution and semantic graph convolution, respectively. In the following, we provide proof that our efficient implementation is equivalent to Eq. 4. a) Temporal Graph Convolution.</p><p>If a 1D convolution has kernel size 3, the weight matrix is a 3D tensor in R 3×C×C . We denote the matrix as W conv1 = [W 1 , W 2 , W 3 ], W 1,2,3 ∈ R C×C . Given the same input X = [x 1 , x 2 , ..., x L ], we pad zero on the input, x 0 = x L+1 = 0 ∈ R C .</p><p>The output of 1D convolution can be written as Y = [y 1 , y 2 , ..., y L ] ∈ R C×L y k = [W 1 x k−1 + W 2 x k + W 3 x k+1 ], k = 1, 2, . . . , L</p><p>We can prove that X t = Y by multiplying e k on both sides in Eq. 11. Please be noted that W * is the trainable weights in the neural network. We can assume W 1 = W b,1 , W 3 = W f,1 , W 2 = W f,0 + W b,0 X t e k = W f,0 Xe k + W f,1 XA f t e k + W b,0 Xe k + W b,1 XA b t e k = (W f,0 + W b,0 )x k + W f,1 X[0, e 1 , e 2 , . . . , e L ] T e k + W b,1 X[e 2 , e 3 , . . . , e L , 0] T e k</p><formula xml:id="formula_18">= (W f,0 + W b,0 )x k + W f,1 Xe k+1 + W b,1 Xe k−1 = (W f,0 + W b,0 )x k + W f,1 x k+1 + W b,1 x k−1 = W 1 x k−1 + W 2 x k + W 3 X k+1<label>(11)</label></formula><p>b) Semantic Graph Convolution. In the semantic graph, edge convolution is directly used, so proof is done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Details</head><p>Semantic Edges from Multiple Levels. In G-TAD, we use multiple GCNeXt blocks to adaptively incorporate multi-level semantic context into video features. After that, SGAlign layer embeds each sub-graph by concatenating aligned features from temporal and semantic graphs. However, it is not necessary to consider only the last GCNeXt semantic graphs to align the semantic feature. Last row in Tab. 6 present one more experiment that takes the union of semantic edges from all GCNeXt blocks to aggregate the semantic feature. We can find that the semantic context also helps to improve model performance under this setup. <ref type="table">Table 6</ref>. Ablating SGAlign Components. We disable the sample-rescale process and the feature concatenation from the semnantic graph for detection on ActivityNet-1.3. The rescaling strategy leads to slight improvement, while the main gain arises from the use of context information (semantic graph 2D Conv. for Sub-Graph Localization. Once we get the sub-graph feature from SGAlign layer, instead of using three fully connected (FC) layers regress to g c , we can arrange the anchors in a 2D L × L map based on the start/end time, and set zeros to the map where is no pre-designed anchors (e.g. t s &gt; t e ). In doing so, we can use 2D CNNs to regress to a g c map that arranged by the same order. We call the predicted matrix IoU map.</p><p>The neighbouring anchors in the 2D IoU map have similar boundary locations. Thus we can use the proposal-proposal relationship in the 2D convolutions. We set kernel size to 1, 3, and 5, and the results are shown in Tab. 7. We do not observe any significant benefit from 2D convolutions. <ref type="table">Table 7</ref>. The model performance when we use 3 2D convolution layers to predict IoU map.We set kernel size to 1, 3, and 5, and collect result on ActivityNet1.3. We do not observe any significant benefit from 2D convolutions.</p><p>Conv </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 3illustrates the architecture of GCNeXt.We build a video graph G = {V, E}, where V = {v l } L l=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>⋈Figure 4 .</head><label>4</label><figDesc>SGAlign layer. SGAlign extracts sub-graph features based on both GCNeXt features (left) and semantic features (right), and concatenates both sub-graph features as output. The dots on top represent sub-graph features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>e 3 , . . . , e L , 0] e 1 , e 2 , . . . , e L−1 ],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Action detection results on validation set of ActivityNet-1.3, measured by mAP (%) at different tIoU thresholds and the average mAP. G-TAD achieves better performance in average mAP than the other methods, even the latest work of BMN and P-GCN shown in the second-to-last block.</figDesc><table><row><cell>Method</cell><cell>0.5</cell><cell>0.75</cell><cell cols="2">0.95 Average</cell><cell></cell></row><row><cell cols="2">Wang et al. [46] 43.65</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell cols="2">Singh et al. [40] 34.47</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>SCC [21]</cell><cell cols="3">40.00 17.90 4.70</cell><cell>21.70</cell><cell></cell></row><row><cell>CDC [36]</cell><cell cols="3">45.30 26.00 0.20</cell><cell>23.80</cell><cell></cell></row><row><cell>R-C3D [52]</cell><cell>26.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>BSN [30]</cell><cell cols="3">46.45 29.96 8.02</cell><cell>30.03</cell><cell></cell></row><row><cell>Chao et al. [9]</cell><cell cols="3">38.23 18.30 1.30</cell><cell>20.22</cell><cell></cell></row><row><cell>P-GCN [55]</cell><cell cols="3">48.26 33.16 3.27</cell><cell>31.11</cell><cell></cell></row><row><cell>BMN [29]</cell><cell cols="3">50.07 34.78 8.29</cell><cell>33.85</cell><cell></cell></row><row><cell>G-TAD (ours)</cell><cell cols="3">50.36 34.60 9.02</cell><cell>34.09</cell><cell></cell></row><row><cell cols="6">Table 2. Action detection results on testing set of THUMOS-</cell></row><row><cell cols="6">14, measured by mAP (%) at different tIoU thresholds. G-TAD</cell></row><row><cell cols="6">achieves the best performance for IoU@0.7, and combined with</cell></row><row><cell cols="6">P-GCN, G-TAD significantly outperforms all the other methods.</cell></row><row><cell>Method</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>SST [5]</cell><cell>-</cell><cell>-</cell><cell>23.0</cell><cell>-</cell><cell>-</cell></row><row><cell>CDC [36]</cell><cell>40.1</cell><cell>29.4</cell><cell>23.3</cell><cell>13.1</cell><cell>7.9</cell></row><row><cell>TURN-TAP[15]</cell><cell>44.1</cell><cell>34.9</cell><cell>25.6</cell><cell>-</cell><cell>-</cell></row><row><cell>CBR [16]</cell><cell>50.1</cell><cell>41.3</cell><cell>31.0</cell><cell>19.1</cell><cell>9.9</cell></row><row><cell>SSN [56]</cell><cell>51.9</cell><cell>41.0</cell><cell>29.8</cell><cell>-</cell><cell>-</cell></row><row><cell>BSN [30]</cell><cell>53.5</cell><cell>45.0</cell><cell>36.9</cell><cell>28.4</cell><cell>20.0</cell></row><row><cell>TCN [11]</cell><cell>-</cell><cell>33.3</cell><cell>25.6</cell><cell>15.9</cell><cell>9.0</cell></row><row><cell>TAL-Net [9]</cell><cell>53.2</cell><cell>48.5</cell><cell>42.8</cell><cell>33.8</cell><cell>20.8</cell></row><row><cell>MGG [32]</cell><cell>53.9</cell><cell>46.8</cell><cell>37.4</cell><cell>29.5</cell><cell>21.3</cell></row><row><cell>DBG [28]</cell><cell>57.8</cell><cell>49.4</cell><cell>39.8</cell><cell>30.2</cell><cell>21.7</cell></row><row><cell>Yeung et al. [53]</cell><cell>36.0</cell><cell>26.4</cell><cell>17.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Yuan et al. [54]</cell><cell>36.5</cell><cell>27.8</cell><cell>17.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Hou et al. [22]</cell><cell>43.7</cell><cell>-</cell><cell>22.0</cell><cell>-</cell><cell>-</cell></row><row><cell>SS-TAD [4]</cell><cell>45.7</cell><cell>-</cell><cell>29.2</cell><cell>-</cell><cell>9.6</cell></row><row><cell>BMN [29]</cell><cell>56.0</cell><cell>47.4</cell><cell>38.8</cell><cell>29.7</cell><cell>20.5</cell></row><row><cell>G-TAD (ours)</cell><cell>54.5</cell><cell>47.6</cell><cell>40.2</cell><cell>30.8</cell><cell>23.4</cell></row><row><cell>BSN+P-GCN [55]</cell><cell>63.6</cell><cell>57.8</cell><cell>49.1</cell><cell>-</cell><cell>-</cell></row><row><cell>G-TAD+P-GCN</cell><cell>66.4</cell><cell>60.4</cell><cell>51.6</cell><cell>37.6</cell><cell>22.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablating GCNeXt Components. We disable temporal/semantic graph convolutions and set different cardinalities for detection on ActivityNet-1.3.</figDesc><table><row><cell cols="2">GCNeXt block</cell><cell cols="3">tIoU on Validation Set</cell></row><row><cell cols="2">Temp. Sem. Card.</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95 Avg.</cell></row><row><cell></cell><cell>1</cell><cell cols="3">48.12 32.16 6.41 31.65</cell></row><row><cell></cell><cell>1</cell><cell cols="3">50.20 34.80 7.35 33.88</cell></row><row><cell></cell><cell>32</cell><cell cols="3">50.13 34.17 8.70 33.67</cell></row><row><cell></cell><cell>32</cell><cell cols="3">49.09 33.32 8.02 32.63</cell></row><row><cell></cell><cell>32</cell><cell cols="3">50.36 34.60 9.02 34.09</cell></row><row><cell cols="5">Table 4. Ablating SGAlign Components. We disable the sample-</cell></row><row><cell cols="5">rescale process and the feature concatenation from the semnan-</cell></row><row><cell cols="5">tic graph for detection on ActivityNet-1.3. The rescaling strategy</cell></row><row><cell cols="5">leads to slight improvement, while the main gain arises from the</cell></row><row><cell cols="4">use of context information (semantic graph).</cell><cell></cell></row><row><cell cols="2">SGAlign</cell><cell cols="3">tIoU on Validation Set</cell></row><row><cell>Samp.</cell><cell>Concat.</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95 Avg.</cell></row><row><cell></cell><cell></cell><cell cols="3">49.84 34.58 8.17 33.78</cell></row><row><cell></cell><cell></cell><cell cols="3">49.86 34.60 9.56 33.89</cell></row><row><cell></cell><cell></cell><cell cols="3">50.36 34.60 9.02 34.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Effect of Video Size. We vary the input video size (window length L) and see that G-TAD performance improves with larger sizes (L = 256). Degradation occurs at L = 512, since GPU memory limits the batch size to be significantly reduced, leading to a noticeable performance drop.</figDesc><table><row><cell>Window</cell><cell></cell><cell cols="3">tIoU on Validation</cell><cell></cell></row><row><cell>Length L</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>64</cell><cell cols="5">47.07 39.29 32.25 23.88 15.17</cell></row><row><cell>128</cell><cell cols="5">51.75 44.90 38.70 29.03 21.32</cell></row><row><cell>256</cell><cell cols="5">54.50 47.61 40.16 30.83 23.42</cell></row><row><cell>512</cell><cell cols="5">48.32 41.71 34.38 26.85 19.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Qualitative results. We show qualitative detection results on ActivityNet-1.3 (top) and THUMOS-14 (bottom).</figDesc><table><row><cell></cell><cell cols="2">Ground Truth</cell><cell>Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Time (sec)</cell></row><row><cell>6</cell><cell>7</cell><cell>Zumba</cell><cell>156</cell><cell>159</cell><cell>38</cell><cell>63</cell><cell cols="2">Preparing Salad</cell><cell></cell><cell>216</cell><cell>219</cell></row><row><cell>132</cell><cell>132</cell><cell>Cliff Diving</cell><cell>136</cell><cell>136</cell><cell></cell><cell>409</cell><cell>409</cell><cell>Long Jump</cell><cell>416</cell><cell>418</cell></row><row><cell cols="3">Figure 6. Context Amount Action-Background Semantic Edge Ratio</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Avg. 49.84 34.58 8.17 33.78 49.86 34.60 9.56 33.89 50.36 34.60 9.02 34.09 all 50.26 34.70 8.52 33.95</figDesc><table><row><cell>).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SGAlign</cell><cell cols="3">tIoU on Validation Set</cell></row><row><cell>Samp.</cell><cell>Concat.</cell><cell>0.5</cell><cell>0.75</cell><cell>0.95</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ) + F (X, A b t , W b t ) + F (X, A s , W s ) + X),(4)whereA f t , A b t , and A s are adjacency matrices, W = {W f t , W b t , W s } are the trainable weights, corresponding to E f t , E b t ,and E s , respectively. ReLU is the rectified linear unit as the activation function. In the supplementary material, we simplify Eq. 4 and prove that it can be efficiently computed by zero-padded 1D convolutions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action search: Spotting actions in videos and its application to temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Soft-nms improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC</title>
		<meeting>the British Machine Vision Conference (BMVC</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02739</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Mesh r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (ICCV)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (ICCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scc: Semantic context cascade for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayner</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time temporal action localization in untrimmed videos by subaction discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deepgcns: Making gcns go as deep as cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulellah</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning video representations from correspondence proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gaussian temporal awareness networks for action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Untrimmed video classification for activity detection: submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01979</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc Val</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">Uts at activitynet 2016. Ac-tivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Point clouds learning with attention-based graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13445</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03252</idno>
		<title level="m">Graph convolutional networks for temporal action localization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
