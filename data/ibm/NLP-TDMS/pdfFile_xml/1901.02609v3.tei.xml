<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequential Attention-based Network for Noetic End-to-End Response Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Lab, DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
							<email>w.wang@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Speech Lab, DAMO Academy, Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequential Attention-based Network for Noetic End-to-End Response Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The noetic end-to-end response selection challenge as one track in Dialog System Technology Challenges 7 (DSTC7) aims to push the state of the art of utterance classification for real world goal-oriented dialog systems, for which participants need to select the correct next utterances from a set of candidates for the multi-turn context. This paper describes our systems that are ranked the top on both datasets under this challenge, one focused and small (Advising) and the other more diverse and large (Ubuntu). Previous state-of-the-art models use hierarchy-based (utterance-level and token-level) neural networks to explicitly model the interactions among different turns' utterances for context modeling. In this paper, we investigate a sequential matching model based only on chain sequence for multi-turn response selection. Our results demonstrate that the potentials of sequential matching approaches have not yet been fully exploited in the past for multi-turn response selection. In addition to ranking the top in the challenge, the proposed model outperforms all previous models, including state-of-the-art hierarchy-based models, and achieves new state-of-the-art performances on two large-scale public multi-turn response selection benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Dialogue systems are gaining more and more attention due to their encouraging potentials and commercial values. With the recent success of deep learning models <ref type="bibr" target="#b15">(Serban et al. 2016)</ref>, building an end-to-end dialogue system became feasible. However, building end-to-end multi-turn dialogue systems is still quite challenging, requiring the system to memorize and comprehend multi-turn conversation context, rather than only considering the current utterance as in single-turn dialogue systems.</p><p>Multi-turn dialogue modeling can be divided into generation-based methods <ref type="bibr" target="#b15">(Serban et al. 2016;</ref>) and retrieval-based methods <ref type="bibr" target="#b9">(Lowe et al. 2015;</ref><ref type="bibr" target="#b20">Wu et al. 2017)</ref>. The latter is the focus of the noetic endto-end response selection challenge in DSTC7 1 <ref type="bibr" target="#b22">(Yoshino et al. 2018)</ref>. Retrieval-based methods select the best response from a candidate pool for the multi-turn context, which can be considered as performing a multi-turn response selection task. The typical approaches for multi-turn response selection mainly consist of sequence-based methods <ref type="bibr" target="#b9">(Lowe et al. 2015;</ref><ref type="bibr" target="#b21">Yan, Song, and Wu 2016)</ref> and hierarchy-based methods <ref type="bibr" target="#b24">(Zhou et al. 2016;</ref><ref type="bibr" target="#b20">Wu et al. 2017;</ref><ref type="bibr" target="#b23">Zhang et al. 2018;</ref><ref type="bibr" target="#b26">Zhou et al. 2018)</ref>. Sequence-based methods usually concatenate the context utterances into a long sequence. Hierarchybased methods normally model each utterance individually and then explicitly model the interactions among the utterances.</p><p>Recently, previous work <ref type="bibr" target="#b20">(Wu et al. 2017;</ref><ref type="bibr" target="#b23">Zhang et al. 2018</ref>) claims that hierarchy-based methods with complicated networks can achieve significant gains over sequencebased methods. However, in this paper, we investigate the efficacy of a sequence-based method, i.e., Enhanced Sequential Inference Model (ESIM) <ref type="bibr" target="#b1">(Chen et al. 2017a</ref>) originally developed for the natural language inference (NLI) task. Our systems are ranked the top on both datasets, i.e., Advising and Ubuntu datasets, under the DSTC7 response selection challenge. In addition, the proposed approach outperforms all previous models, including the previous state-ofthe-art hierarchy-based methods, on two large-scale public benchmark datasets, the Lowe's Ubuntu <ref type="bibr" target="#b9">(Lowe et al. 2015)</ref> and E-commerce datasets <ref type="bibr" target="#b23">(Zhang et al. 2018)</ref>. Our source code is available at https://github.com/alibaba/ esim-response-selection.</p><p>Hierarchy-based methods usually use extra neural networks to explicitly model the multi-turn utterances' relationship. They also usually need to truncate the utterances in the multi-turn context to make them the same length and shorter than the maximum length. However, the lengths of different turns usually vary significantly in real tasks. When using a large maximum length, we need to add a lot of zero padding in hierarchy-based methods, which will increase computation complexity and memory cost drastically. When using a small maximum length, we may throw away some important information in the multi-turn context. We propose to use a sequence-based model, the ESIM model, in the multi-turn response selection task to effectively address the above mentioned problem encountered by hierarchy-based methods. We concatenate the multi-turn context as a long sequence, and convert the multi-turn response selection task into a sentence pair binary classification task, i.e., whether the next sentence is the response for the current context. There are two major advantages of ESIM over hierarchy-based methods. First, since ESIM does not need to make each utterance the same length, it has less zero padding and hence could be more computationally efficient than hierarchy-based methods. Second, ESIM models the interactions between utterances in the context implicitly, yet in an effective way as described in the model description section, without using extra complicated networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Description</head><p>DSTC7 is divided into 3 different tracks, and the proposed approach is developed for the noetic end-to-end response selection track. This track focuses on goal-oriented multi-turn dialogs and the objective is to select the correct response from a set of candidates. Participating systems should not be based on hand-crafted features or rule-based systems. Two datasets are provided, i.e., Ubuntu and Advising, which will be introduced in detail in the experiment section.</p><p>The response selection track provided series of subtasks that have similar structures, but vary in the output space and available context. In Table 1, indicates that the task is evaluated on the marked dataset, and indicates not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Description</head><p>The multi-turn response selection task is to select the next utterance from a candidate pool, given a multi-turn context. We convert the problem into a binary classification task, similar to the previous work <ref type="bibr" target="#b9">(Lowe et al. 2015;</ref><ref type="bibr" target="#b20">Wu et al. 2017)</ref>. Given a multi-turn context and a candidate response, our model needs to determine whether or not the candidate response is the correct next utterance. In this section, we will introduce our model, Enhanced Sequential Inference Model (ESIM) <ref type="bibr" target="#b1">(Chen et al. 2017a</ref>) originally developed for natural language inference. The model consists of three main components, i.e., input encoding, local matching, and matching composition, as shown in <ref type="figure" target="#fig_1">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Encoding</head><p>Input encoding encodes the context information and represents tokens in their contextual meanings. Instead of encoding the context information through complicated hierarchical structures as in hierarchy-based methods, ESIM encodes the context information simply as follows. The multi-turn context is concatenated as a long sequence, which is denoted as c = (c 1 , . . . , c m ). The candidate response is denoted as r = (r 1 , . . . , r n ). Pre-trained word embedding E ∈ R de×|V | is then used to convert c and r to two vector sequences [E(c 1 ), . . . , E(c m )] and [E(r 1 ), . . . , E(r n )], where |V | is the vocabulary size and d e is the dimension of the word embedding. There are many kinds of pre-trained word embeddings available, such as GloVe <ref type="bibr" target="#b14">(Pennington, Socher, and Manning 2014)</ref> and fastText <ref type="bibr" target="#b11">(Mikolov et al. 2018)</ref>. We propose a method to exploit multiple embeddings. Given k kinds of pre-trained word embeddings E 1 , . . . , E k , we concatenate all embeddings for the word i, i.e., E(c i ) = [E 1 (c i ); . . . ; E k (c i )]. Then we use a feed-forward layer with ReLU to reduce dimension from (d e1 + · · · + d e k ) to d h .</p><p>To represent tokens in their contextual meanings, the context and the response are fed into BiLSTM encoders to obtain context-dependent hidden states c s and r s :</p><formula xml:id="formula_0">c s i = BiLSTM 1 (E(c), i) , (1) r s j = BiLSTM 1 (E(r), j) ,<label>(2)</label></formula><p>where i and j indicate the i-th token in the context and the j-th token in the response, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Matching</head><p>Modeling the local semantic relation between a context and a response is the critical component for determining whether the response is the proper next utterance. For instance, a proper response usually relates to some keywords in the context, which can be obtained by modeling the local semantic relation. Instead of directly encoding the context and the response as two dense vectors, we use the cross-attention mechanism to align the tokens from the context and response, and then calculate the semantic relation at the token level. The attention weight is calculated as:</p><formula xml:id="formula_1">e ij = (c s i ) T r s j .<label>(3)</label></formula><p>Soft alignment is used to obtain the local relevance between the context and the response, which is calculated by the attention matrix e ∈ R m×n in Equation <ref type="formula" target="#formula_1">(3)</ref>. Then for the hidden state of the i-th token in the context, i.e., c s i (already encoding the token itself and its contextual meaning), the relevant semantics in the candidate response is identified as a vector c d i , called dual vector here, which is a weighted combination of all the response's states, more specifically as shown in Equation <ref type="formula" target="#formula_3">(4)</ref>.</p><formula xml:id="formula_2">α ij = exp(e ij ) n k=1 exp(e ik )</formula><p>,</p><formula xml:id="formula_3">c d i = n j=1 α ij r s j ,<label>(4)</label></formula><formula xml:id="formula_4">β ij = exp(e ij ) m k=1 exp(e kj ) , r d j = m i=1 β ij c s i ,<label>(5)</label></formula><p>where α ∈ R m×n and β ∈ R m×n are the normalized attention weight matrices with respect to the 2-axis and 1-axis. The similar calculation is performed for the hidden state of each token in the response, i.e., r s j , as in Equation <ref type="formula" target="#formula_4">(5)</ref> to obtain the dual vector r d j . By comparing vector pair &lt; c s i , c d i &gt;, we can model the token-level semantic relation between aligned token pairs. The similar calculation is also applied for vector pair &lt; r s j , r d j &gt;. We collect local matching information as follows:</p><formula xml:id="formula_5">c l i = F ([c s i ; c d i ; c s i − c d i ; c s i c d i ]) ,<label>(6)</label></formula><formula xml:id="formula_6">r l j = F ([r s j ; r d j ; r s j − r d j ; r s j r d j ]) ,<label>(7)</label></formula><p>where a heuristic matching approach <ref type="bibr" target="#b12">(Mou et al. 2016</ref>) with difference and element-wise product is used here to obtain local matching vectors c l i and r l j for the context and response, respectively. F is a one-layer feed-forward neural network with ReLU to reduce the dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub Description</head><p>Ubuntu Advising 1 Select the next utterance from a candidate pool of 100 sentences 2 Select the next utterance from a candidate pool of 120000 sentences 3</p><p>Select the next utterance and its paraphrases from a candidate pool of 100 sentences 4</p><p>Select the next utterance from a candidate pool of 100 which might not contain the correct next utterance 5</p><p>Select the next utterance from a candidate pool of 100 incorporating the external knowledge  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching Composition</head><p>Matching composition is realized as follows. To determine whether the response is the next utterance for the current context, we explore a composition layer to compose the local matching vectors (c l and r l ) collected above:</p><formula xml:id="formula_7">c v i = BiLSTM 2 (c l , i) ,<label>(8)</label></formula><p>r v j = BiLSTM 2 (r l , j) .</p><p>Again we use BiLSTMs as building blocks for the composition layer, but the role of BiLSTMs here is completely different from that in the input encoding layer. The BiLSTMs here read local matching vectors (c l and r l ) and learn to discriminate critical local matching vectors for the overall utterance-level relationship. The output hidden vectors of BiLSTM 2 are converted to fixed-length vectors through pooling operations and fed to the final classifier to determine the overall relationship. Max and mean poolings are used and concatenated altogether to obtain a fixed-length vector. Then the final vector is fed to the multi-layer perceptron (MLP) classifier, with one hidden layer, tanh activation, and softmax output layer. The entire ESIM model is trained via minimizing the cross-entropy loss in an end-to-end manner.</p><formula xml:id="formula_9">y = MLP([c v max ; c v mean ; r v max ; r v mean ]) .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-encoding based methods</head><p>For subtask 2 on the Ubuntu dataset, we need to select the next utterance from a candidate pool of 120000 sentences. If we use the cross-attention based ESIM model directly, the computation cost is unacceptable. Instead, we first use a sentence-encoding based method to select the top 100 candidates from 120000 sentences and then rerank them using ESIM. Sentence-encoding based models use the Siamese architecture <ref type="bibr" target="#b0">(Bromley et al. 1993;</ref><ref type="bibr" target="#b2">Chen et al. 2017b)</ref> shown in <ref type="figure" target="#fig_1">Figure 1</ref> (a). Parameter-tied neural networks are applied to encode both the context and the response. Then a neural network classifier is applied to decide the relationship between the two sentences. Here, we use BiLSTMs with multi-head self-attention pooling to encode sentences , and an MLP to classify. We use the same input encoding process as ESIM. To transform a variable length sentence into a fixed length vector representation, we use a weighted summation of all BiL-STM hidden vectors (H):</p><formula xml:id="formula_10">A = softmax(W 2 ReLU(W 1 H T + b 1 ) + b 2 ) T ,<label>(11)</label></formula><p>where W 1 ∈ R da×2d h and W 2 ∈ R dm×da are weight matrices; b 1 ∈ R da and b 2 ∈ R dm are bias; d a is the dimension of the attention network and d h is the dimension of BiL-STMs. H ∈ R T ×2d h are the hidden vectors of BiLSTMs, where T denotes the length of the sequence. A ∈ R T ×dm is the multi-head attention weight matrix, where d m is a hyperparameter of the head number that needs to be tuned using the held-out set. Instead of using max pooling or mean pooling, we sum up the BiLSTM hidden states H according to the weight matrix A to get a vector representation of the input sentence.</p><formula xml:id="formula_11">V = A T H ,<label>(12)</label></formula><p>where the matrix V ∈ R dm×2d h can be flattened into a vector representation v ∈ R 2d h dm . To enhance the relationship between sentence pairs, similarly to ESIM, we concatenate the embeddings of two sentences and their absolute difference and element-wise product <ref type="bibr" target="#b12">(Mou et al. 2016)</ref> as the input to the MLP classifier:</p><formula xml:id="formula_12">y = MLP([v c ; v r ; |v c − v r |; v c v r ]) .<label>(13)</label></formula><p>The MLP has two hidden layers with ReLU activation, shortcut connections, and softmax output layer. The entire model is trained end-to-end through minimizing the crossentropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>We evaluated our model on both datasets of the DSTC7 response selection track, i.e., the Ubuntu and Advising datasets. In addition, to compare with previous methods, we also evaluated our model on two large-scale public muliturn response selection benchmarks, i.e., the Lowe's Ubuntu dataset <ref type="bibr" target="#b9">(Lowe et al. 2015</ref>) and E-commerce dataset <ref type="bibr" target="#b23">(Zhang et al. 2018)</ref>.</p><p>Ubuntu dataset The Ubuntu dataset includes two party conversations from Ubuntu Internet Relay Chat (IRC) channel . Under this challenge, the context of each dialog contains more than 3 turns and the system is asked to select the next turn from the given set of candidate sentences. Linux manual pages are also provided as external knowledge. We used a similar data augmentation strategy as in <ref type="bibr" target="#b9">(Lowe et al. 2015)</ref>, i.e., we considered each utterance (starting at the second one) as a potential response, with the previous utterances as its context. Hence a dialogue of length 10 yields 9 training examples. To train a binary classifier, we need to sample negative responses from the candidate pool. Initially, we used a 1:1 ratio between positive and negative responses for balancing the samples. Later, we found using more negative responses improved the results, such as 1:4 or 1:9. Considering efficiency, we chose 1:4 in the final configuration for all subtasks except 1:1 for subtask 2.</p><p>Advising dataset The advising dataset includes two-party dialogs that simulate a discussion between a student and an academic advisor. Structured information is provided as a database including course information and personas. The data also includes paraphrases of the sentences and the target responses. We used a similar data augmentation strategy as for the Ubuntu dataset based on original dialogs and their paraphrases. The ratio between positive and negative responses is 1:4.33.</p><p>Lowe's Ubuntu dataset This dataset is similar to the DSTC7 Ubuntu data. The training set contains one million context-response pairs and the ratio between positive and negative responses is 1:1. On both development and test sets, each context is associated with one positive response and 9 negative responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E-commerce dataset</head><p>The E-commerce dataset <ref type="bibr" target="#b23">(Zhang et al. 2018</ref>) is collected from real-word conversations between customers and customer service staff from Taobao 2 , the largest e-commerce platform in China. The ratio between positive and negative responses is 1:1 in both training and development sets, and 1:9 in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>We used spaCy 3 to tokenize text for the two DSTC7 datasets, and used original tokenized text without any further pre-processing for the two public datasets. The multi-turn context was concatenated and two special tokens, eou and eot , were inserted, where eou denotes end-ofutterance and eot denotes end-of-turn.</p><p>The hyperparameters were tuned based on the development set. We used GloVe <ref type="bibr" target="#b14">(Pennington, Socher, and Manning 2014)</ref> and fastText <ref type="bibr" target="#b11">(Mikolov et al. 2018</ref>) as pre-trained word embeddings. For subtask 5 of the Ubuntu dataset, we also used word2vec <ref type="bibr" target="#b10">(Mikolov et al. 2013)</ref> to train word embedding from the provided Linux manual pages. The detail is shown in <ref type="table" target="#tab_2">Table 2</ref>. Note that for subtask 5 of the Advising dataset, we tried using the suggested course information as external knowledge but didn't observe any improvement. Hence, we submitted the results for the Advising dataset without using any external knowledge. For Lowe's Ubuntu and E-commerce datasets, we used pre-trained word embedding on the training data by word2vec <ref type="bibr" target="#b10">(Mikolov et al. 2013</ref>). The pre-trained embeddings were fixed during the training procedure for the two DSTC7 datasets, but fine-tuned for Lowe's Ubuntu and E-commerce datasets.</p><p>Adam <ref type="bibr" target="#b6">(Kingma and Ba 2014)</ref> was used for optimization with an initial learning rate of 0.0002 for Lowe's Ubuntu dataset, and 0.0004 for the rest. The mini-batch size was set to 128 for DSTC7 datasets, 16 for the Lowe's Ubuntu dataset, and 32 for the E-commerce dataset. The hidden size of BiLSTMs and MLP was set to 300.</p><p>To make the sequences shorter than the maximum length, we cut off last tokens for the response but did the cut-off in the reverse direction for the context, as we hypothesized that the last few utterances in the context is more important than the first few utterances. For the Lowe's Ubuntu dataset, the maximum lengths of the context and response were set to 400 and 150, respectively; for the E-commerce dataset, 300 and 50; for the rest datasets, 300 and 30.</p><p>More specially, for subtask 2 of DSTC7 Ubuntu, we used BiLSTM hidden size 400 and 4 heads for sentence-encoding methods. For subtask 4, the candidate pool may not contain the correct next utterance, so we need to choose a threshold. When the probability of positive labels is smaller than the threshold, we predict that candidate pool doesn't contain the correct next utterance. The threshold was selected from the range [0.50, 0.51, .., 0.99] based on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Our results on all DSTC7 response selection subtasks were summarized in   sults rank first on 7 subtasks, rank second on subtask 2 of Ubuntu, and overall rank first on both datasets of the DSTC7 response selection challenge 4 . Subtask 3 may contain multiple correct responses, so Mean Average Precision (MAP) is considered as an extra metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Analysis</head><p>Ablation analysis is shown in <ref type="table" target="#tab_5">Table 4</ref> and 5 for the Ubuntu and Advising datasets, respectively. For Ubuntu subtask 1, ESIM achieved 0.854 R@10 and 0.6401 MRR. If we removed context's local matching and matching composition to accelerate the training process ("-CtxDec"), R@10 and MRR dropped to 0.845 and 0.6210. Further discarding the last words instead of the preceding words for the context ("-CtxDec &amp; -Rev") degraded R@10 and MRR to 0.840 and 0.6174. Ensembling the above three models ("Ensemble") achieved 0.887 R@10 and 0.6790 MRR. Ensembling was performed by averaging output from models trained with   different parameter initializations and different structures. For Ubuntu subtask 2, the sentence-encoding based methods ("Sent-based") achieved 0.082 R@10 and 0.0416 MRR. After ensembling several models with different parameter initializations ("Ensemble1"), R@10 and MRR were increased to 0.091 and 0.0475. Using ESIM to rerank the top 100 candidates predicted by "Ensemble1" achieved 0.125 R@10 and 0.0713 MRR. Removing context's local matching and matching composition ("-CtxDec") degraded R@10 and MRR to 0.117 and 0.0620. Ensembling the above two kinds of ESIM methods ("Ensemble2") achieved 0.134 R@10 and 0.0770 MRR.</p><p>For Ubuntu subtask 4, we observed similar trend with subtask 1. ESIM achieved 0.887 R@10 and 0.6434 MRR, "-CtxDec" degraded performance to 0.877 R@10 and 0.6277 MRR, and "-CtxDec &amp; -Rev" further degraded to 0.875 R@10 and 0.6212 MRR. Ensembling the above three models ("Ensemble") achieved 0.909 R@10 and 0.6771 MRR.</p><p>For Ubuntu subtask 5, the dataset is the same as subtask 1 except for using the external knowledge of Linux manual pages. Adding pre-trained word embeddings derived from Linux manual pages ("+W2V") resulted in 0.858 R@10 and 0.6394 MRR, comparable with ESIM without exploring the external knowledge. Ensembling the ensemble Models Ubuntu E-commerce R@1 R@2 R@5 R@1 R@2 R@5 TF-IDF <ref type="bibr" target="#b9">(Lowe et al. 2015)</ref> 0.410 0.545 0.708 0.159 0.256 0.477 RNN <ref type="bibr" target="#b9">(Lowe et al. 2015)</ref> 0.403 0.547 0.819 0.325 0.463 0.775 CNN <ref type="bibr" target="#b5">(Kadlec, Schmid, and Kleindienst 2015)</ref> 0.549 0.684 0.896 0.328 0.515 0.792 LSTM <ref type="bibr" target="#b5">(Kadlec, Schmid, and Kleindienst 2015)</ref> 0.638 0.784 0.949 0.365 0.536 0.828 BiLSTM <ref type="bibr" target="#b5">(Kadlec, Schmid, and Kleindienst 2015)</ref> 0.630 0.780 0.944 0.355 0.525 0.825 MV-LSTM <ref type="bibr" target="#b18">(Wan et al. 2016)</ref> 0.653 0.804 0.946 0.412 0.591 0.857 Match-LSTM <ref type="bibr" target="#b19">(Wang and Jiang 2016)</ref> 0.653 0.799 0.944 0.410 0.590 0.858 Attentive-LSTM <ref type="bibr" target="#b16">(Tan, Xiang, and Zhou 2015)</ref> 0.633 0.789 0.943 0.401 0.581 0.849 Multi-Channel <ref type="bibr" target="#b20">(Wu et al. 2017)</ref> 0.656 0.809 0.942 0.422 0.609 0.871 Multi-View <ref type="bibr" target="#b24">(Zhou et al. 2016)</ref> 0.662 0.801 0.951 0.421 0.601 0.861 DL2R <ref type="bibr" target="#b21">(Yan, Song, and Wu 2016)</ref> 0.626 0.783 0.944 0.399 0.571 0.842 SMN <ref type="bibr" target="#b20">(Wu et al. 2017)</ref> 0.726 0.847 0.961 0.453 0.654 0.886 DUA <ref type="bibr" target="#b23">(Zhang et al. 2018)</ref> 0.752 0.868 0.962 0.501 0.700 0.921 DAM <ref type="bibr" target="#b26">(Zhou et al. 2018)</ref> 0.767 0.874 0.969 ---Our ESIM 0.796 0.894 0.975 0.570 0.767 0.948 <ref type="table">Table 6</ref>: Comparison of different models on two large-scale public benchmark datasets. All the results except ours are cited from previous work <ref type="bibr" target="#b23">(Zhang et al. 2018;</ref><ref type="bibr" target="#b26">Zhou et al. 2018)</ref>.</p><p>model for subtask 1 (0.887 R@10 and 0.6790 MRR) and the "+W2V" model brought further gain, reaching 0.890 R@10 and 0.6817 MRR. <ref type="table" target="#tab_6">Table 5</ref> showed the ablation analysis on the development set for the Advising dataset. We used ESIM without context's local matching and matching composition for computational efficiency. We observed similar trends as on the Ubuntu data set. "-CtxDec &amp; -Rev" degraded R@10 and MRR over "-CtxDec", yet the ensemble of the two models always produced significant gains over individual models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Previous Work</head><p>The results on two public benchmarks were summarized in <ref type="table">Table 6</ref>. The first group of models includes sentenceencoding based methods. They use hand-craft features or neural network features to encode both context and response, then a cosine classifier or MLP classifier was applied to decide the relationship between the two sequences. Previous work used TF-IDF, RNN <ref type="bibr" target="#b9">(Lowe et al. 2015)</ref> and CNN, LSTM, BiLSTM <ref type="bibr" target="#b5">(Kadlec, Schmid, and Kleindienst 2015)</ref> to encode the context and the response.</p><p>The second group of models consists of sequence-based matching models, which usually use the attention mechanism, including MV-LSTM <ref type="bibr" target="#b18">(Wan et al. 2016</ref>), Matching-LSTM (Wang and Jiang 2016), Attentive-LSTM <ref type="bibr" target="#b16">(Tan, Xiang, and Zhou 2015)</ref>, and Multi-Channels <ref type="bibr" target="#b20">(Wu et al. 2017</ref>). These models compared the token-level relationship between the context and the response, rather than comparing the two dense vectors directly as in sentence-encoding based methods. These kinds of models achieved significantly better performance than the first group of models.</p><p>The third group of models includes more complicated hierarchy-based models, which usually models the tokenlevel and utterance-level information explicitly. Multi-View <ref type="bibr" target="#b24">(Zhou et al. 2016</ref>) model utilized utterance relationships from the word sequence view and utterance sequence view. DL2R model <ref type="bibr" target="#b21">(Yan, Song, and Wu 2016)</ref> employed neural networks to reformulate the last utterance with other utterances in the context. SMN model <ref type="bibr" target="#b20">(Wu et al. 2017</ref>) used CNN and attention to match a response with each utterance in the context. DUA <ref type="bibr" target="#b23">(Zhang et al. 2018</ref>) and DAM <ref type="bibr" target="#b26">(Zhou et al. 2018</ref>) applied a similar framework as SMN <ref type="bibr" target="#b20">(Wu et al. 2017)</ref>, where one improved with gated self attention and the other improved with the Transformer <ref type="bibr" target="#b17">(Vaswani et al. 2017)</ref>.</p><p>Although the previous hierarchy-based work claimed that they achieved the state-of-the-art performance by using the hierarchical structure of multi-turn context, our ESIM sequential matching model outperformed all previous models, including hierarchy-based models. On the Lowe's Ubuntu dataset, the ESIM model brought significant gains on performance over the previous best results from the DAM model, up to 79.6% (from 76.7%) R@1, 89.4% (from 87.4%) R@2 and 97.5% (from 96.9%) R@5. For the E-commerce dataset, the ESIM model also accomplished substantial improvement over the previous state of the art by the DUA model, up to 57.0% (from 50.1%) R@1, 76.7% (from 70.0%) R@2 and 94.8% (from 92.1%) R@5. These results demonstrated the effectiveness of the ESIM model, a sequential matching method, for multi-turn response selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Previous state-of-the-art multi-turn response selection models used hierarchy-based (utterance-level and token-level) neural networks to explicitly model the interactions among the different turns' utterances for context modeling. In this paper, we demonstrated that a sequential matching model based only on chain sequence can outperform all previous models, including hierarchy-based methods, suggesting that the potentials of such sequential matching approaches have not been fully exploited in the past. Specially, the proposed model achieved top one results on both datasets under the noetic end-to-end response selection challenge in DSTC7, and yielded new state-of-the-art performances on two largescale public multi-turn response selection benchmarks. Future work on multi-turn response selection includes exploring the efficacy of external knowledge , such as knowledge graph and user profile.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Two kinds of neural network-based methods for sentence pair classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Task description.</figDesc><table><row><cell></cell><cell></cell><cell>True or False?</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Multilayer Perceptron</cell><cell></cell></row><row><cell>True or False?</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Vector Representation</cell><cell></cell></row><row><cell cols="2">Multilayer Perceptron</cell><cell>Max, Mean</cell><cell>Max, Mean</cell></row><row><cell></cell><cell></cell><cell>BiLSTM</cell><cell>BiLSTM</cell></row><row><cell cols="2">Vector Representation</cell><cell></cell><cell></cell></row><row><cell>Multi-head Self-Attention Pooling</cell><cell>Multi-head Self-Attention Pooling</cell><cell>Local Matching Vectors</cell><cell>Local Matching Vectors</cell></row><row><cell>BiLSTM</cell><cell>BiLSTM</cell><cell>BiLSTM</cell><cell>BiLSTM</cell></row><row><cell>hi &lt;eou&gt; &lt;eot&gt; hello &lt;eou&gt; &lt;eot&gt; May I help you &lt;eou&gt; &lt;eot&gt;</cell><cell>I would like to check in</cell><cell>hi &lt;eou&gt; &lt;eot&gt; hello &lt;eou&gt; &lt;eot&gt; May I help you &lt;eou&gt; &lt;eot&gt;</cell><cell>I would like to check in</cell></row><row><cell cols="2">(a) Sentence encoding-based method.</cell><cell cols="2">(b) Cross attention-based method.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>The challenge ranking considers the average of Recall@10 and Mean Reciprocal Rank (MRR). On the Advising dataset, the test case 2 (Advising2) results were considered for ranking, because test case 1 (Advis-ing1) has some dependency on the training dataset. Our re-</figDesc><table><row><cell>Embedding</cell><cell>Training corpus</cell><cell>#Words</cell></row><row><cell>glove.6B.300d</cell><cell>Wikipedia + Gigaword</cell><cell>0.4M</cell></row><row><cell>glove.840B.300d</cell><cell>Common Crawl</cell><cell>2.2M</cell></row><row><cell>glove.twitter.27B.200d</cell><cell>Twitter</cell><cell>1.2M</cell></row><row><cell cols="2">wiki-news-300d-1M.vec Wikipedia + UMBC</cell><cell>1.0M</cell></row><row><cell>crawl-300d-2M.vec</cell><cell>Common Crawl</cell><cell>2.0M</cell></row><row><cell>word2vec.300d</cell><cell>Linux manual pages</cell><cell>0.3M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">: Statistics of pre-trained word embeddings. The 1-3</cell></row><row><cell cols="5">rows are from GloVe; 4-5 rows are from fastText; 6 is from</cell></row><row><cell>word2vec.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Subtask Measure</cell><cell cols="3">Ubuntu Advising1 Advising2</cell></row><row><cell></cell><cell>Recall@1</cell><cell>0.645</cell><cell>0.398</cell><cell>0.214</cell></row><row><cell>Subtask1</cell><cell>Recall@10 Recall@50</cell><cell>0.902 0.994</cell><cell>0.844 0.986</cell><cell>0.630 0.948</cell></row><row><cell></cell><cell>MRR</cell><cell>0.735</cell><cell>0.5408</cell><cell>0.339</cell></row><row><cell></cell><cell>Recall@1</cell><cell>0.067</cell><cell></cell><cell></cell></row><row><cell>Subtask2</cell><cell>Recall@10 Recall@50</cell><cell>0.185 0.266</cell><cell>NA</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>0.1056</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Recall@1</cell><cell></cell><cell>0.476</cell><cell>0.290</cell></row><row><cell></cell><cell>Recall@10</cell><cell></cell><cell>0.906</cell><cell>0.750</cell></row><row><cell>Subtask3</cell><cell>Recall@50</cell><cell>NA</cell><cell>0.996</cell><cell>0.978</cell></row><row><cell></cell><cell>MRR</cell><cell></cell><cell>0.6238</cell><cell>0.4341</cell></row><row><cell></cell><cell>MAP</cell><cell></cell><cell>0.7794</cell><cell>0.5327</cell></row><row><cell></cell><cell>Recall@1</cell><cell>0.624</cell><cell>0.372</cell><cell>0.232</cell></row><row><cell>Subtask4</cell><cell>Recall@10 Recall@50</cell><cell>0.941 0.997</cell><cell>0.886 0.990</cell><cell>0.692 0.938</cell></row><row><cell></cell><cell>MRR</cell><cell>0.742</cell><cell>0.5409</cell><cell>0.3826</cell></row><row><cell></cell><cell>Recall@1</cell><cell>0.653</cell><cell>0.398</cell><cell>0.214</cell></row><row><cell>Subtask5</cell><cell>Recall@10 Recall@50</cell><cell>0.905 0.995</cell><cell>0.844 0.986</cell><cell>0.630 0.948</cell></row><row><cell></cell><cell>MRR</cell><cell>0.7399</cell><cell>0.5408</cell><cell>0.339</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The submission results on the hidden test sets for the DSTC7 response selection challenge. NA -not applicable. In total, there are 8 test conditions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation analysis on the development set for the DSTC7 Ubuntu dataset.</figDesc><table><row><cell cols="2">Sub Models</cell><cell cols="3">R@1 R@10 R@50 MRR</cell></row><row><cell></cell><cell>-CtxDec</cell><cell>0.222</cell><cell>0.656</cell><cell>0.954 0.3572</cell></row><row><cell>1</cell><cell cols="2">-CtxDec &amp; -Rev 0.214</cell><cell>0.658</cell><cell>0.942 0.3518</cell></row><row><cell></cell><cell>Ensemble</cell><cell>0.252</cell><cell>0.720</cell><cell>0.960 0.4010</cell></row><row><cell></cell><cell>-CtxDec</cell><cell>0.320</cell><cell>0.792</cell><cell>0.978 0.4704</cell></row><row><cell>3</cell><cell cols="2">-CtxDec &amp; -Rev 0.310</cell><cell>0.788</cell><cell>0.978 0.4550</cell></row><row><cell></cell><cell>Ensemble</cell><cell>0.332</cell><cell>0.818</cell><cell>0.984 0.4848</cell></row><row><cell></cell><cell>-CtxDec</cell><cell>0.248</cell><cell>0.706</cell><cell>0.970 0.3955</cell></row><row><cell>4</cell><cell cols="2">-CtxDec &amp; -Rev 0.226</cell><cell>0.714</cell><cell>0.946 0.3872</cell></row><row><cell></cell><cell>Ensemble</cell><cell>0.246</cell><cell>0.760</cell><cell>0.970 0.4110</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation analysis on the development set for the DSTC7 Advising dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.taobao.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://spacy.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The official evaluation allows up to 3 different settings, but we only submitted one setting.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent neural network-based sentence encoder with gated attention for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the 2nd Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural natural language inference models enhanced with external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2406" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhancing sentence embedding with generalized pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018</title>
		<meeting>the 27th International Conference on Computational Linguistics, COLING 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1815" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved deep learning baselines for ubuntu corpus dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleindienst</surname></persName>
		</author>
		<idno>abs/1510.03753</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gouravajhala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athreya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gunasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ganhotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Polymenakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lasecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11118</idno>
		<title level="m">Analyzing assumptions in conversation disentanglement research through the lens of a new dataset and model</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2015 Conference</title>
		<meeting>the SIGDIAL 2015 Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation, LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th</title>
		<meeting>the 54th</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lstm-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2835" to="2841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT 2016</title>
		<meeting>NAACL HLT 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequential matching network: A new architecture for multiturn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrieval-based humancomputer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2016</title>
		<meeting>SIGIR 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Polymenakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gunasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lasecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">The 7th dialog system technology challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling multi-turn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-view response selection for human-computer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mechanism-aware neural machine for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3400" to="3407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-turn response selection for chatbots with deep attention matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
