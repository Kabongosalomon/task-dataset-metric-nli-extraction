<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University DATA61</orgName>
								<address>
									<region>CSIRO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University DATA61</orgName>
								<address>
									<region>CSIRO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University DATA61</orgName>
								<address>
									<region>CSIRO</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Zero-shot learning</term>
					<term>Object detection</term>
					<term>Zero-shot detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current Zero-Shot Learning (ZSL) approaches are restricted to recognition of a single dominant unseen object category in a test image. We hypothesize that this setting is ill-suited for real-world applications where unseen objects appear only as a part of a complex scene, warranting both the 'recognition' and 'localization' of an unseen category. To address this limitation, we introduce a new 'Zero-Shot Detection' (ZSD) problem setting, which aims at simultaneously recognizing and locating object instances belonging to novel categories without any training examples. We also propose a new experimental protocol for ZSD based on the highly challenging ILSVRC dataset, adhering to practical issues, e.g., the rarity of unseen objects. To the best of our knowledge, this is the first end-to-end deep network for ZSD that jointly models the interplay between visual and semantic domain information. To overcome the noise in the automatically derived semantic descriptions, we utilize the concept of meta-classes to design an original loss function that achieves synergy between max-margin class separation and semantic space clustering. Furthermore, we present a baseline approach extended from recognition to detection setting. Our extensive experiments show significant performance boost over the baseline on the imperative yet difficult ZSD problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since its inception, zero-shot learning research has been dominated by the object classification problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. Although it still remains as a challenging task, the zero-shot recognition has a number of limitations that render it unusable in real-life scenarios. First, it is destined to work for simpler cases where only a single dominant object is present in an image. Second, the attributes and semantic descriptions are relevant to individual objects instead of the entire scene composition. Third, zero-shot recognition provides an answer to unseen categories in elementary tasks, e.g., classification and retrieval, yet it is unable to scale to advanced tasks such as scene interpretation and contextual modeling, which require a fundamental reasoning about all salient objects in the scene. Fourth, global attributes are more susceptible to background variations, viewpoint, appearance and scale changes and practical factors such as occlusions and clutter. As a result, image-level ZSL fails for the case of complex scenes where a diverse set of competing attributes that do not belong to a single image-level category would exist.</p><p>To address these challenges, we introduce a new problem setting called the zero-shot object detection. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, instead of merely classifying images, our goal is to simultaneously detect and localize each individual instance of new object classes, even in the absence of any visual examples of those classes during the training phase. In this regard, we propose a new zero-shot detection protocol built on top of the ILSVRC -Object Detection Challenge <ref type="bibr" target="#b39">[40]</ref>. The resulting dataset is very demanding due to its large scale, diversity, and unconstrained nature, and also unique due to its leveraging on WordNet semantic hierarchy <ref type="bibr" target="#b30">[31]</ref>. Taking advantage of semantic relationships between object classes, we use the concept of 'meta-classes' 1 and introduce a novel approach to update the semantic embeddings automatically. Raw semantic embeddings are learned in an unsupervised manner using text mining and therefore they have considerable noise. Our optimization of the class embeddings proves to be an effective way to reduce this noise and learn robust semantic representations.</p><p>ZSD has numerous applications in novel object localization, retrieval, tracking, and reasoning about object's relationships with its environment using only available semantics, e.g., an object name or a natural language description. Although a critical problem, ZSD is remarkably difficult compared to its classification counterpart. While the zero-shot recognition problem assumes only a single primary object in an image and attempts to predict its category, the ZSD task has to predict both the multi-class category label and precise location of each instance in the given image. Since there can be a prohibitively huge number of possible locations for each object in an image and because the semantic class descriptions are noisy, a detection approach is much more susceptible to incorrect predictions compared to classification. Therefore, it would be expected that a ZSD method predicts a class label that might be incorrect but visually and semantically similar to the corresponding true class. For example, wrongly predicting a 'spider' as 'scorpion' where both are semantically similar because of being invertebrates. To address this issue, we relax the original detection problem to independently study the confusions emanating from the visual and semantic resemblance between closely linked classes. For this purpose, alongside the ZSD, we evaluate on zero-shot meta-class detection, zero-shot tagging, and zero-shot meta class tagging. Notably, the proposed network is trained only 'once' for ZSD task and the additional tasks are used during evaluations only.</p><p>Although deep network based solutions have been proposed for zero-shot recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51]</ref>, to the best of our knowledge, we propose the first endto-end trainable network for the ZSD problem that concurrently relates visual image features with the semantic label information. This network considers semantic embedding vector of classes as a fixed embedding within the network to produce prediction scores for both seen and unseen classes. We propose a novel loss formulation that incorporates max-margin learning <ref type="bibr" target="#b52">[53]</ref> and a semantic clustering loss based on class-scores of different meta-classes. While the max-margin loss tries to separate individual classes, semantic clustering loss tries to reduce the noise in semantic vectors by positioning similar classes together and dissimilar classes far apart. Notably, our proposed formulation assumes predefined unseen classes to explore the semantic relationships during model learning phase. This assumption is consistent with recent efforts in the literature which consider class semantics to solve the domain shift problem in ZSL <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> and does not a constitute transductive setting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. Based on the premise that unseen class semantics may be unknown during training in several practical zero-shot scenarios, we also propose a variant of our approach that can be trained without predefined unseen classes. Finally, we propose a comparison method for ZSD by extending a popular zero-shot recognition framework named ConSE <ref type="bibr" target="#b32">[33]</ref> using Faster-RCNN <ref type="bibr" target="#b37">[38]</ref>.</p><p>In summary, this paper reports the following advances:</p><p>-We introduce a new problem for zero-shot learning, which aims to jointly recognize and localize novel objects in complex scenes. -We present a new experimental protocol and design a novel baseline solution extended from conventional recognition to the detection task. -We propose an end-to-end trainable deep architecture that simultaneously considers both visual and semantic information. -We design a novel loss function that achieves synergistic effects for maxmargin class separation and semantic clustering based on meta-classes. Beside that, our approach can automatically tune noisy semantic embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Description</head><p>Given a set of images for seen object categories, ZSD aims at the recognition and localization of previously unseen object categories. In this section, we formally describe the ZSD problem and its associated challenges. We also introduce variants of the detection task, which are natural extensions of the original problem. First, we describe the notations used in the following discussion. Preliminaries: Consider a set of 'seen' classes denoted by S = {1, . . . , S}, whose examples are available during the training stage and S represents their total number. There exists another set of 'unseen' classes U = {S + 1, . . . , S + U}, whose instances are only available during the test phase. We denote the set of all object classes by C = S ∪ U, such that C = S + U denote the cardinality of the label space.</p><p>We define a set of meta (or super) classes by grouping similar object classes into a single meta category. These meta-classes are denoted by M = {z m : m ∈ [1, M]}, where M denote the total number of meta-classes and z m = {k ∈ C s.t., g(k) = m}. Here, g(k) is a mapping function which maps each class k to its corresponding meta-class z g(k) . Note that the meta-classes are mutually exclusive i.e., ∩ M m=1 z m = φ and ∪ M m=1 z m = C. The set of all training images is denoted by X s , which contains examples of all seen object classes. The set of all test images containing samples of unseen object classes is denoted by X u . Each test image x ∈ X u contains at least one instance of an unseen class. Notably, no unseen class object is present in X s , but X u may contain seen objects.</p><p>We define a d dimensional word vector v c (word2vec or GloVe) for every class c ∈ C. The ground-truth label for an i th bounding box is denoted by y i . The object detection task also involves identifying the background class for negative object proposals, we introduce the extended label sets: S = S ∪ y bg , C = C ∪ y bg and M = M ∪ y bg , where y bg = {C + 1} is a singleton set denoting the background label.</p><p>Task Definitions: Given the observed space of images X = X s ∪ X u and the output label space C , our goal is to learn a mapping function f : X → C which gives the minimum regularized empirical risk (R) as follows:</p><formula xml:id="formula_0">arg min f ∈FR (f (x; Θ)) + Ω(Θ),<label>(1)</label></formula><p>where, x ∈ X s during training, Θ denotes the set of parameters and Ω(Θ) denotes the regularization on the learned weights. The mapping function has the following form:</p><formula xml:id="formula_1">f (x; Θ) = arg max y∈C max b∈B(x) F(x, y, b; Θ),<label>(2)</label></formula><p>where F(·) is a compatibility function, B(x) is the set of all bounding box proposals in a given image x. Intuitively, Eq. 2 finds the best scoring bounding boxes for each object category and assigns them the maximum scoring object category. Next, we define the zero-shot learning tasks which go beyond a single unseen category recognition in images. Notably, the training is framed as the challenging ZSD problem, however the remaining task descriptions are used during evaluation to relax the original problem: T1 Zero-shot detection (ZSD): Given a test image x ∈ X u , the goal is to categorize and localize each instance of an unseen object class u ∈ U. T2 Zero-shot meta-class detection (ZSMD): Given a test image x ∈ X u , the goal is to localize each instance of an unseen object class u ∈ U and categorize it into one of the super-classes m ∈ M. T3 Zero-shot tagging (ZST): To recognize one or more unseen classes in a test image x ∈ X u , without identifying their location. T4 Zero-shot meta-class tagging (ZSMT): To recognize one or more meta-classes in a test image x ∈ X u , without identifying their location.</p><p>Among the above mentioned tasks, the ZSD is the most difficult problem and difficulty level decreases as we go down the list. The goal of the later tasks is to distill the main challenges in ZSD by investigating two ways to relax the original problem: (a) The effect of reducing the unseen object classes by clustering similar unseen classes into a single super-class (T2 and T4). (b) The effect of removing the localization constraint. To this end we investigate the zero-shot tagging problem, where the goal is to only recognize all object categories in an image (T3 and T4).</p><p>The state-of-the-art in zero-shot learning deals with only recognition/tagging. The proposed problem settings add the missing detection task which indirectly encapsulates traditional recognition and tagging task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Zero-Shot Detection</head><p>Our proposed model uses Faster-RCNN <ref type="bibr" target="#b37">[38]</ref> as a backbone architecture, due to its superior performance among competitive end-to-end detection models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>. We first provide an overview of our proposed model architecture and then discuss network learning. Finally, we extend a popular ZSL approach to the detection problem, against which we compare our performance in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>The overall architecture is illustrated in <ref type="figure" target="#fig_1">Fig 2.</ref> It has two main components marked in color: the first provides object-level feature descriptions and the second integrates visual information with the semantic embeddings to perform zero-shot detection. We explain these in detail next.</p><p>Object-level Feature Encoding: For an input image x, a deep network (VGG or ResNet) is used to obtain the intermediate convolutional activations. These activations are treated as feature maps, which are forwarded to the Region Proposal Network (RPN). The RPN generates a set of candidate object proposals by automatically ranking the anchor boxes at each sliding window location. The high-scoring candidate proposals can be of different sizes, which are mapped to fixed sized representation using a RoI pooling layer which operates on the initial feature maps and the proposals generated by the RPN. The resulting object level features for each candidate are denoted as 'f '. Note that the RPN generates object proposal based on the objectness measure. Thus, a trained RPN on seen objects can generate proposals for unseen objects also. In the second block of our architecture, these feature representations are used alongside the semantic embeddings to learn useful representations for both the seen and unseen objectcategories.</p><p>Integrating Visual and Semantic Contexts: The object-level feature f is forwarded to two branches in the second module. The top branch is trained to predict the object category for each candidate box. Note that this can assign a class c ∈ C , which can be a seen, unseen or background category. The branch consists of two main sub-networks, which are key to learning the semantic relationships between seen and unseen object classes.</p><p>The first component is the 'Semantic Alignment Network ' (SAN), which consist of an adjustable FC layer, whose parameters are denoted as W 1 ∈ R d×d , that projects the input visual feature vectors to a semantic space with d dimensions. The resulting feature maps are then projected onto the fixed semantic embeddings, denoted by W 2 ∈ R d×(C+1) , which are obtained in an unsupervised manner by text mining (e.g., Word2vec and GloVe embeddings). Note that, here we consider both seen and unseen semantic vectors which require unseen classes to be predefined. This consideration is inline with a very recent effort <ref type="bibr" target="#b11">[12]</ref> which adopt this setting to explore the cluster manifold structure of the semantic embedding space and address domain shift issue. Given a feature representation input to SAN in the top branch, f t , the overall operation can be represented as:</p><formula xml:id="formula_2">o = (W 1 W 2 ) T f t .<label>(3)</label></formula><p>Here, o is the output prediction score. The W 2 is formed by stacking semantic vectors for all classes, including the background class. For background class, we use the mean word vectors v b = 1 C C c=1 v c as its embedding in W 2 . Notably, a non-linear activation function is not applied between the adjustable and fixed semantic embeddings in the SAN. Therefore, the two projections can be understood as a single learnable projection on to the semantic embeddings of object classes. This helps in automatically updating the semantic embeddings to make them compatible with the visual feature domain. It is highly valuable because the original semantic embeddings are often noisy due to the ambiguous nature of closely related semantic concepts and the unsupervised procedure used for their calculation. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we visualize modified embedding space when different loss functions are applied during training.</p><p>The bottom branch is for bounding box regression to add suitable offsets to the proposals to align them with the ground-truths such that the precise location of objects can be predicted. This branch is set up in the same manner as in Faster-RCNN <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training and Inference</head><p>We follow a two step training approach to learn the model parameters. The first part involves training the backbone Faster-RCNN for only seen classes using the training set X s . This training involves initializing weights of shared layers with a pre-trained Vgg/ResNet model, followed by learning the RPN, classification and detection networks. In the second step, we modify the Faster-RCNN model by replacing the last layer of Faster-RCNN classification branch with the proposed semantic alignment network and an updated loss function (see <ref type="figure" target="#fig_1">Fig. 2</ref>). While rest of the network weights are used from the first step, the weights W 1 are randomly initialized and the W 2 are fixed to semantic vectors of the object classes and not updated during training.</p><p>While training in second step, we keep the shared layers trainable but fix the layers specific to RPN since the object proposals requirements are not changed from the previous step. The same seen class images X s are used for training, consistent with the first step. For each given image, we obtain the output of RPN which consists of a total of 'R' ROIs belonging to both positive and negative object proposals. Each proposal has a corresponding ground-truth label given by y i ∈ S . Positive proposals belong to any of the seen class S and negative proposals contain only background. In our implementation, we use an equal number of positive and negative proposals. Now, when object proposals are passed through ROI-Pooling and subsequent dense layers, a feature representation f i is calculated for each ROI. This feature is forwarded to two branches, the classification branch and regression branch. The overall loss is the summation of the respective losses in these two branches, i.e., classification loss and bounding box regression loss.</p><formula xml:id="formula_3">L(o i , b i , y i , b * i ) = arg min Θ 1 T i L cls (o i , y i ) + L reg (b i , b * i )</formula><p>where Θ denotes the parameters of the network, o i is the classification branch output, T = N × R represents the total number of ROIs in the training set with N images. b i and b * i are parameterized coordinates of predicted and groundtruth bounding boxes respectively and y i represents the true class label of the i th object proposal. Classification loss: This loss deals with both seen and unseen classes. It has two components: a max-margin loss (L mm ) and a meta-class clustering loss (L mc ).</p><formula xml:id="formula_4">L cls (o i , y i ) = λL mm (o i , y i ) + (1 − λ)L mc (o i , g(y i )),<label>(4)</label></formula><p>where, λ is a hyper-parameter that controls the trade-off between the two losses. We define,</p><formula xml:id="formula_5">L mm (o i , y i ) = 1 |C \ y i | c∈C \yi log 1 + exp(o c − o yi ) , and L mc (o i , g(y i )) = 1 |M \ z g(yi) ||z g(yi) | c∈M \z g(y i ) j∈z g(y i ) log 1 + exp(o c − o j )</formula><p>where, o k represents the prediction response of class k ∈ S. L mm tries to separate the prediction response of true class from rest of the classes. In contrast, L mc tries to cluster together the members of each super-class and pulls further apart the classes belonging to different meta-classes.</p><p>We illustrate the effect of clustering loss on the learned embeddings in <ref type="figure" target="#fig_2">Fig. 3</ref>. The use of L mc enables us to cluster semantically similar classes together which results in improved embeddings in the semantic space. For example, all animals related meta-classes are in close position whereas food and vehicle are far apart. Such a clear separation in semantic space helps in obtaining a better ZSD performance. Moreover, meta-class based clustering loss does not harm fine-grained detection because the hype-parameter λ is used to put more emphasis on the max-margin loss (L mm ) as compared to the clustering part (L mc ) of the overall loss (L cls ). Still, the clustering loss provides enough guidance to the noisy semantic embeddings (e.g., unsupervised w2v/glove) such that similar classes are clustered together as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Note that w2v/glove try to place similar words nearby with respect to millions of text corpus, it is therefore not fine-tuned for just 200 class recognition setting.</p><p>Regression loss: This part of the loss is similar to faster-RCNN regression loss which fine-tunes the bounding box for each seen class ROI. For each f i , we get 4×S values representing 4 parameterized co-ordinates of the bounding box of each object instance. The regression loss is calculated based on these co-ordinates and parameterized ground truth co-ordinates. During training, no bounding box prediction is done for background and unseen classes due to unavailability of visual examples. As an alternate approach, we approximate the bounding box for an unseen object through the box proposal for a closely related seen object that achieves maximum response. This is a reasonable approximation because visual features of unseen classes are related to that of similar seen classes.</p><p>Prediction: We normalize each output prediction value of classification branch usingô c = oc vc 2 f t 2 . It basically calculates the cosine similarity between modified word vectors and image features. This normalization maps the prediction values within 0 to 1 range. We classify an object proposal as background if maximum responds amongô c where c ∈ C belongs to y bg . Otherwise, we detect an object proposal as unseen object if its maximum prediction response amongô u where u ∈ U is above a threshold α.</p><formula xml:id="formula_6">y u = arg max u∈Uô u s.t.,ô u &gt; α.<label>(5)</label></formula><p>The other detection branch finds b i which is the parameterized co-ordinates of bounding boxes corresponds to S seen classes. Among them, we choose a bounding box corresponding to the class having the maximum prediction response in o s where s ∈ S for the classified unseen class y u . For the tagging tasks, we simply use the mapping function g(.) to assign a meta-class for any unseen label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ZSD without Pre-defined Unseen</head><p>While applying clustering loss in Sec. 3.2, the meta-class assignment adds highlevel supervision in the semantic space. While doing this assignment, we consider both seen and unseen classes. Similarly, the max-margin loss considers the set C consisting of both seen and unseen classes. This problem setting helps to identify the clustering structure of the semantic embeddings to address domain adaptation for zero-shot detection. However, in several practical scenarios, unseen classes may not be known during training. Here, we report a simplified variant of our approach to train the proposed network without pre-defined unseen classes. For this problem setting, we use only seen+bg word vectors (instead of seen+unseen+bg vectors) as the fixed embedding W 2 ∈ R d×(S+1) to train the whole framework with only the max-margin loss, L mm , defined as follows:</p><formula xml:id="formula_7">L mm (o i , y i ) = 1 |S \yi| c∈S \yi log 1 + exp(o c − o yi ) .</formula><p>Since the output classification layer cannot make predictions for unseen classes, we apply a procedure similar to ConSE during the testing phase <ref type="bibr" target="#b32">[33]</ref>. The choice of <ref type="bibr" target="#b32">[33]</ref> here is made due to two main reasons: (a) In contrast to other ZSL methods which train separate models for each class <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref>, ConSE can work on the prediction score of a single end-to-end framework. (b) It is straight-forward to extend a single network to ZSD along with ConSE, since <ref type="bibr" target="#b32">[33]</ref> uses semantic embeddings only during the test phase.</p><p>Suppose, for an object proposal, o ∈ R S+1 is the vector containing final probability values of only seen classes and background. As described earlier, we ignore the object proposal if the background class get highest probability score. For other cases, we sort the vector o in descending order to compute a list of indices l and the sorted listô:</p><formula xml:id="formula_8">o, l = sort(o) s.t., o j =ô lj .<label>(6)</label></formula><p>Then, top K score values (s.t., K ≤ S) fromô are combined with their corresponding word vectors using the equation:</p><formula xml:id="formula_9">e i = K k=1ô k v l k .</formula><p>We consider e i as a semantic space projection of an object proposal which is a combination of word vectors weighted by top K seen class probabilities. The final prediction is made by finding the maximum cosine similarity among e i and all unseen word vectors,</p><formula xml:id="formula_10">y u = arg max u∈U cos(e i , v u ).</formula><p>In this paper, we use K = 10 as proposed in <ref type="bibr" target="#b32">[33]</ref>. For bounding box detection, we choose the box for which corresponding seen class gets maximum score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Experiment Protocol</head><p>Dataset: We evaluate our approach on the standard ILSVRC-2017 detection dataset <ref type="bibr" target="#b39">[40]</ref>. This dataset contains 200 object categories. For training, it includes 456,567 images and 478,807 bounding box annotations around object instances. The validation dataset contains 20,121 images fully annotated with the 200 object categories which include 55,502 object instances. A category hierarchy has been defined in <ref type="bibr" target="#b39">[40]</ref>, where some objects have multiple parents. Since, we also evaluate our approach on meta-class detection and tagging, we define a single parent for each category (see supplementary material for detail).</p><p>Seen/unseen split: Due to lack of an existing ZSD protocol, we propose a challenging seen/unseen split for ILSVRC-2017 detection dataset. Among 200 object categories, we randomly select 23 categories as unseen and rest of the 177 categories are considered as seen. This split is designed to follows the following practical considerations: (a) unseen classes are rare, (b) test categories should be diverse, (c) the unseen classes should be semantically similar with at least some of the seen classes. The details of split are provided in supplementary material.</p><p>Train/test set: A zero-shot setting does not allow any visual example of an unseen class during training. Therefore, we customize the training set of ILSVRC such that images containing any unseen instance are removed. This results in a total of 315,731 training images with 449,469 annotated bounding boxes. For testing, the traditional zero-shot recognition setting is used which considers only unseen classes. As the test set annotations are not available to us, we cannot separate unseen classes for evaluation. Therefore, our test set is composed of the left out data from ILSVRC training dataset plus validation images having at least one unseen bounding box. The resulting test set has 19,008 images and 19,931 bounding boxes. Semantic embedding: Traditionally ZSL methods report performance on both supervised attributes and unsupervised word2vec/glove as semantic embeddings. As manually labeled supervised attributes are hard to obtain, only small-scale datasets with these annotations are available <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. ILSVRC-2017 detection dataset used in the current work is quite huge and does not provide attribute annotations. In this paper, we work on 2 normalized 500 and 300 dimensional unsupervised word2vec <ref type="bibr" target="#b29">[30]</ref> and GloVe <ref type="bibr" target="#b34">[35]</ref> vector respectively to describe the classes. These word vectors are obtained by training on several billion words from Wikipedia dump corpus.</p><p>Evaluation Metric: We report average precision (AP) of individual unseen classes and mean average precision (mAP) for the overall performance of unseen classes.</p><p>Implementation Details: Unlike Faster-RCNN, our first step is trained in one step: after initializing shared layer with pre-trained weights, RPN and detection network of Fast-RCNN layers are learned together. Some other settings includes rescaling shorter size of image as 600 pixels, RPN stride = 16, three anchor box scale 128, 256 and 512 pixels, three aspect ratios 1:1, 1:2 and 2:1, non-maximum suppression (NMS) on proposals class probability with IoU threshold = 0.7. Each mini-batch is obtained from a single image having 16 positive and 16 negative (background) proposals. Adam optimizer with learning rate 10 −5 , β 1 = 0.9 and β 2 = 0.999 is used in both state training. First step is trained over 10 million mini-batches without any data augmentation, but data augmentation through repetition of object proposals is used in second step (details in supplementary material). During testing, the prediction score threshold was 0.1 for baseline and Ours (with L mm ) and 0.2 for clustering method (Ours with L cls ). We implement our model in Keras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ZSD Performance</head><p>We compare different versions of our method (with loss configurations L mm and L cls respectively) to a baseline approach. Note that the baseline is a simple extension of Faster-RCNN <ref type="bibr" target="#b37">[38]</ref> and ConSE <ref type="bibr" target="#b32">[33]</ref>. We apply the inference strategy mentioned in Sec. 3.3 after first step training as we can still get a vector o ∈ R S+1 on the classification layer of Faster-RCNN network. We use two different architectures i.e., VGG-16 (V) <ref type="bibr" target="#b41">[42]</ref> and ResNet-50 (R) <ref type="bibr" target="#b12">[13]</ref> as the backbone of the Faster-RCNN during the first training step. In second step, we experiment   <ref type="table" target="#tab_5">Table 2</ref>. Average precision of individual unseen classes using ResNet+w2v and loss configurations L mm and L cls (cluster based loss with λ = 0.6). We have grouped unseen classes into two groups based on whether visually similar classes present in the seen class set or not. Our proposed method achieve significant performance improvement for the group where similar classes are present in the seen set.</p><p>with both Word2vec and GloVe as the semantic embedding vectors used to define W 2 . <ref type="figure" target="#fig_4">Fig. 4</ref> illustrates some qualitative ZSD examples. More performance results of ZSD on other datasets is provided in the supplementary material.</p><p>Overall results: <ref type="table" target="#tab_0">Table 1</ref> reports the mAP for all approaches on four tasks: ZSD, ZSMD, ZST, and ZSMT across different combinations of network architectures. We can make following observations: (1) Our cluster based method outperforms other competitors on all four tasks because its loss utilizes highlevel semantic relationships from meta-class definitions which are not present in other methods. (2) Performances get improved from baseline to Ours (with L mm ) across all zero-shot tasks. The reason is baseline method did not consider word vectors during the training. Thus, overall detection could not get enough supervision about the semantic embeddings of classes. In contrast, L mm loss formulation considers word vectors. (3) Performances get improved from ZST to ZSMT across all methods whereas similar improvement is not common from ZSD to ZSMD. It's not surprising because ZSMD can get some benefit if meta-class of the predicted class is same as the meta-class of true class. If this is violated frequently, we cannot expect significant performance improvement in ZSMD. (4) In comparison of traditional object detection results, ZSD achieved significantly lower performance. Remarkably, even the state-of-the-art zero-shot classification approaches perform quite low e.g., a recent ZSL method <ref type="bibr" target="#b50">[51]</ref> reported 11% hit@1 rate on ILSVRC 2010/12. This trend does not undermine to significance of ZSD, rather highlights the underlying challenges.</p><p>Individual class detection: Performances of individual unseen classes indicate the challenges for ZSD. In <ref type="table" target="#tab_5">Table 2</ref>, we show performances of individual unseen classes across all tasks with our best (R+w2v) network. We observe that the unseen classes for which visually similar classes are present in their meta-  For the easier tagging tasks (ZST and ZSMT), the cluster method gets superior performance in most of the cases. This indicates that one potential reason for the failure cases of our cluster method for ZSD might be confusions during localization of objects due to ambiguities in visual appearance of unseen classes. Varying λ: The hyperparameter λ controls the weight between L mm and L mc in L cls . In <ref type="figure" target="#fig_4">Fig. 4</ref>, we illustrate the effect of varying λ on four zero-shot tasks for R+w2v and R+glo. It shows that performances has less variation in the range of λ = .5 to .9 than λ = .9 to 1. For a larger λ, mAP starts dropping since the impact of L mc decreases significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Zero Shot Recognition (ZSR)</head><p>Being a detection model, the proposed network can also perform traditional ZSR. We evaluate ZSR performance on popular Caltech-UCSD Birds-200-2011 (CUB) dataset <ref type="bibr" target="#b43">[44]</ref>. This dataset contains 11,788 images from 200 classes and provides single bounding boxes per image. Following standard train/test split <ref type="bibr" target="#b46">[47]</ref>, we use 150 seen and 50 unseen classes for experiments. For semantics embedding, we use 400-d word2vec (w2v) and GloVe (glo) vector <ref type="bibr" target="#b45">[46]</ref>. Note that, we do not use per image part annotation (like <ref type="bibr" target="#b0">[1]</ref>) and descriptions (like <ref type="bibr" target="#b50">[51]</ref>) to enrich semantic embedding. For a given test image, our network predicts unseen class bounding boxes. We pick only one label with the highest prediction score per image. In this way, we report the mean Top1 accuracy of all unseen classes in <ref type="table" target="#tab_3">Table 3</ref>. One can find our proposed solution achieve significant performance improvement in comparison with state-of-the-art methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Challenges and New Directions</head><p>ZSD is challenging: Our empirical evaluations show that ZSD needs to deal with the following challenges: (1) Unseen classes are rare compared to seen classes; (2) Small unseen objects are hard to detect and harder to relate with their semantics; (3) The scarcity of similar seen class leads to an inadequate description of an unseen class; (4) As derived in an unsupervised manner, the noise of semantic space affects ZSD. These issues are discussed in detail in supplementary material. Future challenges: The ZSD problem warrants further investigation. (1) Unlink current work one can consider fine-tuning the bounding box of the both seen and unseen classes based on visual and semantic correspondences. (2) Rather mapping image feature to the semantic space, the reverse mapping may help ZSD similar to ZSR used in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">51]</ref>. (3) One can consider the fusion of different word vectors (word2vec and GloVe) to improve ZSD. (4) Like generalized ZSL <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b23">24]</ref>, one can extend it to a more realistic generalized ZSD. Moreover, weakly supervised or semi-supervised version of zero shot problems is also possible while performing ZSD/GZSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>While traditional ZSL research focuses on only object recognition, we propose to extend the problem to object detection (ZSD). To this end, we offer a new experimental protocol with ILSVRC-2017 dataset specifying the seen-unseen, train-test split. We also develop an end-to-end trainable CNN model to solve this problem. We show that our solution is better than a strong baseline.</p><p>Overall, this research throws some new challenges to ZSL community. To make a long-standing progress in ZSL, the community needs to move forward in the detection setting rather than merely recognition.</p><p>End-to-end Object detection: Though object detection has been extensively studied in the literature, we can only find a few end-to-end learning pipelines capable of simultaneous object localization and classification. Popular examples of such approaches are Faster R-CNN <ref type="bibr" target="#b37">[38]</ref>, R-FCN <ref type="bibr" target="#b16">[17]</ref>, SSD <ref type="bibr" target="#b27">[28]</ref> and YOLO <ref type="bibr" target="#b36">[37]</ref>. The contribution of these methods relies on object localization process. Methods like Faster R-CNN <ref type="bibr" target="#b37">[38]</ref>, R-FCN <ref type="bibr" target="#b16">[17]</ref> are based on Region Proposal Network (RPN) which provides bounding box proposals of possible objects and then classifying and fine tuning the box prediction in the later layers. In contrast, methods like SSD <ref type="bibr" target="#b27">[28]</ref> and YOLO <ref type="bibr" target="#b36">[37]</ref> draw bounding box and classify it in a single step. Unlike RPN; these methods predict bounding box offset of some pre-defined anchors instead of the box co-ordinates itself. The later methods are generally faster than the previous ones. However, RPN based methods are better in terms of accuracy. In current work, we prioritize accuracy over speed. Therefore, we build zero-shot object detection model based on the Faster RCNN.</p><p>Semantic embedding: Any zero-shot task like recognition or tagging requires semantic information of classes. This semantic information works as a bridge among seen and unseen classes. The common way to preserve the semantic information of a class is by using a one-dimensional vector. The vector space that holds semantic information of classes is called 'semantic embedding space'. Visually similar classes reside in a close position in this space. The semantic vector of any class can be generated both manually or automatically. The manually generated semantic vectors are often called attributes <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b20">21]</ref>. Although attributes can describe a class with less noise (than other kinds of embeddings), those are very hard to obtain because of manual annotations. As a workaround, automatic semantic embedding can be generated from a large corpus of unannotated text like (Wikipedia, news article, etc.) or hierarchical relationship of classes in WordNet <ref type="bibr" target="#b30">[31]</ref>. Some popular examples of such kind of semantic embeddings are word2vec <ref type="bibr" target="#b29">[30]</ref>, GloVe <ref type="bibr" target="#b34">[35]</ref>, and hierarchies <ref type="bibr" target="#b45">[46]</ref>. As generated by an unsupervised manner, such embeddings become noisy but provide more flexibility and scalability than manual vectors.</p><p>Zero-shot learning: Humans can recognize an object by relating known objects, without prior visual experience. Simulating this behavior into an automated machine vision system is called Zero-shot learning (ZSL). ZSL attempts to recognize unseen objects without any visual examples of the unseen category. In recent years, numerous effective methods for ZSL have been proposed. Every ZSL strategy has to relate seen and unseen embedding through semantic embedding vector. Based on how this relation is established, we can categorize ZSL strategies into three types. The first type of methods attempt to predict the semantic vector of classes <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50]</ref>. An object is classified as an unseen class based on similarity of predicted vector and semantic vectors of unseen classes. Predicting a high dimensional vector is not an efficient way to related seenunseen classes because it cannot work consistently if the semantic vectors are noisy <ref type="bibr" target="#b14">[15]</ref>. This reason provokes this kind of methods to use attributes as semantic embedding as they are less noisy. The second kind of methods learn a linear <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b38">39]</ref> or non-linear <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b42">43]</ref> compatibility function to relate the seen image feature and corresponding semantic vector. This compatibility function yields high value if visual feature and semantic vector come from the same class and vice versa. A visual feature is classified to an unseen class if it gets the best compatibility score among all possible unseen classes. Such methods work consistently across a wide variety of semantic embedding vectors. The third kind of methods describe unseen classes by mixing seen visual features and semantic embedding <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b51">52]</ref>. For this mixing purpose, sometimes methods perform per class learning and later combine individual class output to decide outputs for unseen classes. While most of the ZSL approaches convert visual feature to semantic spaces, <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">51]</ref> mapped semantic vectors to the visual domain to address the hubness problem during prediction <ref type="bibr" target="#b40">[41]</ref>. Irrespective of method types, attributes work better as semantic embeddings compared to unsupervised word2vec, GloVe, and hierarchies because of less noise. To minimize this performance gap, researchers have investigated transductive setting <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b23">24]</ref>, domain adaptation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref> and class-attribute association <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> techniques. Usually, all ZSL methods are evaluated on a restricted case of recognition problem where test data only contain unseen images. Few recent efforts performed experiments on generalized version of ZSL <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b23">24]</ref>. They found that established ZSL methods perform poorly in such settings. Still, all these methods perform a recognition task in zero-shot settings. In this paper, we extend recognition problem to a more complex detection problem.</p><p>Zero-shot image tagging: Instead of assigning one unseen label to an image during recognition task, zero-shot tagging allows to tag multiple unseen tags to an image and/or ranking the array of unseen tags. Very few papers addressed the zero-shot version of this problem <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b52">53]</ref>. Li et al. <ref type="bibr" target="#b22">[23]</ref> applied the idea of <ref type="bibr" target="#b32">[33]</ref> in tagging. They argued that semantic embeddings (like word2vec) of all possible tags may not be available, and therefore, proposed a hierarchical semantic embedding method for those unavailable tags based on its ancestor classes in WordNet hierarchy. <ref type="bibr" target="#b10">[11]</ref> considered the power set of fixed unseen tags as the label set to perform transductive multi-label learning. Recently, <ref type="bibr" target="#b52">[53]</ref> proposed a fast zero-shot tagging approach that can rank both seen and arbitrary unseen tags during the testing stage. All previous attempts are not end-to-end because they preform training on the top of pre-trained CNN features. In this paper, we propose an end-to-end method for seen detection with zero-shot object tagging.</p><p>Object-level attribute reasoning: Object level attribute reasoning has been studied under two themes in the literature. The first theme advocates the use of object-level semantic representations in a traditional ZSL setting. Li et al. <ref type="bibr" target="#b24">[25]</ref> proposed to use local attributes and employed these shared characteristics to obtain zero-shot classification and segmentations. However, they dealt with fine-grained categorization task, where both seen and unseen objects have similar shapes (and segmentation masks), there is a single dominant category in each image and work with only supervised attributes. Another approach aiming at zero-shot segmentation is to learn a shape space shared with the novel objects. This technique, however, can only segment new object shapes that are very similar to the training set <ref type="bibr" target="#b15">[16]</ref>. Along the second theme, some efforts have more recently been reported for object localization and tracking using natural language descriptions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>. Different to our problem, they assume an accurate semantic description of the object, use supervised examples of objects during training, and therefore do not tackle the zero-shot detection problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset and Experiment Protocol</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Meta-class assignment</head><p>The classes of ILSVRC detection dataset maintain a defined hierarchy <ref type="bibr" target="#b39">[40]</ref>. However, this hierarchy does not follow a tree structure. In this paper, we choose a total of M = 14 meta-classes (including person), in which the 200 object classes are divided. <ref type="table" target="#tab_0">Table 1</ref> describes meta-class assignment of all 200 classes. This assignment mostly follows the hierarchy of question prescribed in the original paper <ref type="bibr" target="#b39">[40]</ref>. Few notable exceptions are (1) the classes of first-aid/medical items, cosmetics, carpentry items, school supplies and bag are grouped as indoor accessory, (2) liquid container related classes are merged with kitchen items, (3) flower pot is considered as furniture similar to MicroSoft COCO super-categories <ref type="bibr" target="#b26">[27]</ref>, (4) All living organisms (other than people) related classes are grouped into three different meta-class categories based on their similarity in word vector embedding space: invertebrate, mammal and non-mammal animal. Although one can argue that all invertebrate are non-mammal, this is just an assignment definition we apply in this paper to obtain a uniform distribution of images across super-classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Train/Test Split</head><p>Since the unseen classes are rare in real life settings and therefore their images are hard to collect, we assume that the training set only contains frequent classes. For ILSVRC detection dataset, number of instances per class follows a long-tail distribution <ref type="figure" target="#fig_0">(Figure 1</ref>). For each of our defined meta-class categories, we first plot the instance distribution of the child classes like <ref type="figure" target="#fig_2">Figure 3</ref>. Then, we randomly select one or two classes (depending on the number of child classes) from the rare second half of the distribution. We choose two unseen classes from the meta-classes which have relatively large (9 or more) number of child classes. In contrast, we choose one class as unseen for the meta-classes having less number of child classes. The only exception is that we do not choose 'Person' meta-class as unseen because it has no similar child class. This random selection procedure avoids biasness, ensures diversity (due to selection from all meta-classes) and conforms to the observation that unseen classes are not frequent.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Data Augmentation</head><p>We visualize the long-tail distribution of ILSVRC detection classes in <ref type="figure" target="#fig_0">Figure 1</ref>. One can find that only 11 highly frequent classes (out of 200) cover top 50% of the distribution. This distribution creates a significant impact on ZSD. To address this problem, in the second step of training, we augment the less frequent data to make a balance among similar seen classes for each unseen category. From the 10 million mini-batches used at the first stage of training, we create a set of over 2.8 million mini-batches for the second stage training. While creating this set, we make sure that every unseen class gets at least 10K similar (positive) instances from classes whose meta-class category is common to that of unseen class. In doing so, for some unseen classes like 'ray', we need to randomly augment data by repetition because the total instances of classes in the meta-class 'non-mammal animal' are not more than 10K. In contrast, the unseen class like 'tiger' has more than 10K similar instances in 'mammal animal' meta-class. Therefore, we randomly pick 10K among those to balance the training set. After this, the rest of instances of 2.8 million mini-batches are chosen as the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ZSD on CUB</head><p>We evaluate the ZSD performance of the baseline and our proposed method based on single bounding box per image provided in CUB dataset <ref type="bibr" target="#b43">[44]</ref>. <ref type="table" target="#tab_5">Table  2</ref> describes the performance comparison between the baseline and our basic method. Our overall loss (L cls ) based method outperforms the baseline in the different network and semantic settings. Note that, we do not define any metaclass for the CUB classes. Therefore, we use λ = 1 for CUB related experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Further Analysis</head><p>ZSD Challenges: In general, detection is a harder task than recognition/tagging because of locating the bounding box at the same time. The strict requirement of not using any unseen class images during training of zero-shot setting is itself a tough condition for recognition/tagging task which gets intensified to a high degree for detection task. We have used ILSVRC-2017 detection dataset to evaluate some baseline performances of the proposed problem. This dataset has 200 classes including a total 478,807 object instances of different shapes/size and distribution (See <ref type="figure" target="#fig_1">Figure 2)</ref>. Within those, we define M = 14 meta classes which contain one or more specific classes. <ref type="figure" target="#fig_2">Figure 3</ref> describes the normalized number of instances per classes within meta class. Considering this challenging dataset, here we describe some other difficulties of the zero shot detection task: Rarity: ILSVRC dataset contains a long-tail distribution issue, i.e., many rare classes get less number of instances. It is apparent that an unseen class should be within the set of rare classes. To address this fact, we randomly choose unseen classes from each meta-class z j which lies in the rarest 50% in the distribution. It affects the zero-shot version of the problem also.</p><p>Object size: Some rare object classes like syringe, ladybug etc. usually have a small size. Smaller objects are difficult to detect as well as recognize.</p><p>High Diversity: Every meta-class gets a different number of classes and there exists a high visual diversity in each meta-class images. Since, being in a same meta-class does not guarantee of the visual similarity, it is difficult to learn relationships for the unseen categories which are quite different from the seen categories in the same super-class. As an example, 'tiger' has many similar classes compared to 'ray'. The scarcity of similar class enables an inadequate description of the unseen class which eventually affect the zero shot detection performance.</p><p>Noise in semantic space: We use unsupervised semantic embedding vectors word2vec/GloVe as the class description. Such embeddings are noisy in general as they are generated automatically from unannotated text mining. It also affects the zero-shot detection performance significantly.</p><p>Seen vs. Unseen Class Performance: The overall performance of ZSD is depended on the learning of seen classes. Therefore, the performance of seen detection can be an indication of how possibly ZSD works. To this end, we also study the detection performance on seen classes of ILSVRC validation dataset after the first step of faster-RCNN training ( <ref type="table" target="#tab_3">Table 3</ref>). It indicates the baseline performance of seen classes necessary to achieve the ZSD performance reported in the paper. The baseline method result is better than our proposed approaches. It is justifiable as both of our proposed methods can generate prediction for both seen and unseen class together which sacrifices the seen performance a bit to achieve distinction among all seen and unseen classes. The <ref type="table" target="#tab_3">Table 3</ref> also compares the seen result with the unseen performance. One can find that performance of selected unseen classes is similar to that of seen classes for our (L cls ) method. It indicates a balanced generalization of ZSD in both seen and unseen classes.</p><p>Learning without meta-class: For some applications, the meta-class based supervision may not be available. In such case, one can define meta-class in an unsupervised manner by applying a clustering mechanism on original semantic embedding.</p><p>ZSL vs ZSD loss: Many traditional non-end-to-end trainable ZSR methods consider different aspects of regularization <ref type="bibr" target="#b31">[32]</ref>, transductive setting <ref type="bibr" target="#b23">[24]</ref>, metric learning <ref type="bibr" target="#b28">[29]</ref>, domain adaptation <ref type="bibr" target="#b17">[18]</ref> and class attribute association <ref type="bibr" target="#b3">[4]</ref> etc. Similarly, the end-to-end trainable ZSR methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b21">22]</ref> employ different nonlinearity in feature and semantic pipeline. But, those traditional loss formulations need to be redesigned in ZSD case to be compatible for both classification and box detection losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative results</head><p>We provide more examples of ZSD in <ref type="figure" target="#fig_4">Fig. 4</ref>. One can find that the prediction score threshold is lower (0.3 used in the examples) than the value (greater than   <ref type="table" target="#tab_3">Table 3</ref>. Comparison of seen and unseen class performance using ResNet as convolution layers. word2vec is used for baseline, our (L mm ) and our (L cls ). Best performance in each row are shown as bold. We refer Unseen (all): mAP of all unseen classes, Unseen (selected): mAP of selected classes for which visually similar classes are present. 0.5) used in traditional object detection like faster-RCNN <ref type="bibr" target="#b37">[38]</ref>. It indicates that the prediction of ZSD has less confidence than that of traditionally seen detection. As zero-shot method does not observe any training instances of unseen classes during the whole learning process, the confidence of prediction cannot be as strong as the seen counterpart. Moreover, a ZSD method needs to correspond visual features with semantic word vectors which are noisy in general. It degrades the overall confidence for ZSD. In the last layer of the box regression branch, our method does not have specified bounding boxes for un-seen classes. Instead, bounding box corresponding to a closely related seen class that has the maximum score is used for un-seen localization. Therefore, a correct unseen class prediction sometimes cannot get very accurate localizations as illustrated in <ref type="figure" target="#fig_5">Fig. 5</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>ZSD deals with a more complex label space (object labels and locations) with considerably less supervision (i.e., no examples of unseen classes). (a) Traditional recognition task only predicts seen class labels. (b) Traditional detection task predicts both seen class labels and bounding boxes. (c) Traditional zero-shot recognition task only predicts unseen class labels. (d) The proposed ZSD predicts both seen and unseen classes and their bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Network Architecture -Left: Image level feature maps are used to propose candidate object boxes and their corresponding features. Right: The features are used for classification and localization of new classes by utilizing their semantic concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The 2D tSNE embedding of modified word vectors W1W2 using only max-margin loss, Lmm (left) and with clustering loss, Lmm + Lmc (right). Semantically similar classes are embedded more closely in cluster based loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ZSD Baseline = 6.3, Ours (L mm) = 6.5, Ours (Lcls) = 4.4 ZSD Baseline = 18.6, Ours (L mm) = 22.7, Ours (Lcls) = 27.4 Zero-Shot Detection (ZSD) Baseline 12.7 0.0 3.9 0.5 0.0 36.3 2.7 1.8 1.7 12.2 2.7 7.0 1.0 0.6 22.0 19.0 1.9 40.9 75.3 0.3 28.4 17.9 12.0 4.0 Ours (L mm) 15.0 0.0 8.0 0.2 0.2 39.2 2.3 1.9 3.2 11.7 4.8 0.0 0.0 7.1 23.3 25.7 5.0 50.5 75.3 0.0 44.8 7.8 28.9 4.5 Ours (Lcls) 16.4 5.6 1.0 0.1 0.0 27.8 1.7 1.5 1.6 7.2 2.2 0.0 4.1 5.3 26.7 65.6 4.0 47.3 71.5 21.5 51.1 3.7 26.2 1.2 Zero-Shot Tagging (ZST) Baseline 23.3 2.9 13.4 9.6 3.1 61.7 20.7 16.3 7.5 29.4 8.6 12.2 8.5 4.9 46.2 30.7 11.0 51.8 77.6 9.0 46.1 39.0 12.7 12.6 Ours (L mm) 27.5 2.9 20.8 10.5 3.3 72.5 27.7 16.7 7.9 22.9 14.3 2.8 6.7 14.5 46.8 42.6 16.0 59.1 80.0 12.9 67.3 34.1 34.0 17.1 Ours (Lcls) 30.6 12.6 10.2 11.9 4.9 48.9 21.8 17.9 29.1 32.2 10.0 4.1 20.7 10.7 52.2 82.6 12.3 58.5 75.5 48.9 72.2 16.9 33.9 15.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Effect of varying λ in different zero-shot tasks for ResNet+w2v (left) and ResNet+glo (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Selected examples of ZSD of our cluster (λ = .6) method with R+w2v, using the prediction score threshold = 0.3. (See supplementary material for more examples)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 1 .</head><label>1</label><figDesc>Long-tail distribution of imageNet dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 2 .</head><label>2</label><figDesc>Word cloud based on (a) number of object instance (b) Mean object size in pixel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 .</head><label>3</label><figDesc>Distribution of instances per classes within each meta class. Two most common (frequent) seen classes and unseen classes are marked in white and black color text respectively. Red dashed line indicates 50 percentile boundary. All unseen classes lie within the rarest half of the instance distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 4 .</head><label>4</label><figDesc>Selected examples of ZSD of our (L cls ) with λ = .6 and R+w2v, using the prediction score threshold = 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of incorrect detection but correct classification. The unseen class 'bow-tie', 'pineapple' and 'bench' are incorrectly localized in these images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>mAP of the unseen classes. Ours (with L mm ) and Ours (with L cls ) denote the performance without predefined unseen and with cluster loss respectively (Sec. 3.3 and Sec. 3.2) . For cluster case, λ = 0.8.</figDesc><table><row><cell>Network</cell><cell>Baseline</cell><cell>ZSD Ours (L mm )</cell><cell>Ours (Lcls)</cell><cell cols="2">ZSMD Baseline Ours (L mm )</cell><cell>Ours (Lcls)</cell><cell>Baseline</cell><cell>ZST Ours (L mm )</cell><cell>Ours (Lcls)</cell><cell cols="2">ZSMT Ours Baseline (L mm )</cell><cell>Ours (Lcls)</cell></row><row><cell>R+w2v</cell><cell>12.7</cell><cell cols="2">15.0 16.0</cell><cell>13.7</cell><cell cols="2">15.4 15.4</cell><cell>23.3</cell><cell cols="2">27.5 30.0</cell><cell>28.8</cell><cell>33.4 39.3</cell></row><row><cell>R+glo</cell><cell>12.0</cell><cell cols="2">12.3 14.6</cell><cell>12.9</cell><cell cols="2">14.1 16.1</cell><cell>22.3</cell><cell cols="2">24.5 26.2</cell><cell>29.2</cell><cell>31.5 36.3</cell></row><row><cell cols="2">V+w2v 10.2</cell><cell cols="2">12.7 11.8</cell><cell>11.4</cell><cell cols="2">12.5 11.8</cell><cell>23.3</cell><cell cols="2">25.6 26.2</cell><cell>29.0</cell><cell>31.3 36.0</cell></row><row><cell>V+glo</cell><cell>9.0</cell><cell cols="2">10.8 11.6</cell><cell>9.7</cell><cell cols="2">11.3 11.8</cell><cell>20.3</cell><cell cols="2">22.9 23.9</cell><cell>27.3</cell><cell>29.2 34.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Zero shot recognition on CUB using λ = 1 because no meta-class assignment is done here. For fairness, we only compared our result with the inductive setting of other methods without per image part annotation and description. We refer V=VGG, R=ResNet, G=GoogLeNet. classes achieve better detection performance (ZSD mAP 18.6, 22.7, 27.4) than those which do not have similar classes (ZSD mAP 6.3, 6.5, 4.4) for the all methods (baseline, our's with L mm and L cls ). Our proposed cluster method with loss L cls outperforms the other versions significantly for the case when visually similar classes are present. For the all classes, our cluster method is still the best (mAP: cluster 16.4 vs. baseline 12.7). However, our's with L</figDesc><table /><note>mm method per- forms better for when case similar classes are not present (mAP 6.5 vs 4.4).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>, band aid, binder, chain saw, cream, crutch, face-powder, hairspray, hammer, lipstick, nail, neck-brace, pencilbox, pencilsharpener, perfume, plastic-bag, power-drill, purse, rubber-eraser, ruler, screwdriver, stethoscope, stretcher, syringe Assigned meta-class to each of the 200 object categories. The unseen classes are presented as bold.</figDesc><table><row><cell cols="4">ID Meta/Super-class</cell><cell>Categories</cell></row><row><cell cols="5">1 axe, backpack2 Indoor Accessory (25) accordion, banjo, cello, chime, drum, flute, french-horn, Musical (17) guitar, harmonica, harp, maraca, oboe, piano, saxophone,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>trombone, trumpet, violin</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>apple, artichoke, bagel, banana, bell-pepper, burrito, cucumber,</cell></row><row><cell>3</cell><cell cols="2">Food (21)</cell><cell></cell><cell>fig, guacamole, hamburger, head-cabbage, hotdog, lemon, mushroom, orange, pineapple, pizza, pomegranate,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>popsicle, pretzel, strawberry</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>computer-keyboard,computer-mouse, digital-clock, electric-fan,</cell></row><row><cell cols="4">4 Electronics (16)</cell><cell>hair-dryer, iPod, lamp, laptop, microphone, printer, vacuum,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>remote-control, tape-player, traffic-light, tv or monitor, washer</cell></row><row><cell>5</cell><cell cols="3">Appliance (7)</cell><cell>coffee-maker, dishwasher, microwave, refrigerator, stove, toaster, waffle-iron</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>beaker, bowl, can-opener, cocktail-shaker, corkscrew,</cell></row><row><cell>6</cell><cell>Kitchen item</cell><cell cols="2">(17)</cell><cell>cup or mug, frying-pan, ladle, milk-can, pitcher, plate-rack, salt or pepper shaker, soap-dispenser, spatula, strainer,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>water-bottle, wine-bottle</cell></row><row><cell>7</cell><cell cols="3">Furniture (8)</cell><cell>baby-bed, bench, bookshelf,chair, filing-cabinet, flower-pot, sofa, table</cell></row><row><cell>8</cell><cell cols="3">Clothing (11)</cell><cell>bathing-cap, bow-tie, brassiere, diaper, hat with a wide brim, helmet, maillot, miniskirt, sunglasses, swimming-trunks, tie</cell></row><row><cell>9</cell><cell>Invertebrate animal</cell><cell cols="2">(14)</cell><cell>ant, bee, butterfly, centipede, dragonfly, goldfish, isopod, jellyfish, ladybug, lobster, scorpion, snail, starfish, tick</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>antelope, armadillo, bear, camel, cattle, dog, domestic-cat,</cell></row><row><cell>10</cell><cell>mammal animal</cell><cell cols="2">(28)</cell><cell>elephant, fox, giant-panda, hamster,hippopotamus, horse, koala-bear, lion, monkey, otter, porcupine, rabbit, red-panda,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>seal, sheep, skunk, squirrel, swine, tiger, whale, zebra</cell></row><row><cell>11</cell><cell cols="2">non-mammal animal</cell><cell>(6)</cell><cell>bird, frog, lizard, ray, snake, turtle</cell></row><row><cell>12</cell><cell cols="2">Vehicle(12)</cell><cell></cell><cell>airplane, bicycle, bus, car, cart, golfcart, motorcycle, snowmobile, snowplow, train, unicycle, watercraft</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>balance-beam, baseball, basketball, bow, croquet-ball, dumbbell,</cell></row><row><cell>13</cell><cell cols="2">Sports (17)</cell><cell></cell><cell>golf-ball, horizontal-bar, ping-pong-ball, puck, punching-bag,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>racket, rugby-ball, ski, soccer-ball, tennis-ball, volleyball</cell></row><row><cell>14</cell><cell cols="2">Person (1)</cell><cell></cell><cell>person</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>ZSD on CUB using λ = 1. We refer V=VGG and R=ResNet</figDesc><table><row><cell>mAP</cell><cell cols="2">Network w2v glo</cell></row><row><cell>Baseline</cell><cell>R</cell><cell>31.0 26.7</cell></row><row><cell>Our (L cls )</cell><cell>R</cell><cell>33.5 32.3</cell></row><row><cell>Baseline</cell><cell>V</cell><cell>30.3 27.9</cell></row><row><cell>Our (L cls )</cell><cell>V</cell><cell>30.4 28.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Meta-classes are obtained by clustering semantically similar classes.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero Shot Object Detection</head><p>A. Related Work</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-cue zero-shot learning with strong supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label-Embedding for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1425" to="1438" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="2927" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-01" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attributes2classname: A discriminative model for attribute-based unsupervised zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Gokberk</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="48" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zero shot learning via multi-scale manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Owechko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transductive multi-label zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.07790</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-shot learning on semantic class prototype graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>cited By 107</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4555" to="4564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3464" to="3472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Straight to shapes: Real-time detection of encoded shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07932</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H J S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">Li</forename><surname>R-Fcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06409</idno>
		<title level="m">Object detection via region-based fully convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic autoencoder for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4247" to="4255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zero-shot image tagging by hierarchical semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="879" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zero-shot recognition using dual visual-semantic mapping paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attributes make sense on segmented objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6495" to="6503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">SSD: Single Shot MultiBox Detector</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving semantic embedding consistency by metric learning for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 14th European Conference on Computer Vision</title>
		<meeting>The 14th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantically consistent regularization for zeroshot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zeroshot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22</title>
		<editor>Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A unified approach for conventional zeroshot, generalized zero-shot and few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08653</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<title level="m">Yolo9000: Better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ridge regression, hubness, and zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A unified probabilistic approach modeling relationships between attributes and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2120" to="2127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Latent embeddings for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Zero-shot learning -the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Matrix trifactorization with manifold regularizations for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Zero-shot classification with discriminative semantic representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Designing category-level attributes for discriminative visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="771" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning a deep embedding model for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Zero-shot learning via semantic similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
