<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FLERT: Document-Level Features for Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schweter</surname></persName>
							<email>schweter.mlstefan@schweter.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
							<email>alan.akbik@hu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FLERT: Document-Level Features for Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state-of-the-art approaches for named entity recognition (NER) using BERT-style transformers typically use one of two different approaches: (1) The first fine-tunes the transformer itself on the NER task and adds only a simple linear layer for word-level predictions.</p><p>(2) The second uses the transformer only to provide features to a standard LSTM-CRF sequence labeling architecture and thus performs no fine-tuning. In this paper, we perform a comparative analysis of both approaches in a variety of settings currently considered in the literature. In particular, we evaluate how well they work when documentlevel features are leveraged. Our evaluation on the classic CoNLL benchmark datasets for 4 languages shows that document-level features significantly improve NER quality and that fine-tuning generally outperforms the featurebased approaches. We present recommendations for parameters as well as several new state-of-the-art numbers. Our approach is integrated into the FLAIR framework to facilitate reproduction of our experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is the well-studied NLP task of predicting shallow semantic labels for sequences of words, used for instance for identifying the names of persons, locations and organizations in text. Current approaches for NER often leverage pre-trained transformer architectures such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> or XLM <ref type="bibr" target="#b8">(Lample and Conneau, 2019)</ref>. Document-level features. While NER is traditionally modeled at the sentence-level, transformerbased models offer a natural option for capturing document-level features by passing a sentence with its surrounding context. As <ref type="figure">Figure 1</ref> shows, this context can then influence the word representations of a sentence: The example sentence "I love Paris" is passed through the transformer together with the next sentence that begins with "The city is", potentially helping to resolve the ambiguity of the word "Paris". A number of prior works have employed such document-level features <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref><ref type="bibr" target="#b21">Virtanen et al., 2019;</ref><ref type="bibr" target="#b25">Yu et al., 2020)</ref> but only in combination with other contributions and thus have not evaluated the impact of using document-level features in isolation. Contributions. With this paper, we close this experimental gap and present an evaluation of document-level features for NER. As there are two conceptually very different approaches for transformer-based NER that are currently used across the literature, we evaluate document-level features in both:</p><p>1. In the first, we fine-tune the transformer itself on the NER task and only add a simple linear layer for word-level predictions <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>.</p><p>2. In the second, we use the transformer only to provide features to a standard LSTM-CRF sequence labeling architecture <ref type="bibr" target="#b7">(Huang et al., 2015)</ref> and thus perform no fine-tuning.</p><p>We discuss the differences between both approaches and explore best hyperparameters for each. In their best determined setup, we then perform a comparative evaluation using documentlevel features from different transformer models. We find that document-level features significantly improve NER quality and that fine-tuning generally outperforms feature-based approaches. We use document-level features in the best determined fine-tuning setup and report a number of new state-of-the-art scores on the classic CoNLL benchmark datasets. Our approach is integrated into the FLAIR framework <ref type="bibr" target="#b0">(Akbik et al., 2019a)</ref>  [CLS] the vote .</p><formula xml:id="formula_0">C [CLS] C 62 C 63 C 64 ... E'' 1 E'' 2 E'' 3 E [SEP]</formula><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The city is [SEP]</head><p>C' 1 C' 2 C' 3 C <ref type="bibr">[SEP]</ref> ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left context</head><p>Right context <ref type="figure">Figure 1</ref>: To obtain document-level features for a sentence that we wish to tag ("I love Paris", shaded green), we add 64 tokens of left and right tokens each (shaded blue). As self-attention is calculated over all input tokens, the representations for the sentence's tokens are influenced by the left and right context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B-LOC O O O</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document-Level Features</head><p>In a transformer-based architecture, documentlevel features can easily be realized by passing a sentence with its surrounding context to obtain word embeddings, as illustrated in <ref type="figure">Figure 1</ref>. Prior approaches. This approach was first employed by <ref type="bibr" target="#b6">Devlin et al. (2019)</ref> with what they described as a "maximal document context", though technical details were not listed. Subsequent work has used variants of this approach. For instance, <ref type="bibr" target="#b21">Virtanen et al. (2019)</ref> experiment with adding the following (but not preceding) sentence as context to each sentence. <ref type="bibr" target="#b25">Yu et al. (2020)</ref> instead use a 64 surrounding token window for each token in a sentence, thus calculating a large context on a per-token basis. By contrast, Luoma and Pyysalo (2020) adopt a multi-sentence view in which they evaluate different ways for passing sequences of multiple complete sentences through a transformer, evaluate the impact of the position of a sentence in this multi-sentence view and present ways to combine predictions from different windows and sentence positions. Our approach. In this paper, we instead use a conceptually simple variant in which we create context on a per-sentence basis. That is, for each sentence we wish to classify, we add 64 subtokens of left and right context, as shown in <ref type="figure">Figure 1</ref>. This has computational and implementation advantages in that each sentence and its context need only be passed through the transformer once and that added context is limited to a relatively small window of 64 subtokens. Furthermore, we can still follow stan-dard procedure in shuffling sentences at each epoch during training, since context is encoded on a persentence level. We use this approach throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline Parameter Experiments</head><p>As mentioned in the introduction, there are two common architectures for transformer-based NER, namely fine-tuning and feature-based approaches.</p><p>In this section, we briefly introduce the differences between both approaches and conduct a study to identify best hyperparameters for each. The best respective setups are then used in the final comparative evaluation in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Data set. We use the development datasets of the CoNLL shared tasks <ref type="bibr" target="#b20">(Tjong Kim Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b19">Tjong Kim Sang, 2002)</ref>   <ref type="bibr" target="#b3">(Bojanowski et al., 2017)</ref> for all other languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">First Approach: Fine-Tuning</head><p>Fine-tuning approaches typically only add a single linear layer to a transformer and fine-tune the entire architecture on the NER task. To bridge the difference between subtoken modeling and token-level predictions, they apply subword pooling to create token-level representations which are then passed to the final linear layer. A common subword pooling strategy is "first" <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> which uses the representation of the first subtoken for the entire token. See <ref type="figure">Figure 2</ref> for an illustration.  To train this architecture, prior works typically use the AdamW <ref type="bibr" target="#b11">(Loshchilov and Hutter, 2019)</ref> optimizer, a very small learning rate and a small, fixed number of epochs as a hard-coded stopping criterion . We adopt a one-cycle training strategy <ref type="bibr" target="#b15">(Smith, 2018)</ref>, inspired from the HuggingFace transformers <ref type="bibr" target="#b24">(Wolf et al., 2019)</ref> implementation, in which the learning rate linearly decreases until it reaches 0 by the end of the training. <ref type="table" target="#tab_3">Table 2</ref> lists the architecture parameters we use across all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Eiffel Tower</head><p>The E ##iff ##el Tower</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input tokens</head><p>Subword tokenization</p><formula xml:id="formula_1">E 1 E 2 E 3 E 4 E 5 T 1 T 2 T 5</formula><p>First subword pooling <ref type="figure">Figure 2</ref>: Illustration of first subword pooling. The input "The Eiffel Tower" is subword-tokenized, splitting "Eiffel" into three subwords (shaded green). Only the first ("E") is used as representation for "Eiffel".</p><p>Conceptually, fine-tuning approaches have the advantage that everything is modeled in a single architecture that is fine-tuned as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Fine-Tuning Variants</head><p>We compare two variants of fine-tuning:</p><p>XLM-R In the first, we use the standard approach of adding a simple linear classifier on top of the transformer to directly predict tags.</p><p>XLM-R-CRF In the second, we evaluate if it is helpful to add a conditional random fields (CRF) decoder between the transformer and the linear classifier <ref type="bibr" target="#b16">(Souza et al., 2019)</ref>.</p><p>We evaluate both in all possible combinations of adding standard word embeddings ("+WE") and using document-level features ("+Context"). Results are listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Best Fine-Tuning Variant</head><p>As  In particular, we find that using additional word embeddings and using a CRF decoder improves results only for some languages, and often only minimally so. On the other hand, the results clearly show document-level context to be of significant importance. We thus select the standard fine-tuning approach with document features (i.e. without word embeddings or the CRF) as fine-tuning architecture for our comparative evaluation in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Second Approach: Feature-Based</head><p>Feature-based approaches instead use the transformer only to generate embeddings for each word in a sentence and use these as input into a standard sequence labeling architecture, most commonly a LSTM-CRF <ref type="bibr" target="#b7">(Huang et al., 2015)</ref>. The transformer weights remain frozen so that training is limited to the LSTM-CRF. Training typically uses SGD with a larger learning rate that is annealed against the development data. Training terminates when the learning rate becomes too small. Advantages of this approach include the use of a real stopping criterion and the relative ease of combining BERT embeddings with other types of word embeddings (e.g. "embedding stacking" <ref type="bibr" target="#b0">(Akbik et al., 2019a)</ref>  The parameters used for training a feature-based model are shown in <ref type="table" target="#tab_7">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Feature-Based Variants</head><p>We compare two variants to produce token features:</p><p>All-layer-mean In the first, we obtain embeddings for each token using mean pooling across all transformer layers, including the word embedding layer. This representation has the same length as the hidden size for each transformer layer. This approach is inspired by the ELMOstyle <ref type="bibr" target="#b14">(Peters et al., 2018)</ref> "scalar mix", that was shown to achieve better results than the best individual layer on most tasks <ref type="bibr" target="#b9">(Liu et al., 2019a;</ref><ref type="bibr" target="#b18">Tenney et al., 2019)</ref>.</p><p>Last-four-layers In the second, we follow <ref type="bibr" target="#b6">Devlin et al. (2019)</ref> to only use the last four transformer layers for each token and concatenate their representations into a final representation for each token. This representation thus has four times the length of the transformer layer hidden size.</p><p>We again evaluate both in all possible combinations of adding standard word embeddings "(+WE)" and using document-level features "(+Context)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Best Feature-Based Variant</head><p>The results of parameter combinations for English 2 are shown in <ref type="table" target="#tab_5">Table 3</ref>. The results clearly show that averaging over all layers outperforms using only the last four layers. In addition, the results clearly show that additionally using word embeddings as well as document-level context improves development set F1 score. For this reason, we chose a setup with all-layer-mean, word embeddings and document features for the feature-based experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Summary of Best Configurations</head><p>This section identified the best configurations for the fine-tuning and feature-based approaches to NER using development data. In both cases, experiments clearly showed that using document-level features improves NER quality. For the featurebased approach, we also find that an all-layer-mean strategy as well as the addition of word embeddings yields the best results. We employ these configurations in our final evaluation in the next section.  of the CoNLL-03 shared task datasets. The results of this evaluation are listed in <ref type="table" target="#tab_9">Table 5</ref>. For a meaningful comparison we also list previous state-of-theart results. We additionally report two ablations:</p><p>(1) fine-tuning with both development and test data, and (2) fine-tuning monolingual transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>We make the following observations: Fine-tuning document-level features best. As <ref type="table" target="#tab_9">Table 5</ref> shows, we find that fine-tuning outperforms the feature-based approach across all experiments (≈↑2 pp on average). Similarly, we find that document-level features clearly outperform sentence-level features (↑1.15 pp on average). Similar to our results on development data, we thus find fine-tuning with document-level features to work best across all languages. New state-of-the-art results. For the best approach, we also report results obtained with a training strategy in which we train over both train and development splits, anneal against training loss (instead of development F1) and perform no model selection using development data. This yields new state-of-the-art F1 scores that in most cases outperform all currently published numbers: For German (original), our approach outperforms the best previous approach <ref type="bibr" target="#b25">(Yu et al., 2020)</ref> by ↑1.81 pp, and ≈↑2 pp on the revised German dataset. For Dutch and English, our approach outperforms the previous best by ↑0.96 pp and ↑0.46 pp respectively. For Spanish, however, our results lie slightly below those reported by <ref type="bibr" target="#b25">(Yu et al., 2020)</ref>, with a difference of ↓0.16 pp.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>To get a better understanding of the impact of document-level features on different entity types, we perform a per-type analysis to compare average results across entity types with and without document context. Results are shown in <ref type="table" target="#tab_11">Table 6</ref>. We find that overall, the ORG (organization) and PER (person) entity types benefit the most from document-level context. In particular, the ORG type consistently improves across all languages. For other entity types, we find a mix of results depending on the language: For instance, documentlevel context reduces F1 score for LOC (location) and PER entities in Dutch, whereas these entities improve the most in all other languages. For German (original), document-features improve all entity types except for MISC (miscellaneous, ↓0.9 pp) whereas for Spanish all entity types improve, albeit only slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation: Monolingual Transformers</head><p>As <ref type="table" target="#tab_11">Table 6</ref> also shows, we surprisingly find monolingual, language-specific transformers to underperform the multilingual XLM-R for most languages: For German, Dutch and Spanish the average F1 score is lower by ↓1.22 pp, ↓1.84 pp and ↓0.98 pp respectively when compared against XLM-R. This could be due to the fact that XML-R was trained over significantly larger data sets. Only for English we find that using RoBERTa <ref type="bibr" target="#b10">(Liu et al., 2019b)</ref> instead of XLM-R leads to a slight increase in F1score (↑0.2 pp).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we evaluated document-level features in two commonly used NER architectures. For each architecture, we conducted an evaluation of different variants to determine the best setup. We derive the following recommendations for transformerbased NER:</p><p>• For fine-tuning approaches, we recommend a straightforward architecture without CRF and additional word embeddings with the hyperparameters as listed in Section 3.2.</p><p>• For feature-based approaches, we recommend an all-layer-mean with additionally stacking classic word embeddings and the hyperparameters as listed in Section 3.3.</p><p>For both architectures, we find document-level features to significantly improve overall F1 score for all four languages in the CoNLL-03 benchmark. We also find fine-tuning to outperform the featurebased approach. We thus recommend the combination of document-level features and fine-tuning for NER. In our experiments, this yields new state-of-theart F1 scores for English, German and Dutch and competitive numbers for Spanish. We integrate our approach into the FLAIR framework and share the recommended parameter settings to enable the research community to reproduce our results and apply our new state-of-the-art models in their tasks. <ref type="figure" target="#fig_0">Figure 3</ref> gives an overview of the feature-based approach: Word representations are extracted from the transformer by either averaging over all layers (all-layer-mean) or by concatenating the representations of the last four layers (last-four-layers). These are then input into a standard sequence labeling model as features. ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix: Figures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The city is [SEP]</head><p>C' 1 C' 2 C' 3 C <ref type="bibr">[SEP]</ref> ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left context</head><p>Right context  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Overview of Feature-based approach. Self-attention is calculated over all input tokens (incl. left and right context). The final representation for each token in the sentence ("I love Paris", shaded green) can be calculated as a) mean over all layers of transformer-based model or b) concatenating the last four layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>96.64 ± 0.14 89.06 ± 0.18 91.86 ± 0.41 93.41 ± 0.19 88.95 ± 0.19 + WE 96.82 ± 0.13 88.96 ± 0.10 92.12 ± 0.10 93.51 ± 0.09 89.09 ± 0.36 + Context 96.82 ± 0.07 89.79 ± 0.13 93.09 ± 0.06 94.19 ± 0.14 90.34 ± 0.27 + WE + Context 97.02 ± 0.09 89.74 ± 0.46 92.83 ± 0.12 94.01 ± 0.27 90.17 ± 0.25 XLM-R-CRF 96.79 ± 0.11 88.52 ± 0.10 92.21 ± 0.07 93.61 ± 0.15 88.77 ± 0.20 + WE 96.79 ± 0.15 88.84 ± 0.15 91.97 ± 0.09 93.36 ± 0.04 88.63 ± 0.47 + Context 96.90 ± 0.06 89.67 ± 0.24 92.87 ± 0.21 94.16 ± 0.07 90.56 ± 0.09 + WE + Context 96.87 ± 0.00 89.69 ± 0.22 92.88 ± 0.26 94.34 ± 0.13 90.37 ± 0.14Table 1: Evaluation of different variants using the fine-tuning approach. The evaluation is performed against the development set of all 4 languages of the CoNLL-03 shared task for NER.</figDesc><table><row><cell>Fine-tuning Approach</cell><cell>EN</cell><cell>DE</cell><cell>DE 06</cell><cell>NL</cell><cell>ES</cell></row><row><cell>XLM-R</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">bik et al. (2018), we use GLOVE embeddings (Pen-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">nington et al., 2014) for the English tasks and FAST-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TEXT embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>for NER on four languages (English, German, Dutch and Spanish). Following Yu et al. (2020) we report results for both original and revisited dataset for German (denoted as DE 06 ). Transformer model. In all experiments in this sec- tion, we employ the multilingual XLM-RoBERTa (XLM-R) transformer model proposed by Conneau et al. (2019). We use the xlm-roberta-large model in our experiments, trained on 2.5TB of data from a cleaned Common Crawl corpus (Wenzek et al., 2020) for 100 different languages Embeddings (+WE). For each setup we experi- ment with adding classic word embeddings that are concatenated to the word-level representations ob- tained from the transformer model. Following Ak-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Parameters used for fine-tuning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of feature-based approach (development set).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Parameters for feature-based approach.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>± 0.06 82.88 ± 0.28 87.35 ± 0.17 89.87 ± 0.45 88.78 ± 0.08 -Feature-based yes 93.12 ± 0.14 84.86 ± 0.11 89.88 ± 0.26 91.73 ± 0.21 88.98 ± 0.11 -Fine-tuning no 92.79 ± 0.10 86.60 ± 0.43 90.04 ± 0.37 93.50 ± 0.15 89.94 ± 0.24 -Fine-tuning yes 93.64 ± 0.05 86.99 ± 0.24 91.55 ± 0.07 93.99 ± 0.16 90.14 ± 0.14</figDesc><table><row><cell>Approach</cell><cell cols="2">Context? EN</cell><cell>DE</cell><cell>DE 06</cell><cell>NL</cell><cell>ES</cell></row><row><cell>XLM-R</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">-Feature-based 91.83 XLM-R (+ Dev) no</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-Fine-tuning</cell><cell>yes</cell><cell cols="5">93.96 ± 0.11 88.21 ± 0.43 92.29 ± 0.18 94.66 ± 0.12 89.93 ± 0.03</cell></row><row><cell>Monolingual*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-Fine-tuning</cell><cell>yes</cell><cell cols="5">93.84 ± 0.12 85.77 ± 0.13 89.56 ± 0.55 92.15 ± 0.12 89.16 ± 0.11</cell></row><row><cell>Best published</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Akbik et al. (2019b)</cell><cell>pooling</cell><cell cols="2">93.18 ± 0.09 -</cell><cell cols="3">88.27 ± 0.30 90.44 ± 0.20 -</cell></row><row><cell>Yu et al. (2020)</cell><cell>yes</cell><cell>93.5</cell><cell>86.4</cell><cell>90.3</cell><cell>93.7</cell><cell>90.3</cell></row><row><cell cols="2">Straková et al. (2019) yes</cell><cell>93.38</cell><cell>85.10</cell><cell>-</cell><cell>92.69</cell><cell>88.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparative evaluation of best configurations of fine-tuning and feature-based approaches on test data. * corresponds to the following monolingual models: for English we use RoBERTa<ref type="bibr" target="#b10">(Liu et al., 2019b)</ref>, DBMDZ 1 -BERT for German, BERTje<ref type="bibr" target="#b22">(Vries et al., 2019)</ref> for Dutch and BETO<ref type="bibr" target="#b4">(Cañete et al., 2020)</ref> for Spanish.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Relative change in F1 for different entity types and languages when adding document-level context.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Comparative EvaluationUsing the best identified configurations, we conduct a final comparative evaluation on the test splits 2 Other languages omitted to preserve space, but they show the same clear results.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FLAIR: An easy-to-use framework for state-of-theart NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pooled contextualized embeddings for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="724" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Spanish pre-trained bert model and evaluation data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Cañete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Chaperon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Pérez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>In to appear in PML4DC at ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF Models for Sequence Tagging. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linguistic knowledge and transferability of contextual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long and Short Papers; Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploring cross-sentence contexts for named entity recognition with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jouni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01563</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fábio</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Lotufo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10649</idno>
		<title level="m">Portuguese named entity recognition using bert-crf</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What do you learn from context? probing for sentence structure in contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jouni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07076</idno>
		<title level="m">Multilingual is not enough: Bert for finnish</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wietse De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Van Cranenburgh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Gertjan Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nissim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09582</idno>
		<title level="m">BERTje: A Dutch BERT Model</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CCNet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page">1910</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.577</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
