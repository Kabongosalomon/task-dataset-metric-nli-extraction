<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Peeking into the Future: Predicting Future Person Activities and Locations in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
							<email>junweil@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
							<email>lujiang@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
							<email>jniebles@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Google AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google AI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Peeking into the Future: Predicting Future Person Activities and Locations in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deciphering human behaviors to predict their future paths/trajectories and what they would do from videos is important in many applications. Motivated by this idea, this paper studies predicting a pedestrian's future path jointly with future activities. We propose an end-to-end, multi-task learning system utilizing rich visual features about human behavioral information and interaction with their surroundings. To facilitate the training, the network is learned with an auxiliary task of predicting future location in which the activity will happen. Experimental results demonstrate our state-of-the-art performance over two public benchmarks on future trajectory prediction. Moreover, our method is able to produce meaningful future activity prediction in addition to the path. The result provides the first empirical evidence that joint modeling of paths and activities benefits future path prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the advancement in deep learning, systems now are able to analyze an unprecedented amount of rich visual information from videos to enable applications such as accident avoidance and smart personal assistance. An important analysis is forecasting the future path of pedestrians, called future person path/trajectory prediction. This problem has received increasing attention in the computer vision community <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>. It is regarded as an essential building block in video understanding because looking at the visual information from the past to predict the future is useful in many applications like self-driving cars, socially-aware robots <ref type="bibr" target="#b18">[19]</ref>, etc.</p><p>Humans navigate through public spaces often with specific purposes in mind, ranging from simple ones like entering a room to more complicated ones like putting things into a car. Such intention, however, is mostly neglected in existing work. Consider the example in <ref type="figure">Fig. 1</ref>, the person (at the top-right corner) might take different paths depend- <ref type="figure">Figure 1</ref>. Our goal is to jointly predict a person's future path and activity. The green and yellow line show two possible future trajectories and two possible activities are shown in the green and yellow boxes. Depending on the future activity, the person (top right) may take different paths, e.g. the yellow path for "loading" and the green path for "object transfer".</p><p>ing on their intention, e.g., they might take the green path to transfer object or the yellow path to load object into the car. Inspired by this, this paper is interested in modeling the future path jointly with such intention in videos. We model the intention in terms of a predefined set of 29 activities provided by NIST such as "loading", "object transfer", etc. See <ref type="table">Table 4</ref> for the full list. The joint prediction model can have two benefits. First, learning the activity together with the path may benefit the future path prediction. Intuitively, humans are able to read from others' body language to anticipate whether they are going to cross the street or continue walking along the sidewalk. After understanding these behaviors, humans can make better predictions. In the example of <ref type="figure">Fig. 1</ref>, the person is carrying a box, and the man at the bottom left corner is waving at the person. Based on common sense, we may agree that the person will take the green path instead of the yellow path. Second, the joint model advances the capability of understanding not only the future path but also the future activity by taking into account the rich semantic context in videos. This increases the capabilities of automated video analytics for social good such as real-time accident alerting, self-driving cars, and smart robot assistance. It may also have safety applications such as anticipating pedestrian movement at traffic intersections or a road robot helping humans transport goods to the trunk of a car. Note that our techniques focus on predicting a few seconds into the future, and should not be useful for non-routine activities.</p><p>To this end, we propose a multi-task learning model called Next which has prediction modules for learning future paths and future activities simultaneously. As predicting future activity is challenging, we introduce two new techniques to address the issue. First, unlike most of the existing work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref> which oversimplifies a person as a point in space, we encode a person through rich semantic features about visual appearance, body movement and interaction with the surroundings, motivated by the fact that humans derive such predictions by relying on similar visual cues. Second, to facilitate the training, we introduce an auxiliary task for future activity prediction, i.e. activity location prediction. In the auxiliary task, we design a discretized grid which we call the Manhattan Grid as location prediction target for the system. Experiments show that the auxiliary task improves the accuracy of future path prediction.</p><p>To the best of our knowledge, our work is the first on joint future path and activity prediction in streaming videos, and more importantly the first to demonstrate such joint modeling can considerably improve the future path prediction. We empirically validate our model on two benchmarks: ETH &amp; UCY <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b15">16]</ref>, and ActEV/VIRAT <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref>. Experimental results show that our method outperforms state-of-the-art baselines, achieving the best-published result on two common benchmarks and producing additional prediction about the future activity. To summarize, the contributions of this paper are threefold: (i) We conduct a pilot study on joint future path and activity prediction in videos. We are the first to empirically demonstrate the benefit of such joint learning. (ii) We propose a multi-task learning framework with new techniques to tackle the challenge of joint future path and activity prediction. (iii) Our model achieves the best-published performance on two public benchmarks. Ablation studies are conducted to verify the contribution of the proposed sub-modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Person-person models for trajectory prediction. Person trajectory prediction models try to predict the future path of people, mostly pedestrians. A large body of work learns to predict person path by considering human social interactions and behaviors in crowded scene <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>. Zou et al. in <ref type="bibr" target="#b35">[36]</ref> learned human behaviors in crowds by imitating a decision-making process. Social-LSTM <ref type="bibr" target="#b0">[1]</ref> added social pooling to model nearby pedestrian trajectory patterns. Social-GAN <ref type="bibr" target="#b6">[7]</ref> added adversarial training on Social-LSTM to improve performance. Different from these previous work, we represent a person by rich visual features in-stead of simply considering a person as points in the scene. Meanwhile we use geometric relation to explicitly model the person-scene interaction and the person-object relations, which have not been used in previous work.</p><p>Person-scene models for trajectory prediction. A number of works focused on learning the effects of the physical scene, e.g., people tend to walk on the sidewalk instead of grass. Kitani et al. in <ref type="bibr" target="#b12">[13]</ref> used Inverse Reinforcement Learning to forecast human trajectory. Xie et al. in <ref type="bibr" target="#b30">[31]</ref> considered pedestrian as "particles" whose motion dynamics are modeled within the framework of Lagrangian Mechanics. Scene-LSTM <ref type="bibr" target="#b20">[21]</ref> divided the static scene into Manhattan Grid and predict pedestrian's location using LSTM. CAR-Net <ref type="bibr" target="#b11">[12]</ref> proposed an attention network on top of scene semantic CNN to predict person trajectory. SoPhie <ref type="bibr" target="#b25">[26]</ref> combined deep neural network features from scene semantic segmentation model and generative adversarial network (GAN) using attention to model person trajectory. A disparity to <ref type="bibr" target="#b25">[26]</ref> is that we explicitly pool scene semantic features around each person at each time instant so that the model can directly learn from such interactions.</p><p>Person visual features for trajectory prediction. Some recent works have attempted to predict person path by utilizing individual's visual features instead of considering them as points in the scene. Kooij et al. in <ref type="bibr" target="#b13">[14]</ref> looked at pedestrian's faces to model their awareness to predict whether they will cross the road using a Dynamic Bayesian Network in dash-cam videos. Yagi et al. in <ref type="bibr" target="#b32">[33]</ref> used person keypoint features with a convolutional neural network to predict future path in first-person videos. Different from these works, we consider rich visual semantics for future prediction that includes both the person behavior and their interactions with soundings .</p><p>Activity prediction/early recognition. Many works have been proposed to anticipate future human actions using Recurrent Neural Network (RNN). <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b1">[2]</ref> proposed different losses to encourage LSTM to recognize actions early in internet videos. Srivastava et al. in <ref type="bibr" target="#b28">[29]</ref> utilized unsupervised learning with LSTM to reconstruct and predict video representations. Another line of works is anticipating human activities in robotic vision <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10]</ref>. Our work differs in that both person behavior and person interaction modeling are used for joint activity and trajectory prediction.</p><p>Multiple cues for tracking/group activity recognition. There are previous works that take into account multiple cues in videos for tracking <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref> and group activity recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref>. Our work differs in that rich visual features and focal attention are used for joint person path and activity prediction. Meanwhile, our work utilizes novel activity location prediction (see Section 3.5) to bridge the two tasks. <ref type="figure" target="#fig_0">Figure 2</ref>. Overview of our model. Given a sequence of frames containing the person for prediction, our model utilizes person behavior module and person interaction module to encode rich visual semantics into a feature tensor. We propose novel person interaction module that takes into account both person-scene and person-object relations for joint activities and locations prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Humans navigate through spaces often with specific purposes in mind. Such purposes may considerably orient the future trajectory/path. This motivates us to study the future path prediction jointly with the intention. In this paper, we model the intention in terms of a predefined set of future activities such as "walk", "open door", "talk", etc. Problem Formulation: Following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>, we assume each scene is first processed to obtain the spatial coordinates of all people at different time instants. Based on the coordinates, we can automatically extract their bounding boxes. Our system observes the bounding box of all the people from time 1 to T obs , and objects if there are any, and predicts their positions (in terms of xy-coordinates) for time T obs+1 to T pred , meanwhile estimating the possibilities of future activity labels at time T pred .  <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref> which oversimplifies a person as a point in space, our model employs two modules to encode rich visual information about each person's behavior and interaction with the surroundings. In summary, it has the following key components:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Person behavior module extracts visual information from the behavioral sequence of the person. Person interaction module looks at the interaction between a person and their surroundings. Trajectory generator summarizes the encoded visual features and predicts the future trajectory by the LSTM decoder with focal attention <ref type="bibr" target="#b16">[17]</ref>. Activity prediction utilizes rich visual semantics to predict the future activity label for the person. In addition, we divide the scene into a discretized grid of multiple scales, which we call the Manhattan Grid, to compute classification and regression for robust activity location prediction.</p><p>In the rest of this section, we will introduce the above modules and the learning objective in details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Person Behavior Module</head><p>This module encodes the visual information about every individual in a scene. As opposed to oversimplifying a person as a point in space, we model the person's the appearance and body movement. To model appearance changes of a person, we utilize a pre-trained object detection model with "RoIAlign" <ref type="bibr" target="#b7">[8]</ref> to extract fixed size CNN features for each person bounding box. See <ref type="figure" target="#fig_1">Fig. 3</ref>. For every person in the scene, we average the feature along the spatial dimensions and feed them into an LSTM encoder. Finally, we obtain a feature representation of T obs × d, where d is the hidden size of the LSTM.</p><p>To capture the body movement, we utilize a person keypoint detection model trained on MSCOCO dataset <ref type="bibr" target="#b5">[6]</ref> to extract person keypoint information. We apply the linear transformation to embed the keypoint coordinates before feeding into the LSTM encoder. The shape of the encoded feature has the shape of T obs × d. These appearance and movement features are commonly used in a wide variety of studies and thus do not introduce new concern on machine learning fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Person Interaction Module</head><p>This module looks at the interaction between a person and their surroundings, i.e. person-scene and person-objects interactions. Person-scene. To encode the nearby scene of a person, we first use a pre-trained scene segmentation model <ref type="bibr" target="#b3">[4]</ref> to extract pixel-level scene semantic classes for each frame. We <ref type="figure">Figure 4</ref>. We show the person interaction module which includes person-scene and person-objects modeling. For person-objects modeling, given the person sequence as the red box in the video frame, we extract the spatial relations between the person and other objects at each time instant. For person-scene modeling, surrounding scene semantic features are pooled around the person into the encoder. See Section 3.3. use totally N s = 10 common scene classes, such as roads, sidewalks, etc. The scene semantic features are integers (class indexes) of the size T obs × h × w, where h, w are the spatial resolution. We first transform the integer tensor into N s binary masks (one mask for each class), and average along the temporal dimension. This results in N s real-valued masks, each of the size of h × w. We apply two convolutional layers on the mask feature with a stride of 2 to get the scene CNN features in two scales.</p><p>Given a person's xy-coordinate, we pool the scene features at the person's current location from the convolution feature map. As the example shown at the bottom of <ref type="figure">Fig. 4</ref>, the red part of the convolution feature is the discretized location of the person at the current time instant. The receptive field of the feature at each time instant, i.e. the size of the spatial window around the person which the model looks at, depends on which scale is being pooled from and the convolution kernel size. In our experiments, we set the scale to 1 and the kernel size to 3, which means our model looks at the 3-by-3 surrounding area of the person at each time instant. The person-scene representation for a person is in R T obs ×C , where C is the number of channels in the convolution layer. We feed this into a LSTM encoder in order to capture the temporal information and get the final person-scene features in R T obs ×d . Person-objects. Unlike previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> which relies on LSTM hidden states to model nearby people, our module explicitly models the geometric relation and the object type of all the objects/persons in the scene. At any time instant, given the observed box of a person (x b , y b , w b , h b ) and K other objects/persons in the scene {(x k , y k , w k , h k )|k ∈ [1, K]}, we encode the geometric relation into G ∈ R K×4 , the k-th row of which equals to:</p><formula xml:id="formula_0">G k = [log( |x b − x k | w b ), log( |y b − y k | h b ), log( w k w b ), log( h k h b )] (1)</formula><p>This encoding computes the geometric relation in terms of the geometric distance and the fraction box size. We use a logarithmic function to reflect our observation that human trajectories are more likely to be affected by close-by objects or people. This encoding has been proven effective in object detection <ref type="bibr" target="#b8">[9]</ref>. For the object type, we simply use one-hot encoding to get the feature in R K×No , where N o is the total number of object classes. We then embed the geometric features and the object type features at the current time into d edimensional vectors and feed the embedded features into an LSTM encoder to obtain the final feature of the shape T obs × d.</p><p>As shown in the example from <ref type="figure">Fig. 4</ref>, the person-objects feature can capture how far away the person is to the other person and the cars (with respect to their own height). The person-scene feature can capture whether the person is near the sidewalk or grass. We feed this information to the model with the hope of learning things like a person walks more often on the sidewalk than the grass and tends to avoid bumping into cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Trajectory Generation with Focal Attention</head><p>As discussed, the above four types of visual features, i.e. appearance, body movement, person-scene, and personobjects, are encoded by separate LSTM encoders into the same dimension. Besides, given a person's trajectory output from the last time instant, we extract the trajectory embedding by</p><formula xml:id="formula_1">e t−1 = tanh{W e [x t−1 , y t−1 ]} + b e ∈ R d ,<label>(2)</label></formula><p>where [x t−1 , y t−1 ] is the trajectory prediction of time t − 1 and W e , b e are learnable parameters. We then feed the embedding e t−1 into another LSTM encoder for the trajectory.</p><p>The hidden states of all encoders are packed into a tensor named Q ∈ R M ×T obs ×d , where M = 5 denotes the total number of features and d is the hidden size of the LSTM. Following <ref type="bibr" target="#b6">[7]</ref>, we use an LSTM decoder to directly predict the future trajectory in the xy-coordinate. The hidden state of this decoder is initialized using the last state of the person's trajectory LSTM encoder. At each time instant, the xy-coordinate will be computed from the decoder state h t = LSTM(h t−1 , [e t−1 ,q t ]) and by a fully connected layer.q t is an important attended feature vector which summarizes salient cues in the input features Q. We employ an effective focal attention <ref type="bibr" target="#b16">[17]</ref> to this end. It was originally proposed to carry out multimodal inference over a sequence of images for visual question answering. The key idea is to project multiple features into a space of correlation, where discriminative features can be easier to capture by the attention mechanism.</p><p>To do so, we compute a correlation matrix S t ∈ R M ×T obs at every time instant t, where each entry S t ij = h t−1 · Q ij: is measured using the dot product similarity and : is a slicing operator that extracts all elements from that dimension. Then we compute two focal attention matrices:</p><formula xml:id="formula_2">A t = softmax( M max i=1 S t i: ) ∈ R M (3) B t = [softmax(S t 1: ), · · · , softmax(S t M : )] ∈ R M ×T obs (4)</formula><p>Then the attended feature vector is given by:</p><formula xml:id="formula_3">q t = M j=1 A t j T obs k=1 B t jk Q jk: ∈ R d<label>(5)</label></formula><p>As shown, the focal attention models the correlation among different features and summarizes them into a lowdimensional attended vector. Section 4 show its benefit in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Activity Prediction</head><p>Since the trajectory generation module outputs one location at a time, errors may accumulate across time and the final destination would deviate from the actual location. Using the wrong location for activity prediction may lead to bad accuracy. To counter this disadvantage, we introduce an auxiliary task, i.e. activity location prediction, in addition to predicting the future activity label of the person. We describe the two prediction modules in the following. Activity location prediction with the Manhattan Grid. To bridge the gap between trajectory generation and activity label prediction, we propose an activity location prediction module to predict the final location of where the person will engage in the future activity. The activity location prediction includes two tasks, location classification and location regression. As illustrated in <ref type="figure" target="#fig_2">Fig. 5</ref>, we first divide a video frame into a discretized h×w grid, namely Manhattan Grid, and learn to classify the correct grid block and at the same time to regress from the center of that grid block to the actual location. Specifically, the aim for the classification task is to predict the correct grid block in which the final location coordinates reside. After classifying the grid block, the aim for the regression task is to predict the deviation of the grid block center (Green Dot in the <ref type="figure">figure)</ref> to the final location coordinate (the end of Green Arrow). The reason for adding the regression task are: (i) it will provide more precise locations than just a grid block area; (ii) it is complementary to the trajectory prediction which requires xy-coordinates lo-calization. We repeat this process on the Manhattan Grid of different scales and use separate prediction heads to model them. These prediction heads are trained end-to-end with the rest of the model. Our idea is partially inspired by the region proposal network <ref type="bibr" target="#b23">[24]</ref> and our intuition is that similar to object detection problem, we need accurate localization using multi-scale features in a cost-efficient way.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, we first concatenate the scene CNN features (see Section 3.3) with the last hidden state of the encoders (see <ref type="bibr">Section 3.4)</ref>. For compatibility, we tile the hidden state Q :T obs : along the height and width dimension resulting in a tensor of the size M × d × w · h, where w · h is the total number of the grid blocks. The hidden state contains rich information from all encoders and allow gradients flow smoothly through from prediction to feature encoders.</p><p>The concatenated features are fed into two separate convolution layers for classification and regression. The convolution output for grid classification cls grid ∈ R w·h×1 indicates the probability of each grid block being the correct destination. In comparison, the convolution output for grid regression rg grid ∈ R w·h×2 denotes the deviation, in the xy-coordinates, between the final destination and every grid block center. A row of rg grid represents the difference to a grid block, calculated from [x t −x ci , y t −y ci ] where (x t , y t ) denotes the predicted location and (x ci , y ci ) is the center of the i-th grid block. The ground truth for the grid regression can be computed in a similar way. During training, only the correct grid block receives gradients for regression. Recent work <ref type="bibr" target="#b20">[21]</ref> also incorporates the grid for location prediction. Our model differs in that we link grid locations to scene semantics, and use a classification layer and a regression layer together to make more robust predictions. Activity label prediction. Given the encoded visual observation sequence, the activity label prediction module predicts the future activity at time instant T pred . We compute the future N a activity probabilities using the concatenated last hidden states of the encoders:</p><formula xml:id="formula_4">cls act = softmax(W a · [Q 1T obs : , · · · , Q M T obs : ])<label>(6)</label></formula><p>where W a is a learnable weight. The future activity of a person could be multi-class, e.g. a person could be "walking" and "carrying" at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training</head><p>The entire network is trained end-to-end by minimizing a multi-task objective. The primary loss is the common L 2 loss between the predicted future trajectories and the ground-truth trajectories <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>. The loss is summed into L xy over all persons from T obs+1 to T pred .</p><p>The second category of loss is the activity location classification and regression loss discussed in Section 3.5. We have L grid cls = N i=1 ce(cls i grid , cls * i grid ), where cls * i grid is the ground-truth final location grid block ID for the i th training trajectory. Likewise L grid reg = N i=1 smooth L1 (rg i grid , rg * i grid ) and rg * i grid is the groundtruth difference to the correct grid block center. This loss is designed to bridge the gap between the trajectory generation task and activity label prediction task.</p><p>The third loss is for activity label prediction. We employ the cross-entropy loss: L act = N i=1 ce(cls i act , cls * i act ). The final loss is then calculated from: L = L xy + λ(L grid cls + L grid reg ) + L act</p><p>We use a balance controller λ = 0.1 for location destination prediction to offset their higher loss values during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed Next model on two common benchmarks for future path prediction: ETH <ref type="bibr" target="#b22">[23]</ref> and UCY <ref type="bibr" target="#b15">[16]</ref>, and ActEV/VIRAT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>. We demonstrate that our model performs favorably against the state-of-theart models on this challenging task. The source code and models will be made available to the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ActEV/VIRAT</head><p>Dataset &amp; Setups. ActEV/VIRAT <ref type="bibr" target="#b2">[3]</ref> is a public dataset released by NIST in 2018 for activity detection research in streaming video (https://actev.nist.gov/). This dataset is an improved version of VIRAT <ref type="bibr" target="#b21">[22]</ref>, with more videos and annotations. It includes 455 videos at 30 fps from 12 scenes, more than 12 hours of recordings. Most of the videos have a high resolution of 1920x1080. We use the official training set for training and the official validation set for testing.</p><p>Following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>, the models observe 3.2 seconds (8 frames) of every person and predict the future 4.8 seconds (12 frames) of person trajectory, we downsample the videos to 2.5 fps and extract person trajectories using the code released in <ref type="bibr" target="#b6">[7]</ref>. Since we do not have the homographic matrix, we use the pixel values for the trajectory coordinates as it is done in <ref type="bibr" target="#b32">[33]</ref>. Evaluation Metrics. Following prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref>, we use two error metrics for person trajectory prediction: i) Average Displacement Error (ADE): The average Euclidean distance between the ground truth coordinates and the prediction coordinates over all time instants,</p><formula xml:id="formula_6">ADE = N i=1 T pred t=1 Ỹ i t − Y i t 2 N * T pred<label>(8)</label></formula><p>ii) Final Displacement Error (FDE): The euclidean distance between the predicted points and the ground truth point at the final prediction time instant T pred ,</p><formula xml:id="formula_7">FDE = N i=1 Ỹ i T pred − Y i T pred 2 N<label>(9)</label></formula><p>The errors are measured in the pixel space on ActEV/VIRAT whereas in meters on ETH and UCY. For future activity prediction, we use mean average precision (mAP). Baseline methods. We compare our method with the two simple baselines and two recent methods: Linear is a single layer model that predicts the next coordinates using a linear regressor based on the previous input point. LSTM is a simple LSTM encoder-decoder model with coordinates input only. Social LSTM <ref type="bibr" target="#b0">[1]</ref>: We train the social LSTM model to directly predict trajectory coordinates instead of Gaussian parameters. SGAN <ref type="bibr" target="#b6">[7]</ref>: We train two model variants (PV &amp; V) detailed in the paper using the released code from Social-GAN <ref type="bibr" target="#b6">[7]</ref> (https://github.com/agrimgupta92/sgan/). Aside from using a single model at test time, Gupta et al. <ref type="bibr" target="#b6">[7]</ref> also used 20 model outputs per frame and selected the best prediction to count towards the final performance. Following the practice, we train 20 identical models using random initializations and report the same evaluation results, which are marked "20 outputs" in <ref type="table" target="#tab_0">Table 1</ref>. Implementation Details. We use LSTM cell for both the encoder and decoder. The embedding size d e is set to 128, and the hidden sizes d of encoder and decoder are both 256. Ground truth bounding boxes of persons and objects are used during the observation period (from time 1 to T obs ). For person keypoint features, we utilize the pre-trained pose estimator from <ref type="bibr" target="#b5">[6]</ref> to extract 17 joints for each ground truth person box. For person appearance feature, we utilize the pre-trained object detection model FPN <ref type="bibr" target="#b17">[18]</ref> to extract appearance features from person bounding boxes. The scene semantic segmentation features are resized to (64, 36) and the scene convolution layers are set to have a kernel size of 3, a stride of 2 and the channel dimension is 64. We resize all videos to 1920x1080 and utilize two grid scales, 32x18 and 16x9. The activation function is tanh if not stated otherwise and we do not use any normalization. For training, we use Adadelta optimizer <ref type="bibr" target="#b34">[35]</ref> with an initial learning rate of 0.1 and the dropout value is 0.3. We use gradient clipping of 10 and weight decay of 0.0001. For Social LSTM, the neighbor is set to 256 pixels as in <ref type="bibr" target="#b32">[33]</ref>. All baselines use the same embedding size and hidden size as our model, therefore all encoder-decoder models have about the same numbers of parameters. Other hyper-parameters we use for the baselines follow the ones in <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr">Figure 6.</ref> (Better viewed in color.) Qualitative comparison between our method and the baselines. Yellow path is the observable trajectory and Green path is the ground truth trajectory during the prediction period. Predictions are shown as Blue heatmaps. Our model also predicts the future activity, which is shown in the text and with the person pose template.</p><p>Main Results. <ref type="table" target="#tab_0">Table 1</ref> lists the testing error, where the top part is the error of a single model output and the bottom shows the best result of 20 model outputs. The "ADE" and "FDE" columns summarize the error over all trajectories, and the last two columns further detail the subset trajectories of moving activities ("walk", "run", and "ride bike"). We report the mean performance of 20 runs of our single model at Row 7. The standard deviation on "ADE" metric is 0.043. Full numbers can be found in supplemental material. As we see, our method performs favorably against other methods, especially in predicting the trajectories of moving activities. For example, our model outperforms Social-LSTM and Social-GAN by a large margin of 10 points in terms of the "move FDE" metric. The results demonstrate the efficacy of the proposed model and its state-of-the-art performance on future trajectory prediction. Additionally, as a step towards real-world application, we train our model with noisy outputs from object detection and tracking during the observation period. For evaluation, following common practise in tracking <ref type="bibr" target="#b29">[30]</ref>, for each trajectory, we assume the person bounding box location at time 1 is close to the ground truth location, and we evaluate the model prediction using tracking inputs and other visual features from time 1 to T obs as shown in <ref type="table" target="#tab_0">Table 1</ref> "Ours-Noisy".</p><p>Qualitative analysis. We visualize and compare our model outputs and the baselines in <ref type="figure">Fig. 6</ref>. In each graph the yellow trajectories are the observable sequences of each person and the green trajectories are the ground truth future trajectories. The predicted trajectories are shown in the blue heatmap. To better visualize the predicted future activities of our method, we plot the person keypoint template for each predicted activity at the end of the predicted trajectory. As we see, our method outputs more accurate trajectories for each person, especially for the two persons on the right that were about to accelerate their movement. Our method is also able to predict most of the activities correct except one (walk versus run). Our model successfully predicts the activity "carry" and the static trajectory of the person near the car, while in <ref type="figure">Fig 6(c)</ref>, SGAN predicts several moving trajectories in different directions.</p><p>We further provide a qualitative analysis of our model predictions. (i) Successful cases: <ref type="figure" target="#fig_3">In Fig 7(a) and 7(b)</ref>, both the trajectory prediction and future activity prediction are correct. (ii) Imperfect case: <ref type="figure" target="#fig_3">In Fig 7(c)</ref>, although the trajectory prediction is mostly correct, our model predicts that the person is going to open the door of the car, given the observation that he is walking towards the side of the car. (iii) Failed case: <ref type="figure" target="#fig_3">In Fig 7(d)</ref>, our model fails to capture the subtle interactions between the two persons and predicts that they will go separate ways, while in fact they are going to stop and talk to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Model</head><p>In <ref type="table">Table 2</ref>, we systematically evaluate our method through a series of ablation experiments, where "ADE" and   <ref type="table">Table 3</ref>. Comparison of different methods on ETH (Column 3 and 4) and UCY datasets (Column 5-7). * We use a smaller test set on UNIV since 1 video is unable to download.</p><p>"FDE" denotes the errors thus lower are better. "Act" is the mean Average Precision (mAP) of the activity label prediction over 29 activities and higher are better. Efficacy of rich visual features. We investigate the feature contribution of person behavior and person interactions by separately ablating them. As shown in the first three rows in <ref type="table">Table 2</ref>, both features are important to trajectory prediction while person behavior features are more essential for activity prediction. Individual feature ablations are shown in <ref type="table" target="#tab_4">Table 7</ref>. Effect of focal attention. In the fourth row of <ref type="table">Table 2</ref>, we replace focal attention in Eq. (5) with a simple average of the last hidden states from all encoders. Both trajectory and activity prediction hurt as a result. Impact of multi-task learning. In the last three rows of <ref type="table">Table 2</ref>, we remove the additional tasks of predicting the activity label or the activity location or both to see the impact of multi-task learning. Results show the benefit of our multi-task learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">ETH &amp; UCY</head><p>Dataset. ETH <ref type="bibr" target="#b22">[23]</ref> and UCY <ref type="bibr" target="#b15">[16]</ref> are common datasets for person trajectory prediction benchmark <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>. Same as previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>, we report performance by averaging over both datasets. We use the same data processing method and settings detailed in <ref type="bibr" target="#b6">[7]</ref>. This benchmark includes videos from five scenes: ETH, HO-TEL, UNIV, ZARA1 and ZARA2. Leave-one-scene-out data split is used and we evaluate our model on 5 sets of data. We follow the same testing scenario and baselines as in the previous section. We have also cited the latest stateof-the-art results from <ref type="bibr" target="#b25">[26]</ref>. Due to 1 video cannot be downloaded, we use a smaller test set for UNIV and a smaller training set across all splits. The other 4 test sub-datasets are the same as in <ref type="bibr" target="#b6">[7]</ref> so the numbers are comparable.</p><p>Since there is no activity annotation, we do not use activity label prediction module in our model. Since the annotation is only a point for each person and the human scale in each video doesn't change much, we apply a fixed size expansion from points for each video to get the person bounding box annotation for feature pooling. We do not use any other bounding box. We don't use any additional annotation compared to baselines to ensure a fair comparison. Implementation Details. We do not use person keypoint feature. Final location loss and trajectory L2 loss are used. Unlike <ref type="bibr" target="#b25">[26]</ref>, we don't utilize any data augmentation. We train our model for 40 epochs with the adadelta optimizer. Other hyper-parameters are the same as in Section 4.1.</p><p>Results &amp; Analysis. Experiments are shown in <ref type="table">Table 3</ref>. Our model outperforms other methods in both evaluations, where we obtain the best-published single model on ETH and best average performance on the ETH &amp; UCY benchmark. As shown in the table, our model performs much better on HOTEL and ZARA2. The average movement at each time-instant in these two scenes are 0.18 and 0.22, respectively, much lower than others: 0.389 (ZARA1), 0.460 (ETH), 0.258 (UNIV). Recall that the leave-one-scene-out data split is used in training. The results suggest other methods are more likely to overfit to the trajectories of large movements, e.g. Social-GAN <ref type="bibr" target="#b6">[7]</ref> often "over-shoot" when predicting the future trajectories. In comparison, our method uses attention to find the "right" visual signal and show better performance for trajectories of small movements on HOTEL and ZARA2 while still being competitive for trajectories of large movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a new neural network model for predicting human trajectory and future activity simultaneously. We first encode a person through rich visual features capturing human behaviors and interactions with their surroundings. Then we add an auxiliary task of predicting the activity locations to facilitate the joint training process. We refer to the resulting model as Next. We showed the efficacy of our model on both popular and recent large-scale video benchmarks on person trajectory prediction. In addition, we quantitatively and qualitatively demonstrated that our Next model successfully predicts meaningful future activities.</p><p>Our research goal is to promote human safety in applications such as robotics or autonomous driving. We experiment on the public benchmark ActEV, the primary driver of which is to support public safety and traffic monitoring and management by automatic activity detection in streaming video 2 . Our approach works on a predefined set of 30 activities provided by the NIST, such as "loading", "object transfer". See <ref type="table">Table 4</ref> for the full list. Our system may not work beyond these predefined activities.</p><p>Future research into activity and path prediction may implicate ethical issues around privacy, safety and fairness and ought to be considered carefully before being used in realworld applications. Our method for predicting trajectory and activity has not been tested for different populations of people. As such, it is important to further evaluate these issues before employing the model in situations that may differentially impact people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">ActEV/VIRAT Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Object &amp; Activity Class</head><p>We show the object classes we used for our person interaction module and the activity classes for our activity prediction module in <ref type="table">Table 4</ref>. Detailed class definition can be found on https://actev.nist.gov/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Trajectory Type</head><p>In ActEV/VIRAT dataset, there are two distinctive types of trajectory: relatively static and the moving ones. We label the person trajectory as moving if at time T obs there is an activity label of one of the following: "Walk", "Run", "Ride Bike", otherwise we label it as static trajectory. Table 5.1.4 shows the mean displacement in pixels between the last observed point and the prediction trajectory points. As we see, there is a large difference between the two types of trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Nearest Neighbor Experiment</head><p>Since the ActEV/VIRAT experiment is not cameraindependent, we conduct a nearest neighbor experiment. Specifically, for each observed sequence in the test set, we use the nearest sequence in the training set as future predictions. As shown in <ref type="table">Table 3</ref>, it is non-trivial to predict human trajectory as people navigate differently even in the same scene. Please refer to the paper for evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Single Model Experiment</head><p>We train 20 identical Next models with different initialization for the single output experiment. We show the mean and standard deviation numbers in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Single Feature Ablation Experiments</head><p>We experiment with ablating person-object, person-scene, person keypoint and person appearance feature, as shown in <ref type="table" target="#tab_4">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Activity Detection Experiment</head><p>Since we are predicting activities in the not so distant future, a system may perform well enough if it just outputs the current activity labels as the future prediction. We train an identical model to detect the activity labels at time T obs as the future prediction outputs, which results in a performance of 0.155 mAP for activity prediction and 18.27 ADE for trajectory prediction as shown in <ref type="table" target="#tab_4">Table 7</ref>. Such a significant performance drop (0.192 vs. 0.155) suggests that activity prediction even for 4.8 seconds into the future is not a trivial task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">More Qualitative Analysis</head><p>We show more qualitative analysis in <ref type="figure" target="#fig_4">Fig. 8</ref>. In each graph the yellow trajectories are the observable sequences of each person and the green trajectories are the ground truth future trajectories. The predicted trajectories are shown in the blue heatmap. To better visualize the predicted future activities of our method, we plot the person keypoint template for each predicted activity at the end of the predicted trajectory. Successful cases: <ref type="figure" target="#fig_4">In Fig 8(a)</ref>, <ref type="figure" target="#fig_4">Fig 8(b), Fig 8(c)</ref> and <ref type="figure" target="#fig_4">Fig 8(d)</ref>, both the trajectory prediction and future activity prediction are correct. In <ref type="figure" target="#fig_4">Fig 8(d)</ref>, our model successfully predicts the two persons at the bottom is going to walk past the car and also one of them is going to gesture at the other people by the trunk of the car. Imperfect cases: <ref type="figure" target="#fig_4">In Fig 8(e)</ref> and <ref type="figure" target="#fig_4">Fig 8(f)</ref>, although the activity predictions are correct, our model predicts the wrong trajectories. <ref type="figure" target="#fig_4">In Fig 8(e)</ref>, our model fails to predict that the person is going to the other direction. <ref type="figure" target="#fig_4">In Fig 8(</ref> the front door instead of the back door. Failed cases: <ref type="figure" target="#fig_4">In Fig 8(g)</ref> and <ref type="figure" target="#fig_4">Fig 8(h)</ref>, our model fails to predict both trajectories and activities. In <ref type="figure" target="#fig_4">Fig 8(h)</ref>, the person on the bike is going to turn to avoid the incoming car while our model predicts a straight direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Comparing ActEV/VIRAT to ETH &amp; UCY Benchmark</head><p>We compare the ActEV/VIRAT dataset and the ETH &amp; UCY trajectory benchmark in <ref type="table">Table 5</ref>.2.2. As we see, the ActEV/VIRAT dataset is much larger compared to the other benchmark. Also, the ActEV/VIRAT includes bounding box and activity annotations that could be used for multitask learning. The ActEV/VIRAT is inherently different from the crow dataset since it includes diverse annotation of human activities rather than just passers-by, which makes trajectory prediction more purpose-oriented. We show the trajectory numbers after processing based on the setting of eight-second-length sequences. Note that in the public benchmark it is unbalanced since there is one crowded scene called "University" that contains over half of the trajectories in 4 scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">ETH &amp; UCY Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Dataset Difference Compared to SGAN</head><p>The dataset we use is slightly different from the one in <ref type="bibr" target="#b6">[7]</ref>, as some original videos are unavailable even though their trajectory annotations are provided. Specifically, two videos from UNIV scene, "students001", "uni examples", and one video from ZARA3, "crowds zara03", which is used in training for all corresponding splits in <ref type="bibr" target="#b6">[7]</ref>, cannot be downloaded from the dataset website. Therefore, the test set for UNIV we use is smaller than previous methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref> while the training set we use is about 34% smaller. Test sets for other 4 splits are the same therefore the numbers are comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Pre-Processing Details</head><p>Since the annotation is only a point for each person and the human scale in each video doesn't change much, we apply a fixed size expansion from the annotated points for each video to get the person bounding box annotation for appearance and person-scene feature pooling. Specifically, we use a bounding box size of 50 pixels by 80 pixels with the original annotation point putting at the center of the bottom line. All videos are resized to 720x576. The spatial dimension of the scene semantic segmentation feature is (64, 51) and two grid scales are used: <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b25">26)</ref>, <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b12">13)</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>shows the overall network architecture of our Next model. Unlike most of the existing work</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>We show the person behavior module given a sequence of person frames. We extract person appearance features and pose features to model the changes of a person's behavior. See Section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Activity location prediction with classification and regression on the multi-scale Manhattan Grid. See Section 3.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>(Better viewed in color.) Qualitative analysis of our model. Please refer to Fig. 6 for legends.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>(Better viewed in color.) Qualitative analysis of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison to baseline methods on the ActEV/VIRAT validation set. Top uses the single model output. Bottom uses 20 outputs. Numbers denote errors thus lower are better.</figDesc><table><row><cell></cell><cell>Method</cell><cell>ADE</cell><cell>FDE</cell><cell>move ADE</cell><cell>move FDE</cell></row><row><cell></cell><cell>Linear</cell><cell cols="2">32.19 60.92</cell><cell>42.82</cell><cell>80.18</cell></row><row><cell>Single Model</cell><cell>LSTM Social LSTM SGAN-PV SGAN-V</cell><cell cols="2">23.98 44.97 23.10 44.27 30.51 60.90 30.48 62.17</cell><cell>30.55 28.59 37.65 35.41</cell><cell>56.25 53.75 73.01 68.77</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">17.99 37.24</cell><cell>20.34</cell><cell>42.54</cell></row><row><cell></cell><cell>Ours-Noisy</cell><cell cols="2">34.32 57.04</cell><cell>40.33</cell><cell>66.73</cell></row><row><cell>20 Outputs</cell><cell>SGAN-PV-20 SGAN-V-20 Ours-20</cell><cell cols="2">23.11 41.81 21.16 38.05 16.00 32.99</cell><cell>29.80 26.97 17.97</cell><cell>53.04 47.57 37.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>e), our model fails to predict that the person near the car is going to open Single feature ablation &amp; activity detection experiments on the ActEV/VIRAT benchmark.</figDesc><table><row><cell>Metric</cell><cell cols="4">Nearest Neighbor Our-Single-Model</cell></row><row><cell>ADE</cell><cell></cell><cell>40.04</cell><cell></cell><cell>17.99±0.043</cell></row><row><cell>FDE</cell><cell></cell><cell>73.69</cell><cell></cell><cell>37.24±0.102</cell></row><row><cell>move ADE</cell><cell></cell><cell>39.52</cell><cell></cell><cell>20.34±0.059</cell></row><row><cell>move FDE</cell><cell></cell><cell>72.67</cell><cell></cell><cell>42.54±0.146</cell></row><row><cell cols="5">Table 6. Our single model experiment on the ActEV/VIRAT</cell></row><row><cell>benchmark.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="3">ADE ↓ FDE ↓ Act mAP ↑</cell></row><row><cell cols="2">Our full model</cell><cell>17.91</cell><cell>37.11</cell><cell>0.192</cell></row><row><cell>No p-object</cell><cell></cell><cell>18.17</cell><cell>37.13</cell><cell>0.198</cell></row><row><cell>No p-scene</cell><cell></cell><cell>18.18</cell><cell>37.75</cell><cell>0.206</cell></row><row><cell cols="2">No p-keypoint</cell><cell>18.25</cell><cell>37.96</cell><cell>0.190</cell></row><row><cell cols="2">No p-appearance</cell><cell>18.20</cell><cell>37.79</cell><cell>0.154</cell></row><row><cell>Act Detect</cell><cell></cell><cell>18.27</cell><cell>37.68</cell><cell>0.155</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Comparison to commonly used person trajectory benchmark datasets.</figDesc><table><row><cell></cell><cell>ActEV</cell><cell>ETH, UCY</cell></row><row><cell>#Scene</cell><cell>5</cell><cell>4</cell></row><row><cell>Dataset Length</cell><cell>4 hours 22 minutes</cell><cell>38 minutes</cell></row><row><cell>Resolutions</cell><cell>1920x1080, 1280x720</cell><cell>640x480, 720x576</cell></row><row><cell>FPS</cell><cell>30</cell><cell>25</cell></row><row><cell>Annotation FPS</cell><cell>30</cell><cell>2.5</cell></row><row><cell>#Traj</cell><cell>84600</cell><cell>19359, (10039 in Univ)</cell></row><row><cell>Annotations</cell><cell>Person+object bounding boxes, activities</cell><cell>Person coordinates</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we present more details and analysis for our experiments on the ActEV/VIRAT and ETH &amp; UCY Benchmarks. We also provide statistical comparisons of the two datasets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human tra-2 https://actev.nist.gov/1B-Evaluation jectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Encouraging lstms to anticipate actions very early</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andersson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trecvid 2018: Benchmarking video activity detection, video captioning and matching, video storytelling linking and video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qunot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Semedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In TRECVID</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Understanding collective activitiesof people from videos. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1242" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Car that knows before you do: Anticipating maneuvers via learning temporal driving models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A transferable pedestrian motion prediction model for intersections with different geometries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaipuria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Habibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09444</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-based pedestrian path prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F P</forename><surname>Kooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Flohr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="14" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal visual-text attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">People tracking with human motion predictions from social forces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Tipaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scene-lstm: A model for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Manh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alaghband</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04018</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A large-scale benchmark dataset for event recognition in surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cuntoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving data association by joint modeling of pedestrian trajectories and groupings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="300" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01482</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cern: confidence-energy recurrent network for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint inference of groups, events and human roles in aerial videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4576" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning and inferring dark matter and predicting human intents and trajectories in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Encoding crowd interaction with deep neural network for pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Future person localization in first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yonetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pedestrian behavior understanding and prediction with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Understanding human behaviors in crowds by imitating the decisionmaking process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08391</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
