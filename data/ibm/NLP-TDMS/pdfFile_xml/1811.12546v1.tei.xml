<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ho</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Hyuk</forename><surname>Kim</surname></persName>
							<email>junhyuk.kim@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manri</forename><surname>Cheon</surname></persName>
							<email>manri.cheon@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Seok</forename><surname>Lee</surname></persName>
							<email>jong-seok.lee@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Integrated Technology</orgName>
								<orgName type="institution">Yonsei University</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lightweight and Efficient Image Super-Resolution with Block State-based Recursive Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Super-resolution</term>
					<term>deep learning</term>
					<term>recursive neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, several deep learning-based image super-resolution methods have been developed by stacking massive numbers of layers. However, this leads too large model sizes and high computational complexities, thus some recursive parameter-sharing methods have been also proposed. Nevertheless, their designs do not properly utilize the potential of the recursive operation. In this paper, we propose a novel, lightweight, and efficient super-resolution method to maximize the usefulness of the recursive architecture, by introducing block state-based recursive network. By taking advantage of utilizing the block state, the recursive part of our model can easily track the status of the current image features. We show the benefits of the proposed method in terms of model size, speed, and efficiency. In addition, we show that our method outperforms the other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Single-image super-resolution is a task to obtain a high-resolution image from a given low-resolution image. It is a kind of ill-posed problems since it has to estimate image details under the lack of spatial information. Many researchers have proposed various approaches that can generate upscaled images having better quality than the simple interpolation methods such as nearest-neighbor, bilinear, and bicubic upscaling.</p><p>Recently, the emergence of deep learning techniques has flowed into the superresolution field. For example, Dong et al. <ref type="bibr" target="#b7">[8]</ref> proposed the super-resolution convolutional neural network (SRCNN) model, which showed much improved performance in comparison to the previous approaches. Lim et al. <ref type="bibr" target="#b18">[19]</ref> suggested the enhanced deep super-resolution (EDSR) model, which employs residual connections and various optimization techniques.</p><p>Many recent deep learning-based super-resolution methods tend to stack much more numbers of layers to obtain better upscaled images, but this dramatically increases the number of involved model parameters. <ref type="bibr">For</ref>   <ref type="figure">Fig. 1</ref>. Number of parameters and peak signal-to-noise ratio (PSNR) values of the state-of-the-art and the proposed methods for an upscaling factor of 2 on the Urban100 dataset <ref type="bibr" target="#b10">[11]</ref>.</p><p>than those of the SRCNN model. To deal with this, recursive approaches that use some parameters repeatedly have been proposed, including deeply-recursive convolutional network (DRCN) <ref type="bibr" target="#b13">[14]</ref>, deep recursive residual network (DRRN) <ref type="bibr" target="#b23">[24]</ref>, and dual-state recurrent network (DSRN) <ref type="bibr" target="#b8">[9]</ref>. The recursive super-resolution methods can be regarded as kinds of recurrent neural networks (RNNs) <ref type="bibr" target="#b8">[9]</ref>. RNNs have been usually employed when sequential relation of the data is significant, such as language modeling <ref type="bibr" target="#b25">[26]</ref> and human activity recognition <ref type="bibr" target="#b6">[7]</ref>. The beauty of RNNs comes from their two-fold structure: the recurrent unit handles not only the current input data but also the previously processed features. Since the previously processed features contain historical information, RNNs can deal with sequential dependency of the inputs properly.</p><p>However, two characteristics of the existing recursive super-resolution methods hinder them from fully exploiting the usefulness of the RNNs. First, there are no intermediate inputs and only the previously processed features are provided to the recurrent unit. Second, the final output of the recurrent unit is directly used to obtain the final upscaled image. In this situation, the output of the recurrent unit has to contain not only the super-resolved features, but also the historical information that is not useful in the non-recursive post-processing part.</p><p>To alleviate this problem, we propose a novel super-resolution method using block state-based recursive network (BSRN). Our method employs so-called "block state" along with the input features in the recursive part, which is a separate information storage to keep historical features. Thanks to the elaborate design, our method achieves various benefits on top of the previous recursive super-resolution methods, in terms of image quality, lightness, speed, and effi-ciency. As shown in <ref type="figure">Fig. 1</ref>, our method achieves the best performance in terms of image quality, while the model complexity is significantly reduced. In addition, the BSRN model can generate the super-resolved images in a progressive manner, which is useful for real-world applications such as progressive image loading.</p><p>The rest of the paper is organized as follows. First, we discuss the related work in Section 2. Then, the overall structure of the proposed method is explained in Section 3. We present several experiments for in-depth analysis of our method in Section 4, including examining effectiveness of the newly introduced recursive structure and comparison with the other state-of-the-art methods. Finally, we conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Before deep learning has emerged, feature extraction-based methods have been widely used for super-resolution, such as sparse representation-based <ref type="bibr" target="#b27">[28]</ref> and Bayes forest-based <ref type="bibr" target="#b21">[22]</ref> approaches. This trend has changed since deep learning showed significantly better performance in image classification tasks <ref type="bibr" target="#b16">[17]</ref>. Dong et al. <ref type="bibr" target="#b7">[8]</ref> pioneered the deep learning-based super-resolution by introducing SR-CNN, which enhances the interpolated image via three convolutional layers. Kim et al. <ref type="bibr" target="#b12">[13]</ref> proposed very deep super-resolution (VDSR), which stacks 20 convolutional layers to improve the performance. Lim et al. <ref type="bibr" target="#b18">[19]</ref> suggested the EDSR model, which employs more than 64 convolutional layers. These methods share the basic empirical rule of deep learning: deeper and larger models can achieve better performance <ref type="bibr" target="#b20">[21]</ref>.</p><p>As we addressed in the introduction, super-resolution methods sharing model parameters have been proposed. DRCN introduced by Kim et al. <ref type="bibr" target="#b13">[14]</ref> proves the effectiveness of parameter sharing, which recursively applies the feature extraction layer for 16 times. Tai et al. <ref type="bibr" target="#b23">[24]</ref> proposed DRRN that employs residual network (ResNet) <ref type="bibr" target="#b9">[10]</ref> with sharing the model parameters. They also proposed the memory network (MemNet) model <ref type="bibr" target="#b24">[25]</ref>, which contains groups of recursive parts called "memory blocks" with skip connections across them. Han et al. <ref type="bibr" target="#b8">[9]</ref> considered DRCN and DRRN as the RNNs employing recurrent states, and proposed DSRN, which uses dual recurrent states. Ahn et al. <ref type="bibr" target="#b2">[3]</ref> developed the cascading residual network (CARN) model, which employs cascading residual blocks with sharing their model parameters. Although these methods can be regarded as RNNs as Han et al. mentioned <ref type="bibr" target="#b8">[9]</ref>, none of them uses a separate state, which is used in only the recursive part and not in the non-recursive postprocessing part.</p><p>Some researchers proposed super-resolution methods that do not rely on shared parameters but have small numbers of model parameters. For example, Lai et al. <ref type="bibr" target="#b17">[18]</ref> introduced the Laplacian pyramid super-resolution network (LapSRN) method, which progressively upscales the input image by a factor of 2. Hui et al. <ref type="bibr" target="#b11">[12]</ref> proposed the information distillation network (IDN) method, which employs long and short feature extraction paths to maximize the amount of extracted information from the given low-resolution image. Along with the recursive super-resolution methods, the performance of these methods is also compared with that of our proposed method in Section 4.5. We observe the following three common techniques from the previous work. First, increasing the spatial resolution at the latter stage can reduce the computational complexity than upscaling at the initial stage <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>. Second, employing multiple residual connections is beneficial to obtain better upscaled images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. Third, obtaining multiple upscaled images from the same superresolution model and combining them into one provides better quality than acquiring a single image directly <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>. Along with the newly introduced block state-based architecture, our proposed method is built with considering the aforementioned empirical knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this section, we present how our super-resolution model works in detail. As similar to the existing super-resolution methods, our BSRN model can be divided into three parts: initial feature extraction, feature processing in a recursive manner, and upscaling. <ref type="figure">Fig. 2</ref> shows the overall structure of our method. As shown in the figure, the main objective of the super-resolution task is to obtain an image Y , which is upscaled from a given low-resolution image X, where we want Y to be the same as the ground-truth image Y . Briefly, the initial features are extracted from the given input image. Then, the extracted features are further processed via a recursive residual block (RRB, <ref type="figure" target="#fig_3">Fig. 3</ref>), which is employed multiple times with the same parameters. The final image is obtained from the upscaling module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Initial feature extractor</head><p>The BSRN model takes a low-resolution input image X ∈ R w×h×3 consisting of three channels of the RGB color space, where w × h is the resolution of the image. Before we recursively process it, a convolutional layer extracts the initial features of the image, which can be represented as</p><formula xml:id="formula_0">H 0 = W I * X + b I<label>(1)</label></formula><p>where W I ∈ R 3×3×3×c and b I ∈ R c are the weight and bias matrices, respectively, and the operator * denotes the convolution operation. A variable c determines the number of convolutional channels, thus the last dimension of H 0 is c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recursive residual block</head><p>Starting from the initial features H 0 , our model performs the recursive operations in the shared part named "recursive residual block (RRB)," which is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. The RRB takes two matrices as inputs at a given iteration t: the feature matrix H t that has been processed at previous iterations from the original input image and an additional matrix S t ∈ R w×h×s called "block state," where s determines the feature dimension of S t . As shown in <ref type="figure">Fig. 2</ref>, the block state matrix is not derived from the input image features. Instead, the initial block state matrix S 0 is initialized by zero values. Note that S t and H t have the same spatial dimension but different feature dimensions. A RRB consists of three concatenated convolution (C-Conv) layers and one concatenated rectified linear unit (C-ReLU) layer. A C-Conv layer first concatenates two input matrices along the last dimension, performs a convolutional operation, and splits the result into two output matrices having the sizes of the input matrices. In other words, when H t and S t are given, a C-Conv layer concatenates them (i.e., [H t , S t ]), applies convolution as</p><formula xml:id="formula_1">[H t , S t ] = W C * [H t , S t ] + b C<label>(2)</label></formula><p>and splits them into H t ∈ R w×h×c and S t ∈ R w×h×s , where W C ∈ R 3×3×(c+s)×(c+s) and b C ∈ R (c+s) are the weight and bias matrices, respectively. A C-ReLU layer performs element-wise ReLU operations for the two inputs. In addition, two residual connections are involved for better performance as in the previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>. After processing H t and S t with three C-Conv layers, one C-ReLU layer, and two residual connections for the H t part, the RRB outputs H t+1 and S t+1 , which then serve as the inputs of the same RRB for the next recursion. This recursive process is performed R times, which produces H R and S R . There are two ways of configuring the BSRN model to get better performance: increasing the number of convolutional channels c and increasing the feature dimension of the block state s. When c increases, the number of model parameters increases across all parts of the model, including the initial feature extraction, RRB, and upscaling parts. On the other hand, increased s affects the number of model parameters only in the RRB part because the block state is involved only in RRB. Therefore, employing the block state is more beneficial to make the model compact than using a larger number of the convolutional channels.</p><p>In addition, because the block state can serve as a "memory," the RRB can keep track of the status of the current image features over the recursive operations. When the block state does not exist, it is hard to track the current status because it has to be latently written on the image features (i.e., H t ). It may lead to quality degradation of upscaled images, since both the image features and the current status are inputted to the upscaling part. We investigate the effectiveness of employing the block state in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Upscaling</head><p>Finally, the BSRN model upscales the processed feature matrix H R to generate an upscaled image Y R . In particular, we use the depth-to-space operation as in the previous super-resolution models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, which is also known as sub-pixel convolution <ref type="bibr" target="#b22">[23]</ref>. For instance, in the upscaling part by a factor of 2, the first convolutional layer outputs the processed matrix having a size of w × h × 4c, the depth-to-space operator modifies the shape of the matrix to 2w × 2h × c, and the last convolutional layer outputs the final upscaled image having a shape of 2w × 2h × 3. Note that the block state S t is not used in the upscaling part.</p><p>Our model can generate upscaled images not only from the final processed feature matrix H R but also from the intermediate feature matrices H t , ∀t ∈ {1, ..., R − 1}. Therefore, with our model, it is possible to generate the upscaled images in a progressive manner. In addition, it is known that combining multiple outputs can improve the quality of the super-resolved images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>. Thus, we adopt a similar approach to obtain the final upscaled image Y by combining the intermediate outputs via the weighted sum as:</p><formula xml:id="formula_2">Y = R/r t=1 2 (rt−1) Y rt R/r t=1 2 (rt−1)<label>(3)</label></formula><p>where r is a so-called "frequency control variable," which will be explained later. The term 2 (rt−1) controls the amount of contribution of each intermediate output, where the later outputs contribute to Y more than the earlier outputs. This facilitates our model to generate intermediate upscaled images, which have progressively improved quality.</p><p>The variable r in (3) controls the frequency of the progressive upscaling. For example, when R = 16 and r = 4, Y is obtained from the weighted sum of Y 4 , Y 8 , Y 12 , and Y 16 . Since a larger value of r reduces the number of times to employ the upscaling part, it is beneficial to reduce the processing time for generating the final super-resolved image. We discuss the influence of changing r in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss function</head><p>The loss function of our model is calculated from the weighted sum of the pixelby-pixel L1 loss, i.e.,</p><formula xml:id="formula_3">L Y , Y = 1 w × h w x=1 h y=1 Y (x, y) − Y (x, y)<label>(4)</label></formula><p>where w × h is the spatial resolution of Y and Y , and Y (x, y) and Y (x, y) are the pixel values at (x, y) of the upscaled and ground-truth images, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct three experiments to investigate the advantages of the BSRN model. First, we examine the effectiveness of employing the block state. Second, we explore the role of the frequency control variable r. Finally, we compare our models with the other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and evaluation metrics</head><p>We employ the DIV2K dataset <ref type="bibr" target="#b1">[2]</ref> for training the BSRN models, which is widely used for training the recent super-resolution models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>. For evaluating the performance of our models, we use four benchmark datasets, including Set5 <ref type="bibr" target="#b4">[5]</ref>, Set14 <ref type="bibr" target="#b28">[29]</ref>, BSD100 <ref type="bibr" target="#b19">[20]</ref>, and Urban100 <ref type="bibr" target="#b10">[11]</ref>. We employ peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) <ref type="bibr" target="#b26">[27]</ref> for measuring quality of the upscaled images. As in the previous work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>, both metrics are calculated on the Y channel of the YCbCr channels converted from the RGB channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training details</head><p>We build both single-scale (×4) and multi-scale (×2, ×3, and ×4) BSRN models. The single-scale models are used to find out the benefits of the block state and frequency control variable, and the multi-scale model is used to evaluate the performance of our model in comparison to the other super-resolution methods across different scales. The number of the recursive operations R and the frequency control variable r are set to 16 and 1, respectively. Then, the loss is calculated using (4) and the Adam optimization method <ref type="bibr" target="#b15">[16]</ref> with β 1 = 0.9, β 2 = 0.999, andˆ = 10 −8 is used to update the model parameters. To prevent the vanishing or exploding gradients problem <ref type="bibr" target="#b3">[4]</ref>, we employ the L2 norm-based gradient clipping method, which clips each gradient so as to fit its L2 norm within [−θ, θ]. In this study, we set θ = 5. The initial learning rate is set to 10 −4 and reduced by a half at every 2 × 10 5 training steps. A total of 1.0 × 10 6 and 1.5 × 10 6 steps are executed for training the single-scale and multi-scale BSRN models, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benefits of employing the block state</head><p>As explained in Section 3.2, the BSRN model can be trained with various numbers of the convolutional channels (i.e., c) and the block state channels (i.e., s).</p><p>Here, we investigate the effectiveness of employing the block state by comparing the single-scale BSRN models having an upscaling factor of 4, which are trained with and without using the block state. For the models with the block state, the number of the convolutional channels c is fixed to 64 and the number of the block state channels s is changed from 1 to 64. For the models without the block state, on the other hand, s is fixed to 0 and c is changed from 64 to 96. All models are tested with r = 1. <ref type="figure" target="#fig_1">Fig. 4</ref> compares the performance of the trained BSRN models in terms of the number of parameters and the PSNR values measured for the BSD100 dataset <ref type="bibr" target="#b19">[20]</ref>. Overall, both the models with and without having the block states have a tendency to show better performance as the feature dimension (and consequently the number of parameters) increases. However, the BSRN models with the block state outperform the models without the block state, when the same numbers of parameters are used. This strongly supports that differentiating the place to store historical information from that for the image features helps to improve the quality of the upscaled images.  <ref type="table">Table 1</ref>. Performance comparison of the ×4-scale BSRN model tested with different values of r in terms of the average processing time to obtain Y , PSNR, and SSIM for the BSD100 dataset <ref type="bibr" target="#b19">[20]</ref>.</p><p>We further examine changes of the activation patterns of the BSRN models over the recursive iterations. <ref type="figure">Fig. 5</ref> shows H t , S t , and Y t of the BSRN models trained with c = 64, s = 0 and c = 64, s = 64, where the corresponding PSNR values are also reported. The values of the intermediate features and block states are averaged along the last dimension. Both the models with and without the block state generate the upscaled images with gradually improved quality in terms of the PSNR values over the iteration t. However, the changes of the intermediate features are largely different. When the block state is not employed <ref type="figure">(Fig. 5 (a)</ref>), both the patterns of the activation and range of the values drastically change, even though the super-resolved images are not. This implies that the RRB of the model without the block state has difficulty in generating progressively improved features and highly relies on the latter part (i.e., upscaling part) to generate good quality of the upscaled images. On the other hand, employing the block state ( <ref type="figure">Fig. 5 (b)</ref>) results in much more stable activations of H t than the model without the block state. Instead, the block states S t have major changes of the details, which provide historical information that can be used to produce gradually improved upscaled images over the iterations. This confirms that our model properly utilizes the block state along with the intermediate output features, which leads to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Role of frequency control variable (r)</head><p>Our model can be configured with the frequency of progressive outputs via r, along with the number of recursive iterations R. While R determines how many times the RRB is used to generate the final upscaled image, r determines how many intermediate images are obtained from the model to generate the final image (i.e., how many times the upscaling part is employed), which is R/r. Note that both R and r do not affect the number of model parameters.</p><p>In our proposed model, the upscaling part spends most of the computation time due to its increased number of the convolutional filters for the depth-tospace operation and increased spatial resolution after the depth-to-space operation. To verify this, we examine the BSRN model trained with c = 64 and s = 64 by testing with different values of r and compare their efficiency in terms of speed and quality of the upscaled images (i.e., PSNR and SSIM).  <ref type="table">Table 2</ref>. Performance comparison of the state-of-the-art methods and our model evaluated on the Set5 <ref type="bibr" target="#b4">[5]</ref>, Set14 <ref type="bibr" target="#b28">[29]</ref>, BSD100 <ref type="bibr" target="#b19">[20]</ref>, and Urban100 <ref type="bibr" target="#b10">[11]</ref> datasets. Red and blue colors indicate the best and second best performance, respectively. <ref type="table">Table 1</ref> shows the average processing time spent on upscaling an image by a factor of 4, PSNR values, and SSIM values for the BSD100 dataset <ref type="bibr" target="#b19">[20]</ref> for various values of r. The processing time is measured on a NVIDIA GeForce GTX 1080 GPU. As expected, the processing time largely decreases when r increases. For example, the BSRN model tested with r = 16 requires more than 5 times less processing time than the model tested with r = 1. Nevertheless, the PSNR value decreases by only 0.002 dB and SSIM value even remains the same. This confirms that increasing r significantly increases the processing speed with only negligible quality degradation. In addition, the experimental result implies that our proposed model has a capability of real-time processing. For example, when r = 16, our model can upscale more than 30 images per second, which is a common frame rate of videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with the other methods</head><p>Finally, we compare the performance of the multi-scale BSRN model with the other state-of-the-art super-resolution methods, including VDSR <ref type="bibr" target="#b12">[13]</ref>, DRCN <ref type="bibr" target="#b13">[14]</ref>, LapSRN <ref type="bibr" target="#b17">[18]</ref>, DRRN <ref type="bibr" target="#b23">[24]</ref>, MemNet <ref type="bibr" target="#b24">[25]</ref>, DSRN <ref type="bibr" target="#b8">[9]</ref>, IDN <ref type="bibr" target="#b11">[12]</ref>, and CARN   <ref type="table">Table 2</ref> shows the performance of the state-of-the-art methods and ours in terms of the PSNR and SSIM values on the four benchmark datasets. The number of model parameters required to obtain the super-resolved image with the given upscaling factor for each method is also provided. First, the BSRN model outperforms the other methods that do not employ any recursive operations or parameter-sharing, including VDSR, LapSRN, and IDN. For example, our method achieves a quality gain of 0.31 dB for a scale factor of 2 on the BSD100 dataset over the LapSRN model. It confirms that recursive processing helps to obtain better super-resolved images with keeping the number of model parameters small enough.</p><p>In addition, our model employs much less numbers of parameters than DRCN, DSRN, and CARN. For instance, the BSRN model uses up to 70% less numbers of model parameters than the DRCN model. Nevertheless, our proposed model outperforms DRCN and DSRN, and shows comparable performance to CARN. In particular, BSRN shows almost the same performance as CARN despite the smaller model size. This proves that the proposed method handles the image features better than the other state-of-the-art methods. <ref type="figure" target="#fig_2">Fig. 6</ref> provides a showcase of the images reconstructed by our proposed model and the other state-of-the-art methods. The figure shows that the BSRN model is highly reliable in recovering textures from the low-resolution images. For example, our method successfully upscales fine details of the structures in the Urban100 dataset, which results in clearer outputs, while the other methods produce highly blurred images or images containing large amounts of artifacts. This confirms that the BSRN model produces images having visually nice superresolved images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduced the BSRN model, which employs a novel way of recursive operation using the block state, for the super-resolution tasks. We explained the benefits and efficiency of employing our model in terms of the number of model parameters, quality measures (i.e., PSNR and SSIM), and speed. In addition, comparison with the other state-of-the-art methods also showed that our method can generate better quality of the upscaled images than the others.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Overall structure of the proposed BSRN model. Structure of a recursive residual block (RRB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Number of parameters and PSNR values of the ×4-scale BSRN models with and without the block state for the BSD100 dataset [20]. We implement the training and evaluation code of the BSRN model on the TensorFlow framework [1] 1 . For each training step, eight image patches are randomly cropped from the training images. A cropping size of 32×32 pixels is used for training the single-scale BSRN model and 48×48 pixels is used for the multiscale BSRN model. For data augmentation, the image patches are then randomly flipped and rotated. For the multi-scale BSRN model, one of the upscaling paths (i.e., ×2, ×3, and ×4) is randomly selected for every training step. The superresolved images are obtained from our model by feeding the image patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of the upscaled images obtained by the BSRN model and the other state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>[ 3 ]</head><label>3</label><figDesc>. The DRCN, DRRN, MemNet, DSRN, and CARN models contain parametersharing parts. The VDSR, LapSRN, and IDN methods are also included in the comparison, since they have been recently proposed and have similar numbers of model parameters to ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>instance, the EDSR model requires about 43M parameters, which are at least 400 times more arXiv:1811.12546v1 [cs.CV] 30 Nov 2018</figDesc><table><row><cell></cell><cell>32.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">BSRN (Ours)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>31.5</cell><cell></cell><cell>CARN</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MemNet</cell><cell></cell></row><row><cell>(dB)</cell><cell>DRRN</cell><cell>IDN</cell><cell></cell><cell></cell></row><row><cell>PSNR</cell><cell>31.0</cell><cell></cell><cell>DSRN</cell><cell></cell></row><row><cell></cell><cell></cell><cell>VDSR</cell><cell></cell><cell>DRCN</cell></row><row><cell></cell><cell>30.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">LapSRN</cell><cell></cell><cell></cell></row><row><cell></cell><cell>30.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0K</cell><cell>500K</cell><cell>1,000K</cell><cell>1,500K</cell><cell>2,000K</cell></row><row><cell></cell><cell></cell><cell cols="2"># parameters</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Intermediate features Ht, block states St, upscaled images Yt, and PSNR values of the BSRN models from an image of the BSD100 dataset<ref type="bibr" target="#b19">[20]</ref>. Enlarged versions of the regions marked with red rectangles are also shown.</figDesc><table><row><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell></row><row><cell>�</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR (dB)</cell><cell>24.72</cell><cell>25.01</cell><cell>25.42</cell><cell>25.74</cell><cell>26.00</cell><cell>26.19</cell><cell>26.31</cell><cell>26.34</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell></row><row><cell>�</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PSNR (dB)</cell><cell>24.90</cell><cell>25.50</cell><cell>25.93</cell><cell>26.27</cell><cell>26.52</cell><cell>26.67</cell><cell>26.71</cell><cell>26.74</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Fig. 5. (a) Without block state</cell></row><row><cell cols="6">(c = 64, s = 0) (b) With block state (c = 64, s = 64)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is available at https://github.com/idearibosome/tf-bsrn-sr.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NTIRE 2017 challenge on single image superresolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast, accurate, and lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="252" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network with selection units for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1150" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Impact of three-dimensional video scalability on multi-view activity recognition using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thematic Workshops of ACM Conference on Multimedia</title>
		<meeting>the Thematic Workshops of ACM Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image superresolution via dual-state recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1654" to="1663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast and accurate single image super-resolution via information distillation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual network with enhanced upscaling module for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="800" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proccedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>cedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2924" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Naive Bayes super-resolution forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez-Pellitero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MemNet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multitask dictionary learning and sparse representation based single-image super-resolution reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="3193" to="3203" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparserepresentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Curves and Surfaces</title>
		<meeting>the International Conference on Curves and Surfaces</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
