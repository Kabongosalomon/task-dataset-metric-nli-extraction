<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-Learning with Differentiable Convex Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
							<email>smmaji@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
							<email>ravinash@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<email>soattos@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Web</forename><surname>Services</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>San Diego</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umass</forename><surname>Amherst</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-Learning with Differentiable Convex Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many meta-learning approaches for few-shot learning rely on simple base learners such as nearest-neighbor classifiers. However, even in the few-shot regime, discriminatively trained linear predictors can offer better generalization. We propose to use these predictors as base learners to learn representations for few-shot learning and show they offer better tradeoffs between feature size and performance across a range of few-shot recognition benchmarks. Our objective is to learn feature embeddings that generalize well under a linear classification rule for novel categories. To efficiently solve the objective, we exploit two properties of linear classifiers: implicit differentiation of the optimality conditions of the convex problem and the dual formulation of the optimization problem. This allows us to use highdimensional embeddings with improved generalization at a modest increase in computational overhead. Our approach, named MetaOptNet, achieves state-of-the-art performance on miniImageNet, tieredImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. Our code is available online 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to learn from a few examples is a hallmark of human intelligence, yet it remains a challenge for modern machine learning systems. This problem has received significant attention from the machine learning community recently where few-shot learning is cast as a meta-learning problem (e.g., <ref type="bibr" target="#b24">[22,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b35">33,</ref><ref type="bibr" target="#b30">28]</ref>). The goal is to minimize generalization error across a distribution of tasks with few training examples. Typically, these approaches are composed of an embedding model that maps the input domain into a feature space and a base learner that maps the feature space to task variables. The meta-learning objective is to learn an embedding model such that the base learner generalizes well across tasks.</p><p>While many choices for base learners exist, nearestneighbor classifiers and their variants (e.g., <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b35">33]</ref>) are 1 https://github.com/kjunelee/MetaOptNet popular as the classification rule is simple and the approach scales well in the low-data regime. However, discriminatively trained linear classifiers often outperform nearestneighbor classifiers (e.g., <ref type="bibr" target="#b6">[4,</ref><ref type="bibr" target="#b18">16]</ref>) in the low-data regime as they can exploit the negative examples which are often more abundant to learn better class boundaries. Moreover, they can effectively use high dimensional feature embeddings as model capacity can be controlled by appropriate regularization such as weight sparsity or norm.</p><p>Hence, in this paper, we investigate linear classifiers as the base learner for a meta-learning based approach for fewshot learning. The approach is illustrated in <ref type="figure">Figure 1</ref> where a linear support vector machine (SVM) is used to learn a classifier given a set of labeled training examples and the generalization error is computed on a novel set of examples from the same task. The key challenge is computational since the meta-learning objective of minimizing the generalization error across tasks requires training a linear classifier in the inner loop of optimization (see Section 3). However, the objective of linear models is convex and can be solved efficiently. We observe that two additional properties arising from the convex nature that allows efficient metalearning: implicit differentiation of the optimization <ref type="bibr" target="#b4">[2,</ref><ref type="bibr" target="#b13">11]</ref> and the low-rank nature of the classifier in the few-shot setting. The first property allows the use of off-the-shelf convex optimizers to estimate the optima and implicitly differentiate the optimality or Karush-Kuhn-Tucker (KKT) conditions to train embedding model. The second property means that the number of optimization variables in the dual formation is far smaller than the feature dimension for fewshot learning.</p><p>To this end, we have incorporated a differentiable quadratic programming (QP) solver <ref type="bibr" target="#b3">[1]</ref> which allows endto-end learning of the embedding model with various linear classifiers, e.g., multiclass support vector machines (SVMs) <ref type="bibr" target="#b7">[5]</ref> or linear regression, for few-shot classification tasks. Making use of these properties, we show that our method is practical and offers substantial gains over nearest neighbor classifiers at a modest increase in computational costs (see <ref type="table">Table 3</ref>). Our method achieves state-of-the-art performance on 5-way 1-shot and 5-shot classification for popu-</p><formula xml:id="formula_0">! " ! " ℒ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embeddings of Training Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weights of Linear Classifier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score (logit) for Each Class</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Examples</head><p>Test Examples SVM Loss <ref type="figure">Figure 1</ref>. Overview of our approach. Schematic illustration of our method MetaOptNet on an 1-shot 3-way classification task. The meta-training objective is to learn the parameters φ of a feature embedding model f φ that generalizes well across tasks when used with regularized linear classifiers (e.g., SVMs). A task is a tuple of a few-shot training set and a test set (see Section 3 for details).</p><p>lar few-shot benchmarks including miniImageNet <ref type="bibr" target="#b35">[33,</ref><ref type="bibr" target="#b24">22]</ref>, tieredImageNet <ref type="bibr" target="#b25">[23]</ref>, CIFAR-FS <ref type="bibr" target="#b5">[3]</ref>, and FC100 <ref type="bibr" target="#b22">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Meta-learning studies what aspects of the learner (commonly referred to as bias or prior) effect generalization across a distribution of tasks <ref type="bibr" target="#b28">[26,</ref><ref type="bibr" target="#b33">31,</ref><ref type="bibr" target="#b34">32]</ref>. Meta-learning approaches for few-shot learning can be broadly categorized these approaches into three groups. Gradient-based methods <ref type="bibr" target="#b24">[22,</ref><ref type="bibr" target="#b10">8]</ref> use gradient descent to adapt the embedding model parameters (e.g., all layers of a deep network) given training examples. Nearest-neighbor methods <ref type="bibr" target="#b35">[33,</ref><ref type="bibr" target="#b30">28]</ref> learn a distance-based prediction rule over the embeddings. For example, prototypical networks <ref type="bibr" target="#b30">[28]</ref> represent each class by the mean embedding of the examples, and the classification rule is based on the distance to the nearest class mean. Another example is matching networks <ref type="bibr" target="#b35">[33]</ref> that learns a kernel density estimate of the class densities using the embeddings over training data (the model can also be interpreted as a form of attention over training examples). Model-based methods <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b21">19]</ref> learn a parameterized predictor to estimate model parameters, e.g., a recurrent network that predicts parameters analogous to a few steps of gradient descent in parameter space. While gradient-based methods are general, they are prone to overfitting as the embedding dimension grows <ref type="bibr" target="#b20">[18,</ref><ref type="bibr" target="#b27">25]</ref>. Nearest-neighbor approaches offer simplicity and scale well in the few-shot setting. However, nearestneighbor methods have no mechanisms for feature selection and are not very robust to noisy features.</p><p>Our work is related to techniques for backpropagation though optimization procedures. Domke <ref type="bibr" target="#b8">[6]</ref> presented a generic method based on unrolling gradient-descent for a fixed number of steps and automatic differentiation to compute gradients. However, the trace of the optimizer (i.e., the intermediate values) needs to be stored in order to compute the gradients which can be prohibitive for large problems. The storage overhead issue was considered in more detail by Maclaurin et al. <ref type="bibr" target="#b17">[15]</ref> where they studied low precision representations of the optimization trace of deep networks. If the argmin of the optimization can be found analytically, such as in unconstrained quadratic minimization problems, then it is also possible to compute the gradients analytically. This has been applied for learning in low-level vision problems <ref type="bibr" target="#b32">[30,</ref><ref type="bibr" target="#b29">27]</ref>. A concurrent and closely related work <ref type="bibr" target="#b5">[3]</ref> uses this idea to learn few-shot models using ridgeregression base learners which have closed-form solutions. We refer readers to Gould et al. <ref type="bibr" target="#b13">[11]</ref> which provides an excellent survey of techniques for differentiating argmin and argmax problems.</p><p>Our approach advocates the use of linear classifiers which can be formulated as convex learning problems. In particular, the objective is a quadratic program (QP) which can be efficiently solved to obtain its global optima using gradient-based techniques. Moreover, the solution to convex problems can be characterized by their Karush-Kuhn-Tucker (KKT) conditions which allow us to backpropagate through the learner using the implicit function theorem <ref type="bibr" target="#b14">[12]</ref>. Specifically, we use the formulation of Amos and Kolter <ref type="bibr" target="#b3">[1]</ref> which provides efficient GPU routines for computing solutions to QPs and their gradients. While they applied this framework to learn representations for constraint satisfaction problems, it is also well-suited for few-shot learning as the problem sizes that arise are typically small.</p><p>While our experiments focus on linear classifiers with hinge loss and 2 regularization, our framework can be used with other loss functions and non-linear kernels. For example, the ridge regression learner used in <ref type="bibr" target="#b5">[3]</ref> can be implemented within our framework allowing a direct comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Meta-learning with Convex Base Learners</head><p>We first derive the meta-learning framework for few-shot learning following prior work (e.g., <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b10">8]</ref>) and then discuss how convex base learners, such as linear SVMs, can be incorporated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation</head><p>Given the training set D train = {(x t , y t )} T t=1 , the goal of the base learner A is to estimate parameters θ of the predictor y = f (x; θ) so that it generalizes well to the unseen test set D test = {(x t , y t )} Q t=1 . It is often assumed that the training and test set are sampled from the same distribution and the domain is mapped to a feature space using an embedding model f φ parameterized by φ. For optimizationbased learners, the parameters are obtained by minimizing the empirical loss over training data along with a regularization that encourages simpler models. This can be written as:</p><formula xml:id="formula_1">θ = A(D train ; φ) = arg min θ L base (D train ; θ, φ) + R(θ) (1)</formula><p>where L base is a loss function, such as the negative loglikelihood of labels, and R(θ) is a regularization term. Regularization plays an important role in generalization when training data is limited.</p><p>Meta-learning approaches for few-shot learning aim to minimize the generalization error across a distribution of tasks sampled from a task distribution. Concretely, this can be thought of as learning over a collection of tasks:</p><formula xml:id="formula_2">T = {(D train i , D test i )} I i=1 , often referred to as a meta- training set. The tuple (D train i , D test i</formula><p>) describes a training and a test dataset, or a task. The objective is to learn an embedding model φ that minimizes generalization (or test) error across tasks given a base learner A. Formally, the learning objective is:</p><formula xml:id="formula_3">min φ E T L meta (D test ; θ, φ), where θ = A(D train ; φ) .</formula><p>(2) <ref type="figure">Figure 1</ref> illustrates the training and testing for a single task. Once the embedding model f φ is learned, its generalization is estimated on a set of held-out tasks (often referred to as a meta-test set)</p><formula xml:id="formula_4">S = {(D train j , D test j )} J j=1 computed as: E S L meta (D test ; θ, φ), where θ = A(D train ; φ) . (3)</formula><p>Following prior work <ref type="bibr" target="#b24">[22,</ref><ref type="bibr" target="#b10">8]</ref>, we call the stages of estimating the expectation in Equation 2 and 3 as meta-training and meta-testing respectively. During meta-training, we keep an additional held-out meta-validation set to choose the hyperparameters of the meta-learner and pick the best embedding model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Episodic sampling of tasks</head><p>Standard few-shot learning benchmarks such as miniIm-ageNet <ref type="bibr" target="#b24">[22]</ref> evaluate models in K-way, N -shot classification tasks. Here K denotes the number of classes, and N denotes the number of training examples per class. Fewshot learning techniques are evaluated for small values of N , typically N ∈ {1, 5}. In practice, these datasets do not explicitly contain tuples (D train i , D test i ), but each task for meta-learning is constructed "on the fly" during the metatraining stage, commonly described as an episode.</p><p>For example, in prior work <ref type="bibr" target="#b35">[33,</ref><ref type="bibr" target="#b24">22]</ref>, a task (or episode)</p><formula xml:id="formula_5">T i = (D train i , D test i )</formula><p>is sampled as follows. The overall set of categories is C train . For each episode, categories C i containing K categories from the C train are first sampled (with replacement); then training (support) set D train</p><formula xml:id="formula_6">i = {(x n , y n ) | n = 1, . . . , N × K, y n ∈ C i } consisting of N images per category is sampled; and finally, the test (query) set D test i = {(x n , y n ) | n = 1, . . . , Q × K, y n ∈ C i } consisting of Q images per category is sampled.</formula><p>We emphasize that we need to sample without replacement, i.e., D train i ∩ D test i = Ø, to optimize the generalization error. In the same manner, meta-validation set and meta-test set are constructed on the fly from C val and C test , respectively. In order to measure the embedding model's generalization to unseen categories, C train , C val , and C test are chosen to be mutually disjoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convex base learners</head><p>The choice of the base learner A has a significant impact on Equation 2. The base learner that computes θ = A(D train ; φ) has to be efficient since the expectation has to be computed over a distribution of tasks. Moreover, to estimate parameters φ of the embedding model the gradients of the task test error L meta (D test ; θ, φ) with respect to φ have to be efficiently computed. This has motivated simple base learners such as nearest class mean <ref type="bibr" target="#b30">[28]</ref> for which the parameters of the base learner θ are easy to compute and the objective is differentiable.</p><p>We consider base learners based on multi-class linear classifiers (e.g., support vector machines (SVMs) <ref type="bibr" target="#b7">[5,</ref><ref type="bibr" target="#b36">34]</ref>, logistic regression, and ridge regression) where the baselearner's objective is convex. For example, a K-class linear SVM can be written as θ = {w k } K k=1 . The Crammer and Singer <ref type="bibr" target="#b7">[5]</ref> formulation of the multi-class SVM is:</p><formula xml:id="formula_7">θ = A(D train ; φ) = arg min {w k } min {ξi} 1 2 k ||w k || 2 2 + C n ξ n subject to w yn · f φ (x n ) − w k · f φ (x n ) ≥ 1 − δ yn,k − ξ n , ∀n, k<label>(4)</label></formula><p>where D train = {(x n , y n )}, C is the regularization parameter and δ ·,· is the Kronecker delta function.</p><p>Gradients of the SVM objective. From <ref type="figure">Figure 1</ref>, we see that in order to make our system end-to-end trainable, we require that the solution of the SVM solver should be differentiable with respect to its input, i.e., we should be able to compute { ∂θ ∂f φ (xn) } N ×K n=1 . The objective of SVM is convex and has a unique optimum. This allows for the use of implicit function theorem (e.g., <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b9">7,</ref><ref type="bibr" target="#b4">2]</ref>) on the optimality (KKT) conditions to obtain the necessary gradients. For the sake of completeness, we derive the form of the theorem for convex optimization problems as stated in <ref type="bibr" target="#b4">[2]</ref>. Consider the following convex optimization problem:</p><formula xml:id="formula_8">minimize f 0 (θ, z) subject to f (θ, z) 0 h(θ, z) = 0.<label>(5)</label></formula><p>where the vector θ ∈ R d is the optimization variable of the problem, the vector z ∈ R e is the input parameter of the optimization problem, which is {f φ (x n )} in our case. We can optimize the objective by solving for the saddle point (θ,λ,ν) of the following Lagrangian:</p><formula xml:id="formula_9">L(θ, λ, ν, z) = f 0 (θ, z) + λ T f (θ, z) + ν T h(θ, z). (6)</formula><p>In other words, we can obtain the optimum of the objective function by solving g(θ,λ,ν, z) = 0 where</p><formula xml:id="formula_10">g(θ, λ, ν, z) =   ∇ θ L(θ, λ, ν, z) diag(λ)f (θ, z) h(θ, z)   .<label>(7)</label></formula><p>Given a function f (x) :</p><formula xml:id="formula_11">R n → R m , denote D x f (x) as its Jacobian ∈ R m×n .</formula><p>Theorem 1 (From Barratt <ref type="bibr" target="#b4">[2]</ref>) Suppose g(θ,λ,ν, z) = 0. Then, when all derivatives exist,</p><formula xml:id="formula_12">D zθ = −D θ g(θ,λ,ν, z) −1 D z g(θ,λ,ν, z).<label>(8)</label></formula><p>This result is obtained by applying the implicit function theorem to the KKT conditions. Thus, once we compute the optimal solutionθ, we can obtain a closed-form expression for the gradient ofθ with respect to the input data. This obviates the need for backpropagating through the entire optimization trajectory since the solution does not depend on the trajectory or initialization due to its uniqueness. This also saves memory, an advantage that convex problems have over generic optimization problems.</p><p>Time complexity. The forward pass (i.e., computation of Equation 4) using our approach requires the solution to the QP solver whose complexity scales as O(d 3 ) where d is the number of optimization variables. This time is dominated by factorizing the KKT matrix required for primaldual interior point method. Backward pass requires the solution to <ref type="bibr">Equation 8</ref> in Theorem 1, whose complexity is O(d 2 ) given the factorization already computed in the forward pass. Both forward pass and backward pass can be expensive when the dimension of embedding f φ is large.</p><p>Dual formulation. The dual formulation of the objective in Equation 4 allows us to address the poor dependence on the embedding dimension and can be written as follows. Let</p><formula xml:id="formula_13">w k (α k ) = n α k n f φ (x n ) ∀ k.<label>(9)</label></formula><p>We can instead optimize in the dual space:</p><formula xml:id="formula_14">max {α k } − 1 2 k ||w k (α k )|| 2 2 + n α yn n subject to α yn n ≤ C, α k n ≤ 0 ∀k = y n , k α k n = 0 ∀n.<label>(10)</label></formula><p>This results in a quadratic program (QP) over the dual variables {α k } K k=1 . We note that the size of the optimization variable is the number of training examples times the number of classes. This is often much smaller than the size of the feature dimension for few-shot learning. We solve the dual QP of Equation 10 using <ref type="bibr" target="#b3">[1]</ref> which implements a differentiable GPU-based QP solver. In practice (as seen in <ref type="table">Table 3</ref>) the time taken by the QP solver is comparable to the time taken to compute features using the ResNet-12 architectures so the overall speed per iteration is not significantly different from those based on simple base learners such as nearest class prototype (mean) used in Prototypical Networks <ref type="bibr" target="#b30">[28]</ref>.</p><p>Concurrent to our work, Bertinetto et al. <ref type="bibr" target="#b5">[3]</ref> employed ridge regression as the base learner which has a closed-form solution. Although ridge regression may not be best suited for classification problems, their work showed that training models by minimizing squared error with respect to one-hot labels works well in practice. The resulting optimization for ridge regression is also a QP and can be implemented within our framework as: </p><formula xml:id="formula_15">max {α k } − 1 2 k ||w k (α k )|| 2 2 − λ 2 k ||α k || 2 2 +</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Meta-learning objective</head><p>To measure the performance of the model we evaluate the negative log-likelihood of the test data sampled from the same task. Hence, we can re-express the meta-learning objective of Equation 2 as:</p><formula xml:id="formula_16">L meta (D test ; θ, φ, γ) = (x,y)∈D test [−γw y · f φ (x) + log k exp(γw k · f φ (x))] (12) where θ = A(D train ; φ) = {w k } K k=1</formula><p>and γ is a learnable scale parameter. Prior work in few-shot learning <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b5">3,</ref><ref type="bibr" target="#b12">10]</ref> suggest that adjusting the prediction score by a learnable scale parameter γ provides better performance under nearest class mean and ridge regression base learners.</p><p>We empirically find that inserting γ is beneficial for the meta-learning with SVM base learner as well. While other choices of test loss, such as hinge loss, are possible, loglikelihood worked the best in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first describe the network architecture and optimization details used in our experiments (Section 4.1). We then present results on standard few-shot classification benchmarks including derivatives of ImageNet (Section 4.2) and CIFAR (Section 4.3), followed by a detailed analysis of the impact of various base learners on accuracy and speed using the same embedding network and training setup (Section 4.4-4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>Meta-learning setup. We use a ResNet-12 network following <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b20">18]</ref> in our experiments. Let Rk denote a residual block that consists of three {3×3 convolution with k filters, batch normalization, Leaky ReLU(0.1)}; let MP denote a 2×2 max pooling. We use DropBlock regularization <ref type="bibr" target="#b11">[9]</ref>, a form of structured Dropout. Let DB(k, b) denote a DropBlock layer with keep rate=k and block size=b. The network architecture for ImageNet derivatives is: R64-MP-DB(0.9,1)-R160-MP-DB(0.9,1)-R320-MP-DB(0.9,5)-R640-MP-DB(0.9,5), while the network architecture used for CIFAR derivatives is: R64-MP-DB(0.9,1)-R160-MP-DB(0.9,1)-R320-MP-DB(0.9,2)-R640-MP-DB(0.9,2). We do not apply a global average pooling after the last residual block.</p><p>As an optimizer, we use SGD with Nesterov momentum of 0.9 and weight decay of 0.0005. Each mini-batch consists of 8 episodes. The model was meta-trained for 60 epochs, with each epoch consisting of 1000 episodes. The learning rate was initially set to 0.1, and then changed to 0.006, 0.0012, and 0.00024 at epochs 20, 40 and 50, respectively, following the practice of <ref type="bibr" target="#b12">[10]</ref>.</p><p>During meta-training, we adopt horizontal flip, random crop, and color (brightness, contrast, and saturation) jitter data augmentation as in <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b23">21]</ref>. For experiments on mini-ImageNet with ResNet-12, we use label smoothing with = 0.1. Unlike <ref type="bibr" target="#b30">[28]</ref> where they used higher way classification for meta-training than meta-testing, we use a 5way classification in both stages following recent works <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b22">20]</ref>. Each class contains 6 test (query) samples during meta-training and 15 test samples during meta-testing. Our meta-trained model was chosen based on 5-way 5-shot test accuracy on the meta-validation set. Meta-training shot. For prototypical networks, we match the meta-training shot to meta-testing shot following the usual practice <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b12">10]</ref>. For SVM and ridge regression, we observe that keeping meta-training shot higher than metatesting shot leads to better test accuracies as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Hence, during meta-training, we set training shot to 15 for miniImageNet with ResNet-12; 5 for miniImageNet with 4-layer CNN (in <ref type="table">Table 3</ref>); 10 for tieredImageNet; 5 for CIFAR-FS; and 15 for FC100. Base-learner setup. For linear classifier training, we use the quadratic programming (QP) solver OptNet <ref type="bibr" target="#b3">[1]</ref>. Regularization parameter C of SVM was set to 0.1. Regularization parameter λ of ridge regression was set to 50.0. For the nearest class mean (prototypical networks), we use squared Euclidean distance normalized with respect to the feature dimension. Early stopping. Although we can run the optimizer until convergence, in practice we found that running the QP solver for a fixed number of iterations (just three) works well in practice. Early stopping acts an additional regularizer and even leads to a slightly better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on ImageNet derivatives</head><p>The miniImageNet dataset <ref type="bibr" target="#b35">[33]</ref> is a standard benchmark for few-shot image classification benchmark, consisting of 100 randomly chosen classes from ILSVRC-2012 <ref type="bibr" target="#b26">[24]</ref>. These classes are randomly split into 64, 16 and 20 classes for meta-training, meta-validation, and meta-testing respectively. Each class contains 600 images of size 84×84. Since the class splits were not released in the original publication <ref type="bibr" target="#b35">[33]</ref>, we use the commonly-used split proposed in <ref type="bibr" target="#b24">[22]</ref>.</p><p>The tieredImageNet benchmark <ref type="bibr" target="#b25">[23]</ref> is a larger subset of ILSVRC-2012 <ref type="bibr" target="#b26">[24]</ref>, composed of 608 classes grouped into 34 high-level categories. These are divided into 20 categories for meta-training, 6 categories for meta-validation, and 8 categories for meta-testing. This corresponds to 351, 97 and 160 classes for meta-training, meta-validation, and meta-testing respectively. This dataset aims to minimize the semantic similarity between the splits. All images are of size 84 × 84. Results. <ref type="table">Table 1</ref> summarizes the results on the 5-way mini-ImageNet and tieredImageNet. Our method achieves stateof-the-art performance on 5-way miniImageNet and tiered-ImageNet benchmarks. Note that LEO <ref type="bibr" target="#b27">[25]</ref> make use of encoder and relation network in addition to the WRN-28-10 backbone network to produce sample-dependent initializa- <ref type="table">Table 1</ref>. Comparison to prior work on miniImageNet and tieredImageNet. Average few-shot classification accuracies (%) with 95% confidence intervals on miniImageNet and tieredImageNet meta-test splits. a-b-c-d denotes a 4-layer convolutional network with a, b, c, and d filters in each layer. * Results from <ref type="bibr" target="#b24">[22]</ref>. † Used the union of meta-training set and meta-validation set to meta-train the meta-learner. "RR" stands for ridge regression. tion of gradient descent. TADAM <ref type="bibr" target="#b22">[20]</ref> employs a task embedding network (TEN) block for each convolutional layer -which predicts element-wise scale and shift vectors.</p><p>We also note that <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b23">21]</ref> pretrain the WRN-28-10 feature extractor <ref type="bibr" target="#b38">[36]</ref> to jointly classify all 64 classes in mini-ImageNet meta-training set; then freeze the network during the meta-training. <ref type="bibr" target="#b22">[20]</ref> make use of a similar strategy of using standard classification: they co-train the feature embedding on few-shot classification task (5-way) and standard classification task (64-way). In contrast, our system is meta-trained end-to-end, explicitly training the feature extractor to work well on few-shot learning tasks with regularized linear classifiers. This strategy allows us to clearly see the effect of meta-learning. Our method is arguably simpler and achieves strong performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on CIFAR derivatives</head><p>The CIFAR-FS dataset <ref type="bibr" target="#b5">[3]</ref> is a recently proposed fewshot image classification benchmark, consisting of all 100 classes from CIFAR-100 <ref type="bibr" target="#b15">[13]</ref>. The classes are randomly split into 64, 16 and 20 for meta-training, meta-validation, and meta-testing respectively. Each class contains 600 images of size 32 × 32.</p><p>The FC100 dataset <ref type="bibr" target="#b22">[20]</ref> is another dataset derived from CIFAR-100 <ref type="bibr" target="#b15">[13]</ref>, containing 100 classes which are grouped into 20 superclasses. These classes are partitioned into 60 classes from 12 superclasses for meta-training, 20 classes from 4 superclasses for meta-validation, and 20 classes from 4 superclasses for meta-testing. The goal is to minimize semantic overlap between classes similar to the goal  Results. <ref type="table">Table 2</ref> summarizes the results on the 5-way classification tasks where our method MetaOptNet-SVM achieves the state-of-the-art performance. On the harder FC100 dataset, the gap between various base learners is more significant, which highlights the advantage of complex base learners in the few-shot learning setting. <ref type="table">Table 2</ref>. Comparison to prior work on CIFAR-FS and FC100. Average few-shot classification accuracies (%) with 95% confidence intervals on CIFAR-FS and FC100. a-b-c-d denotes a 4-layer convolutional network with a, b, c, and d filters in each layer. * CIFAR-FS results from <ref type="bibr" target="#b5">[3]</ref>. † FC100 result from <ref type="bibr" target="#b22">[20]</ref>. ¶ Used the union of meta-training set and meta-validation set to meta-train the meta-learner. "RR" stands for ridge regression.</p><p>CIFAR-FS 5-way FC100 5-way model backbone 1-shot 5-shot 1-shot 5-shot MAML * <ref type="bibr" target="#b10">[8]</ref> 32-32-32-32 58.9 ± 1.9 71.5 ± 1.0 --Prototypical Networks * † <ref type="bibr" target="#b30">[28]</ref> 64-64-64-64 55.5 ± 0.7 72.0 ± 0.6 35.3 ± 0.6 48.6 ± 0.6 Relation Networks * <ref type="bibr" target="#b31">[29]</ref> 64-96-128-256 55.0 ± 1.0 69.3 ± 0.8 --R2D2 <ref type="bibr" target="#b5">[3]</ref> 96-192-384-512 65.3 ± 0.2 79.4 ± 0.1 --TADAM <ref type="bibr" target="#b22">[20]</ref> ResNet-12 --40.1 ± 0.4 56.1 ± 0.4 ProtoNets (our backbone) <ref type="bibr" target="#b30">[28]</ref> ResNet-12 72.2 ± 0.7 83.5 ± 0.5 37.5 ± 0.6 52.5 ± 0.  <ref type="table">Table 3</ref>. Effect of the base learner and embedding network architecture. Average few-shot classification accuracy (%) and forward inference time (ms) per episode on miniImageNet and tieredImageNet with varying base learner and backbone architecture. The former group of results used the standard 4-layer convolutional network with 64 filters per layer used in <ref type="bibr" target="#b35">[33,</ref><ref type="bibr" target="#b30">28]</ref>, whereas the latter used a 12-layer ResNet without the global average pooling. "RR" stands for ridge regression.  <ref type="table">Table 3</ref> shows the results where we vary the base learner for two different embedding architectures. When we use a standard 4-layer convolutional network where the feature dimension is low (1600), we do not observe a substantial benefit of adopting discriminative classifiers for few-shot learning. Indeed, nearest class mean classifier <ref type="bibr" target="#b19">[17]</ref> is proven to work well under a low-dimensional feature as shown in Prototypical Networks <ref type="bibr" target="#b30">[28]</ref>. However, when the embedding dimensional is much higher (16000), SVMs yield better few-shot accuracy than other base learners. Thus, regularized linear classifiers provide robustness when highdimensional features are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons between base learners</head><p>The added benefits come at a modest increase in com-putational cost. For ResNet-12, compared to nearest class mean classifier, the additional overhead is around 13% for the ridge regression base learner and around 30-50% for the SVM base learner. As seen in from <ref type="figure" target="#fig_2">Figure 2</ref>, the performance of our model on both 1-shot and 5-shot regimes generally increases with increasing meta-training shot. This makes the approach more practical as we can meta-train the embedding once with a high shot for all meta-testing shots.</p><p>As noted in the FC100 experiment, SVM base learner seems to be beneficial when the semantic overlap between test and train is smaller. We hypothesize that the class embeddings are more significantly more compact for training data than test data (e.g., see <ref type="bibr" target="#b37">[35]</ref>); hence flexibility in the base learner allows robustness to noisy embeddings and improves generalization.   <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b23">21]</ref>, we use the union of the meta-training and meta-validation sets to meta-train the embedding, keeping the hyperparameters, such as the number of epochs, identical to the previous setting. In particular, we terminate the meta-training after 21 epochs for mini-ImageNet, 52 epochs for tieredImageNet, 21 epochs for CIFAR-FS, and 21 epochs for FC100. <ref type="table">Tables 1 and 2</ref> show the results with the augmented meta-training sets, denoted as MetaOptNet-SVM-trainval. On minImageNet, CIFAR-FS, and FC100 datasets, we observe improvements in test accuracies. On tieredImageNet dataset, the difference is negligible. We suspect that this is because our system has not yet entered the regime of overfitting (In fact, we observe ∼94% test accuracy on tieredImageNet meta-training set). Our results suggest that meta-learning embedding with more meta-training "classes" helps reduce overfitting to the meta-training set. Various regularization techniques. <ref type="table" target="#tab_2">Table 4</ref> shows the effect of regularization methods on MetaOptNet-SVM with ResNet-12. We note that early works on few-shot learning <ref type="bibr" target="#b30">[28,</ref><ref type="bibr" target="#b10">8]</ref> did not employ any of these techniques. We observe that without the use of regularization, the performance of ResNet-12 reduces to the one of the 4-layer convolutional network with 64 filters per layer shown in <ref type="table">Table 3</ref>. This shows the importance of regularization for meta-learners. We expect that performances of few-shot learning systems would be further improved by introducing novel regularization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Efficiency of dual optimization</head><p>To see whether the dual optimization is indeed effective and efficient, we measure accuracies on meta-test set with varying iteration of the QP solver. Each iteration of QP solver <ref type="bibr" target="#b3">[1]</ref>  are shown in <ref type="figure">Figure 3</ref>. The QP solver reaches the optima of ridge regression objective in just one iteration. Alternatively one can use its closed-form solution as used in <ref type="bibr" target="#b5">[3]</ref>. Also, we observe that for 1-shot tasks, the QP SVM solver reaches optimal accuracies in 1 iteration, although we observed that the KKT conditions are not exactly satisfied yet. For 5-shot tasks, even if we run QP SVM solver for 1 iteration, we achieve better accuracies than other base learners. When the iteration of SVM solver is limited to 1 iteration, 1 episode takes 69 ± 17 ms for an 1-shot task, and 80 ± 17 ms for a 5shot task, which is on par with the computational cost of the ridge regression solver <ref type="table">(Table 3)</ref>. These experiments show that solving dual objectives for SVM and ridge regression is very effective under few-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented a meta-learning approach with convex base learners for few-shot learning. The dual formulation and KKT conditions can be exploited to enable computational and memory efficient meta-learning that is especially well-suited for few-shot learning problems. Linear classifiers offer better generalization than nearestneighbor classifiers at a modest increase in computational costs (as seen in <ref type="table">Table 3</ref>). Our experiments suggest that regularized linear models allow significantly higher embedding dimensions with reduced overfitting. For future work, we aim to explore other convex base-learners such as kernel SVMs. This would allow the ability to incrementally increase model capacity as more training data becomes available for a task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where w k is defined as Equation 9. A comparison of linear SVM and ridge regression in Section 4 shows a slight advantage of the linear SVM formation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Test accuracies (%) on meta-test sets with varying meta-training shot. Shaded region denotes 95% confidence interval. In general, the performance of MetaOptNet-SVM on both 1-shot and 5-shot regimes increases with increasing meta-training shot. of tieredImageNet. Each class contains 600 images of size 32 × 32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>6</head><label>6</label><figDesc>MetaOptNet-RR (ours) ResNet-12 72.6 ± 0.7 84.3 ± 0.5 40.5 ± 0.6 55.3 ± 0.6 MetaOptNet-SVM (ours) ResNet-12 72.0 ± 0.7 84.2 ± 0.5 41.1 ± 0.6 55.5 ± 0.6 MetaOptNet-SVM-trainval (ours) ¶ ResNet-12 72.8 ± 0.7 85.0 ± 0.5 47.2 ± 0.6 62.5 ± 0.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>%) time (ms) acc. (%) time (ms) acc. (%) time (ms) acc. (%) time (ms) 4-layer conv (feature dimension=1600)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>involves computing updates for primal and dual variables via LU decomposition of KKT matrix. The results Ablation study. Various regularization techniques improves test accuracy regularization techniques improves test accuracy (%) on 5-way miniImageNet benchmark. We use MetaOptNet-SVM with ResNet-12 for results. 'Data Aug.', 'Label Smt.', and 'Larger Data' stand for data augmentation, label smoothing on the meta-learning objective, and merged dataset of meta-training split and meta-test split, respectively.</figDesc><table><row><cell>Data Aug.</cell><cell>Weight Decay</cell><cell>Drop Block</cell><cell>Label Smt.</cell><cell>Larger Data</cell><cell>1-shot 5-shot</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51.13 70.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.80 75.76</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56.65 73.72</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60.33 76.61</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>61.11 77.40</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.64 78.63</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64.09 80.00</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors thank Yifan Xu, Jimmy Yan, Weijian Xu, Justin Lazarow, and Vijay Mahadevan for valuable discussions. Also, we appreciate the anonymous reviewers for their helpful and constructive comments and suggestions. Finally, we would like to thank Chuyi Sun for help with <ref type="figure">Figure 1</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaoptnet-Rr</surname></persName>
		</author>
		<idno>ours) 53.23±0.59 20±0.03 69.51±0.48 27±0.05 54.63±0.67 21±0.05 72.11±0.59 28±0.06</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Metaoptnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svm</surname></persName>
		</author>
		<idno>ours) 52.87±0.57 28±0.02 68.76±0.48 37±0.05 54.71±0.67 28±0.07 71.79±0.59 38±0.08</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Metaoptnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svm</surname></persName>
		</author>
		<idno>ours) 62.64±0.61 78±17 78.63±0.46 89±17 65.99±0.72 78±17 81.56±0.53 90±17</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">OptNet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the Differentiability of the Solution to Convex Optimization Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Barratt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05098</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closedform solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical evaluation of supervised learning in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generic methods for optimization-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Implicit functions and solution mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Asen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dontchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tyrrell Rockafellar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Monogr. Math</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">On differentiating parameterized argmin and argmax problems with application to bi-level optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edison</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05447</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The implicit function theorem: history, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">R</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cifar-100 (canadian institute for advanced research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based hyperparameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distance-based image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2624" to="2637" />
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Evolutionary principles in selfreferential learning. on learning now to learn: The metameta-meta...-hook. Diploma thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987-05-14" />
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technische Universitat Munchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning gaussian conditional random fields for low-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Lifelong Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="181" to="209" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A perspective view and survey of meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Vilalta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Drissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="95" />
			<date type="published" when="2002-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Support Vector Machines for Multiclass Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium On Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
