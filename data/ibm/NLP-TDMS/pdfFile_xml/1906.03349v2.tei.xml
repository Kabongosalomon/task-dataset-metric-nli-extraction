<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Modeling with Correlation Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
							<email>hengwang@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
							<email>trandu@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
							<email>torresani@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Video Modeling with Correlation Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motion is a salient cue to recognize actions in video. Modern action recognition models leverage motion information either explicitly by using optical flow as input or implicitly by means of 3D convolutional filters that simultaneously capture appearance and motion information. This paper proposes an alternative approach based on a learnable correlation operator that can be used to establish frame-toframe matches over convolutional feature maps in the different layers of the network. The proposed architecture enables the fusion of this explicit temporal matching information with traditional appearance cues captured by 2D convolution. Our correlation network compares favorably with widely-used 3D CNNs for video modeling, and achieves competitive results over the prominent two-stream network while being much faster to train. We empirically demonstrate that correlation networks produce strong results on a variety of video datasets, and outperform the state of the art on four popular benchmarks for action recognition: Kinetics, Something-Something, Diving48 and Sports1M.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>After the breakthrough of AlexNet <ref type="bibr" target="#b29">[30]</ref> on ImageNet <ref type="bibr" target="#b6">[7]</ref>, convolutional neural networks (CNNs) have become the dominant model for still-image classification <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b20">21]</ref>. In the video domain, CNNs were initially adopted as image-based feature extractor on individual frames of the video <ref type="bibr" target="#b26">[27]</ref>. More recently, CNNs for video analysis have been extended with the capability of capturing not only appearance information contained in individual frames but also motion information extracted from the temporal dimension of the image sequence. This is usually achieved by one of two possible mechanisms. One strategy involves the use of a two-stream network <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b4">5]</ref> where one stream operates on RGB frames to model appearance information and the other stream extracts motion features from optical flow provided as input. The representations obtained from these two distinct inputs are then fused, typically in a late layer of the network. An alternative strategy is to use 3D convolutions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b8">9]</ref> which couple appearance and temporal modeling by means of spatiotemporal kernels.</p><p>In this paper we propose a new scheme based on a novel correlation operator inspired by the correlation layer in FlowNet <ref type="bibr" target="#b10">[11]</ref>. While in FlowNet the correlation layer is only applied once to convert the video information from the RGB pixel space to the motion displacement space, we propose a learnable correlation operator to establish frameto-frame matches over convolutional feature maps to capture different notions of similarity in different layers of the network. Similarly to two-stream models, our model enables the fusion of explicit motion cues with appearance information. However, while in two-stream models the motion and appearance subnets are disjointly learned and fused only in a late layer of the model, our network enables the efficient integration of appearance and motion information throughout the network. Compared to 3D CNNs, which extract spatiotemporal features, our model factorizes the computation of appearance and motion, and learns distinct filters capturing different measures of patch similarity. The learned filters can match pixels moving in different directions. Through our extensive experiments on four action recognition datasets (Kinetics, Something-Something, Div-ing48 and Sports1M), we demonstrate that our correlation network compares favorably with widely-used 3D CNNs for video modeling, and achieves competitive results over the prominent two-stream network while being much faster to train. We summarize our contributions as follows:</p><p>• A new correlation operator with learnable filters. By making use of dilation and grouping, the operator is highly efficient to compute. Compared to 3D convolution or optical flow, it provides an alternative way to model temporal information in video. • A new correlation network which is designed to integrate motion and appearance information in every block. A rigorous study of the new architecture and comparisons with strong baselines provide insights for the different design choices. • Our correlation network outperforms the state-of-the-art on four different video datasets without using optical  flow.</p><p>In the rest of the paper, we introduce related work in Section 2, and detail the proposed correlation operator in Section 3. We present the correlation network in Section 4. Experimental setups are in Section 5. We discuss the experimental results in Section 6 and conclude the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Architectures for video classification. Among the popular video models, there are two major categories: two-stream networks <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b4">5]</ref> and 3D CNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>. Since the introduction of two-stream networks <ref type="bibr" target="#b45">[46]</ref>, further improvements have been achieved by adding connections between the two streams <ref type="bibr" target="#b15">[16]</ref>, or inflating a 2D model to 3D <ref type="bibr" target="#b3">[4]</ref>. 3D CNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b52">53</ref>] learn appearance and motion information simultaneously by convolving 3D filters in space and time. Successful image architectures <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b20">21]</ref> have been extended to video using 3D convolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b62">63]</ref>. Recent research <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> shows that decomposing 3D convolution into 2D spatial convolution and 1D temporal convolution leads to better performance. Our correlation network goes beyond twostream networks and 3D convolution, and we propose a new operator that can better learn the temporal dynamics of video sequences. Motion information for action recognition. Before the popularity of deep learning, various video features <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b55">56]</ref> were hand-designed to encode motion information in video. Besides two-stream networks and 3D CNNs, ActionFlowNet <ref type="bibr" target="#b38">[39]</ref> proposes to jointly estimate optical flow and recognize actions in one network. Fan et al. <ref type="bibr" target="#b11">[12]</ref> and Piergiovanni et al. <ref type="bibr" target="#b39">[40]</ref> also introduced networks to learn optical flow end-to-end for action recognition.</p><p>There is also work <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22]</ref> seeking alternatives to optical flow. Sun et al. <ref type="bibr" target="#b50">[51]</ref> extracted features guided by optical flow to capture the transformation between adjacent frames. Lee et al. <ref type="bibr" target="#b33">[34]</ref> designed motion filters by computing the difference of adjacent frames. Hommos et al. <ref type="bibr" target="#b21">[22]</ref> proposed to use phase instead of optical flow as the motion representation for action recognition. Our paper is along the line of designing architectures to directly learn motion information from raw RGB pixels. Applications of correlation operation. Deep matching <ref type="bibr" target="#b60">[61]</ref> computes the correlation of image patches to find dense correspondence to improve optical flow. Unlike deep matching using hand-crafted features, FlowNet <ref type="bibr" target="#b10">[11]</ref> is a network, where a correlation layer performs multiplicative patch comparisons. Correlation layers were also used in other CNN-based optical flow algorithms <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b23">24]</ref>. Besides optical flow, Rocco et al. <ref type="bibr" target="#b43">[44]</ref> used it to estimate the geometric transformation of two images, whereas Feichtenhofer et al. <ref type="bibr" target="#b16">[17]</ref> applied it to object tracking.</p><p>In the context of action recognition, Zhao et al. <ref type="bibr" target="#b66">[67]</ref> utilize the correlation layer to compute a cost volume to estimate the displacement map as in optical flow. The Spatio-Temporal Channel Correlation Network <ref type="bibr" target="#b7">[8]</ref> adapts the Squeeze-and-Excitation block <ref type="bibr" target="#b22">[23]</ref> to a ResNeXt <ref type="bibr" target="#b61">[62]</ref> backbone. The notion of correlation in <ref type="bibr" target="#b7">[8]</ref> refers to the relationship among the spatial and temporal dimensions of the feature maps, which is different from the matching of ad- <ref type="table">Table 1</ref>: A comparison of the correlation operator with 3D convolution. When the size K of the filter is similar (i.e., K * K ≈ K t * K y * K x ), the parameters of 3D convolution is about C out /L times more than the correlation operator, and its FLOPs is about C out times higher. jacent frames studied in our work. We compare our results with <ref type="bibr" target="#b7">[8]</ref> in Section 6.3. Our paper extends this line of ideas by introducing a learnable operator based on correlation. Instead of trying to explicitly or implicitly estimate optical flow, the correlation operator is used repeatedly in combination with other operators to build a new architecture that can learn appearance and motion information simultaneously and that achieves state of the art accuracy on various video datasets.</p><formula xml:id="formula_0">Operator Correlation 3D convolution Input C in × L × H × W C in × L × H × W Filter L × C in × K × K C out × C in × K t × K y × K x Output (G * K * K) × L × H × W C out × L × H × W # params L * C in * K * K C out * C in * K t * K y * K x FLOPs C in * K * K * L * H * W C out * C in * K t * K y * K x * L * H * W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Correlation Operator</head><p>This section describes the proposed correlation operator. We start by reviewing the existing correlation operator over image pairs used in optical flow <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b48">49]</ref> and geometric matching <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b43">44]</ref>. We then propose to inject filters into the operator to make it learnable. We discuss how to increase the number of output channels while retaining efficiency and low number of parameters by means of a groupwise variant. We finally generalize the operator to work on sequences of video frames. Correlation operator for matching. As shown in Figure 1 (a), each image is represented by a 3D tensor of size C ×H ×W , where C is the number of channels and H ×W is the spatial resolution. Given a feature patch P B (i, j) in image B, we compute the similarity of this patch with another patch P A (i , j ) in image A, where (i, j) is the spatial location of the patch. To make the computation more tractable, the size of the feature patch can be reduced to a single pixel, thus P A (i , j ) and P B (i, j) becomes Cdimensional vectors. The similarity is defined as the dot product of the two vectors:</p><formula xml:id="formula_1">S(i, j, i , j ) = 1/C * C c=1 (P B c (i, j) * P A c (i , j )),<label>(1)</label></formula><p>where 1/C is for normalization. (i , j ) is often limited to be within a K ×K neighborhood of (i, j). K is the maximal displacement for patch matching. Considering all possible locations of (i, j) and (i , j ) in Eq. 1, the output S is a tensor of size K × K × H × W , where K × K can be flattened to play the role of channel to generate a 3D feature tensor (</p><formula xml:id="formula_2">K 2 × H × W ) like the input image.</formula><p>Learnable correlation operator. Computer vision has achieved impressive results by moving from hand-crafted features <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b5">6]</ref> to learnable deep neural networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>The original correlation operator <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b43">44]</ref> does not include learnable parameters and thus it is quite limited in terms of the types of representations it can generate. We propose to endow the operator with a learnable filter as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b). Our motivation is to learn to select informative channels during matching. To achieve this goal we introduce a weight vector W c to Eq. 1 in the dot product computation:</p><formula xml:id="formula_3">W c * P B c (i, j) * P A c (i , j ).</formula><p>The similarity of two feature patches (i.e., P B (i, j) and P A (i , j )) is often related to how close their spatial location is. We thus apply different weight vectors W c to different locations in the K × K neighbor to take into account the spatial distribution of the matching disparity. Thus, the size of each filter is C × K × K as summarized in <ref type="table">Table 1</ref>.</p><p>K indicates the maximal displacement when matching two patches. Larger valued K can cover larger regions and encode more information. The downside is that the computational cost grows quadratically w.r.t. K. Inspired by the dilated convolution <ref type="bibr" target="#b63">[64]</ref>, we propose to perform dilated correlation to handle large displacement without increasing the computational cost. We enlarge the matching region in image A by a dilation factor D. In practice, we set K = 7 with a dilation factor of D = 2 to cover a region of 13 × 13 pixels. Besides dilation, we also apply the operator at different spatial scales (as discussed in Section 4), which is a popular strategy to handle large displacements in optical flow <ref type="bibr" target="#b42">[43]</ref>. From <ref type="figure" target="#fig_3">Figure 4</ref>, filters do learn to select discriminative channels as filters from certain channels are more active than the other. Having different weights in the K × K neighborhood also enables the filter to learn pixel movements in different directions. Groupwise correlation operator. The correlation operator converts a feature map from C × H × W to K 2 × H × W . In popular CNNs, C can be one to two orders of magnitude larger than K 2 . This means that the correlation operator may cause a great reduction in the number of channels. This is not a problem for applications such as optical flow or geometric matching, where the correlation operator is only applied once. If we want to design a network based on the correlation operator and apply it repeatedly, it will reduce the dimension of the channels dramatically, and degrade the representation power of the learned features, as shown by the results in Section 6.2.</p><p>Similar to <ref type="bibr" target="#b19">[20]</ref>, we propose a groupwise version of the correlation operator that avoids shrinking the number of channels while maintaining efficiency. Groupwise convolution <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b61">62]</ref> was introduced to reduce the computational cost of convolution by constraining each kernel to span a subset of feature channels. Here we utilize this idea to increase the number of output channels without increasing the computational cost. For the groupwise correlation operator, all C channels are split into G groups for both input images and filters, and the correlation operation is computed within each group. The outputs of all groups are stacked together as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (c). This increases the number of output channels by a factor of G, to a total of K 2 G channels. The size of each group is g = C/G. By choosing the group size properly, we can control the number of channels without additional cost. From two images to a video clip. The original correlation operator is designed for matching a pair of images. In this paper, we apply it for video classification where the input is a sequence of L video frames. We extend the operator to video by computing correlation for every pair of adjacent frames of the input sequence. As the number of adjacent frame pairs is L − 1 (i.e., one fewer than the number of frames), we propose to compute self-correlation for the first frame in addition to the cross-correlation of adjacent frame pairs, shown in <ref type="figure" target="#fig_0">Figure 1 (d)</ref>. It can keep the length L of the output feature map consistent with the input, and make the correlation operator easier to use when designing new architectures. The gradual change of filters within each column of <ref type="figure" target="#fig_3">Figure 4</ref> shows filters learn to follow the motion of pixels across frames when extending the correlation operator to a video clip. <ref type="table">Table 1</ref> summarizes our final proposed correlation operator and compares it with the standard 3D convolution. Intuitively, 3D convolution seeks to learn both spatial and temporal representation by convolving a 3D filter in space and time. The correlation operator however, is intentionally designed to capture matching information between adjacent frames. The correlation operator provides an alternative way to model temporal information for video classification, and it has much fewer parameters and FLOPs than the popular 3D convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Correlation Network</head><p>The correlation operator is designed to learn temporal information, and needs to be combined with other operators  capturing appearance information in order to yield a comprehensive set of features for video classification. We first briefly introduce the backbone architecture adapted from R(2+1)D <ref type="bibr" target="#b54">[55]</ref>, then discuss how to build the correlation network to leverage the matching information by incorporating the correlation operator into the backbone. R(2+1)D backbone. The R(2+1)D network <ref type="bibr" target="#b54">[55]</ref> was recently introduced and shown to yield state-of-the-art action recognition results on several video datasets. R(2+1)D factorizes the traditional 3D convolution (i.e., 3 × 3 × 3) into a 2D spatial convolution (i.e., 1 × 3 × 3) and an 1D temporal convolution (i.e., 3 × 1 × 1). Decoupling the spatial and temporal filtering is beneficial for both hand-crafted features <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b9">10]</ref> and 3D CNNs <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b40">41]</ref>. Compared with the original R(2+1)D <ref type="bibr" target="#b54">[55]</ref>, we make a few changes to further simplify and improve its efficiency, e.g., using bottleneck layers, supporting higher input resolution, keeping the number of channels consistent, less temporal striding, etc. <ref type="table" target="#tab_2">Table 2</ref> provides the details of the R(2+1)D backbone used in this paper. Correlation network. To incorporate the correlation operator into the backbone network, we propose the two types of correlation blocks shown in <ref type="figure">Figure 2</ref>. The design of these blocks is similar in spirit to that of the bottleneck block <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure">Figure 2</ref> (a) illustrates the correlation-sum block. It first uses an 1 × 1 × 1 convolution to reduce the number of channels, then applies a correlation operator for feature matching. Finally another 1 × 1 × 1 is used to restore the original number of channels. A shortcut connection <ref type="bibr" target="#b20">[21]</ref> is applied The output of the two branches are combined together by concatenation in the channel dimension. We compare the two different designs in Section 6.2. We obtain the final correlation network by inserting the correlation block into the R(2+1)D backbone architecture. In this paper, we insert one correlation block after res 2 , res 3 and res 4 in <ref type="table" target="#tab_2">Table 2</ref>. We omit res 5 as its spatial resolution is low (i.e., 7 × 7). Note that the number of FLOPs of the correlation operator is much lower than 3D convolution. The correlation network only adds a small overhead to the computational cost of the backbone network. Section 6.1 provides a more quantitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setups</head><p>Video Datasets. We evaluate our model on four video datasets that have rather different properties, emphasizing distinct aspects of action recognition. Kinetics <ref type="bibr" target="#b27">[28]</ref> is among the most popular datasets for video classification. It consists of about 300K YouTube videos covering 400 categories. Something-Something <ref type="bibr" target="#b18">[19]</ref> is created by crowdsourcing. This dataset focuses on humans performing predefined basic actions with everyday objects. The same action is performed with different objects ("something") so that models are forced to understand the basic actions instead of recognizing the objects. It includes about 100K videos covering 174 classes. We note this dataset as Something for short. Diving48 <ref type="bibr" target="#b34">[35]</ref> was recently introduced and includes videos from diving competitions. The dataset is designed to reduce the bias of scene and object context in action recognition, and force the model to focus on understanding temporal dynamics of video data. It has a fine-grained taxonomy covering 48 different types of diving with 18K videos in total. The annotations of Sports1M <ref type="bibr" target="#b26">[27]</ref> are produced automatically by analyzing the text metadata surrounding the videos. As there are many long videos in Sports1M, we cut them into shorter clips to better utilize the data and end up with a training set of about 5M samples. For Kinetics and Something, annotations on the testing set are not public available, so we report accuracy on the validation set like others. For Diving48 and Sports1M, we report accuracy on the testing set following the setup by the authors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b26">27]</ref>. Training and Testing. To train the correlation network, we sample a clip of L (16 or 32) frames with a resolution of 224 × 224 from a given video. Some videos in Something do not have enough frames. We simply repeat each frame twice for those videos. For data augmentation, we resize the input video to have shorter side randomly sampled in [256, 320] pixels, following <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b46">47]</ref>, and apply temporal jittering when sampling clips for training. For the default configuration of our correlation network, we use the correlationsum block, and set the filter size to K = 7 and group size to g = 32. Training is done with synchronous distributed SGD on GPU clusters using Caffe2 <ref type="bibr" target="#b2">[3]</ref> with a cosine learning rate schedule <ref type="bibr" target="#b35">[36]</ref>. We train the model for 250 epochs in total with the first 40 epochs for warm-up <ref type="bibr" target="#b17">[18]</ref> on Kinetics. As Something and Diving48 are smaller datasets, we reduce the training epochs from 250 to 150 on them. For Sports1M, we train 500 epochs since it is the largest dataset. For testing, we sample 10 clips uniformly spaced out in the video and average the clip-level predictions to generate the video-level results. Except in Section 6.3, all reported results are obtained by training from scratch without pretraining on ImageNet <ref type="bibr" target="#b6">[7]</ref> or other large-scale video datasets. We only use RGB as the input to our model, unlike two-stream networks <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b57">58]</ref> which use both RGB and optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Evaluation</head><p>To demonstrate the advantages of the proposed correlation network, we first compare the correlation operator with temporal convolution in Section 6.1. We evaluate the correlation network under different settings to justify our design choices and compare with the two-stream network in Section 6.2. We show that our correlation network outperforms the state of the art on all four datasets in Section 6.3. Finally, we visualize the learned filters in Section 6.4.   tions (i.e., 3 × 1 × 1), and adding a 3 × 1 × 1 max pooling when we need to do temporal striding. CorrNet-26 is obtained by inserting one correlation-sum block after res 2 , res 3 and res 4 of R(2+1)D-26 as described in Section 4. As the correlation block adds a small overhead to the FLOPs, we further reduce the number of filters for conv 1 from 64 to 32, and remove the 3 × 1 × 1 temporal convolutions from res 2 for CorrNet. This reduces the accuracy of CorrNet only slightly (less than 0.5%). The resulting CorrNet-26 has similar FLOPs as R(2+1)D-26, as shown in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Correlation network vs baseline backbones</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R2D vs R(2+1)D. The gap between R2D and R(2+1)D varies dramatically on different datasets. On Kinetics and</head><p>Diving48, R(2+1)D is only 2-5% better than R2D, but the gap widens up to 20% on Something. This is consistent with findings in <ref type="bibr" target="#b62">[63]</ref> and is due to the design of Something where objects are not predictive of the action label. This also highlights the challenges of designing new architectures that can generalize well to different types of datasets. R(2+1)D vs CorrNet. We observe a consistent improvement of over 3% on three datasets when comparing CorrNet with R(2+1)D in <ref type="table" target="#tab_3">Table 3</ref>. We achieve the most significant gain on Diving48, i.e., 4.3%, using 16 frames. Note that our improved R(2+1)D is a very strong baseline and its performance is already on par with the best results (listed in   <ref type="table">Table 5</ref>: Action recognition accuracy (%) of CorrNet vs two-stream network. <ref type="table" target="#tab_8">Table 6</ref> and 7) reported. A significant 3% improvement on three datasets shows the power of the information learned from pixel matching and the general applicability of the correlation network to model video of different characteristics. Moreover, CorrNet only increases the GFLOPs of the network by a very small margin, from 71.9 to 74.8, comparing with R(2+1)D. Input clip length. <ref type="table" target="#tab_3">Table 3</ref> also compares different models using different input length L. As expected, increasing L from 16 to 32 frames can boost the performance across all datasets. Something and Diving48 benefit more from using longer inputs. It is noteworthy that the improvements of CorrNet over R(2+1)D are largely carried over when using 32 frames. To simplify, we use L = 32 frames in all the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluating design choices and comparison to two-stream network</head><p>To justify our design choices, we experimentally compare different configurations of CorrNet-26 in <ref type="table" target="#tab_6">Table 4</ref>. We consider the following modifications: 1) remove filters in the correlation operator; 2) remove grouping to reduce the number of channels from C to K 2 ; 3) swap the correlationsum block with correlation-concat. Note that we only change one thing at a time.</p><p>Removing filters results in an accuracy drop of 1% on both datasets, as it significantly reduces the power of the learned representations. Similarly, the aggressive channel reduction introduced by removing grouping also causes an accuracy drop of about 1%. The correlation-concat block performs worse than correlation-sum, which leverages the shortcut connection to ease optimization. <ref type="figure" target="#fig_2">Figure 3</ref> shows the performance of CorrNet-26 for K ∈ {3, 5, 7, 9}. As expected, a larger K can cover a larger</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretrain</head><p>Two GFLOPs Kinetics stream × crops STC-ResNext-101 <ref type="bibr" target="#b7">[8]</ref> N/A 68.7 R(2+1)D <ref type="bibr" target="#b54">[55]</ref> 152×115 72.0 MARS+RGB <ref type="bibr" target="#b4">[5]</ref> N/A 74.8 ip-CSN-152 <ref type="bibr" target="#b53">[54]</ref> 109×30 77.8 DynamoNet <ref type="bibr" target="#b8">[9]</ref> N/A 77.9 SlowFast-101 <ref type="bibr" target="#b14">[15]</ref> 213×30 78.9 SlowFast-101+NL <ref type="bibr" target="#b14">[15]</ref> 234×30  neighborhood while matching pixels, thus yields a higher accuracy. But the improvements become marginal beyond K = 7, possibly due to the low resolution of the feature maps.</p><p>We compare CorrNet-26 with the two-stream network using the R(2+1)D backbone in <ref type="table">Table 5</ref>. We use the Farneback <ref type="bibr" target="#b12">[13]</ref> algorithm for computing optical flow. The two-stream network of R(2+1)D is implemented by concatenating the features after global average pooling. For R(2+1)D, the accuracy gap between RGB and optical flow is smaller on Something, as Kinetics is arguably more biased towards appearance information. Our CorrNet-26 alone is on par with R(2+1)D-26 using two streams. Note that two-stream network effectively doubles the FLOPs of the backbone and the cost of computing optical flow (not considered here) can be very high as well. This shows that our correlation network is more efficient by learning motion information from RGB pixels directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparison to the state of the art</head><p>The correlation network discussed in the previous sections is based on R(2+1)D-26 with a block configuration of [2, 2, 2, 2] for res 2 , res 3 , res 4 and res 5 . To compare with the state-of-the-art, we simply add more layers to the backbone. Following the design of ResNet <ref type="bibr" target="#b20">[21]</ref>, CorrNet-50 uses a block configuration of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3]</ref>, whereas CorrNet-101 uses <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b2">3]</ref>. Like in CorrNet-26, a correlation block is inserted after res 2 , res 3 and res 4 for CorrNet-50. For CorrNet-101, we insert an additional correlation block in the middle of res 4 , so there are 4 correlation blocks in total.  of <ref type="table" target="#tab_8">Table 6</ref> and 7) at test time, we sample more clips (30 instead of 10), as done in <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref> . As expected, using deeper models or sampling more clips can further improve the accuracy. Comparing with CorrNet-26 in <ref type="table" target="#tab_3">Table 3</ref>, CorrNet-101 is 4.1%, 4.3% and 3.1% better on Kinetics, Something and Diving48, respectively. As Diving48 is the smallest dataset among the four, increasing model capacity may lead to overfitting, thus the improvement is less significant. We also experiment with pre-training CorrNet-101 using the Sports1M dataset <ref type="bibr" target="#b26">[27]</ref>. This time we achieve the most significant improvement on Diving48, i.e., 6.1%. Smaller datasets are likely to benefit more from pre-training, as we have seen in the case of UCF101 <ref type="bibr" target="#b47">[48]</ref> and HMDB51 <ref type="bibr" target="#b30">[31]</ref>. On both Kinetics and Something, we observe a modest improvement of 1-2% by pre-training on Sports1M.</p><p>On Kinetics, CorrNet-101 significantly outperforms the previous models using the same setup (i.e., no pretraining and only using RGB) except for the recently introduced SlowFast network <ref type="bibr" target="#b14">[15]</ref> augmented with non-local network (NL) <ref type="bibr" target="#b58">[59]</ref>. In fact, compared to SlowFast-101, CorrNet-101 achieves slightly higher accuracy (79.2% vs 78.9%), and it is only 0.6% lower in accuracy when SlowFast-101 is combined with NL. Comparing with results using pre-training, CorrNet-101 is 1.6% better than LGD-3D <ref type="bibr" target="#b41">[42]</ref>, i.e., 81.0% vs 79.4%. The two-stream LGD-3D improves the accuracy to 81.2% by extracting the computationally expensive TV-L1 optical flow <ref type="bibr" target="#b65">[66]</ref>.</p><p>Comparing CorrNet-101 with other approaches trained from scratch in <ref type="table" target="#tab_10">Table 7</ref>, we observe a significant improvement of 7.8% on Something (51.7% for CorrNet-101 vs. 43.9% for MFNet-C101 <ref type="bibr" target="#b33">[34]</ref>). On Diving48 <ref type="bibr" target="#b34">[35]</ref>, the improvement is even more substantial, i.e., over 17% (38.6% from CorrNet-101 vs. 21.4% from R(2+1)D). With pre-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Pretrain Two stream Sports1M C3D <ref type="bibr" target="#b52">[53]</ref> 61.1 P3D <ref type="bibr" target="#b40">[41]</ref> 66.4 R(2+1)D <ref type="bibr" target="#b54">[55]</ref> 73.0 ip-CSN-152 <ref type="bibr" target="#b53">[54]</ref> 75.5 Conv Pool <ref type="bibr" target="#b64">[65]</ref> 71.7 R(2+1)D <ref type="bibr" target="#b54">[55]</ref> 73.3 CorrNet-101 77.1 To sum up, CorrNet is a new versatile backbone that outperforms the state-of-the-art on a wide variety of video datasets. Thanks to the efficient design of the correlation operator and our improved R(2+1)D backbone, the FLOPs of CorrNet is also lower than those of previous models, such as NL I3D <ref type="bibr" target="#b58">[59]</ref>. FLOPs can further be significantly reduced (i.e., 3x decrease) by sampling fewer clips during testing with only a small drop in accuracy, as shown in the third last row of <ref type="table" target="#tab_8">Table 6</ref> and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Visualizing Correlation Filters</head><p>In this section, we visualize the filters (i.e., the yellow tensor in <ref type="figure" target="#fig_0">Fig. 1</ref>) from the correlation operator to better understand the model. We choose the CorrNet-101 trained from scratch on Kinetics from <ref type="table" target="#tab_8">Table 6</ref>, and the correlation operator with the highest output resolution, i.e., from the correlation block after res 2 . The size of the filter is L×C ×K ×K as listed in <ref type="table">Table 1</ref>, which is 32×64×7×7 in this case. We visualize filters for l = 0, . . . , 7 and c = 0, . . . , 7 in <ref type="figure" target="#fig_3">Figure 4</ref>. The color coding indicates the weights in the learned filters, and the white arrows point to the directions with largest weights.</p><p>Zooming into filters in <ref type="figure" target="#fig_3">Figure 4</ref>, we observe that each filter learns a specific motion pattern (i.e., the 7 × 7 grid) for matching. The filters in each column are sorted in time and exhibit similar patterns. The white arrows often point to similar directions for the filters in the same column. This suggests that our network learns the temporal consistency We visualize the correlation filters, which is a 4D tensor of shape L × C × K × K. Filters in each column are aligned in time, and each column represents a different channel dimension. White arrows point to locations with highest weights, showing that different filters learn to match pixels moving in different directions. of motion, i.e., pixels usually move in the same direction across frames. Comparing filters in different columns, we observe that some columns are more active than others, which indicates that our filters learns which channels are more discriminative for matching. Filter weights for these channels can be larger than channels that are less informative for matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>This paper explores a novel way to learn motion information from video data. Unlike previous approaches based on optical flow or 3D convolution, we propose a learnable correlation operator which establishes frame-to-frame matches over convolutional feature maps in the different layers of the network. Differently from the standard 3D convolution, the correlation operator makes the computation of motion information explicit. We design the correlation network based on this novel operator and demonstrate its superior performance on various video datasets for action recognition. Potential future work includes the application of the learnable correlation operator to other tasks, such as action localization, optical flow, and geometry matching.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of the proposed correlation operator. (a) Correlation operator used for optical flow and geometric matching. (b) The introduction of filters renders the operator "learnable." (c) Groupwise correlation increases the number of output channels without adding computational cost. (d) Extending the correlation operator to work on a sequence of video frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 CFigure 2 :</head><label>22</label><figDesc>Two types of correlation blocks. We mark the number of channels for each operator. for residual learning. The correlation-concat block in Figure 2 (b) has two branches within the block: one branch with a correlation operator and another branch passing the input feature maps through an 1 × 1 × 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Effect of filter size K on classification accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of CorrNet-101 trained on Kinetics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The R(2+1)D backbone for building correlation network.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 compares</head><label>3</label><figDesc>the correlation network with different baselines. We denote the backbone architecture from Table 2 as R(2+1)D-26. To demonstrate the importance of temporal learning on different datasets, we create R2D-26, which is obtained by removing all 1D temporal convolu-</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Top-1 accuracy (%)</cell><cell></cell></row><row><cell>Model</cell><cell>Length</cell><cell>GFLOPs</cell><cell>Kinetics</cell><cell>Something</cell><cell>Diving</cell></row><row><cell>R2D-26</cell><cell>16</cell><cell>27.5</cell><cell>67.8</cell><cell>15.8</cell><cell>17.5</cell></row><row><cell>R(2+1)D-26</cell><cell>16</cell><cell>36.0</cell><cell>69.9</cell><cell>35.4</cell><cell>22.7</cell></row><row><cell>CorrNet-26</cell><cell>16</cell><cell>37.4</cell><cell>73.4</cell><cell>38.5</cell><cell>27.0</cell></row><row><cell>R2D-26</cell><cell>32</cell><cell>55.0</cell><cell>70.1</cell><cell>28.1</cell><cell>29.2</cell></row><row><cell>R(2+1)D-26</cell><cell>32</cell><cell>71.9</cell><cell>72.3</cell><cell>45.0</cell><cell>32.2</cell></row><row><cell>CorrNet-26</cell><cell>32</cell><cell>74.8</cell><cell>75.1</cell><cell>47.4</cell><cell>35.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Correlation networks vs baselines. Our CorrNet significantly outperforms the two baseline architectures on three datasets, at a very small increase in FLOPs compared to R(2+1)D. Using longer clip length L leads to better accuracy on all three datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Action recognition accuracy (%) for different con-</cell></row><row><cell>figurations of CorrNet.</cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell cols="2">Kinetics Something</cell></row><row><cell>CorrNet-26</cell><cell>75.1</cell><cell>47.4</cell></row><row><cell>R(2+1)D-26 (RGB)</cell><cell>72.3</cell><cell>45.0</cell></row><row><cell>R(2+1)D-26 (OF)</cell><cell>66.5</cell><cell>42.5</cell></row><row><cell>R(2+1)D-26 (Two-stream)</cell><cell>74.4</cell><cell>47.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Compare with the state-of-the-art on Kinetics-400.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>, 7 and 8 compare the accuracy of CorrNet-50 and CorrNet-101 with several recently published results under different settings. For CorrNet-101 (the last two rows</figDesc><table><row><cell>Methods</cell><cell>Pretrain</cell><cell>Two stream</cell><cell cols="2">Something Diving</cell></row><row><cell>R(2+1)D [55]</cell><cell></cell><cell></cell><cell></cell><cell>21.4</cell></row><row><cell>TRN [68]</cell><cell></cell><cell></cell><cell>34.4</cell></row><row><cell>MFNet-C101 [34]</cell><cell></cell><cell></cell><cell>43.9</cell></row><row><cell>NL I3D-50 [59]</cell><cell>ImageNet</cell><cell></cell><cell>44.4</cell></row><row><cell>R(2+1)D [55]</cell><cell>Sports1M</cell><cell></cell><cell>45.7</cell><cell>28.9</cell></row><row><cell>NL I3D-50+GCN [60]</cell><cell>ImageNet</cell><cell></cell><cell>46.1</cell></row><row><cell>DiMoFs [2]</cell><cell>Kinetics</cell><cell></cell><cell></cell><cell>31.4</cell></row><row><cell>Attention-LSTM [26]</cell><cell>ImageNet</cell><cell></cell><cell></cell><cell>35.6</cell></row><row><cell>GST-50 [38]</cell><cell>ImageNet</cell><cell></cell><cell>48.6</cell><cell>38.8</cell></row><row><cell>MARS+RGB [5]</cell><cell>Kinetics</cell><cell></cell><cell>51.7</cell></row><row><cell>S3D-G [63]</cell><cell>ImageNet</cell><cell></cell><cell>48.2</cell></row><row><cell>TRN [68]</cell><cell>ImageNet</cell><cell></cell><cell>42.0</cell><cell>22.8</cell></row><row><cell>MARS+RGB+Flow [5]</cell><cell>Kinetics</cell><cell></cell><cell>53.0</cell></row><row><cell>CorrNet-50</cell><cell></cell><cell></cell><cell>49.3</cell><cell>37.9</cell></row><row><cell>CorrNet-101</cell><cell></cell><cell></cell><cell>50.9</cell><cell>38.2</cell></row><row><cell>CorrNet-101</cell><cell></cell><cell></cell><cell>51.7</cell><cell>38.6</cell></row><row><cell>CorrNet-101</cell><cell>Sports1M</cell><cell></cell><cell>53.3</cell><cell>44.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Compare with the state-of-the-art on Something-Something v1 and Diving48.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Comparison with the state-of-the-art on Sports1M. Something, i.e., 53.3 vs 53.0.Table 8provides a comparison with the state of the art on Sports1M. We only evaluate our best model CorrNet-101 to limit the training time. All the methods inTable 8are trained from scratch since Sports1M is already a very large scale video dataset. Our CorrNet-101 established a new state of the art, i.e. 77.1%, outperforming the very recent ip-CSN-152 [54] by 1.6%. CorrNet-101 also significantly outperforms R(2+1)D [55] by 3.8% which uses both RGB and optical flow.</figDesc><table><row><cell>training, CorrNet-101 is still 1.6% and 5.9% better on</cell></row><row><cell>Something and Diving48. CorrNet-101 even slightly out-</cell></row><row><cell>performs MARS [5] augmented with RGB and optical flow</cell></row><row><cell>streams on</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequential deep learning for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atilla</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Human Behavior Understanding</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning discriminative motion features through detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04172</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Caffe2: A new lightweight, modular, and scalable deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe2-Team</surname></persName>
		</author>
		<ptr target="https://caffe2.ai/.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mars: Motion-augmented rgb stream for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nieves</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7882" to="7891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahman</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dynamonet: Dynamic action and motion network. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV VS-PETS</title>
		<meeting>ICCV VS-PETS</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><forename type="middle">Ermon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6016" to="6025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on Image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04730</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detect to track and track to detect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3038" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The something something video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3273" to="3282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using phase instead of optical flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Hommos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silvia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV Workshops</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attentive spatio-temporal representation learning for diving classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhakar</forename><surname>Gagan Kanojia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanmuganathan</forename><surname>Kumawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR Workshops</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">HMDB51: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunggi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Sungjoon Son, Gyutae Park, and Nojun Kwak</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grouped spatial-temporal aggregation for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5512" to="5521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1616" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="12056" to="12065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: a fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="214" to="223" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Recognize actions by disentangling components of dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6566" to="6575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
