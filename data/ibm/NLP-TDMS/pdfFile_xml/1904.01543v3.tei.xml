<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
						</author>
						<title level="a" type="main">Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph kernels based on the 1-dimensional Weisfeiler-Leman algorithm and corresponding neural architectures recently emerged as powerful tools for (supervised) learning with graphs. However, due to the purely local nature of the algorithms, they might miss essential patterns in the given data and can only handle binary relations. The k-dimensional Weisfeiler-Leman algorithm addresses this by considering k-tuples, defined over the set of vertices, and defines a suitable notion of adjacency between these vertex tuples. Hence, it accounts for the higher-order interactions between vertices. However, it does not scale and may suffer from overfitting when used in a machine learning setting. Hence, it remains an important open problem to design WL-based graph learning methods that are simultaneously expressive, scalable, and non-overfitting. Here, we propose local variants and corresponding neural architectures, which consider a subset of the original neighborhood, making them more scalable, and less prone to overfitting. The expressive power of (one of) our algorithms is strictly higher than the original algorithm, in terms of ability to distinguish non-isomorphic graphs. Our experimental study confirms that the local algorithms, both kernel and neural architectures, lead to vastly reduced computation times, and prevent overfitting. The kernel version establishes a new state-of-the-art for graph classification on a wide range of benchmark datasets, while the neural version shows promising performance on large-scale molecular regression tasks. tween two k-tuples. However, it fixes the cardinality of this neighborhood to k · n, where n denotes the number of vertices of a given graph. Hence, the running time of each iteration does not take the sparsity of a given graph into account. Further, new neural architectures <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b77">78]</ref> that possess the same power as the k-WL in terms of separating non-isomorphic graphs suffer from the same drawbacks, i.e., they have to resort to dense matrix multiplications. Moreover, when used in a machine learning setting with real-world graphs, the k-WL may capture the isomorphism type, which is the complete structural information inherent in a graph, after only a couple of iterations, which may lead to overfitting, see <ref type="bibr" target="#b81">[82]</ref>, and the experimental section of the present work.</p><p>Present work To address this, we propose a local version of the k-WL, the local δ-k-dimensional Weisfeiler-Leman algorithm (δ-k-LWL), which considers a subset of the original neighborhood in each iteration. The cardinality of the local neighborhood depends on the sparsity of the graph, i.e., the degrees of the vertices of a given k-tuple. We theoretically analyze the strength of a variant of our local algorithm and prove that it is strictly more powerful in distinguishing non-isomorphic graphs compared to the k-WL. Moreover, we devise a hierarchy of pairs of non-isomorphic graphs that a variant of the δ-k-LWL can separate while the k-WL cannot. On the neural side, we devise a higher-order graph neural network architecture, the δ-k-LGNN, and show that it has the same expressive power as the δ-k-LWL. Moreover, we connect it to recent advancements in learning theory for GNNs <ref type="bibr" target="#b40">[41]</ref>, which show that the δ-k-LWL architecture has better generalization abilities compared to dense architectures based on the k-WL. See <ref type="figure">Figure 1</ref> for an overview of the proposed algorithms.</p><p>Experimentally, we apply the discrete algorithms (or kernels) and the (local) neural architectures to supervised graph learning, and verify that both are several orders of magnitude faster than the global, discrete algorithms or dense, neural architectures, and prevent overfitting. The discrete algorithms establish a new state-of-the-art for graph classification on a wide range of small-and mediumscale classical datasets. The neural version shows promising performance on large-scale molecular regression tasks.</p><p>Related work In the following, we review related work from graph kernels and GNNs. We refer to Appendix A for an in-depth discussion of related work, as well as a discussion of theoretical results for the k-WL.</p><p>Historically, kernel methods-which implicitly or explicitly map graphs to elements of a Hilbert space-have been the dominant approach for supervised learning on graphs. Important early work in this area includes kernels based on random-walks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b69">70]</ref>, shortest paths [13], and kernels based on the 1-WL [100]. Morris et al.</p><p>[82] devised a local, set-based variant of the k-WL. However, the approach is (provably) weaker than the tuple-based algorithm, and they do not prove convergence to the original algorithm. For a thorough survey of graph kernels, see <ref type="bibr" target="#b70">[71]</ref>. Recently, graph neural networks (GNNs) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b96">97]</ref> emerged as an alternative to graph kernels. Notable instances of this architecture include, e.g., <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b104">105]</ref>, and the spectral approaches proposed in, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b80">81</ref>]-all of which descend from early work in <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b96">97]</ref>. A survey of recent advancements in GNN techniques can be found, e.g., in [19, 113, 125]. Recently, connections to Weisfeiler-Leman type algorithms have been shown <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b114">115]</ref>. Specifically, the authors of <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b114">115]</ref> showed that the expressive power of any possible GNN architecture is limited by the 1-WL in terms of distinguishing non-isomorphic graphs. Morris et al. <ref type="bibr" target="#b82">[83]</ref> introduced k-dimensional GNNs (k-GNN) which rely on a message-passing scheme between subgraphs of cardinality k. Similar to <ref type="bibr" target="#b81">[82]</ref>, the paper employed a local, set-based (neural) variant of the k-WL, which is (provably) weaker than the variant considered here. Later, this was refined in [77] by introducing k-order invariant graph networks (k-IGN), based on Maron et al. <ref type="bibr" target="#b77">[78]</ref>, which are equivalent to the folklore variant of the k-WL <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref> in terms of distinguishing non-isomorphic graphs. However, k-IGN may not scale since they rely on dense linear algebra routines. Chen et al. <ref type="bibr" target="#b23">[24]</ref> connect the theory of universal approximation of permutation-invariant functions and the graph isomorphism viewpoint and introduce a variation of the 2-WL, which is more powerful than the former. Our comprehensive treatment of higher-order, sparse, (graph) neural networks for arbitrary k subsumes all of the algorithms and neural architectures mentioned above.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph-structured data is ubiquitous across application domains ranging from chemo-and bioinformatics <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b102">103]</ref> to image <ref type="bibr" target="#b100">[101]</ref> and social network analysis <ref type="bibr" target="#b26">[27]</ref>. To develop successful machine learning models in these domains, we need techniques that can exploit the rich information inherent in the graph structure, as well as the feature information contained within nodes and edges. In recent years, numerous approaches have been proposed for machine learning with graphs-most notably, approaches based on graph kernels <ref type="bibr" target="#b70">[71]</ref> or using graph neural networks (GNNs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. Here, graph kernels based on the 1-dimensional Weisfeiler-Leman algorithm (1-WL) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b110">111]</ref>, and corresponding GNNs <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b114">115]</ref> have recently advanced the state-of-the-art in supervised node and graph learning. Since the 1-WL operates via simple neighborhood aggregation, the purely local nature of these approaches can miss important patterns in the given data. Moreover, they are only applicable to binary structures, and therefore cannot deal with general t-ary structures, e.g., hypergraphs <ref type="bibr" target="#b123">[124]</ref> or subgraphs, in a straight-forward way. A provably more powerful algorithm (for graph isomorphism testing) is the k-dimensional Weisfeiler-Leman algorithm (k-WL) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b76">77]</ref>. The algorithm can capture more global, higher-order patterns by iteratively computing a coloring (or discrete labeling) for k-tuples, instead of single vertices, based on an appropriately defined notion of adjacency be- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We briefly describe the Weisfeiler-Leman algorithm and, along the way, introduce our notation, see Appendix B for expanded preliminaries. As usual, let [n] = {1, . . . , n} ⊂ N for n ≥ 1, and let { {. . . } } denote a multiset. We also assume elementary definitions from graph theory (such as graphs, directed graphs, vertices, edges, neighbors, trees, and so on). The vertex and the edge set of a graph G are denoted by V (G) and E(G) respectively. The neighborhood</p><formula xml:id="formula_0">of v in V (G) is denoted by δ(v) = N (v) = {u ∈ V (G) | (v, u) ∈ E(G)}. Moreover, its complement δ(v) = {u ∈ V (G) | (v, u) / ∈ E(G)}.</formula><p>We say that two graphs G and H are isomorphic (G H) if there exists an adjacency preserving bijection ϕ : V (G) → V (H), i.e., <ref type="bibr">(u, v)</ref> is in E(G) if and only if (ϕ(u), ϕ(v)) is in E(H), call ϕ an isomorphism from G to H. If the graphs have vertex/edges labels, the isomorphism is additionally required to match these labels. A rooted tree is a tree with a designated vertex called root in which the edges are directed in such a way that they point away from the root. Let p be a vertex in a directed tree then we call its out-neighbors children with parent p. Given a k-tuple of vertices v = (v 1 , . . . , v k ), let G[v] denote the subgraph induced on the set {v 1 , . . . , v k }, where, the vertex v i is labeled with i, for i in [k].</p><p>Vertex refinement algorithms For a fixed positive integer k, let V (G) k denote the set of k-tuples of vertices of G. A coloring of V (G) k is a mapping C : V (G) k → N, i.e., we assign a number (or color) to every tuple in V (G) k . The initial coloring C 0 of V (G) k is specified by the isomorphism types of the tuples, i.e., two tuples v and w in V (G) k get a common color iff the mapping v i → w i induces an isomorphism between the labeled subgraphs G[v] and G <ref type="bibr">[w]</ref>. A color class corresponding to a color c is the set of all tuples colored c, i.e., the set C −1 (c). For j in [k], let φ j (v, w) be the k-tuple obtained by replacing the j th component of v with the vertex w. That is, φ j (v, w) = (v 1 , . . . , v j−1 , w, v j+1 , . . . , v k ). If w = φ j (v, w) for some w in V (G), call w a j-neighbor of v (and vice-versa). The neighborhood of v is then defined as the set of all tuples w such that w = φ j (v, w) for some j in [k] and w in V (G). The refinement of a coloring C : V (G) k → N, denoted by C, is a coloring C : V (G) k → N defined as follows. For each j in [k], collect the colors of the j-neighbors of v as a multiset S j = { {C(φ j (v, w)) | w ∈ V (G)} }. Then, for a tuple v, define C(v) = (C(v), M (v)), where M (v) is the k-tuple (S 1 , . . . , S k ). For consistency, the strings C(v) thus obtained are lexicographically sorted and renamed as integers. Observe that the new color C(v) of v is solely dictated by the color histogram of its neighborhood and the previous color of v. In general, a different mapping M (·) could be used, depending on the neighborhood information that we would like to aggregate.</p><p>The k-dim. Weisfeiler-Leman For k ≥ 2, the k-WL computes a coloring C ∞ : V (G) k → N of a given graph G, as follows. <ref type="bibr" target="#b3">4</ref> To begin with, the initial coloring C 0 is computed. Then, starting with C 0 , successive refinements C i+1 = C i are computed until convergence. That is,</p><formula xml:id="formula_1">C i+1 (v) = (C i (v), M i (v)), where M i (v) = { {C i (φ 1 (v, w)) | w ∈ V (G)} }, . . . , { {C i (φ k (v, w)) | w ∈ V (G)} } .<label>(1)</label></formula><p>The successive refinement steps are also called rounds or iterations. Since the disjoint union of the color classes form a partition of V (G) k , there must exist a finite ≤ |V (G)| k such that C = C . In the end, the k-WL outputs C as the stable coloring C ∞ . The k-WL distinguishes two graphs G and H if, upon running the k-WL on their disjoint union G∪ H, there exists a color c in N in the stable coloring such that the corresponding color class S c satisfies |V (G) k ∩ S c | = |V (H) k ∩ S c |, i.e., there exist an unequal number of c-colored tuples in V (G) k and V (H) k . Hence, two graphs distinguished by the k-WL must be non-isomorphic. See Appendix C for its relation to the folklore k-WL.</p><formula xml:id="formula_2">The δ-k-dim. Weisfeiler-Leman Let w = φ j (v, w) be a j-neighbor of v. Call v a local j-neighbor of w if w is adjacent to the replaced vertex v j . Otherwise, call v a global j-neighbor of w.</formula><p>For tuples v and w in V (G) k , let the function adj((v, w)) evaluate to L or G, depending on whether w is a local or a global neighbor, respectively, of v. The δ-k-dimensional Weisfeiler-Leman algorithm, denoted by δ-k-WL, is a variant of the classic k-WL which differentiates between the local and the global neighbors during neighborhood aggregation <ref type="bibr" target="#b75">[76]</ref>. Formally, the δ-k-WL algorithm refines a coloring C i (obtained after i rounds) via the aggregation function</p><formula xml:id="formula_3">M δ,δ i (v) = { {(C i (φ 1 (v, w), adj(v, φ 1 (v, w))) | w ∈ V (G)} }, . . . , { {(C i (φ k (v, w), adj(v, φ k (v, w))) | w ∈ V (G)} } ,<label>(2)</label></formula><p>instead of the k-WL aggregation specified by Equation <ref type="formula" target="#formula_1">(1)</ref>. We define the 1-WL to be the δ-1-WL, which is commonly known as color refinement or naive vertex classification.</p><p>Comparison of k-WL variants Let A 1 and A 2 denote two vertex refinement algorithms, we write A 1 A 2 if A 1 distinguishes between all non-isomorphic pairs A 2 does, and A 1 ≡ A 2 if both directions hold. The corresponding strict relation is denoted by . The following result shows that the δ-k-WL is strictly more powerful than the k-WL for k ≥ 2 (see Appendix C.1.1 for the proof). Proposition 1. For k ≥ 2, the following holds: δ-k-WL k-WL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Local δ-k-dimensional Weisfeiler-Leman algorithm</head><p>In this section, we define the new local δ-k-dimensional Weisfeiler-Leman algorithm (δ-k-LWL). This variant of the δ-k-WL considers only local neighbors during the neighborhood aggregation process, and discards any information about the global neighbors. Formally, the δ-k-LWL algorithm refines a coloring C i (obtained after i rounds) via the aggregation function,</p><formula xml:id="formula_4">M δ i (v) = { {C i (φ 1 (v, w)) | w ∈ N (v 1 )} }, . . . , { {C i (φ k (v, w)) | w ∈ N (v k )} } ,<label>(3)</label></formula><p>instead of Equation <ref type="formula" target="#formula_3">(2)</ref>, hence considering only the local j-neighbors of the tuple v in each iteration. The indicator function adj used in Equation <ref type="formula" target="#formula_3">(2)</ref> is trivially equal to L here, and is thus omitted. The coloring function for the δ-k-LWL is then defined by</p><formula xml:id="formula_5">C k,δ i+1 (v) = (C k,δ i (v), M δ i (v)).</formula><p>We also define the δ-k-LWL + , a minor variation of the δ-k-LWL. Later, we will show that the δk-LWL + is equivalent in power to the δ-k-WL (Theorem 2). Formally, the δ-k-LWL + algorithm refines a coloring C i (obtained after i rounds) via the aggregation function,</p><formula xml:id="formula_6">M δ,+ (v) = { {(C i (φ 1 (v, w)), # 1 i (v, φ 1 (v, w))) | w ∈ N (v 1 )} }, . . . , { {(C i (φ k (v, w)), # k i (v, φ k (v, w))) | w ∈ N (v k )} } ,<label>(4)</label></formula><p>instead of δ-k-LWL aggregation defined in Equation <ref type="formula" target="#formula_4">(3)</ref>. Here, the function</p><formula xml:id="formula_7"># j i (v, x) = {w : w ∼ j v, C i (w) = C i (x)} ,<label>(5)</label></formula><p>where w ∼ j v denotes that w is j-neighbor of v, for j in [k]. Essentially, # j i (v, x) counts the number of j-neighbors (local or global) of v which have the same color as x under the coloring C i (i.e., after i rounds). For a fixed v, the function # j i (v, ·) is uniform over the set S ∩ N j , where S is a color class obtained after i iterations of the δ-k-LWL + and N j denotes the set of j-neighbors of v. Note that after the stable partition has been reached # j i (v) will not change anymore. Intuitively, this variant captures local and to some extent global information, while still taking the sparsity of the underlying graph into account. Moreover, observe that each iteration of the δ-k-LWL + has the same asymptotic running time as an iteration of the δ-k-LWL, and that the information of the # function is already implicitly contained in Equation <ref type="formula" target="#formula_3">(2)</ref>.</p><p>The following theorem shows that the local variant δ-k-LWL + is at least as powerful as the δ-k-WL when restricted to the class of connected graphs. The possibly slower convergence leads to advantages in a machine learning setting, see Sections 4 and 6, and also Section 5 for a discussion of practicality, running times, and remaining challenges. Theorem 2. For the class of connected graphs, the following holds for all k ≥ 1:</p><formula xml:id="formula_8">δ-k-LWL + ≡ δ-k-WL.</formula><p>Along with Proposition 1, this establishes the superiority of the δ-k-LWL + over the k-WL. Corollary 3. For the class of connected graphs, the following holds for all k ≥ 2:</p><formula xml:id="formula_9">δ-k-LWL + k-WL.</formula><p>In fact, the proof of Proposition 1 shows that the infinite family of graphs G k , H k witnessing the strictness condition can even be distinguished by the δ-k-LWL, for each corresponding k ≥ 2. We note here that the restriction to connected graphs can easily be circumvented by adding a specially marked vertex, which is connected to every other vertex in the graph.</p><p>Kernels based on vertex refinement algorithms After running the δ-k-LWL (and the other vertex refinements algorithms), the concatenation of the histogram of colors in each iteration can be used as a feature vector in a kernel computation. Specifically, in the histogram for every color c in N there is an entry containing the number of nodes or k-tuples that are colored with c.</p><p>Local converges to global: proof of Theorem 2 The main technique behind the proof is to construct tree-representations of the colors assigned by the k-WL (or its variants). Given a graph G, a tuple v, and an integer ≥ 0, the unrolling tree of the graph G at v of depth is a rooted directed tree UNR [G, s, ] (with vertex and edge labels) which encodes the color assigned by k-WL to the tuple v after rounds, see Appendix D.2 for a formal definition and <ref type="figure" target="#fig_1">Figure 4</ref> for an illustration. The usefulness of these tree representations is established by the following lemma. Formally, let s and t be two k-vertex-tuples in V (G) k . For different k-WL variants, the construction of these unrollings are slightly different, since an unrolling tree needs to faithfully represent the corresponding aggregation process for generating new colors. For the variants δ-k-WL, δ-k-LWL, and δ-k-LWL + , we define respective unrolling trees δ-UNR [G, s, ], L-UNR [G, s, ], and L + -UNR [G, s, ] along with analogous lemmas, as above, stating their correctness/usefulness. Finally, we show that for connected graphs, the δ-UNR unrolling trees (of sufficiently large depth) at two tuples s and t are identical only if the respective δ-k-LWL + unrolling trees (of sufficiently larger depth) are identical, as shown in the following lemma. Lemma 5. Let G be a connected graph, and let s and t in V (G) k . If the stable colorings of s and t under δ-k-LWL + are identical, then the stable colorings of s and t under δ-k-WL are also identical.</p><p>Hence, the local algorithm δ-k-LWL + is at least as powerful as the global δ-k-WL, for connected graphs, i.e., δ-k-LWL + δ-k-WL. The exact details and parameters of this proof can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Higher-order neural architectures</head><p>Although the discrete kernels defined in the previous section are quite powerful, they are limited due to their fixed feature construction scheme, hence suffering from poor adaption to the learning task at hand and the inability to handle continuous node and edge labels in a meaningful way. Moreover, they often result in high-dimensional embeddings forcing one to resort to non-scalable, kernelized optimization procedures. This motivates our definition of a new neural architecture, called local δ-k-GNN (δ-k-LGNN). Given a labeled graph G, let each tuple v in V (G) k be annotated with an initial feature f (0) (v) determined by its isomorphism type. In each layer t &gt; 0, we compute a new feature</p><formula xml:id="formula_10">f (t) (v) as f W1 mrg f (t−1) (v), f W2 agg { {f (t−1) (φ 1 (v, w)) | w ∈ δ(v 1 )} }, . . . , { {f (t−1) (φ k (v, w)) | w ∈ δ(v k )} } , in R 1×e for a tuple v, where W (t) 1 and W (t)</formula><p>2 are learnable parameter matrices from R d×e . <ref type="bibr" target="#b4">5</ref> . Moreover, f W2 mrg and the permutation-invariant f W1 agg can be arbitrary (permutation-invariant) differentiable functions, responsible for merging and aggregating the relevant feature information, respectively. Initially, we set f (0) (v) to a one-hot encoding of the (labeled) isomorphism type of G <ref type="bibr">[v]</ref>. Note that we can naturally handle discrete node and edge labels as well as directed graphs, see Section 4 on how to deal with continuous information. The following result demonstrates the expressive power of the δ-k-GNN, in terms of distinguishing non-isomorphic graphs. Theorem 6. Let (G, l) be a labeled graph. Then for all t ≥ 0 there exists a sequence of weights</p><formula xml:id="formula_11">W (t) such that C k,δ t (v) = C k,δ t (w) ⇐⇒ f (t) (v) = f (t) (w)</formula><p>. Hence, for all graphs, the following holds for all k ≥ 1:</p><formula xml:id="formula_12">δ-k-LGNN ≡ δ-k-LWL.</formula><p>Moreover, the δ-k-GNN inherits the main strength of the δ-k-LWL, i.e., it can be implemented using sparse matrix multiplication. Note that it is not possible to come up with an architecture, i.e., instantiations of f W1 mrg and f W2 agg , such that it becomes more powerful than the δ-k-LWL, see <ref type="bibr" target="#b82">[83]</ref>. However, all results from the previous section can be lifted to the neural setting. That is, one can derive neural architectures based on the δ-k-LWL + , δ-k-WL, and k-WL, called δ-k-LGNN + , δ-k-GNN, and k-WL-GNN, respectively, and prove results analogous to Theorem 6.</p><p>Incorporating continous information Since many real-world graphs, e.g., molecules, have continuous features (real-valued vectors) attached to vertices and edges, using a one-hot encoding of the (labeled) isomorphism type is not a sensible choice. Let a : V (G) → R 1×d be a function such that each vertex v is annotated with a feature a(v) in R 1×d , and let v = (v 1 , . . . , v k ) be a k-tuple of vertices. Then we can compute an inital feature</p><formula xml:id="formula_13">f (0) (v) = f W3 enc (a(v 1 ), . . . , a(v k )) ,<label>(6)</label></formula><p>for the tuple v. Here, f enc : R 1×d k → R 1×e is an arbitrary differentiable, parameterized function, e.g., a multi-layer perceptron or a standard GNN aggregation function, that computes a joint representation of the k node features a(v 1 ), . . . , a(v k ). Moreover, it is also straightforward to incorporate the labeled isomorphism type and continuous edge label information. We further explore this in the experimental section.</p><p>Generalization abilities of the neural architecture Garg et al. <ref type="bibr" target="#b40">[41]</ref>, studied the generalization abilities of a standard GNN architecture for binary classification using a margin loss. Under mild conditions, they bounded the empirical Rademacher complexity asÕ( rdL / √ mγ), where d is the maximum degree of the employed graphs, r is the number of components of the node features, L is the number of layers, and γ is a parameter of the loss function. It is straightforward to transfer the above bound to the higher-order (local) layer from above. Hence, this shows that local, sparsityaware, higher-order variants, e.g., δ-k-LGNN, exhibit a smaller generalization error compared to dense, global variants like the k-WL-GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Practicality, barriers ahead, and possible road maps</head><p>As Theorem 2 shows, the δ-k-LWL + and its corresponding neural architecture, the δ-k-LGNN + , have the same power in distinguishing non-isomorphic graphs as δ-k-WL. Although for dense graphs, the local algorithms will have the same running time, for sparse graphs, the running time for each iteration can be upper-bounded by |n k | · kd, where d denotes the maximum or average degree of the graph. Hence, the local algorithm takes the sparsity of the underlying graph into account, resulting in improved computation times compared to the non-local δ-k-WL and the k-WL (for the same number of iterations). These observations also translate into practice, see Section 6. The same arguments can be used in favor of the δ-k-LWL and δ-k-LGNN, which lead to even sparser algorithms.</p><p>Obstacles The biggest obstacle in applying the algorithms to truly large graphs is the fact that the algorithm considers all possible k-tuples leading to a lower bound on the running time of Ω(n k ).</p><p>Lifting the results to the folklore k-WL, e.g., <ref type="bibr" target="#b76">[77]</ref>, only "shaves off one dimension". Moreover, applying higher-order algorithms for large k might lead to overfitting issues, see also Section 6.</p><p>Possible solutions Recent sampling-based approaches for graph kernels or GNNs, see, e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b81">82]</ref> address the dependence on n k , while appropriate pooling methods along the lines of Equation <ref type="formula" target="#formula_13">(6)</ref> address the overfitting issue. Finally, new directions from the theory community, e.g., <ref type="bibr" target="#b49">[50]</ref> paint further directions, which might result in more scalable algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental evaluation</head><p>Our intention here is to investigate the benefits of the local, sparse algorithms, both kernel and neural architectures, compared to the global, dense algorithms, and standard kernel and GNN baselines. More precisely, we address the following questions: Q1 Do the local algorithms, both kernel and neural architectures, lead to improved classification and regression scores on real-world benchmark datasets compared to global, dense algorithms and standard baselines? Q2 Does the δ-k-LWL + lead to improved classification accuracies compared to the δ-k-LWL? Does it lead to higher computation times? Q3 Do the local algorithms prevent overfitting to the training set? Q4 How much do the local algorithms speed up the computation time compared to the non-local algorithms or dense neural architectures?</p><p>The source code of all methods and evaluation procedures is available at https://www.github. com/chrsmrrs/sparsewl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>To evaluate kernels, we use the following, well-known, small-scale datasets: EN-ZYMES <ref type="bibr" target="#b97">[98,</ref><ref type="bibr">13]</ref>, IMDB-BINARY, IMDB-MULTI <ref type="bibr" target="#b118">[119]</ref>, NCI1, NCI109 <ref type="bibr" target="#b108">[109]</ref>, PTC_FM <ref type="bibr" target="#b52">[53]</ref>, PROTEINS <ref type="bibr" target="#b30">[31,</ref><ref type="bibr">13]</ref>, and REDDIT-BINARY <ref type="bibr" target="#b118">[119]</ref>. To show that our kernels also scale to larger datasets, we additionally used the mid-scale datasets: YEAST, YEASTH, UACC257, UACC257H, OVCAR-8, OVCAR-8H <ref type="bibr" target="#b116">[117]</ref>. For the neural architectures, we used the large-scale molecular regression datasets ZINC <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b56">57]</ref> and ALCHEMY <ref type="bibr" target="#b20">[21]</ref>. To further compare to the (hierarchical) k-GNN <ref type="bibr" target="#b82">[83]</ref> and k-IGN <ref type="bibr" target="#b76">[77]</ref>, and show the benefits of our architecture in presence of continuous features, we used the QM9 <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b111">112]</ref> regression dataset. <ref type="bibr" target="#b5">6</ref> All datasets can be obtained from http://www.graphlearning.io <ref type="bibr" target="#b83">[84]</ref>. See Appendix E.1 for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernels</head><p>We implemented the δ-k-LWL, δ-k-LWL + , δ-k-WL, and k-WL kernel for k in {2, 3}. We compare our kernels to the Weisfeiler-Leman subtree kernel (1-WL) <ref type="bibr" target="#b99">[100]</ref>, the Weisfeiler-Leman Optimal Assignment kernel (WLOA) <ref type="bibr" target="#b67">[68]</ref>, the graphlet kernel (GR) <ref type="bibr" target="#b98">[99]</ref>, and the shortest-path kernel <ref type="bibr">[13]</ref> (SP). All kernels were (re-)implemented in C ++ 11. For the graphlet kernel, we counted (labeled) connected subgraphs of size three. We followed the evaluation guidelines outlined in <ref type="bibr" target="#b83">[84]</ref>. We also provide precomputed Gram matrices for easier reproducability.</p><p>Neural architectures We used the GIN and GIN-ε architecture <ref type="bibr" target="#b114">[115]</ref> as neural baselines. For data with (continuous) edge features, we used a 2-layer MLP to map them to the same number of components as the node features and combined them using summation (GINE and GINE-ε). For the   evaluation of the neural architectures of Section 4, δ-k-LGNN, δ-k-GNN, and k-WL-GNN, we implemented them using PYTORCH GEOMETRIC <ref type="bibr" target="#b35">[36]</ref>, using a Python-wrapped C ++ 11 preprocessing routine to compute the computational graphs for the higher-order GNNs. <ref type="bibr" target="#b6">7</ref> We used the GIN-ε layer to express f W1 mrg and f W2 aggr of Section 4. See Appendix E.2 for a detailed description of all evaluation protocols and hyperparameter selection routines.</p><p>Results and discussion In the following we answer questions Q1 to Q4.</p><p>A1 Kernels See <ref type="table" target="#tab_1">Table 1</ref>. The local algorithm, for k = 2 and 3, severely improves the classification accuracy compared to the k-WL and the δ-k-WL. For example, on the ENZYMES dataset the δ-2-LWL achieves an improvement of almost 20%, and the δ-3-LWL achieves the best accuracies over all employed kernels, improving over the 3-WL and the δ-3-WL by more than 13%. This observation holds over all datasets. Our algorithms also perform better than neural baselines. See <ref type="table" target="#tab_7">Table 5</ref> in the appendix for additional results on the mid-scale datasets. However, it has to be noted that increasing k does not always result in increased accuracies. For example, on all datasets (excluding ENZYMES), the performance of the δ-2-LWL is better or on par with the δ-3-LWL. Hence, with increasing k the local algorithm is more prone to overfitting. Neural architectures See <ref type="table" target="#tab_2">Table 2b</ref> and <ref type="figure" target="#fig_4">Figure 2</ref>. On the ZINC and ALCHEMY datasets, the δ-2-</p><p>LGNN is on par or slightly worse than the δ-2-GNN. Hence, this is in contrast to the kernel variant. We assume that this is due to the δ-2-GNN being more flexible than its kernel variant in weighing the importance of global and local neighbors. This is further highlighted by the worse performance of the 2-WL-GNN, which even performs worse than the (1-dimensional) GINE-ε on the ZINC dataset. On the QM9 dataset, see <ref type="figure" target="#fig_4">Figure 2a</ref>, the δ-2-LGNN performs better than the higher-order methods from <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b82">83]</ref> while being on par with the MPNN architecture. We note here that the MPNN was specifically tuned to the QM9 dataset, which is not the case for the δ-2-LGNN (and the other higher-</p><formula xml:id="formula_14">Method QM9 Baseline GINE-ε 0.081 ±0.003 MPNN 0.034 ±0.001 1-2-GNN 0.068 ±0.001 1-3-GNN 0.088 ±0.007 1-2-3-GNN 0.062 ±0.001 3-IGN 0.046 ±0.001 δ-2-LGNN 0.029 ±0.001</formula><p>(a) Mean std. MAE compared to <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b82">83]</ref>. Epoch   </p><formula xml:id="formula_15">0.2 0.4 0.6 MAE δ-k-LGNN train δ-k-LGNN test δ-k-GNN train δ-k-GNN test k-WL-GNN train k-WL-GNN test (b) ZINC</formula><formula xml:id="formula_16">mean std. MAE δ-k-LGNN train δ-k-LGNN test δ-k-GNN train δ-k-GNN test k-WL-GNN train k-WL-GNN test (c) ALCHEMY</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Dataset  <ref type="table">Table 8</ref> in the appendix. Similar results can be observed on the midscale datasets, see <ref type="table" target="#tab_7">Tables 5 and 9</ref> in the appendix. A3 Kernels As <ref type="table" target="#tab_2">Table 2a (Table 6</ref> for all datasets) shows the δ-2-WL reaches slightly higher training accuracies over all datasets compared to the δ-2-LWL, while the testing accuracies are much lower (excluding PTC_FM and PROTEINS). This indicates that the δ-2-WL overfits on the training set.</p><formula xml:id="formula_17">ZINC ALCHEMY Dense 2-WL-GNN 2.2 1.1 δ-2-GNN 2.5 1.7 GINE-ε 0.2 0.4 δ-2-LGNN 1.0 1.0</formula><p>The higher test accuracies of the local algorithm are likely due to the smaller neighborhood, which promotes that the number of colors grow slower compared to the global algorithm. The δ-k-LWL + inherits the strengths of both algorithms, i.e., achieving the overall best training accuracies while achieving state-of-the-art testing accuracies. Neural architectures See <ref type="figure" target="#fig_4">Figure 2</ref>. In contrast to the kernel variants, the 2-WL and the δ-2-WL, the corresponding neural architectures, the 2-WL-GNN and the δ-2-GNN, seem less prone to overfitting. However, especially on the ALCHEMY dataset, the δ-2-LGNN overfits less. A4 Kernels See <ref type="table" target="#tab_4">Table 3a (Tables 8 and 9</ref> for all datasets). The local algorithm severely speeds up the computation time compared to the δ-k-WL and the k-WL for k = 2 and 3. For example, on the ENZYMES dataset the δ-2-LWL is over ten times faster than the δ-2-WL. The improvement of the computation times can be observed across all datasets. For some datasets, the {2, 3}-WL and the δ-{2, 3}-WL did not finish within the given time limit or went out of memory. For example, on four out of eight datasets, the δ-3-WL is out of time or out of memory. In contrast, for the corresponding local algorithm, this happens only two out of eight times. Hence, the local algorithm is more suitable for practical applications.</p><p>Neural architectures <ref type="table" target="#tab_4">See Table 3b</ref>. The local algorithm severely speeds up the computation time of training and testing. Especially, on the ZINC dataset, which has larger graphs compared to the ALCHEMY dataset, the δ-2-LGNN achieves a computation time that is more than two times lower compared to the δ-2-GNN and the 2-WL-GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduced local variants of the k-dimensional Weisfeiler-Leman algorithm. We showed that one variant and its corresponding neural architecture are strictly more powerful than the k-WL while taking the underlying graph's sparsity into account. To demonstrate the practical utility of our findings, we applied them to graph classification and regression. We verified that our local, sparse algorithms lead to vastly reduced computation times compared to their global, dense counterparts while establishing new state-of-the-art results on a wide range of benchmark datasets. We believe that our local, higher-order kernels and GNN architectures should become a standard approach in the regime of supervised learning with small graphs, e.g., molecular learning.</p><p>Future work includes a more fine-grained analysis of the proposed algorithm, e.g., moving away from the restrictive graph isomorphism objective and deriving a deeper understanding of the neural architecture's capabilities when optimized with stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader impact</head><p>We view our work mainly as a methodological contribution. It studies the limits of current (supervised) graph embeddings methods, commonly used in chemoinformatics <ref type="bibr" target="#b102">[103]</ref>, bioinformatics <ref type="bibr" target="#b9">[10]</ref>, or network science <ref type="bibr" target="#b26">[27]</ref>. Currently, methods used in practice, such as GNNs or extendedconnectivity fingerprints <ref type="bibr" target="#b92">[93]</ref> have severe limitations and might miss crucial patterns in today's complex, interconnected data. We investigate how to scale up graph embeddings that can deal with higher-order interactions of vertices (or atom of molecules, users in social networks, variables in optimization, . . .) to larger graphs or networks. Hence, our method paves the way for more resourceefficient and expressive graph embeddings.</p><p>We envision that our (methodological) contributions enable the design of more expressive and scalable graph embeddings in fields such as quantum chemistry, drug-drug interaction prediction, insilicio, data-driven drug design/generation, and network analysis for social good. However, progress in graph embeddings might also trigger further advancements in hostile social network analysis, e.g., extracting more fine-grained user interactions for social tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example impact</head><p>We are actively cooperating with chemists on drug design to evaluate further our approach to new databases for small molecules. Here, the development of new databases is quite tedious, and graph embeddings can provide hints to the wet lab researcher where to start their search. However, still, humans need to do much of the intuition-driven, manual wet lab work. Hence, we do not believe that our methods will result in job losses in the life sciences in the foreseeable future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related work (Expanded)</head><p>In the following, we review related work from graph kernels, GNNs, and theory.</p><p>Graph kernels Historically, kernel methods-which implicitly or explicitly map graphs to elements of a Hilbert space-have been the dominant approach for supervised learning on graphs. Important early work in this area includes random-walk based kernels <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b69">70]</ref> and kernels based on shortest paths <ref type="bibr">[13]</ref>. More recently, graph kernels' developments have emphasized scalability, focusing on techniques that bypass expensive Gram matrix computations by using explicit feature maps, see, e.g., <ref type="bibr" target="#b99">[100]</ref>. Morris et al. <ref type="bibr" target="#b81">[82]</ref> devised a local, set-based variant of the k-WL. However, the approach is (provably) weaker than the tuple-based algorithm, and they do not prove convergence to the original algorithm. Yanardag and Vishwanathan successfully employed Graphlet <ref type="bibr" target="#b98">[99]</ref>, and Weisfeiler-Leman kernels within frameworks for smoothed <ref type="bibr" target="#b117">[118]</ref> and deep graph kernels <ref type="bibr" target="#b118">[119]</ref>. Other recent works focus on assignment-based <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b87">88]</ref>, spectral <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b105">106]</ref>, graph decomposition <ref type="bibr" target="#b88">[89]</ref>, randomized binning approaches <ref type="bibr" target="#b51">[52]</ref>, and the extension of kernels based on the 1-WL <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b103">104]</ref>. For a theoretical investigation of graph kernels, see <ref type="bibr" target="#b68">[69]</ref>, for a thorough survey of graph kernels, see <ref type="bibr" target="#b70">[71]</ref>.</p><p>GNNs Recently, graph neural networks (GNNs) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b96">97]</ref> emerged as an alternative to graph kernels. Notable instances of this architecture include, e.g., <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b104">105]</ref>, and the spectral approaches proposed in, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b80">81</ref>]-all of which descend from early work in <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b96">97]</ref>. Recent extensions and improvements to the GNN framework include approaches to incorporate different local structures (around subgraphs), e.g., <ref type="bibr">[1,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b113">114]</ref>, novel techniques for pooling node representations in order perform graph classification, e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b121">122]</ref>, incorporating distance information <ref type="bibr" target="#b120">[121]</ref>, and non-euclidian geometry approaches <ref type="bibr" target="#b17">[18]</ref>. Moreover, recently empirical studies on neighborhood aggregation functions for continuous vertex features <ref type="bibr" target="#b25">[26]</ref>, edge-based GNNs leveraging physical knowledge <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b65">66]</ref>, and sparsification methods <ref type="bibr" target="#b93">[94]</ref> emerged. Loukas <ref type="bibr" target="#b73">[74]</ref> and Sato et al. studied the limits of GNNs when applied to combinatorial problems. A survey of recent advancements in GNN techniques can be found, e.g., in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b124">125]</ref>. Garg et al. <ref type="bibr" target="#b40">[41]</ref> and Verma and Zhang <ref type="bibr" target="#b106">[107]</ref> studied the generalization abilities of GNNs, and <ref type="bibr" target="#b31">[32]</ref> related wide GNNs to a variant of the neural tangent kernel <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b55">56]</ref>. Murphy et al. <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b85">86]</ref> and Sato et al. <ref type="bibr" target="#b95">[96]</ref> extended the expressivity of GNNs by considering all possible permutations of a graph's adjacency matrix, or adding random node features, respectivley. The connection between random colorings and universality was investigated in <ref type="bibr" target="#b27">[28]</ref>.</p><p>Recently, connections to Weisfeiler-Leman type algorithms have been shown <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b114">115]</ref>. Specifically, <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b114">115]</ref> showed that the expressive power of any possible GNN architecture is limited by the 1-WL in terms of distinguishing non-isomorphic graphs. Morris et al. <ref type="bibr" target="#b82">[83]</ref> also introduced k-dimensional GNNs (k-GNN) which rely on a message-passing scheme between subgraphs of cardinality k. Similar to <ref type="bibr" target="#b81">[82]</ref>, the paper employed a local, set-based (neural) variant of the k-WL, which is (provably) weaker than the variant considered here. Later, this was refined in <ref type="bibr" target="#b76">[77]</ref> by introducing k-order invariant graph networks (k-IGN), based on Maron et al. <ref type="bibr" target="#b77">[78]</ref>, and references therein, which are equivalent to the folklore variant of the k-WL <ref type="bibr" target="#b45">[46]</ref> in terms of distinguishing nonisomorphic graphs. However, k-IGN may not scale since they rely on dense linear algebra routines. Chen et al. <ref type="bibr" target="#b23">[24]</ref> connect the theory of universal approximation of permutation-invariant functions and the graph isomorphism viewpoint and introduce a variation of the 2-WL, which is more powerful than the former. Our comprehensive treatment of higher-order, sparse, neural networks for arbitrary k subsumes all of the algorithms and neural architectures mentioned above.</p><p>Finally, there exists a new line of work focusing on extending GNNs to hypergraphs, see, e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b122">123]</ref>, and a line of work in the data mining community incorporating global or higher-order information into graph or node embeddings, see, e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b78">79]</ref>.</p><p>Theory The Weisfeiler-Leman algorithm constitutes one of the earliest approaches to isomorphism testing <ref type="bibr" target="#b109">[110,</ref><ref type="bibr" target="#b110">111]</ref>, having been heavily investigated by the theory community over the last few decades <ref type="bibr" target="#b48">[49]</ref>. Moreover, the fundamental nature of the k-WL is evident from a variety of connections to other fields such as logic, optimization, counting complexity, and quantum computing. The power and limitations of k-WL can be neatly characterized in terms of logic and descriptive complexity <ref type="bibr" target="#b54">[55]</ref>, Sherali-Adams relaxations of the natural integer linear program for the graph isomorphism problem <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b75">76]</ref>, homomorphism counts <ref type="bibr" target="#b29">[30]</ref>, and quantum isomorphism games <ref type="bibr" target="#b6">[7]</ref>.</p><p>In their seminal paper <ref type="bibr" target="#b54">[55]</ref>, Cai et al. showed that for each k there exists a pair of non-isomorphic graphs of size O(k) each that cannot be distinguished by the k-WL. Grohe et al. <ref type="bibr" target="#b48">[49]</ref> gives a thorough overview of these results. For k = 1, the power of the algorithm has been completely characterized <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b62">63]</ref>. Moreover, upper bounds on the running time for k = 1 <ref type="bibr">[12,</ref><ref type="bibr" target="#b60">61]</ref>, and the number of iterations for the folklore k = 2 <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b72">73]</ref> have been shown. For k = 1 and 2, Arvind et al. <ref type="bibr" target="#b4">[5]</ref> studied the abilities of the (folklore) k-WL to detect and count fixed subgraphs, extending the work of Fürer <ref type="bibr" target="#b38">[39]</ref>. The former was refined in <ref type="bibr" target="#b24">[25]</ref>. The algorithm (for logarithmic k) plays a prominent role in the recent result of Babai <ref type="bibr" target="#b7">[8]</ref> improving the best-known running time for the graph isomorphism problem. Recently, Grohe et al. <ref type="bibr" target="#b49">[50]</ref> introduced the framework of Deep Weisfeiler Leman algorithms, which allow the design of a more powerful graph isomorphism test than Weisfeiler-Leman type algorithms. Finally, the emerging connections between the Weisfeiler-Leman paradigm and graph learning are described in a recent survey of Grohe <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Preliminaries (Expanded)</head><p>We briefly describe the Weisfeiler-Leman algorithm and, along the way, introduce our notation. We also state a variant of the algorithm, introduced in <ref type="bibr" target="#b75">[76]</ref>. As usual, let [n] = {1, . . . , n} ⊂ N for n ≥ 1, and let { {. . . } } denote a multiset.</p><p>Graphs A graph G is a pair (V, E) with a finite set of vertices V and a set of edges E ⊆ {{u, v} ⊆ V | u = v}. We denote the set of vertices and the set of edges of G by V (G) and E(G), respectively. For ease of notation, we denote the edge {u, v} in E(G) by (u, v) or (v, u). In the case of directed</p><formula xml:id="formula_18">graphs E ⊆ {(u, v) ∈ V × V | u = v}. A labeled graph G is a triple (V, E, l) with a label function l : V (G) ∪ E(G) → Σ, where Σ is some finite alphabet. Then l(v) is a label of v for v in V (G) ∪ E(G). The neighborhood of v in V (G) is denoted by δ(v) = N (v) = {u ∈ V (G) | (v, u) ∈ E(G)}. Moreover, its complement δ(v) = {u ∈ V (G) | (v, u) / ∈ E(G)}. Let S ⊆ V (G) then G[S] = (S, E S ) is the subgraph induced by S with E S = {(u, v) ∈ E(G) | u, v ∈ S}.</formula><p>A tree is a connected graph without cycles. A rooted tree is a tree with a designated vertex called root in which the edges are directed in such a way that they point away from the root. Let p be a vertex in a directed tree then we call its out-neighbors children with parent p.</p><p>We say that two graphs G and H are isomorphic if there exists an edge preserving bijection ϕ : V (G) → V (H), i.e., (u, v) is in E(G) if and only if (ϕ(u), ϕ(v)) is in E(H). If G and H are isomorphic, we write G H and call ϕ an isomorphism between G and H. Moreover, we call the equivalence classes induced by isomorphism types, and denote the isomorphism type of G by τ G . In the case of labeled graphs, we additionally require that l(v) = l(ϕ(v)) for v in V (G) and l((u, v)) = l((ϕ(u), ϕ(v))) for (u, v) in E(G). Let v be a tuple in V (G) k for k &gt; 0, then G[v] is the subgraph induced by the components of v, where the vertices are labeled with integers from {1, . . . , k} corresponding to indices of v.</p><p>Kernels A kernel on a non-empty set X is a positive semidefinite function k : X × X → R. Equivalently, a function k is a kernel if there is a feature map φ : X → H to a Hilbert space H with inner product ·, · , such that k(x, y) = φ(x), φ(y) for all x and y in X . Let G be the set of all graphs, then a (positive semidefinite) function G × G → R is called a graph kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Vertex refinement algorithms (Expanded)</head><p>Let k be a fixed positive integer. As usual, let V (G) k denote the set of k-tuples of vertices of G.</p><p>A coloring of V (G) k is a mapping C : V (G) k → N, i.e., we assign a number (color) to every tuple in V (G) k . The initial coloring C 0 of V (G) k is specified by the isomorphism types of the tuples, i.e., two tuples v and w in V (G) k get a common color iff the mapping v i → w i induces an isomorphism between the labeled subgraphs G[v] and G[w]. A color class corresponding to a color c is the set of all tuples colored c, i.e., the set C −1 (c).</p><p>The neighborhood of a vertex tuple v in V (G) k is defined as follows. For j in [k], let φ j (v, w) be the k-tuple obtained by replacing the j th component of v with the vertex w. That is, φ j (v, w) = (v 1 , . . . , v j−1 , w, v j+1 , . . . , v k ). If w = φ j (v, w) for some w in V (G), call w a j-neighbor of v. The neighborhood of v is thus defined as the set of all tuples w such that w = φ j (v, w) for some j in [k] and w in V (G).</p><p>The refinement of a coloring C : V (G) k → N, denoted by C, is a coloring C : V (G) k → N defined as follows. For each j in [k], collect the colors of the j-neighbors of v as a multiset</p><formula xml:id="formula_19">S j = { {C(φ j (v, w)) | w ∈ V (G)} }. Then, for a tuple v, define C(v) = (C(v), M (v)),</formula><p>where M (v) is the k-tuple (S 1 , . . . , S k ). For consistency, the strings C(v) thus obtained are lexicographically sorted and renamed as integers. Observe that the new color C(v) of v is solely dictated by the color histogram of its neighborhood. In general, a different mapping M (·) could be used, depending on the neighborhood information that we would like to aggregate. We will refer to a mapping M (·) as an aggregation map.</p><p>k-dimensional Weisfeiler-Leman For k ≥ 2, the k-WL computes a coloring C ∞ : V (G) k → N of a given graph G, as follows. <ref type="bibr" target="#b7">8</ref> To begin with, the initial coloring C 0 is computed. Then, starting with C 0 , successive refinements C i+1 = C i are computed until convergence. That is,</p><formula xml:id="formula_20">C i+1 (v) = (C i (v), M i (v)), where M i (v) = { {C i (φ 1 (v, w)) | w ∈ V (G)} }, . . . , { {C i (φ k (v, w)) | w ∈ V (G)} } .<label>(7)</label></formula><p>The successive refinement steps are also called rounds or iterations. Since the disjoint union of the color classes form a partition of V (G) k , there must exist a finite ≤ |V (G)| k such that C = C . In the end, the k-WL outputs C as the stable coloring C ∞ .</p><p>The k-WL distinguishes two graphs G and H if, upon running the k-WL on their disjoint union G∪ H, there exists a color c in N in the stable coloring such that the corresponding color class S c satisfies</p><formula xml:id="formula_21">|V (G) k ∩ S c | = |V (H) k ∩ S c |,</formula><p>i.e., there exist an unequal number of c-colored tuples in V (G) k and V (H) k . Hence, two graphs distinguished by the k-WL must be non-isomorphic.</p><p>In fact, there exist several variants of the above defined k-WL. These variants result from the application of different aggregation maps M (·). For example, setting M (·) to be</p><formula xml:id="formula_22">M F (v) = { { C(φ 1 (v, w)), . . . , C(φ k (v, w)) | w ∈ V (G)} },</formula><p>yields a well-studied variant of the k-WL (see, e.g., <ref type="bibr" target="#b14">[15]</ref>), commonly known as "folklore" k-WL in machine learning literature. It holds that the k-WL using Equation <ref type="formula" target="#formula_20">(7)</ref> is as powerful as the folklore (k−1)-WL <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 δ-Weisfeiler-Leman algorithm</head><p>Let w = φ j (v, w) be a j-neighbor of v. Call w a local j-neighbor of v if w is adjacent to the replaced vertex v j . Otherwise, call w a global j-neighbor of v. The δ-k-dimensional Weisfeiler-Leman algorithm, denoted by δ-k-WL, is a variant of the classic k-WL which differentiates between the local and the global neighbors during neighborhood aggregation <ref type="bibr" target="#b75">[76]</ref>. Formally, the δ-k-WL algorithm refines a coloring C i (obtained after i rounds) via the aggregation map</p><formula xml:id="formula_23">M δ,δ i (v) = { {(C i (φ 1 (v, w), adj(v, φ 1 (v, w))) | w ∈ V (G)} }, . . . , { {(C i (φ k (v, w), adj(v, φ k (v, w))) | w ∈ V (G)} } ,<label>(8)</label></formula><p>instead of the k-WL aggregation specified by Equation <ref type="formula" target="#formula_20">(7)</ref>. We define the 1-WL to be the δ-1-WL, which is commonly known as color refinement or naive vertex classification. Comparing k-WL variants Given that there exist several variants ofthe k-WL, corresponding to different aggregation maps M (·), it is natural to ask whether they are equivalent in power, vis-a-vis distinguishing non-isomorphic graphs. Let A 1 and A 2 denote two vertex refinement algorithms, we write A 1 A 2 if A 1 distinguishes between all non-isomorphic pairs A 2 does, and A 1 ≡ A 2 if both directions hold. The corresponding strict relation is denoted by .</p><formula xml:id="formula_24">w v x u (a) Underlying graph G, with tuple (u, v, w) w v x u (b) (u, v, x) is a local 3- neighbor of (u, v, w) w v x u (c) (x, v, w) is a global 1- neighbor of (u, v, w)</formula><p>The following result relates the power of the k-WL and δ-k-WL. Since for a graph G = (V, E),</p><formula xml:id="formula_25">M δ,δ i (v) = M δ,δ i (w) implies M i (v) = M i (w)</formula><p>for all v and w in V (G) k and i ≥ 0, it immediately follows that δ-k-WL k-WL. For k = 1, these two algorithms are equivalent by definition. For k ≥ 2, this relation can be shown to be strict, see the next section.</p><p>Proposition 7 (restated, Proposition 1 in the main text). For all graphs and k ≥ 2, the following holds:</p><p>δ-k-WL k-WL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 Proof of Proposition 1</head><p>It suffices to show an infinite family of graphs (G k , H k ), k ∈ N, such that (a) k-WL does not distinguish G k and H k , although (b) δ-k-WL distinguishes G k and H k .</p><p>We proceed to the construction of this family. The graph family is based on the classic construction of <ref type="bibr" target="#b14">[15]</ref>, commonly referred to as Cai-Furer-Immermman (CFI) graphs.</p><p>Construction. Let K denote the complete graph on k + 1 vertices (there are no loops in K). The vertices of K are numbered from 0 to k. Let E(v) denote the set of edges incident to v in K: clearly, |E(v)| = k for all v ∈ V (K). Define the graph G as follows:</p><p>1. For the vertex set V (G), we add (a) (v, S) for each v ∈ V (K) and for each even subset S of E(v), (b) two vertices e 1 , e 0 for each edge e ∈ E(K). A set S of vertices is said to form a distance-two-clique if the distance between any two vertices in S is exactly two. Lemma 8. The following holds for graphs G and H defined above.</p><p>• There exists a distance-two-clique of size (k + 1) inside G. • There does not exist a distance-two-clique of size (k + 1) inside H.</p><p>Hence, G and H are non-isomorphic.</p><p>Proof. In the graph G, consider the vertex subset S = {(0, ∅), (1, ∅), . . . , (k, ∅)} of size (k + 1). That is, from each "cloud" of vertices of the form (v, S) for a fixed v, we pick the vertex corresponding to the trivial even subset, the empty set denoted by ∅. Observe that any two vertices in S are at distance two from each other. This holds because for any i, j ∈ V (K), (i, ∅) is adjacent to {i, j} 0 which is adjacent to (j, ∅) (e.g. see <ref type="figure" target="#fig_0">Figure 1</ref>). Hence, the vertices in S form a distancetwo-clique of size k + 1.</p><p>On the other hand, for the graph H, suppose there exists a distance-two-clique, say (0, S 0 ), . . . , (k, S k ) in H, where each S i ⊆ E(i). If we compute the parity-sum of the parities of |S 0 |, . . . , |S k |, we end up with 1 since there is exactly one odd subset in this collection, viz. S 0 . On the other hand, we can also compute this parity-sum in an edge-by-edge manner: for each edge (i, j) ∈ E(K), since (i, S i ) and (j, S j ) are at distance two, either both S i and S j contain the edge {i, j} or neither of them contains {i, j}: hence, the parity-sum contribution of S i and S j to the term corresponding to e is zero. Since the contribution of each edge to the total parity-sum is 0, the total parity-sum must be zero. This is a contradiction, and hence, there does not exist a distance-two-clique in H.</p><p>Next, we show that the local algorithm δ-k-LWL can distinguish G and H. Since δ-k-WL δ-k-LWL, the above lemma implies the strictness condition δ-k-WL k-WL. Lemma 9. δ-k-LWL distinguishes G and H.</p><p>Proof. The proof idea is to show that δ-k-LWL algorithm is powerful enough to detect distance-twocliques of size (k+1), which ensures the distinguishability of G and H. Indeed, consider the k-tuple P = ((1, ∅), (2, ∅), . . . , (k, ∅)) in V (G) k . We claim that there is no tuple Q in V (H) k such that the unrolling of P is isomorphic to the unrolling of Q. Indeed, for the sake of contradiction, assume that there does exist Q in V (H) k such that the unrolling of Q is isomorphic to the unrolling of P . Comparing isomorphism types, we know that the tuple Q must be of the form <ref type="figure" target="#fig_0">((1, S 1 )</ref>, . . . , (k, S k )).</p><p>Consider the depth-two unrolling of P : from the root vertex P , we can go down via two local-edges labeled 1, to hit the tuple P 2 = ((2, ∅), (2, ∅), . . . , (k, ∅)). If we consider the depth-two unrolling of Q, the isomorphism type of P 2 implies that the vertices (1, S 1 ) and (2, S 2 ) must be at distance-two in the graph H. Repeating this argument, we obtain that (1, S 1 ), . . . , (k, S k ) form a distance-twoclique in H of size k. Our goal is to produce a distance-two-clique in H of size k, for the sake of contradiction.</p><p>For that, consider the depth-four unrolling of P : from the root vertex P , we can go down via two local-edges labeled 1 to hit the tuple R = ((0, ∅), (2, ∅), . . . (k, ∅). For each 2 ≤ j ≤ k, we can further go down from R via two local edges labeled j to reach a tuple whose 1 st and j th entry is (0, ∅). Similarly, for the unrolling of Q, there exists a subset S 0 ⊆ E(0) and a corresponding tuple R = ((0, S 0 ), (2, S 2 ), . . . , (k, S k )), such that for each 2 ≤ j ≤ k, we can further go down from R via two local edges labeled j to reach a tuple whose 1 st and j th entry is (0, S 0 ). Comparing the isomorphism types of all these tuples, we deduce that (0, S 0 ) must be at distance two from each of (i, S i ) for i ∈ [k]. This implies that the vertex set {(0, S 0 ), (1, S 1 ), . . . , (k, S k )} is a distancetwo-clique of size k + 1 in H, which is impossible. Hence, there does not exist any k-tuple Q in V (H) k such that the unrolling of P and the unrolling of Q are isomorphic. Hence, the δ-k-LWL distinguishes G and H.</p><p>Finally, we note that CFI graphs are standard tools from graph isomorphism theory, and are often used to analyze the power and limitations of WL-type algorithms. It follows from results of <ref type="bibr" target="#b14">[15]</ref> that for every k ≥ 0, k-WL fails to distinguish the graphs G k and H k of our constructed family. This finishes the proof of the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Local δ-k-dimensional Weisfeiler-Leman algorithm (Expanded)</head><p>In this section, we define the new local δ-k-dimensional Weisfeiler-Leman algorithm (δ-k-LWL). This variant of δ-k-WL considers only local neighbors during the neighborhood aggregation process, and discards any information about the global neighbors. Formally, the δ-k-LWL algorithm refines a coloring C i (obtained after i rounds) via the aggregation map,</p><formula xml:id="formula_26">M δ i (v) = { {C i (φ 1 (v, w)) | w ∈ N (v 1 )} }, . . . , { {C i (φ k (v, w)) | w ∈ N (v k )} } ,<label>(9)</label></formula><p>instead of Equation <ref type="bibr" target="#b7">(8)</ref>. That is, the algorithm only considers the local j-neighbors of the vertex v in each iteration. Therefore, the indicator function adj used in Equation <ref type="formula" target="#formula_23">(8)</ref> is trivially equal to L here, and is hence omitted. The coloring function for the δ-k-LWL is defined by</p><formula xml:id="formula_27">C k,δ i+1 (v) = (C k,δ i (v), M δ i (v)</formula><p>). We also define δ-k-LWL + , a minor variation of δ-k-LWL. Later, we will show that δ-k-LWL + is equivalent in power to δ-k-WL (Theorem 10). Formally, the δ-k-LWL + algorithm refines a coloring C i (obtained after i rounds) via the aggregation function,</p><formula xml:id="formula_28">M δ,+ (v) = { {(C i (φ 1 (v, w)), # 1 i (v, φ 1 (v, w))) | w ∈ N (v 1 )} }, . . . , { {(C i (φ k (v, w)), # k i (v, φ k (v, w))) | w ∈ N (v k )} } ,<label>(10)</label></formula><p>instead of δ-k-LWL aggregation defined in Equation <ref type="bibr" target="#b8">(9)</ref>. Here, the function</p><formula xml:id="formula_29"># j i (v, x) = {w : w ∼ j v, C i (w) = C i (x)} , where w ∼ j v denotes that w is j-neighbor of v, for j in [k]. Essentially, # j i (v, x)</formula><p>counts the number of j-neighbors (local or global) of v which have the same color as x under the coloring C i (i.e., after i rounds). For a fixed v, the function # j i (v, ·) is uniform over the set S ∩ N j , where S is a color class obtained after i iterations of the δ-k-LWL + and N j denotes the set of j-neighbors of v. Note that after the stable partition has been reached # j i (v) will not change anymore. Observe that each iteration of the δ-k-LWL + has the same asymptotic running time as an iteration of the δ-k-LWL.</p><p>The following theorem shows that the local variant δ-k-LWL + is at least as powerful as the δ-k-WL when restricted to the class of connected graphs. In other words, given two connected graphs G and H, if these graphs are distinguished by δ-k-WL, then they must also be distinguished by the δ-k-LWL + . On the other hand, it is important to note that, in general, the δ-k-LWL + might need a larger number of iterations to distinguish two graphs, as compared to δ-k-WL. However, this leads to advantages in a machine learning setting, see Section 6. Theorem 10 (restated, Theorem 2 in the main text). For the class of connected graphs, the following holds for all k ≥ 1:</p><p>δ-k-LWL + ≡ δ-k-WL.</p><p>Along with Proposition 1, we obtain the following corollary relating the power of k-WL and δ-k-LWL + . Corollary 11 (restated, Corollary 3 in the main text). For the class of connected graphs, the following holds for all k ≥ 2: δ-k-LWL + k-WL.</p><p>In fact, the proof of Proposition 1 shows that the infinite family of graphs G k , H k witnessing the strictness condition can even be distinguished by δ-k-LWL, for each corresponding k ≥ 2. We note here that the restriction to connected graphs can easily be circumvented by adding a specially marked vertex, which is connected to every other vertex in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Kernels based on vertex refinement algorithms</head><p>The idea for a kernel based on the δ-k-LWL (and the other vertex refinements algorithms) is to compute it for h ≥ 0 iterations resulting in a coloring function C k,δ : V (G) → Σ i for each iteration i. Now, after each iteration, we compute a feature vector φ i (G) in R |Σi| for each graph G. Each component φ i (G) c counts the number of occurrences of k-tuples labeled by c in Σ i . The overall feature vector φ LWL (G) is defined as the concatenation of the feature vectors of all h iterations, i.e., φ LWL (G) = φ 0 (G), . . . , φ h (G) . The corresponding kernel for h iterations then is computed as k LWL (G, H) = φ LWL (G), φ LWL (H) , where ·, · denotes the standard inner product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Local converges to global: Proof of Theorem 2</head><p>The main technique behind the proof is to encode the colors assigned by the k-WL (or its variants) as rooted directed trees, called unrolling trees. The exact construction of the unrolling tree depends     on the aggregation map M (·) used by the k-WL variant under consideration. We illustrate this construction for the k-WL. For other variants such as the δ-k-WL, δ-k-LWL, and δ-k-LWL + , we will specify analogous constructions.</p><p>Unrollings ("Rolling in the deep") Given a graph G, a tuple v in V (G) k , and an integer ≥ 0, the unrolling UNR [G, v, ] is a rooted, directed tree with vertex and edge labels, defined recursively as follows.</p><p>-For = 0, UNR [G, v, 0] is defined to be a single vertex, labeled with the isomorphism type τ (v). This lone vertex is also the root vertex. -For &gt; 0, UNR [G, v, ] is defined as follows. First, introduce a root vertex r, labeled with the isomorphism type τ (v). Next, for each j ∈ [k] and for each j-neighbor w of v, append the rooted subtree UNR [G, w, − 1] below the root r. Moreover, the directed edge e from r to the root of UNR [G, w, − 1] is labeled j iff w is a j-neighbor of v.</p><p>We refer to UNR [G, v, ] as the unrolling of the graph G at v of depth . <ref type="figure" target="#fig_1">Figure 4</ref> partially illustrates the recursive construction of unrolling trees: it describes the unrolling tree for the graph in <ref type="figure" target="#fig_5">Figure 3</ref> at the tuple (u, v, w), of depth 1. Each node w in the unrolling tree is associated with some k-tuple w, indicated alongside the node in the figure. We call w the tuple corresponding to the node w.</p><p>Analogously, we can define unrolling trees δ-UNR , L-UNR , and L + -UNR for the k-WL-variants δk-WL, δ-k-LWL, and δ-k-LWL + respectively. The minor differences lie in the recursive step above, since the unrolling construction needs to faithfully represent the aggregation process.</p><p>-For δ-UNR , we additionally label the directed edge e with (j, L) or (j, G) instead of just j, depending on whether the neighborhood is local or global. -For L-UNR , we consider only the subtrees L-UNR [G, w, − 1] for local j-neighbors w.</p><p>-For L + -UNR , we again consider only the subtrees L + -UNR [G, w, − 1] for local jneighbors w. However, the directed edge e to this subtree is also labeled with the # counter value # j −1 (v, w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding colors as trees</head><p>The following Lemma shows that the computation of the k-WL can be faithfully encoded by the unrolling trees. Formally, let s and t be two k-vertex-tuples in V (G) k . Proof. By induction on . For the base case = 0, observe that the initial colors of s and t are equal to the respective isomorphism types τ (s) and τ (t). On the other hand, the vertex labels for the single-vertex graphs UNR [G, s, 0] and UNR [G, t, 0] are also the respective isomorphism types τ (s) and τ (t). Hence, the statement holds for = 0. Equivalence The following Lemma establishes that the local algorithm δ-k-LWL + is at least as powerful as the global δ-k-WL, for connected graphs, i.e., δ-k-LWL + δ-k-WL. Proof. Let r * denote the number of rounds needed to attain the stable coloring under δ-k-LWL + . Consider unrollings L 1 = L + -UNR [G, s, q] and L 2 = L + -UNR [G, t, q] of sufficiently large depth q = r * + |V (G)| + 1. Since s and t have the same stable coloring under δ-k-LWL + , the trees L 1 and L 2 are isomorphic (by <ref type="bibr">Lemma 13)</ref>. Let θ be an isomorphism from L 1 to L 2 .</p><p>We prove the following equivalent statement. If L 1 and L 2 are isomorphic, then for all i ≥ 0,</p><formula xml:id="formula_30">δ-UNR [G, s, i] = δ-UNR [G, t, i].</formula><p>The proof is by induction on i. The base case i = 0 follows trivially by comparing the isomorphism types of s and t.</p><p>For the inductive case, let j ∈ [k]. Let X j be the set of j-neighbors of s. Similarily, let Y j be the set of j-neighbors of t. Our goal is to construct, for every j ∈ [k], a corresponding bijection σ j between X j and Y j satisfying the following conditions.</p><p>Hence, we can devise the required bijection σ j = σ L j∪ σ G j as follows. We pick an arbitrary bijection σ L j between the set of local j-neighbors of s inside C and the set of local j-neighbors of t inside C. We also pick an arbitrary bijection σ G j between the set of global j-neighbors of s inside C and the set of global j-neighbors of t inside C. Clearly, σ j satisfies the first stipulated condition. By induction hypothesis, the second condition is also satisifed. Hence, we can obtain a desired bijection σ j satisfying the two stipulated conditions. Since we obtain the desired bijections σ 1 , . . . , σ k , this finishes the proof of the lemma. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Details on experiments and additional results</head><p>Here we give details on the experimental study of Section 6.    <ref type="table">Table 6</ref>: Training versus test accuracy of local and global kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural architectures</head><p>We used the GIN and GIN-ε architecture <ref type="bibr" target="#b114">[115]</ref> as neural baselines. For data with (continuous) edge features, we used a 2-layer MLP to map them to the same number of components as the node features and combined them using summation (GINE and GINEε). For the evaluation of the neural architectures of Section 4, δ-k-LGNN, δ-k-GNN, k-WL-GNN, we implemented them using PYTORCH GEOMETRIC <ref type="bibr" target="#b35">[36]</ref>, using a Pythonwrapped C ++ 11 preprocessing routine to compute the computational graphs for the higherorder GNNs. We used the GIN-ε layer to express f W1 mrg and f W2 aggr of Section 4. Finally, we used the PYTORCH <ref type="bibr" target="#b89">[90]</ref> implementations of the 3-IGN <ref type="bibr" target="#b76">[77]</ref>, and 1-2-GNN, 1-3-GNN, 1-2-3-GNN <ref type="bibr" target="#b82">[83]</ref> made available by the respective authors. For the QM9 dataset, we additionally used the MPNN architecture as a baseline, closely following the setup of <ref type="bibr" target="#b44">[45]</ref>. For the GINE-ε and the MPNN architecture, following Gilmer et al. <ref type="bibr" target="#b44">[45]</ref>, we used a complete graph, computed pairwise 2 distances based on the 3D-coordinates, and concatenated them to the edge features. We note here that our intent is not the beat state-of-the-art, physical knowledge-incorporating architectures, e.g., DimeNet <ref type="bibr" target="#b65">[66]</ref> or Cormorant <ref type="bibr" target="#b1">[2]</ref>, but to solely show the benefits of the (local) higher-order architectures compared to the corresponding (1-dimensional) GNN. For the δ-2-GNN, to implement Equation <ref type="formula" target="#formula_13">(6)</ref>, for each 2-tuple we concatenated the (two) node and edge features, computed pairwise 2 distances based on the 3D-coordinates, and a one-hot encoding of the (labeled) isomorphism type. Finally, we used a 2-layer MLP to learn a joint, initial vectorial representation.</p><p>The source code of all methods and evaluation procedures is available at https://www.github. com/chrsmrrs/sparsewl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Experimental protocol and model configuration</head><p>In the following, we describe the experimental protocol and hyperparameter setup.</p><p>Kernels For the smaller datasets (first third of <ref type="table" target="#tab_6">Table 4</ref>), for each kernel, we computed the (cosine) normalized gram matrix. We computed the classification accuracies using the C-SVM implementation of LIBSVM <ref type="bibr" target="#b19">[20]</ref>, using 10-fold cross-validation. We repeated each 10fold cross-validation ten times with different random folds, and report average accuracies and standard deviations. For the larger datasets (second third of <ref type="table" target="#tab_6">Table 4</ref>), we computed explicit feature vectors for each graph and used the linear C-SVM implementation of LI-BLINEAR <ref type="bibr" target="#b34">[35]</ref>, again using 10-fold cross-validation (repeated ten times). Following the evaluation method proposed in <ref type="bibr" target="#b83">[84]</ref>, in the both cases, the C-parameter was selected from {10 −3 , 10 −2 , . . . , 10 2 , 10 3 } using a validation set sampled uniformly at random from the training fold (using 10% of the training fold). Similarly, the number of iterations of the   <ref type="table">Table 8</ref>: Overall computation times for the whole datasets in seconds (Number of iterations for 1-WL, 2-WL, 3-WL, δ-2-WL, WLOA, δ-3-WL, δ-2-LWL, and δ-3-LWL: 5), OOT-Computation did not finish within one day (24h), OOM-Out of memory.</p><p>1-WL, WLOA, δ-k-LWL, δ-k-LWL + , and k-WL were selected from {0, . . . , 5} using the validation set. Moreover, for the δ-k-LWL + , we only added the additional label function # on the last iteration to prevent overfitting. We report computation times for the 1-WL, WLOA, δ-k-LWL, δ-k-LWL + , and k-WL with five refinement steps. All kernel experiments were conducted on a workstation with an Intel Xeon E5-2690v4 with 2.60GHz and 384GB of RAM running Ubuntu 16.04.6 LTS using a single core. Moreover, we used the GNU C ++ Compiler 5.5.0 with the flag -O2. Neural architectures For comparing to kernel approaches, see <ref type="table" target="#tab_1">Tables 1 and 5</ref>, we used 10-fold cross-validation, and again used the approach outlined in <ref type="bibr" target="#b83">[84]</ref>. The number of components of the (hidden) node features in {32, 64, 128} and the number of layers in {1, 2, 3, 4, 5} of the GIN (GINE) and GIN-ε (GINE-ε) layer were again selected using a validation set sampled uniformly at random from the training fold (using 10% of the training fold). We used mean pooling to pool the learned node embeddings to a graph embedding and used a 2-layer MLP for the final classification, using a dropout layer with p = 0.5 after the first layer of the MLP. We repeated each 10-fold cross-validation ten times with different random folds, and report the average accuracies and standard deviations. Due to the different training methods, we do not provide computation times for the GNN baselines. For the larger molecular regression tasks, ZINC and ALCHEMY, see <ref type="table" target="#tab_9">Table 7</ref>, we closely followed the hyperparameters found in <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b20">[21]</ref>, respectively, for the GINE-ε layers. That is, for ZINC, we used four GINE-ε layers with a hidden dimension of 256 followed by batch norm and a 4-layer MLP for the joint regression of the twelve targets, after applying mean pooling. For ALCHEMY and QM9, we used six layers with 64 (hidden) node features and a set2seq layer <ref type="bibr" target="#b107">[108]</ref> for graph-level pooling, followed by a 2-layer MLP for the joint regression of the twelve targets. We used exactly the same hyperparameters for the (local) δ-2-LGNN, and the dense variants δ-2-GNN and 2-WL-GNN.</p><p>For ZINC, we used the given train, validation split, test split, and report the MAE over the test set. For the ALCHEMY and QM9 datasets, we uniformly and at random sampled 80% of the graphs for training, and 10% for validation and testing, respectively. Moreover, following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45]</ref>, we normalized the targets of the training split to zero mean and unit variance. We used a single model to predict all targets. Following [66, Appendix C],  <ref type="table">Table 9</ref>: Overall computation times for the whole datasets in seconds on medium-scale datasets (Number of iterations for 1-WL, δ-2-LWL, and δ-3-LWL: 2).</p><p>we report mean standardized MAE and mean standardized logMAE. We repeated each experiment five times (with different random splits in case of ALCHEMY and QM9) and report average scores and standard deviations.</p><p>To compare training and testing times between the δ-2-LGNN, the dense variants the δ-2-GNN and 2-WL-GNN, and the (1-dimensional) GINE-ε layer, we trained all four models on ZINC (10K) and ALCHEMY (10K) to convergence, divided by the number of epochs, and calculated the ratio with regard to the average epoch computation time of the δ-2-</p><p>LGNN (average computation time of dense or baseline layer divided by average computation time of the δ-2-LGNN). All neural experiments were conducted on a workstation with four Nvidia Tesla V100 GPU cards with 32GB of GPU memory running Oracle Linux Server 7.7.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the power of proposed algorithms and neural architectures. The green and dark red nodes represent algorithms proposed in the present work. The grey region groups dense algorithms and neural architectures. * -Follows directly from the proof of Theorem 6. A B (A B, A ≡ B): algorithm A is more powerful (strictly more powerful, equally powerful) than B, † -Follows by definition, strictness open.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Lemma 4 .</head><label>4</label><figDesc>The colors of s and t after rounds of k-WL are identical if and only if the unrolling tree UNR [G, s, ] is isomorphic to the unrolling tree UNR [G, t, ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Additional results for neural architectures.(a) Speed up ratios of local kernel computations for a subset of the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 illustrates this definition for a 3 -</head><label>3</label><figDesc>tuple (u, v, w). For tuples v and w in V (G) k , the function adj((v, w)) = L if w is a local neighbor of v G if w is a global neighbor of v indicates whether w is a local or global neighbor of v.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the local and global neighborhood of the 3-tuple (u, v, w).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 . 2 · 2 .</head><label>222</label><figDesc>For the edge set E(G), we add (a) an edge {e 0 , e 1 } for each e ∈ E(K), (b) an edge between (v, S) and e 1 if v ∈ e and e ∈ S, (c) an edge between (v, S) and e 0 if v ∈ e and e S, Define a companion graph H, in a similar manner to G, with the following exception: in Step 1(a), for the vertex 0 ∈ V (K), we choose all odd subsets of E(0). Counting vertices, we find that |V (G)| = |V (H)| = (k + 1) · 2 k−1 + k This finishes the construction of graphs G and H. We set G k := G and H k := H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 :</head><label>4</label><figDesc>Unrolling at the tuple (u, v, w) of depth one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Lemma 12 .</head><label>12</label><figDesc>The colors of s and t after rounds of k-WL are identical if and only if the unrolling tree UNR [G, s, ] is isomorphic to the unrolling tree UNR [G, t, ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>ForFigure 5 :Lemma 13 . 1 .</head><label>5131</label><figDesc>the inductive case, we proceed with the forward direction. Suppose that k-WL assigns the same color to s and t after rounds. For each j in [k], the j-neighbors of s form a partition C 1 , . . . , C p corresponding to their colors after − 1 rounds of k-WL. Similarly, the j-neighbors of t form a Unrollings L 1 = L + -UNR [G, s, q] and L 2 = L + -UNR [G, t, q] of sufficiently large depth. partition D 1 , . . . , D p corresponding to their colors after − 1 rounds of k-WL, where for i in [p], C i and D i have the same size and correspond to the same color. By inductive hypothesis, the corresponding depth − 1 unrollings UNR [G, c, − 1] and UNR [G, d, − 1] are isomorphic, for every c in C i and d in D i . Since we have a bijective correspondence between the depth − 1 unrollings of the j-neighbors of s and t, respectively, there exists an isomorphism between UNR [G, s, ] and UNR [G, t, ]. Moreover, this isomorphism preserves vertex labels (corresponding to isomorphism types) and edges labels (corresponding to j-neighbors).For the backward direction, suppose that UNR [G, s, ] is isomorphic to UNR [G, t,]. Then, we have a bijective correspondence between the depth − 1 unrollings of the j-neighbors of s and of t, respectively. For each j in [k], the j-neighbors of s form a partition C 1 , . . . , C p corresponding to their unrolling trees after − 1 rounds of k-WL. Similarly, the j-neighbors of t form a partition D 1 , . . . , D p corresponding to their unrolling trees after − 1 rounds of k-WL, where for i in [p], C i , and D i have the same size and correspond to the same isomorphism type of the unrolling tree. By induction hypothesis, the j-neighborhoods of s and t have an identical color profile after − 1 rounds. Finally, since the depth − 1 trees UNR [G, s, − 1] and UNR [G, t, − 1] are trivially isomorphic, the tuples s and t have the same color after − 1 rounds. Therefore, k-WL must assign the same color to s and t after rounds.Using identical arguments, we can state the analogue of Lemma 12 for the algorithms δ-k-WL, δ-k-LWL, δ-k-LWL + , and their corresponding unrolling constructions δ-UNR , L-UNR and L + -UNR . The proof is identical and is hence omitted. The following statements hold.The colors of s and t after rounds of δ-k-WL are identical if and only if the unrolling tree δ-UNR [G, s, ] is isomorphic to the unrolling tree δ-UNR [G, t, ]. 2. The colors of s and t after rounds of δ-k-LWL are identical if and only if the unrolling tree L-UNR [G, s, ] is isomorphic to the unrolling tree L-UNR [G, t, ]. 3. The colors of s and t after rounds of δ-k-LWL + are identical if and only if the unrolling tree L + -UNR [G, s, ] is isomorphic to the unrolling tree L + -UNR [G, t, ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Lemma 14 .</head><label>14</label><figDesc>Let G be a connected graph, and let s and t in V (G) k . If the stable colorings of s and t under the δ-k-LWL + are identical, then the stable colorings of s and t under δ-k-WL are also identical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Finally</head><label></label><figDesc>, since for a graph G = (V, E), M δ,δ i (v) = M δ,δ i (w) implies M δ,+ i (v) = M δ,+ i (w) for all v and w in V (G) k and i ≥ 0, it holds that δ-k-WL δ-k-LWL + . Together with Lemma 14 above, this finishes the proof of Theorem 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>±0.6 58.9 ±1.0 39.0 ±0.8 66.1 ±0.4 66.3 ±0.2 61.3 ±1.1 71.2 ±0.6 60.0 ±0.2 SP 40.7 ±0.9 58.5 ±0.4 39.4 ±0.3 74.0 ±0.3 73.0 ±0.4 61.3 ±1.3 75.6 ±0.5 84.6 ±0.3 1-WL 50.7 ±1.2 72.5 ±0.5 50.0 ±0.5 84.2 ±0.3 84.3 ±0.3 62.6 ±2.0 72.6 ±1.2 72.8 ±0.5 WLOA 56.8 ±1.6 72.7 ±0.9 50.1 ±0.7 84.9 ±0.3 85.2 ±0.3 61.8 ±1.5 73.2 ±0.6 88.1 ±0.4 Neural Gin-0 38.8 ±1.7 72.7 ±0.9 49.9 ±0.8 78.5 ±0.5 76.7 ±0.8 58.2 ±3.3 71.3 ±0.9 89.8 ±0.6 Gin-ε 39.4 ±1.7 72.9 ±0.6 49.6 ±0.9 78.6 ±0.3 77.0 ±0.5 57.7 ±2.0 71.1 ±0.8 90.3 ±0.3 ±0.3 67.5 ±0.2 62.3 ±1.6 ±0.5 67.2 ±0.4 61.9 ±0.9 ±0.3 84.2 ±0.4 60.3 ±3.2 75.1 ±0.3 89.7 ±0.4 δ-2-LWL + 52.9 ±1.4 75.7 ±0.7 62.5 ±1.0 91.4 ±0.2 89.3 ±0.2 62.6 ±1.6 ±0.2 82.4 ±0.4 61.3 ±1.6 ±0.5 81.9 ±0.4 61.3 ±2.0 OOM OOM</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell cols="3">ENZYMES IMDB-BINARY IMDB-MULTI</cell><cell>NCI1</cell><cell cols="4">NCI109 PTC_FM PROTEINS REDDIT-BINARY</cell></row><row><cell cols="3">Baseline 29.7 Global GR 2-WL 36.7 ±1.7 3-WL 42.3 ±1.1 δ-2-WL 37.5 ±1.2</cell><cell>68.2 ±1.1 67.8 ±0.8 68.1 ±1.1</cell><cell>48.1 ±0.5 47.0 ±0.7 47.9 ±0.7</cell><cell cols="4">67.1 75.0 ±0.8 OOT OOT 61.5 ±1.7 OOM 67.0 75.0 ±0.4</cell><cell>OOM OOM OOM</cell></row><row><cell></cell><cell>δ-3-WL</cell><cell>43.0 ±1.4</cell><cell>67.5 ±1.0</cell><cell>47.3 ±0/9</cell><cell>OOT</cell><cell>OOT</cell><cell>61.2 ±2.0</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell>Local</cell><cell>δ-2-LWL δ-3-LWL</cell><cell>56.6 ±1.2 57.6 ±1.2</cell><cell>73.3 ±0.5 72.8 ±1.2</cell><cell>50.2 ±0.6 49.3 ±1.0</cell><cell cols="4">84.7 79.3 ±1.1 83.4 OOM</cell><cell>91.1 ±0.5 OOM</cell></row><row><cell></cell><cell>δ-3-LWL +</cell><cell>56.8 ±1.2</cell><cell>76.2 ±0.8</cell><cell>64.2 ±0.9</cell><cell>82.7</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracies in percent and standard deviations, OOT-Computation did not finish within one day, OOM-Out of memory.</figDesc><table><row><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dataset</cell></row><row><cell>Set</cell><cell cols="3">ENZYMES IMDB-BINARY IMDB-MULTI</cell><cell></cell><cell>Method</cell><cell>ZINC (FULL)</cell><cell>ALCHEMY (FULL)</cell></row><row><cell>δ -2 -W L Train Test δ -2 -L W L Train Test</cell><cell>91.2 37.5 98.8 56.6</cell><cell>83.8 68.1 83.5 73.3</cell><cell>57.6 47.9 59.9 50.2</cell><cell>Baseline</cell><cell>GINE-ε</cell><cell>0.084</cell></row><row><cell>+ Train δ -2 -L W L Test</cell><cell>99.5 52.9</cell><cell>95.1 75.7</cell><cell>86.5 62.5</cell><cell></cell><cell></cell><cell></cell></row></table><note>(a) Training versus test accuracy of local and global kernels for a subset of the datasets.(b) Mean MAE (mean std. MAE, logMAE) on large- scale (multi-target) molecular regression tasks.±0.004 0.103 ±0.001 -2.956 ±0.029 2-WL-GNN 0.133 ±0.013 0.093 ±0.001 -3.394 ±0.035 δ-2-GNN 0.042 ±0.003 0.080 ±0.001 -3.516 ±0.021 δ-2-LGNN 0.045 ±0.006 0.083 ±0.001 -3.476 ±0.025</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Additional results for kernel and neural approaches.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Speed up ratios of local over global algorithms. order architectures). A2 See Table 1. The δ-2-LWL + improves over the δ-2-LWL on all datasets excluding ENZYMES. For example, on IMDB-MULTI, NCI1, NCI109, and PROTEINS the algorithm achieves an improvement over of 4%, respectively, achieving a new state-of-the-art. The computation times are only increased slightly, see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>E.1 Datasets, graph kernels, and neural architectures Dataset Properties Number of graphs Number of classes/targets ∅ Number of vertices ∅ Number of edges Vertex labels Edge labels</figDesc><table><row><cell>ENZYMES</cell><cell>600</cell><cell>6</cell><cell>32.6</cell><cell>62.1</cell><cell></cell><cell></cell></row><row><cell>IMDB-BINARY</cell><cell>1 000</cell><cell>2</cell><cell>19.8</cell><cell>96.5</cell><cell></cell><cell></cell></row><row><cell>IMDB-MULTI</cell><cell>1 500</cell><cell>3</cell><cell>13.0</cell><cell>65.9</cell><cell></cell><cell></cell></row><row><cell>NCI1</cell><cell>4 110</cell><cell>2</cell><cell>29.9</cell><cell>32.3</cell><cell></cell><cell></cell></row><row><cell>NCI109</cell><cell>4 127</cell><cell>2</cell><cell>29.7</cell><cell>32.1</cell><cell></cell><cell></cell></row><row><cell>PTC_FM</cell><cell>349</cell><cell>2</cell><cell>14.1</cell><cell>14.5</cell><cell></cell><cell></cell></row><row><cell>PROTEINS</cell><cell>1 113</cell><cell>2</cell><cell>39.1</cell><cell>72.8</cell><cell></cell><cell></cell></row><row><cell>REDDIT-BINARY</cell><cell>2 000</cell><cell>2</cell><cell>429.6</cell><cell>497.8</cell><cell></cell><cell></cell></row><row><cell>YEAST</cell><cell>79 601</cell><cell>2</cell><cell>21.5</cell><cell>22.8</cell><cell></cell><cell></cell></row><row><cell>YEASTH</cell><cell>79 601</cell><cell>2</cell><cell>39.4</cell><cell>40.7</cell><cell></cell><cell></cell></row><row><cell>UACC257</cell><cell>39 988</cell><cell>2</cell><cell>26.1</cell><cell>28.1</cell><cell></cell><cell></cell></row><row><cell>UACC257H</cell><cell>39 988</cell><cell>2</cell><cell>46.7</cell><cell>48.7</cell><cell></cell><cell></cell></row><row><cell>OVCAR-8</cell><cell>40 516</cell><cell>2</cell><cell>26.1</cell><cell>28.1</cell><cell></cell><cell></cell></row><row><cell>OVCAR-8H</cell><cell>40 516</cell><cell>2</cell><cell>46.7</cell><cell>48.7</cell><cell></cell><cell></cell></row><row><cell>ZINC</cell><cell>249 456</cell><cell>12</cell><cell>23.1</cell><cell>24.9</cell><cell></cell><cell></cell></row><row><cell>ALCHEMY</cell><cell>202 579</cell><cell>12</cell><cell>10.1</cell><cell>10.4</cell><cell></cell><cell></cell></row><row><cell>QM9</cell><cell>129 433</cell><cell>12</cell><cell>18.0</cell><cell>18.6</cell><cell>(13+3D)  †</cell><cell>(4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Dataset statistics and properties, † -Continuous vertex labels following<ref type="bibr" target="#b44">[45]</ref>, the last three components encode 3D coordinates.In the following, we give an overview of employed datasets, (baselines) kernels, and (baseline) neural architectures.&lt; 0.1 88.8 &lt; 0.1 96.8 &lt; 0.1 96.9 &lt; 0.1 96.1 &lt; 0.1 96.2 &lt; 0.1 Neural GINE 88.3 &lt; 0.1 88.3 &lt; 0.1 95.9 &lt; 0.1 95.9 &lt; 0.1 94.9 &lt; 0.1 94.9 &lt; 0.1 GINE-ε 88.3 &lt; 0.1 88.3 &lt; 0.1 95.9 &lt; 0.1 95.9 &lt; 0.1 94.9 &lt; 0.1 94.9 &lt; 0.1 Local δ-2-LWL 89.2 &lt; 0.1 88.9 &lt; 0.1 97.0 &lt; 0.1 96.9 &lt; 0.1 96.4 &lt; 0.1 96.3 &lt; 0.1 δ-2-LWL + 95.0 &lt; 0.1 95.7 &lt; 0.1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell></cell></row><row><cell>Method</cell><cell>YEAST</cell><cell cols="4">YEASTH UACC257 UACC257H OVCAR-8 OVCAR-8H</cell></row><row><cell>1-WL</cell><cell cols="2">88.8 97.4 &lt; 0.1</cell><cell>98.1 &lt; 0.1</cell><cell>97.4 &lt; 0.1</cell><cell>97.7 &lt; 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracies in percent and standard deviations on medium-scale datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Set</cell><cell cols="8">ENZYMES IMDB-BINARY IMDB-MULTI NCI1 NCI109 PTC_FM PROTEINS REDDIT-BINARY</cell></row><row><cell>δ -2 -W L Train Test</cell><cell>91.2 37.5</cell><cell>83.8 68.1</cell><cell>57.6 47.9</cell><cell>91.5 67.0</cell><cell>92.4 67.2</cell><cell>74.1 61.9</cell><cell>85.4 75.0</cell><cell>--</cell></row><row><cell>δ -2 -L W L Train Test</cell><cell>98.8 56.6</cell><cell>83.5 73.3</cell><cell>59.9 50.2</cell><cell>98.6 84.7</cell><cell>99.1 84.2</cell><cell>84.0 60.3</cell><cell>84.5 75.1</cell><cell>92.0 89.7</cell></row><row><cell>+ Train δ -2 -L W L Test</cell><cell>99.5 52.9</cell><cell>95.1 75.7</cell><cell>86.5 62.5</cell><cell>95.8 91.4</cell><cell>94.4 89.3</cell><cell>96.1 62.6</cell><cell>90.9 79.3</cell><cell>96.2 91.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>±0.022 0.145 ±0.006 0.084 ±0.004 0.185 ±0.007 -1.864 ±0.062 0.127 ±0.004 -2.415 ±0.053 0.103 ±0.001 -2.956 ±0.029 2-WL-GNN 0.399 ±0.006 0.357 ±0.017 0.133 ±0.013 0.149 ±0.004 -2.609 ±0.029 0.105 ±0.001 -3.139 ±0.020 0.093 ±0.001 -3.394 ±0.035 δ-2-GNN 0.374 ±0.022 0.150 ±0.064 0.042 ±0.003 0.118 ±0.001 -2.679 ±0.044 0.085 ±0.001 -3.239 ±0.023 0.080 ±0.001 -3.516 ±0.021 δ-2-LGNN 0.306 ±0.044 0.100 ±0.005 0.045 ±0.006 0.122 ±0.003 -2.573 ±0.078 0.090 ±0.001 -3.176 ±0.020 0.083 ±0.001 -3.476 ±0.025 o</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell>ZINC (10k)</cell><cell>ZINC (50k) ZINC (FULL)</cell><cell>ALCHEMY (10K)</cell><cell>ALCHEMY (50K)</cell><cell>ALCHEMY (FULL)</cell></row><row><cell>Baseline</cell><cell>GINE-ε</cell><cell>0.278</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Mean MAE (mean std. MAE, logMAE) on large-scale (multi-target) molecular regression tasks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Graph Kernel</cell><cell cols="8">ENZYMES IMDB-BINARY IMDB-MULTI NCI1 NCI109 PTC_FM PROTEINS REDDIT-BINARY</cell></row><row><cell>Baseline</cell><cell>GR SP 1-WL WLOA</cell><cell>&lt;1 &lt;1 &lt;1 &lt;1</cell><cell>&lt;1 &lt;1 &lt;1 &lt;1</cell><cell>&lt;1 &lt;1 &lt;1 &lt;1</cell><cell>1 2 2 14</cell><cell>1 2 2 14</cell><cell>&lt;1 &lt;1 &lt;1 &lt;1</cell><cell>&lt;1 &lt;1 &lt;1 1</cell><cell>2 1 035 2 15</cell></row><row><cell>Global</cell><cell>2-WL 3-WL δ-2-WL</cell><cell>302 74 712 294</cell><cell>89 18 180 89</cell><cell>44 5 346 44</cell><cell>1 422 OOT 1 469</cell><cell>1 445 OOT 1 459</cell><cell>11 5 346 11</cell><cell>14 755 OOM 14 620</cell><cell>OOM OOM OOM</cell></row><row><cell></cell><cell>δ-3-WL</cell><cell>64 486</cell><cell>17 464</cell><cell>5 321</cell><cell>OOT</cell><cell>OOT</cell><cell>1119</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell></cell><cell>δ-2-LWL</cell><cell>29</cell><cell>25</cell><cell>20</cell><cell>101</cell><cell>102</cell><cell>1</cell><cell>240</cell><cell>59 378</cell></row><row><cell>Local</cell><cell>δ-2-LWL + δ-3-LWL</cell><cell>35 4 453</cell><cell>31 3 496</cell><cell>24 2 127</cell><cell>132 18 035</cell><cell>132 17 848</cell><cell>1 98</cell><cell>285 OOM</cell><cell>84 044 OOM</cell></row><row><cell></cell><cell>δ-3-LWL +</cell><cell>4 973</cell><cell>3 748</cell><cell>2 275</cell><cell>20 644</cell><cell>20 410</cell><cell>105</cell><cell>OOM</cell><cell>OOM</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We define the 1-WL in the next subsection.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For clarity of presentation we omit biases.<ref type="bibr" target="#b5">6</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We opted for comparing on the QM9 dataset to ensure a fair comparison concerning hyperparameter selection.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We opted for not implementing the δ-k-LGNN + as it would involve precomputing #.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We define the 1-WL in the next subsection.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and disclosure of funding</head><p>We thank Matthias Fey for answering our questions with regard to PYTORCH GEOMETRIC and Xavier Bresson for providing the ZINC dataset. This work is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy -EXC-2047/1 -390685813 and under DFG Research Grants Program-RA 3242/1-1-411032549.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cormorant: Covariant molecular neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14510" to="14519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On exact computation with an infinitely wide neural net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8139" to="8148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the power of color refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köbler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Fundamentals of Computation Theory</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="339" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On Weisfeiler-Leman invariance: Subgraph counts and related graph properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fuhlbrück</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köbler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Fundamentals of Computation Theory</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="111" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sherali-adams relaxations and indistinguishability in counting logics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atserias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Maneva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="137" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quantum and non-signalling graph isomorphisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atserias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Mancinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sámal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varvitsiotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="289" to="328" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph isomorphism in quasipolynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGACT Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="684" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hypergraph convolution and hypergraph attention. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Network biology: Understanding the cell&apos;s functional organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-L</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">N</forename><surname>Oltvai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The logical expressiveness of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barceló</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Kostylev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Reutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Pablo</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tight lower and upper bounds for the complexity of canonical colour refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berkholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Bonsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual European Symposium on Algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fürer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards sparse hierarchical graph classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jovanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<idno>abs/1811.01287</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GraRep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4869" to="4880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Machine learning on graphs: A model and comprehensive taxonomy. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<idno>27:1-27:27</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Alchemy: A quantum chemistry dataset for benchmarking AI models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with GNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15868" to="15876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Can graph neural networks count substructures? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno>abs/2002.04025</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<idno>abs/2004.05718</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Networks</surname></persName>
		</author>
		<title level="m">Crowds, and Markets: Reasoning About a Highly Connected World</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coloring graph neural networks for node disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Virmaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2126" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lovász meets Weisfeiler and Leman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<idno>1-40:14</idno>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph Neural Tangent Kernel: Fusing graph neural networks with graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5723" to="5733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Benchmarking graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno>abs/2003.00982</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with PyTorch Geometric. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SplineCNN: Fast geometric deep learning with continuous B-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Flam-Shepherd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Friederich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<title level="m">Neural message passing on high order paths. CoRR, abs</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the combinatorial power of the Weisfeiler-Lehman algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fürer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithms and Complexity</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph U-Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>abs/2002.06157</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Let&apos;s agree to degree: Comparing graph convolutional networks in the message-passing framework. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Geerts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mazowiecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Pérez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The expressive power of kth-order invariant graph networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Floris</forename><surname>Geerts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Descriptive Complexity, Canonisation, and Definable Graph Structure Theory. Lecture Notes in Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Word2vec, Node2vec, Graph2vec, X2vec: Towards a theory of vector embeddings of structured data. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pebble games and linear equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Symbolic Logic</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="797" to="844" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dimension reduction via colour refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mladenov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Algorithms</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="505" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wiebking</forename><forename type="middle">D</forename><surname>Deep Weisfeiler Leman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Distribution of node embeddings as multiresolution features for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The Predictive Toxicology Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Helma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="108" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4563" to="4572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Describing Graphs: A First-Order Approach to Graph Canonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="59" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural Tangent kernel: convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8580" to="8589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2328" to="2337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">GraLSP: Graph neural networks with local structural patterns. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning with similarity functions on graphs using matchings of geometric embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="467" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">The iteration number of colour refinement. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Upper bounds on the quantifier depth for graph differentiation in first order logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Symposium on Logic in Computer Science</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graphs identified by logics with counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Mathematical Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="319" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Chemnet: A novel neural network based method for graph/property mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Kireev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="180" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The multiscale Laplacian graph kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2982" to="2990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1615" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A property testing framework for the theoretical expressivity of graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2348" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A unifying view of explicit and implicit feature maps of graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Minining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1505" to="1547" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A survey on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Graph convolutional networks with motif-based attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th ACM International Conference on Information</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Walk refinement, walk logic, and the iteration number of the Weisfeiler-Leman algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Annual ACM/IEEE Symposium on Logic in Computer Science</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">A simple proof of the universality of invariant/equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<idno>abs/1910.03802</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Sherali-adams relaxations of graph isomorphism polytopes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Malkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Optimization</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="73" to="97" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Subgraph pattern neural networks for high-order graph evolution prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Mouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3778" to="3787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Automatic generation of complementary descriptors with molecular graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Merkwirth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lengauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Glocalized Weisfeiler-Lehman kernels: Global-local feature maps of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Weisfeiler and Leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">TUDataset: A collection of benchmark datasets for learning with graphs. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Relational pooling for graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4663" to="4673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Matching node embeddings for graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A degeneracy framework for graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2595" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A persistent Weisfeiler-Lehman procedure for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5448" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">DropEdge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Approximation ratios of graph neural networks for combinatorial problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4083" to="4092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Random features strengthen graph neural networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">BRENDA, the enzyme database: updates and major new developments. Nucleic acids research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schomburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ebeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gremse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schomburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-01" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="431" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="714" to="749" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">A deep learning approach to antibiotic discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cubillos-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Donghia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macnair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carfrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bloom-Ackerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chiappino-Pepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Badran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collins</surname></persName>
		</author>
		<idno>e13</idno>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="688" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Wasserstein Weisfeiler-Lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Togninalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ghisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Llinares-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6436" to="6446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Hunt for the unique, stable, sparse and fast feature learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="88" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1539" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">On Construction and Identification of Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Mathematics</title>
		<imprint>
			<biblScope unit="volume">558</biblScope>
			<date type="published" when="1976" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leman</surname></persName>
		</author>
		<ptr target="https://www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf" />
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
	<note>English translation by G. Ryabov is available at</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">MoleculeNet: A benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1901.00596</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">HyperGCN: A new method for training graph convolutional networks on hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nimishakavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1509" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Mining significant graph patterns by leap search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">A structural smoothing framework for robust graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2125" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yixin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4428" to="4435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Hyper-SAGNN: A self-attention based graph neural network for hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Learning with hypergraphs: Clustering, classification, and embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">For all x in X j , x is a local j-neighbor of s if and only if σ j (x) is a local j-neighbor of t. 2. For all x in X j</title>
		<imprint/>
	</monogr>
	<note>G, x, i − 1] = δ-UNR [G, σ j (x), i − 1], i.e., x and σ j (x) are identically colored after i − 1 rounds of δ-k-WL</note>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">From the definition of δ-UNR trees, the existence of such σ 1 , . . . , σ k immediately implies the desired claim δ-UNR</title>
		<imprint/>
	</monogr>
	<note>G, s, i] = δ-UNR [G, t, i</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">First, we show the following claim. in C. Finally, the set of j-neighbors of w is equal to the set of j-neighbors of s, which is X j . Similarily, the set of j-neighbors of z is equal to the set of j-neighbors of t, which is Y j . Hence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>|c ∩ X J | = |c ∩ Y J |</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<monogr>
		<title level="m" type="main">the number of local j-neighbors of s in C ∩ X j is equal to the number of local j-neighbors of t in C ∩ Y j . Otherwise, we could perform one more round of δ-k-LWL + and derive different colors for s and t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreover</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>a contradiction</note>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">REDDIT-BINARY [119] datasets. To show that our kernels also scale to larger datasets, we additionally used the mid-scale YEAST, YEASTH, UACC257, UACC257H, OVCAR-8, OVCAR-8H [117] 10 datasets. For the neural architectures we used the large-scale molecular regression datasets ZINC [34, 57] and ALCHEMY [21]. We opted for not using the 3D-coordinates of the ALCHEMY dataset to solely show the benefits of the (sparse) higher-order structures concerning graph structure and discrete labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imdb-Binary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imdb-Multi</surname></persName>
		</author>
		<idno>PRO- TEINS [31, 13</idno>
		<imprint>
			<biblScope unit="volume">119</biblScope>
		</imprint>
	</monogr>
	<note>] regression dataset. 11 To study data efficiency, we also used smaller subsets of the ZINC and ALCHEMY dataset. That is, for the ZINC 10K (ZINK 50K) dataset, following [34], we sampled 10 000 (50 000) graphs from the training, and 1 000 (5 000) from the training and validation split, respectively. For ZINC 10K, we used the same splits as provided by [34</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">For the ALCHEMY 10K (ALCHEMY 50K) dataset, as there is no fixed split available for the full dataset 12 , we sampled the (disjoint) training, validation, and test splits uniformly and at random from the full dataset</title>
	</analytic>
	<monogr>
		<title level="m">See Table 4 for dataset statistics and properties</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">All kernels were (re-)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-Wl</forename><surname>K-Lwl, Δ-K-Lwl + , Δ-K-Wl</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>kernel for k in {2, 3}. We compare our kernels to the Weisfeiler-Leman subtree kernel (1-WL) [100], the Weisfeiler-Leman Optimal Assignment kernel (WLOA) [68], the graphlet kernel [99] (GR), and the shortest-path kernel [13] (SP). implemented in C ++ 11. For the graphlet kernel we counted (labeled) connected subgraphs of size three</note>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
				<ptr target="https://alchemy.tencent.com/" />
		<title level="m">Note that the full dataset is different from the contest dataset</title>
		<imprint/>
	</monogr>
	<note>it does not provide normalized targets</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title/>
		<ptr target="http://www.graphlearning.io" />
	</analytic>
	<monogr>
		<title level="j">All datasets can</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
