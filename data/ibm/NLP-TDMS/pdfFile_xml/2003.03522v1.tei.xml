<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MobilePose: Real-Time Pose Estimation for Unseen Objects with Weak Shape Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingbo</forename><surname>Hou</surname></persName>
							<email>tingbo@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Ahmadyan</surname></persName>
							<email>ahmadyan@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangkai</forename><surname>Zhang</surname></persName>
							<email>liangkai@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Wei</surname></persName>
							<email>jianingwei@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
							<email>grundman@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Pkwy</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MobilePose: Real-Time Pose Estimation for Unseen Objects with Weak Shape Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pose estimation</term>
					<term>3D detection</term>
					<term>shape</term>
					<term>segmentation</term>
					<term>mobile</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the problem of detecting unseen objects from RGB images and estimating their poses in 3D. We propose two mobile friendly networks: MobilePose-Base and MobilePose-Shape. The former is used when there is only pose supervision, and the latter is for the case when shape supervision is available, even a weak one. We revisit shape features used in previous methods, including segmentation and coordinate map. We explain when and why pixel-level shape supervision can improve pose estimation. Consequently, we add shape prediction as an intermediate layer in the MobilePose-Shape, and let the network learn pose from shape. Our models are trained on mixed real and synthetic data, with weak and noisy shape supervision. They are ultra lightweight that can run in real-time on modern mobile devices (e.g. 36 FPS on Galaxy S20). Comparing with previous single-shot solutions, our method has higher accuracy, while using a significantly smaller model (2 ∼ 3% in model size or number of parameters).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Detecting unseen objects from images and estimating their poses in 3D remain a challenge in computer vision, which have not been fully explored in previous work. Solving this problem enables many applications across computer vision, augmented reality (AR), autonomous driving, and robotics. Furthermore, mobile-friendly solutions add their own layers of complexity to the problem: first, they should run in real-time with limited model size; second, unlike self-driving cars where cameras are fixed <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref>, on-device models have to cope with various device rotations.</p><p>Towards the challenges, existing methods often simplify the problem by assuming objects are known. Most of the methods from literature are instanceaware <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b7">8]</ref>. Meaning that, they are trained on a set of specific objects, and expected to work on the same instances. Object-specific features including appearance and geometry can be learned to determine poses, and hence, applications are mostly limited to grabbing known objects in robotics. Recent progress arXiv:2003.03522v1 [cs.CV] 7 Mar 2020 in pose estimation has been made by leveraging 2D-3D correspondence at inference time <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>. These methods require CAD models of the objects, which are not applicable to unseen ones. Recently, there are a few attempts for removing the requirement of known objects. As an example, <ref type="bibr" target="#b29">[30]</ref> uses depth images to align unseen objects at inference time. While relieving prior knowledge of objects, it relies on depth sensors, which requires extra hardware that is not available on general mobile devices.</p><p>For unseen objects, we want the model to learn intra-category features, e.g. common shape or geometry of a category. We categorize geometry related representations and name them shape features, which can be mapped to images with pixel-level signals. Shape features have been previously used in pose estimation, e.g. segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8]</ref>, parameterization map <ref type="bibr" target="#b31">[32]</ref>, and coordinate map <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b14">15]</ref>. These methods train Convolutional Neural Networks (CNNs) to infer shape features, which are then used out of the networks. They rely on highly accurate predictions of shape features, preferably with high resolutions, to align with objects' CAD models. However, estimating shapes of unseen objects is as hard as, or maybe even harder than estimating poses. Instead of post-processing, we use shape prediction as an intermediate layer, and have the network learn pose from shape. Besides, pixel-level shape labels are expensive to annotate, which brings another challenge to the problem. This motivates us to look for weakly supervised solutions.</p><p>Although there are methods claiming real-time <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b14">15]</ref>, none of them haven been deployed to mobile devices. To run on mobile, the CNN model needs to be ultra lightweight, e.g. MobileNet <ref type="bibr" target="#b6">[7]</ref> and MobileNetv2 <ref type="bibr" target="#b23">[24]</ref>, which only have a few million parameters. Another requirement is post-processing, whose runtime also counts. This is often overlooked in previous methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30]</ref>, where expensive algorithms are widely used, e.g. RANdom Sample Consensus (RANSAC), Perspective-n-Point (PnP), and Iterative Closest Point (ICP).</p><p>In this paper, we address the aforementioned challenges and limitations by proposing two mobile-friendly networks: MobilePose-Base and MobilePose-Shape. MobilePose-Base is our baseline network with minimal model size, which detects unseen objects and estimates their poses in a single shot. The detection is anchor-free, following a rising trend in 2D object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref>. It regresses projected vertices of a 3D bounding box to estimate object's pose. Since pose estimation is in a low-dimensional space, high-resolution features need supervision in order to make a positive contribution. Therefore, we also propose MobilePose-Shape, which predicts shape features in an intermediate layer. Running on mobile devices, the two networks only require cheap operations in postprocessing to fully recover object's rotation, translation, and size up to a scale. In this work, we are particularly interested in shoes. Following fashion trends, shoes have changing appearances and shapes with a number of sub-categories, e.g. sneakers, boots, flip-flops, high heels, etc.</p><p>To summarize, our contributions in this paper are -We propose two novel MobilePose networks for detecting unseen objects from RGB images and estimating their poses. Unlike previous methods, we do not require any prior knowledge of the objects or additional hardware at inference time. -We revisit shape supervision by exploring when and why it can improve pose estimation. Comparing with previous methods that have shape prediction in parallel to other streams and use it in post-processing, we insert it as an intermediate layer and let the network learn pose from shape. -We train models with weak shape supervision, which transfers shape learning from synthetic data to real data. With the lightweight models, we develop end-to-end applications that can run on mobile devices in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Given a large literature on pose estimation, we categorize recent work by the prior knowledge the methods require at inference time.</p><p>Instance-aware methods learn poses from a set of known objects and are expected to work on the same instances. SSD-6D <ref type="bibr" target="#b8">[9]</ref> extends the classic architecture in 2D object detection to 6DoF pose estimation. It predicts 2D bounding box, discretized viewpoint, and in-plane rotation in a single shot. PoseCNN <ref type="bibr" target="#b30">[31]</ref> estimates 3D translation of an object as image locations and depth while regressing over 3D rotation. <ref type="bibr" target="#b27">[28]</ref> predicts the location of the 3D bounding box's vertices in an image using heatmaps, then recovers the orientation and translation using PnP by knowing the object size. In <ref type="bibr" target="#b25">[26]</ref>, an Augmented Autoencoder (AAE) is used to extract orientation from other factors, e.g. translation and scale. A codebook is created for each object, which contains AAE embeddings of all orientations. YOLO-6D <ref type="bibr" target="#b26">[27]</ref> predicts image locations of projected box vertices, and recovers 6DoF pose using PnP. BB8 <ref type="bibr" target="#b20">[21]</ref> uses coarse segmentation to roughly locate objects, subsequently estimating the corners of a 3D bounding box. The method in <ref type="bibr" target="#b7">[8]</ref> parallels segmentation and bounding box estimation as two branches of the network. Pose estimation is improved by letting the network learn the entire shape of an object. <ref type="bibr" target="#b24">[25]</ref> utilizes a hybrid intermediate representation to provide more supervision during training.</p><p>Model-aware methods require 3D CAD models of objects in post-processing. This is a much stronger prior, which leverages 2D-3D correspondences and yields higher accuracy. PVNet <ref type="bibr" target="#b19">[20]</ref> finds 2D-3D correspondence of object features, and formulates pose estimation as a PnP problem. By assuming known 3D models of target objects, iterative mechanism <ref type="bibr" target="#b13">[14]</ref> has been utilized to refine estimated pose by comparing rendered images with inputs. Another recent method <ref type="bibr" target="#b31">[32]</ref> computes the UV map of the object from a single RGB method and uses PnP + RANSAC to estimate the 6DOF object pose. The UV map is a parameterization of 3D models, which also provides 2D-3D correspondence. <ref type="bibr" target="#b14">[15]</ref> estimates rotation and translation separately, where rotation is computed, again, by RANSAC + PnP from coordinate map. Similarly, Pix2Pose <ref type="bibr" target="#b18">[19]</ref> also predicts 3D coordinates of objects from images, and uses RANSAC + PnP to recover poses. For a better prediction, it adopts Generative Adversarial Network (GAN) in training to discriminate predicted coordinates and rendered coordinates.</p><p>Depth-aware methods require depth images in addition to RGB images for pose estimation. In <ref type="bibr" target="#b9">[10]</ref>, pose is estimated by searching over the nearest neighbors in a codebook of encoded RGB-D patches. DenseFusion <ref type="bibr" target="#b28">[29]</ref> processes the RGB image and depth image individually, and uses a dense fusion network to extract pixel-wise dense features. Since it has 3D coordinates, it directly predicts translation and rotation. In <ref type="bibr" target="#b12">[13]</ref>, a multi-view framework was proposed using viewpoint alignment and pose voting. It adopts depth image as an optional input. A recent work <ref type="bibr" target="#b29">[30]</ref> predicts object's normalized coordinates, and aligns them with a depth image to compute the pose. The normalized coordinates compose yet another 2D-3D correspondence mapping.</p><p>Detection-aware methods rely on existing 2D detectors to find a bounding box or ROI of an object. <ref type="bibr" target="#b17">[18]</ref> estimates 3D bounding boxes of vehicles by applying geometric constraints on 2D bounding boxes. The SSD-6D also detects 2D bounding boxes using a SSD-style network. <ref type="bibr" target="#b29">[30]</ref> employees the Mask R-CNN <ref type="bibr" target="#b4">[5]</ref> to detect objects and find their 2D locations. <ref type="bibr" target="#b14">[15]</ref> adopts YOLOv3 for 2D detector to crop images with objects during inference. Pix2Pose <ref type="bibr" target="#b18">[19]</ref> also assumes cropped images of objects, which are obtained by a modified Fast R-CNN <ref type="bibr" target="#b22">[23]</ref> and Retinanet <ref type="bibr" target="#b15">[16]</ref>.</p><p>Shape features have been employed in estimating poses, e.g. segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8]</ref>, 3D features <ref type="bibr" target="#b19">[20]</ref>, parameterization map <ref type="bibr" target="#b31">[32]</ref>, coordinate map <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>, etc. These methods use shape prediction in post-processing, instead of in network. That is, they first infer shape features by CNN, and then use PnP or ICP to align them with the 3D models. On the contrary, we adopt shape prediction as an intermediate layer, and let the CNN learn 3D bounding boxes from them. This generalizes our method to unseen objects.</p><p>Real-time solutions have been proposed to push the technique closer to applications. The models need to be lightweight in order to run in real-time, preferably in a single shot. <ref type="bibr" target="#b8">[9]</ref> uses Inceptionv4 as backbone to build a SSD-style architecture. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8]</ref> both adopt YOLOv2 <ref type="bibr" target="#b21">[22]</ref> as backbone. In a recent work <ref type="bibr" target="#b14">[15]</ref>, YOLOv3 is used to detect objects in the first stage, while pose is estimated in the second stage. With the limitations of model size and runtime, none of them has been deployed to mobile devices and runs in real-time there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MobilePose-Base</head><p>In this section, we propose MobilePose-Base as our baseline network, which detects unseen objects without anchors and estimate their poses in a single shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Backbone</head><p>We devise the backbone as a popular encoder-decoder architecture. To build an ultra lightweight model, we select the MobileNetv2 <ref type="bibr" target="#b23">[24]</ref> to build our encoder, which is proven to run real-time on mobile, and outperforms YOLOv2 <ref type="bibr" target="#b21">[22]</ref>. The MobileNetv2 is built upon inverted residual blocks, where shortcut connections are between thin bottleneck layers. An expansion-and-squeeze scheme is used in  the blocks. To make it even lighter, we remove some blocks with large channels at the bottleneck, reducing half of the parameters. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the blue and purple boxes are convolutional and deconvolutional blocks, respectively. An orange box represents an inverted residual block. The numbers of blocks and their dimensions shown in the figure are exactly the same in our implementation. The input is an image with size 640 × 480 × 3. The encoder starts with a convolutional layer, followed by five levels of inverted residual blocks. At the bottleneck, we use four 128-channel blocks, instead of four 160-channel blocks and one 320-channel block in MobileNetv2 <ref type="bibr" target="#b23">[24]</ref>. The decoder is composed by a deconvolution layer, a concatenation layer with skip connection from the same scale in the encoder, and two inverted residual blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Heads</head><p>We attach two heads after the backbone: detection and regression. The detection head is inspired by the anchor-free methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref> in 2D object detection. We model objects as distributions around their centers. The detection head outputs a 40 × 30 × 1 heatmap. Specifically, for image I with pixels {p}, its heatmap is computed as a bivariate normal distribution <ref type="bibr" target="#b2">[3]</ref> </p><formula xml:id="formula_0">H(p) = max i∈O (N (p − µ i , σ i )),<label>(1)</label></formula><p>where O is the set of all object instances in the image, µ i is the centroid location of object i, and σ i is the kernel size that is proportional to object size. We keep the fractions when computing projections µ i to preserve accuracy. For multiple objects in an image, we select the max heat for each pixel, as the examples shown in <ref type="figure" target="#fig_1">Fig. 1</ref> and <ref type="figure" target="#fig_2">Fig. 2</ref>. By modeling objects as distributions, we end up using a simple L2 loss (mean squared error) for this head. In <ref type="figure" target="#fig_2">Fig. 2</ref>, we compare different detection methods used in single-shot pose estimation. Anchor-based methods (e.g. <ref type="bibr" target="#b26">[27]</ref>) set anchors at grid cells, and regresses bounding boxes at positive anchors (marked as green dots). It handles multiple objects in the same cell by assigning a number of anchors ad hocly. Segmentation methods (e.g. <ref type="bibr" target="#b7">[8]</ref>) find objects by segmented instances. For multiple objects from the same category, it needs instance segmentation to distinguish objects. We model objects as Gaussian distributions, and detect them by finding peaks. We use a high resolution in the figure for better illustration, while the actual resolution in our model is (40 × 30).</p><p>The regression head estimates displacement fields of bounding box vertices, similar with the vertex offsets in <ref type="bibr" target="#b7">[8]</ref>. Specifically, for a box vertex X i , let x i denote its projection on the image plane. We compute the displacement vector as</p><formula xml:id="formula_1">D i (p) = x i − p.<label>(2)</label></formula><p>Displacement fields of multiple objects in an image are merged according to their heats, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. The regression head outputs a 40 × 30 × 16 tensor, where each box vertex contributes two channels of displacements. In the figure, we only show two displacement fields for illustration. To tolerate errors in peak extraction, we regress displacements at all pixels with significant heats. We use L1 loss (mean absolute error) for this head, which is more robust to outliers. With predicted D i (p), the loss is computed as</p><formula xml:id="formula_2">L reg = mean H(p)&gt; (||D i (p) + p − x i || 1 ),<label>(3)</label></formula><p>where ||·|| 1 denotes the L1-norm, and is a threshold (0.2 in our implementation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MobilePose-Shape</head><p>When shape supervision is available, even a weak one, we provide MobilePose-Shape, which predicts shape features at an intermediate layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Shape Features</head><p>The motivation is to guide the network learn high-resolution shape features that are related to pose estimation. We found that simply introducing highresolution features without supervision does not improve our pose estimation. This is because the regression of bounding box vertices is in a low dimensional space. Without supervision, the network may overfit on object-specific features at small scales. The problem is not valid for instance-aware pose estimation, but not our case of unseen objects.</p><p>Similar with previous methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>, we select coordinate map and segmentation as our intra-category shape features, as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. The coordinate map has three channels, corresponding to the axes of 3D coordinates. If we have the CAD model of an object in training data, we can render coordinate map using normalize coordinates as colors. Coordinate map is a strong feature with pixel-level signals. However, it requires object's CAD model and pose, which are difficult to acquire. Therefore, we add segmentation as another shape feature. For simplicity, we use semantic segmentation, resulting in one additional channel in our shape supervision. Segmentation is a weak feature for pose estimation. That is, given a segmentation of an unseen object, it is not sufficient to determine its pose. Yet, it does not need object's CAD model and pose, and is easier to acquire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Shape-supervised Network</head><p>With the shape features, we modify the network with high-resolution layers in the decoder and a shape prediction layer. In previous work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>, they add another branch for shape prediction in parallel to other streams. The predicted shape is used out of network for building 2D-3D correspondence. In a contrary, we add an intermediate layer for shape prediction, whose output is further used in network. Meanwhile, shape prediction is useful in many applications, making our network a joint learning of multi-tasks: object detection, pose estimation, and shape prediction.</p><p>As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, we combine multi-scale features in the decoder. A shape layer is added at the end of the decoder, predicting shape features. It is then concatenated with the decoder to connect pose heads after downsampling. Specifically, we utilize four inverted residual blocks to reduce the resolution, and finally attach the detection head and regression head, described in Section 3.2. The shape head (160 × 120 × 4) has four channels with L2 loss (mean squared error). Training examples without shape labels are skipped when computing this loss. Through experiments, we show that even with a weak supervision, the pose estimation is improved by introducing high-resolution shape prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Post-processing</head><p>Despite the lightweight model, post-processing is another component that is critical to mobile applications. Expensive algorithms e.g. RANSAC, large-sized PnP, and ICP, are not in our consideration. As a result, we simplify the postprocessing to only two cheap operations: peak extraction and EPnP <ref type="bibr" target="#b11">[12]</ref>.</p><p>To compute projected vertices of a 3D bounding box, we extract peaks of the detection output, a 40 × 30 heatmap. For a peak pixel p, which may not necessarily be the center pixel, the eight vertices {x i } of the projected bounding box can be simply computed by</p><formula xml:id="formula_3">x i = p + D i (p),<label>(4)</label></formula><p>where D i (p) is the displacement field of vertex x i computed by Eq. 2. Given the projected 2D box vertices and the camera intrinsics, we employ the EPnP <ref type="bibr" target="#b11">[12]</ref> algorithm to recover a 3D bounding box up to scale. The algorithm has constant complexity, which solves eigen-decomposition of a 12 × 12 matrix. It does not require known object's size. We choose four control points {C j } as the origin (at object's coordinate system), and three points along the coordinate axes. These control points form an orthogonal basis of the object frame. The eight vertices of a 3D bounding box can be represented by these four control points,</p><formula xml:id="formula_4">X i = 3 j=0 α ij C j ,<label>(5)</label></formula><p>where {α ij } are coefficients that are held under rigid transformations. From camera projection, we obtain a linear system of 16 equations, with each box vertex contributing two equations for u i and v i . By re-writing control points in camera frame as a 12-vector C c , this linear system can be formulated as</p><formula xml:id="formula_5">M · C c = 0,<label>(6)</label></formula><p>where M is a 16 × 12 matrix composed by 2D vertices x i , camera intrinsics, and coefficients α ij . For details, please refer to our supplementary material. The solution to this linear system is the null eigenvectors of matrix M T M. With this solution, we can recover a 3D bonding box in camera frame by Eq. 5, and further estimate object's pose and size up to a scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Training Data</head><p>Since there is no existing large dataset of shoes with annotated poses, we build our dataset using on-device AR techniques. We annotate real data with 3D bounding boxes, and rely on synthetic data to provide weak supervision for shape. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Real Data</head><p>The lack of training data is a remaining challenge in 6DoF pose estimation. The majority of previous methods are instance-aware with full supervision from a small dataset. To solve the problem for unseen objects, we develop a pipeline to collect and annotate video clips recorded by mobile devices equipped with AR. The cutting-edge AR solutions (e.g. ARKit and ARCore) can estimate camera poses and sparse 3D features on the fly using Visual Inertial Odometry (VIO). This on-device technology enables an affordable and scalable way to generate 3D training data.</p><p>The key to our data pipeline is efficient and accurate 3D bounding box annotation. We build a tool to visualize both 2D and 3D views of recorded sequences. Annotators draw 3D bounding boxes in the 3D view, and verify them in multiple 2D views across the sequence. The drawn bounding boxes are automatically populated to all frames in the video sequence, using estimated camera poses from AR.</p><p>As a result, we annotated 1800 video clips of shoes. Clips are several-seconds long of different shoes under various environments. We only accepted one clip for one or a pair of shoes, and hence, the objects are completely different from clip to clip. Among the clips, 1500 were randomly selected for training, and the rest 300 were reserved for evaluation. Finally, considering adjacent frames from the same clip are very similar, we randomly selected 100K images for training, and 1K images for evaluation. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our real data only has 3D bounding box labels. This is because annotating pixel-level shape labels frame by frame is much more expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Synthetic Data</head><p>To provide shape supervision and enrich the real dataset, we generate two sets of synthetic data. The first one synthetic-3D has 3D labels. We collect AR video clips of background scenes, and place virtual objects into the scenes. Specifically, we render virtual objects with random poses on a detected plane in the scene, e.g. a table or a floor. We reuse estimated lights in AR sessions for lighting. Measurements in AR session data are in metric scale. Therefore, virtual objects are rendered coherently with the surrounding geometries. We collected 100 video clips of common scenes: home, office, and outdoor. For each scene, we generated 100 sequences by rendering 50 scanned shoes with random poses. Each sequence contains a number of shoes. From the generated images, we randomly selected  <ref type="figure" target="#fig_4">Fig. 4</ref> the synthetic-3D data has 3D bounding box, segmentation, and coordinate map labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>80K for training. As some examples shown in</head><p>Although the synthetic-3D data has accurate labels, the numbers of objects and backgrounds are still limited. Therefore, we build a synthetic-2D dataset by images crawled from the internet. We crawled 75K images of shoes with transparent background, and 40K images of backgrounds (e.g. office and home). Shoe images with trivial errors (e.g. no transparent background) are filtered. We segmented shoes by the alpha channel, and randomly paste them to the background images. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, the generated images are not realistic, and the labels are noisy. We roughly estimated that there are about 20% images with mild label errors (e.g. small missing parts and shadows), and about 10% images with severe label errors (e.g. non-shoe objects and large extra background).</p><p>We summarize our datasets in <ref type="table" target="#tab_0">Table 1</ref>. The three datasets are complementary to each other. The real data have real images collected at different places, the synthetic-3D data have accurate and complete labels, while the synthetic-2D data cover a large number of objects. With a low cost, we demonstrate a method to prepare training data with 2D and 3D labels, which can be used in other computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Implementation Details and Evaluation Metric</head><p>Our training pipeline is implemented in TensorFlow. We train our networks using the Adam optimizer with batch size 128 on 8 GPUs. The initial learning rate is 0.01 and gradually decays to 0.001 in 30 epochs. To deploy the trained models on mobile devices, we convert them to TFLite models. The conversion process will remove some layers such as batch normalization as they are not necessary during inference. Based on the MediaPipe framework <ref type="bibr" target="#b16">[17]</ref>, we build an application that runs on various mobile devices.</p><p>For evaluation metric, we adopt the average precision (AP) of 3D Intersectionover-Union (IoU). In previous work, the computation of 3D IoU is overly simplified. For example, <ref type="bibr" target="#b29">[30]</ref> assumes two 3D boxes are axis-aligned, and rotates one of them to get the best IoU. <ref type="bibr" target="#b26">[27]</ref> thought computing convex hull is tedious, and they used 2D IoU of projections. These simplifications do not hold in general 3D oriented boxes and often result in over-estimation of the 3D IoU metric. On the contrary, we compute the exact 3D IoU by finding intersecting points of two oriented boxes, and computing the volume of the intersection as a convex hull. Recall that our post-processing recovers 3D bounding boxes up to a scale. Although the scale is not necessary in our applications, it is needed in evaluation. We reuse the detected planes in AR session data to determine the metric scale of our estimations. Please refer to the supplementary material for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Ablation Studies</head><p>We conduct ablation studies on decoder scale, shape supervision, and dataset. <ref type="table">Table 2</ref> shows the study on decoder scale of the MobilePose-Base. To compare different scales, we build the decoder as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, and output at different levels. The detection and regression heads are also in the same experimenting scale. The result of this study shows that the MobilePose-Base achieves the best accuracy at scale 40 × 30, in terms of AP at 0.5 3D IoU. As the model has more layers with high-resolution features, its accuracy drops. This motivates us to look for shape supervision with pixel-level signals.</p><p>To study the contributions of our datasets and network components, we compare different configurations and document the results in <ref type="table">Table 3</ref>. In this experiment, we compare four network configurations: MobilePose-Base (Base), MobilePose-Shape without shape supervision (Shape-No), MobilePose-Shape with coordinate map (CM) in shape supervision (Shape-CM), and the MobilePose-Shape with full shape supervision (Shape-Full). The configurations are evaluated by training on three data combinations by AP at 0.5 3D IoU. On the real dataset, we observe that the MoblePose-Shape is no better than the MobilePose-Base. This is consistent with our experiment on decoder scales, that high-resolution features without supervision are not helpful in pose estimation. Trained on the real dataset and synthetic-3D dataset, the MobilePose-Shape with coordinate map as supervision has better performance than other configurations. This proves that shape supervision can improve pose estimation. We also notice that with full (segmentation + coordinate map) supervision, the accuracy goes down. This indicates that segmentation is a weak feature, which is redundant when coupling with coordinate map.</p><p>Finally, trained on the all three datasets, the MobilePose-Shape has the best performance with full supervision. Although synthetic-2D dataset only has noisy segmentation label, the network is able to evolve with this weak and noisy supervision. Segmentation makes a positive contribution when there are data with even noisy supervision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Results</head><p>In <ref type="figure" target="#fig_5">Fig. 5</ref>, we show some results from our real evaluation dataset. To visualize the 3D bounding boxes, we reproject them to the 2D image plane. We also show the segmentation and coordinate map predictions from our model. We would like to remind readers that both segmentation and coordinate map are learned purely from synthetic data. Coordinate map only has 50 scanned shoes for supervision, while segmentation has very noisy labels. Recall that we argue learning accurate shape is more difficult than pose. We show that our model can infer accurate poses from weakly learned coarse shape features. Meanwhile, we show that our model also predicts reasonably well segmentation masks by transfer learning from synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Comparisons</head><p>Shoe Dataset. We compare our method with three other methods on our shoe dataset. Two of them are state-of-the-art single-shot solutions: YOLO-6D <ref type="bibr" target="#b26">[27]</ref> and YOLO-Seg <ref type="bibr" target="#b7">[8]</ref>. Both of the two methods use YOLOv2 <ref type="bibr" target="#b21">[22]</ref> as their backbones. The YOLO-6D predicts object's class and a confidence value at each grid cell, which are used for detecting the object. The YOLO-Seg uses a semantic segmentation branch for detection, parallel to its regression branch. Besides the two previous methods, we also compare with the MobileNetv2 <ref type="bibr" target="#b23">[24]</ref> attached with our decoder and heads. Recall that our encoder removes several blocks with large number of channels with nearly half parameters from MobileNetv2. This is to verify the effect of this optimization. For all the methods, we follow the implementation details and uploaded code if there is any. As shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, our MobilePose-Shape has the best AP at 0.5 3D IoU. We also compare their model sizes and speeds on a smartphone (Galaxy S20 with Adreno 650 GPU). We benchmark models by running inference on mobile GPUs using TFLite with GPU delegate. Our MobilePose-Base runs at 36 FPS with the smallest model size (16MB), and MobilePose-Shape runs at 26.5 FPS with a slightly larger model size (18MB) and 10% higher AP. Interestingly, using only half of the model size, our MobilePose-Shape is comparable with the MobileNetv2 plus our decoder and heads. This indicates that by introducing high-resolution features with even a weak supervision, low-resolutions features can be compressed by using a shallower and thinner network. Finally, comparing with the two previous models, ours are about 2 ∼ 3% in model size or number of parameters, and 3 ∼ 12 times in FPS.</p><p>Public Datasets. We also compare the methods on two popular public datasets: Linemod <ref type="bibr" target="#b5">[6]</ref> and Occlusion <ref type="bibr" target="#b0">[1]</ref>. We adopt the standard metrics of reprejection error (REP-5px) and average distance (ADD-0.1d), same as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8]</ref>. The results are shown in <ref type="table">Table 4</ref> and 5, where superscript * indicates the object is symmetric. We use our MobilePose-Base for the two experiments, because objects in the two datasets are relatively small. Without increasing model size with higher resolution, or leveraging detection-aware cropping, shape features do not provide sufficient supervision. Since our model is designed for unseen objects from a single category, we trained a model for each object category. Comparing with YOLO-Seg <ref type="bibr" target="#b7">[8]</ref> that uses a multi-object model for Occlusion, ours has much higher accuracy, and the total model size is still smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we address the problem of pose estimation from two different angles that have not been explored before. First, we do not assume any prior knowledge of the unseen objects. Second, our MobilePose models are ultra lightweight that can run on mobile devices in real-time. Additionally, we reveal that pixel-level shape supervision can guide the network to learn poses from high-resolution features. With a particular interest, we demonstrate our models on various unseen shoes through a mobile application. Furthermore, the proposed method can be easily extended to other object categories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>MobilePose-Base network. The blue and purple boxes are convolutional and deconvolutional blocks, respectively. Orange boxes represent inverted residual blocks from MobileNetv2<ref type="bibr" target="#b23">[24]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Detection methods (from left to right): anchor, segmentation, and distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>MobilePose-Shape network with shape prediction at an intermediate layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of our synthetic-3D (left three columns), synthetic-2D (right three columns), and their shape labels (bottom row). The last two examples are considered with mild and severe label errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Example results on our real evaluation data. We show the reprojected bounding boxes of detected shoes, as well as the segmentation masks and coordinate maps learned from weak supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Comparison of state-of-the-art single-shot methods: AP vs 3D IoU (left), and FPS vs model size (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets with their labels and sizes.</figDesc><table><row><cell>Dataset</cell><cell>Pose</cell><cell cols="4">Segmentation Coordinate Map Size # of Objects</cell></row><row><cell>Real</cell><cell>annotation</cell><cell>N/A</cell><cell>N/A</cell><cell>100K</cell><cell>1500</cell></row><row><cell cols="2">Synthetic-3D accurate</cell><cell>accurate</cell><cell>accurate</cell><cell>80K</cell><cell>50</cell></row><row><cell>Synthetic-2D</cell><cell>N/A</cell><cell>noisy</cell><cell>N/A</cell><cell>50K</cell><cell>75K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Study of decoder scales on AP@0.5IoU. Study of datasets and network configurations on AP@0.5IoU.</figDesc><table><row><cell>Scale</cell><cell cols="5">20 × 15 40 × 30 80 × 60 160 × 120</cell></row><row><cell>AP@0.5IoU</cell><cell>0.5026</cell><cell cols="2">0.5343</cell><cell cols="2">0.5174</cell><cell>0.4916</cell></row><row><cell>Dataset</cell><cell cols="2">Base</cell><cell cols="3">Shape-No Shape-CM Shape-Full</cell></row><row><cell>Real</cell><cell cols="2">0.5343</cell><cell cols="2">0.5102</cell></row><row><cell>Real + Syn3D</cell><cell cols="2">0.5541</cell><cell cols="2">0.5378</cell><cell>0.5793</cell><cell>0.5554</cell></row><row><cell cols="2">Real + Syn3D + Syn2D</cell><cell></cell><cell></cell><cell></cell><cell>0.5434</cell><cell>0.5939</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>MetricMethod Ape Can Cat Driller Duck Eggbox * Glue * Holep.REP-5px YOLO-6D 92.10 97.44 97.41 79.41 94.65 90.33 96.53 92.86 MobilePose 98.92 95.56 99.44 85.47 97.86 99.47 95.63 97.85 ADD-0.1d YOLO-6D 21.62 68.80 41.82 63.51 27.23 69.58 80.02 42.63 MobilePose 42.70 72.78 50.28 68.16 39.57 91.98 93.44 56.45 Table 4. Comparison with YOLO-6D [27] on Linemod dataset. Metric Method Ape Can Cat Driller Duck Eggbox * Glue * Holep. MobilePose 29.0 56.0 33.5 70.3 25.6 55.2 58.5 48.1 Comparison with YOLO-Seg [8] on Occlusion dataset.</figDesc><table><row><cell>REP-5px</cell><cell cols="3">YOLO-Seg 59.1 59.8 46.9 59.0 42.6 MobilePose 95.9 87.9 89.8 84.1 86.0 72.1 54.6 88.0 11.9 16.5 63.6</cell></row><row><cell>ADD-0.1d</cell><cell>YOLO-Seg 12.1 39.9 8.2 45.2 17.2</cell><cell>22.1</cell><cell>35.8 36.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning 6d object pose estimation using 3d object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How do neural networks see depth in single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Croon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object as distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fridman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CenterNet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation-driven 6D object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hugonot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SSD-6D: Making RGBbased 3D detection and 6D pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning of local RGB-D patches for 3D object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">EPnP: An accurate O(N) solution to the PnP problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified framework for multi-view multi-class object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeepIM: Deep iterative matching for 6D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cdpn: Coordinates-based disentangled pose network for real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mediapipe: A framework for building perception pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lugaresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcclanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Uboweja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Georg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<idno>abs/1906.08172</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6DoF pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<title level="m">Yolo9000: Better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01869</idno>
		<title level="m">Hybridpose: 6d object pose estimation under hybrid representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicit 3D orientation learning for 6D object detection from RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time seamless single shot 6D object pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DenseFusion: 6D object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martn-Martn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6D object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DPOD: 6D pose object detector and refiner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno>abs/1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
