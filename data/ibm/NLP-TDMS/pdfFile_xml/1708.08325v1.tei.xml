<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
							<email>oberweger@icg.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
							<email>lepetit@icg.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computer Graphics and Vision</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratoire Bordelais de Recherche en Informatique</orgName>
								<orgName type="institution">Université de Bordeaux</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>DeepPrior <ref type="bibr" target="#b16">[18]</ref> is a simple approach based on Deep Learning that predicts the joint 3D locations of a hand given a depth map. Since its publication early 2015, it has been outperformed by several impressive works. Here we show that with simple improvements: adding ResNet layers, data augmentation, and better initial hand localization, we achieve better or similar performance than more sophisticated recent methods on the three main benchmarks (NYU, ICVL, MSRA) while keeping the simplicity of the original method. Our new implementation is available at https: //github.com/moberweger/deep-prior-pp.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate hand pose estimation is an important requirement for many Human Computer Interaction or Augmented Reality tasks, and has attracted lots of attention in the Computer Vision research community <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b41">43]</ref>. Even with 3D sensors such as structured-light or timeof-flight sensors, it is still very challenging, as the hand has many degrees of freedom, and exhibits self-similarity and self-occlusions in images.</p><p>One popular method for 3D hand pose estimation is DeepPrior, introduced by <ref type="bibr" target="#b16">[18]</ref>. DeepPrior is a Deep Network-based approach that uses a single depth image as input and directly predicts the 3D joint locations of the hand skeleton. The key idea in DeepPrior is to explicitly integrate a prior on 3D hand poses computed by Principal Component Analysis (PCA) directly into a Convolutional Neural Network. This offers a simple, yet accurate and fast method for 3D hand pose estimation.</p><p>Since the publication of the original paper, there has been tremendous advances in the field of Machine Learning and Deep Neural Networks. We leverage recent progress in this field and update the original approach. Thus we call the resulting approach DeepPrior++. Specifically:</p><p>• we updated the model architecture to make the model more powerful by introducing a Residual Network <ref type="bibr" target="#b5">[7]</ref> for extracting feature maps;</p><p>• we improved the initial hand localization method. This step in DeepPrior was based on a heuristics. Here we use a trained method;</p><p>• we improved the training procedure to leverage more information from the available data.</p><p>We released the code with our improvements at https:// github.com/moberweger/deep-prior-pp with the hope that it will be useful for the community.</p><p>In the following, we shortly review the original Deep-Prior approach in Section 3, then introduce our modifications in Section 4. The modifications are evaluated in Section 5 with a comparison to state-of-the-art methods on public benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is a significant amount of early work that deals with hand pose estimation, and we refer to <ref type="bibr" target="#b1">[3]</ref> for an overview. In 2015, an evaluation of several works on benchmark datasets <ref type="bibr" target="#b28">[30]</ref> has shown that DeepPrior performed state-of-the-art in terms of accuracy and speed. There have been tremendous advances since then, and here we shortly review related works. We compare against all the works that report results using the commonly used error metrics on at least one of the three major benchmark datasets, i.e. NYU <ref type="bibr" target="#b38">[40]</ref>, MSRA <ref type="bibr" target="#b26">[28]</ref>, and ICVL <ref type="bibr" target="#b32">[34]</ref>. These works are marked in this section with a star * .</p><p>Many recent approaches exploit the hierarchy of the hand kinematic tree. <ref type="bibr" target="#b33">[35]</ref> * proceeds along the skeleton tree and predicts the positions of the child joints within the tree. Similarly, <ref type="bibr" target="#b42">[44]</ref> (Lie-X) * predicts updates along the skeleton tree that correct an initial pose and use Lie-algebra to constrain these updates. Sun et al. <ref type="bibr" target="#b26">[28]</ref> (HPR) * estimate the joint locations in normalized coordinate frames for each finger, and <ref type="bibr" target="#b23">[25]</ref> uses a separate regressor for each finger to predict spatial and temporal features that are combined in a nearest-neighbor formulation. <ref type="bibr" target="#b44">[46]</ref> introduces a spatial attention mechanism that specializes on each joint and an additional optimization step to enforce kinematic constraints. <ref type="bibr" target="#b12">[14]</ref> splits the hand into smaller sub-regions along the kinematic tree. <ref type="bibr" target="#b43">[45]</ref> * predicts a gesture class for each pose and trains a separate pose regressor for each class. All these approaches require multiple predictors, one for each joint or finger, and often additional regressors for different iterations of the algorithms. Thus the number of regression models ranges from tens to more than 50 different models that have to be trained and evaluated.</p><p>To overcome this shortcoming, there are several works that integrate the kinematic hierarchy into a single CNN structure. Guo et al. <ref type="bibr" target="#b4">[6]</ref> (REN) * train an ensemble of subnetworks for different spatial regions of input features, and Madadi et al. <ref type="bibr" target="#b13">[15]</ref> * use a tree-shaped CNN architecture that predicts different parts of the kinematic tree. However, this requires a specifically designed CNN architecture depending on the annotation.</p><p>Different data representations of the input depth image were also proposed. Deng et al. <ref type="bibr" target="#b0">[2]</ref> (Hand3D) * convert the depth image to a 3D volume and use a 3D CNN to predict joint locations. However, 3D networks show a low computational efficiency <ref type="bibr" target="#b21">[23]</ref>. Differently, <ref type="bibr" target="#b40">[42]</ref> * uses surface normals instead of the depth image, but surface normals are not readily accessible from current depth sensors and thus introduce an additional computational overhead. Neverova et al. <ref type="bibr" target="#b15">[17]</ref> * combine a segmentation of the hand parts with a regression of joint locations, but the segmentation is sensitive to the sensor noise.</p><p>Instead of predicting the 3D joint locations directly, <ref type="bibr" target="#b38">[40]</ref> * proposed an approach to predict 2D heatmaps for the different joints. <ref type="bibr" target="#b3">[5]</ref> * extended this work and use multiple CNNs to predict heatmaps from different reprojections of the depth image, which requires a separate CNN for each reprojection. Also, these approaches require complex postprocessing to fit a kinematic model to the heatmaps.</p><p>A probabilistic framework was proposed by Bouchacourt et al.</p><p>[1] (DISCO) * , who use a network to learn the posterior distribution of hand poses and one can sample from this distribution. However, it is unclear how to combine these samples in practice. Wan et al. <ref type="bibr" target="#b39">[41]</ref> (Crossing Nets) * use two generative networks, one for the hand pose and one for the depth image, and learn a shared mapping between these two networks, which involves training several networks in a complex procedure.</p><p>Oberweger et al. <ref type="bibr" target="#b17">[19]</ref> (Feedback) * learn a CNN to synthesize depth image of a hand and use the synthesized depth image to predict updates for an initial hand pose. Again, this requires training three different networks.</p><p>Zhou et al. <ref type="bibr" target="#b46">[48]</ref> (DeepModel) * integrate a hand model into a CNN, by introducing an additional layer that enforces the physical constraints of a 3D hand model, where the constraints have to be manually defined beforehand.</p><p>Fourure et al. <ref type="bibr" target="#b2">[4]</ref> (JTSC) * exploit different annotations from different datasets by introducing a shared representation, which is an interesting idea for harvesting more training samples, but has shortcomings when dealing with sensor characteristics.</p><p>Zhang et al. <ref type="bibr" target="#b45">[47]</ref> * formulate pose estimation as a multivariate regression problem that, however, requires solving a complex optimization problem during runtime.</p><p>There are also generative model-based approaches that recently raised much attention. Although being very accurate, the works of [12] * , <ref type="bibr" target="#b24">[26]</ref>, <ref type="bibr" target="#b30">[32]</ref> * , <ref type="bibr" target="#b35">[37]</ref> * require a 3D model of the hand, which should be adjusted to the users' hand <ref type="bibr" target="#b24">[26]</ref>, <ref type="bibr" target="#b31">[33]</ref> * , and run a complex optimization during inference.</p><p>Comparing to these recent approaches, our method is easier and faster to train, has a simpler architecture, is more accurate, and runs at a comparable speed, i.e. realtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Original DeepPrior</head><p>In this section, we briefly review the original DeepPrior method. More details can be found in <ref type="bibr" target="#b16">[18]</ref>.</p><p>DeepPrior aims at estimating the 3D hand joint locations from a single depth image. It requires a set of depth images labeled with the 3D joint locations for training. To simplify the regression task, DeepPrior first performs a 3D detection of the hand. It then estimates a coarse 3D bounding box containing the hand. Following <ref type="bibr" target="#b32">[34]</ref>, Deep-Prior assumes the hand is the closest object to the camera, and extracts a fixed-size cube centered on the center of mass of this object from the depth map. It then resizes the extracted cube to a 128×128 patch of depth values normalized to [−1, 1].</p><p>Points for which the depth is not available-which may happen with structured light sensors for example-or the depth values are farther than the back face of the cube, are assigned a depth of 1. This normalization is important for the learning stage in order to be invariant to different distances from the hand to the camera.</p><p>Given the physical constraints over the hand, there are strong correlation between the different 3D joint locations. Instead of directly predicting the 3D joint locations, Deep-Prior therefore predicts the parameters of the pose in a lower dimensional space. As this enforces constraints of the hand pose, this improves the reliability of the predictions. <ref type="figure" target="#fig_0">Figure 1</ref>, DeepPrior implements the pose prior into the network structure by initializing the weights of the last layer with the major components from a PCA of the 3D hand pose data. Then, the full network is trained using standard back-propagation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DeepPrior++</head><p>In this section, we describe our changes to enhance the original DeepPrior approach, which includes improved training data augmentation, better hand localization, and a more powerful network architecture. For implementation level details we refer to the code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Improved Training Data Augmentation</head><p>Since our approach is data-driven, we aim at leveraging as much information as possible from the available data. There have been many different augmentation methods used in literature <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b38">40]</ref>, such as scaling, flipping, mirroring, rotating, etc. In this work we use depth images, which give rise to specific data augmentation methods. Specifically, we use rotation, scaling, and translation, as well as different combinations of them.</p><p>Rotation: The hand can be rotated easily around the forearm. This rotation can be approximated by simple in-plane rotation of the depth images. We use random in-plane rotations of the image, and change the 3D annotations accordingly by projecting the 3D annotations onto the 2D image, applying the same in-plane rotation, and projecting the 2D annotations back to 3D coordinates. The rotation angle is sampled from a uniform distribution with the interval</p><formula xml:id="formula_0">[−180 • , 180 • ].</formula><p>Scaling: The MSRA <ref type="bibr" target="#b26">[28]</ref> and NYU <ref type="bibr" target="#b38">[40]</ref> datasets contain different persons, with different hand size and shape. Although DeepPrior is not explicitly invariant to scale, we can train the network to be invariant to hand size by varying the size of the crop in the training data. Therefore, we scale the 3D bounding box for the crop from the depth image by a random factor sampled from a normal distribution with mean of 1 and variance of 0.02. This changes the appearance of the hand size in the cropped cube, and we scale the 3D joint locations according to the random factor.</p><p>Translation: Since the hand 3D localization is not perfect, we augment the training set by adding random 3D offsets to the hand 3D location, and center the crops from the depth images on these 3D locations. We sample the random offsets from a normal distribution with a variance of 5mm, which is comparable to the error of the hand 3D detector we use. We also modify the 3D annotations according to this offset.</p><p>Online Augmentation: The augmentation is performed online during training and thus the network sees different samples at each epoch. This leads to more than 10M different samples in total. The augmentation helps to prevent overfitting and to be more robust to deviations of the hand from the training set. Although the samples are correlated, it significantly helps at test time, as we show in the experiments.</p><p>Robust Prior: Similarly, we also improve the prior, which is obtained by applying PCA to the 3D hand poses. We sample 1M poses, by randomly using rotation, scaling, and translation of the original poses in 3D. We use this augmented set of 3D poses for calculating the prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Refined Hand Localization</head><p>The original DeepPrior used a very simple hand detection. It was based on the center of mass of the depth segmentation of the hand. Therefore, the hand was segmented using depth-thresholding, and the 3D center of mass was calculated. Then, a 3D bounding box was extracted around the center of mass. DeepPrior++ still uses this method but introduces a refinement step that significantly improves the final accuracy. This refinement step relies on a regression CNN. This CNN is applied to the 3D bounding box centered on the center of mass, and is trained to predict the location of the Metacarpophalangeal (MCP) joint of the middle finger, which we use as referential. We also use augmented training data to train this CNN as described in Section 4.1.</p><p>For real-time applications, instead of extracting the center of mass from each frame, we apply this regression CNN to the hand location of the previous frame. This remains accurate while being faster in practice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">More Powerful Network Architecture</head><p>Residual Networks. Since the introduction of DeepPrior, there has been much research on better deep architectures <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b29">31]</ref>, and the Residual Network (ResNet) architecture <ref type="bibr" target="#b6">[8]</ref> appears to be one of the best performing models. Our model is similar to the 50-layer ResNet model of <ref type="bibr" target="#b6">[8]</ref>. Since ResNet was originally proposed for image classification, we adapt the architecture to fit our regression problem. Most importantly, we remove the global average pooling, and add two fully-connected layers. The input to the network is 128 × 128 pixel, with values normalized to [−1, 1]. The adapted ResNet model is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The network contains an initial convolution layer with 64 filters and 2 × 2 max-pooling. This convolutional layer is followed by four residual modules, each with a stride of 2 × 2, and with {64, 128, 256, 256} filters. The much simpler model used for refining the hand localization is shown in <ref type="figure">Fig. 3</ref>. It consists of three convolutional layers with max-pooling, and two fully-connected layers with Dropout.</p><p>We optimize the network parameters using the gradient descent algorithm ADAM <ref type="bibr" target="#b9">[11]</ref> with standard hyperparameters and a learning rate of 0.0001, and train for 100 epochs.</p><p>Regularization using Dropout. The ResNet model can overfit, and we experienced this behavior especially on datasets with small hand pose variation <ref type="bibr" target="#b32">[34]</ref>. Therefore, we introduce Dropout <ref type="bibr" target="#b25">[27]</ref> to the model, which was shown to provide an effective way of regularizing a neural network. We apply binary Dropout with a dropout rate of 0.3 on both fully-connected layers after the residual modules. This enables training high capacity ResNet models while avoiding overfitting and achieving highly accurate predictions.  <ref type="figure">Figure 3</ref>: The network architecture used for refining hand localization. As in <ref type="figure" target="#fig_1">Fig. 2</ref>, C denotes a convolutional layer, FC a fully-connected layer, D a Dropout layer, and P a maxpooling layer. The initial hand crop from the depth image is fed to the network that predicts an offset to correct an inaccurate hand localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>We evaluate our DeepPrior++ approach on three public benchmark datasets for hand pose estimation: the NYU dataset <ref type="bibr" target="#b38">[40]</ref>, the ICVL dataset <ref type="bibr" target="#b32">[34]</ref>, and the MSRA dataset <ref type="bibr" target="#b26">[28]</ref>. For the comparison with other methods, we focus here on works that were published after the original DeepPrior paper. There are different evaluation metrics used in the literature for hand pose estimation, and we report the numbers stated in the papers or measured from the graphs if provided, and/or plot the relevant graphs for comparison.</p><p>For all experiments, we report the results for a 30dimensional PCA prior. By using an efficient implementation for data augmentation, the training time is the same for all experiments, approximately 10 hours on a computer with an Intel i7 with 3.2GHz and 64GB of RAM, and an nVidia GTX 980 Ti graphics card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Metrics</head><p>We use two different metrics to evaluate the accuracy:</p><p>• First, we evaluate the accuracy of the 3D hand pose estimation as average 3D joint error. This is established as the most commonly used metric in literature, and allows comparison with many other works due to simplicity of evaluation.</p><p>• As a second, more challenging metric, we plot the fraction of frames where all predicted joints are below a given maximum Euclidean distance from the ground truth <ref type="bibr" target="#b36">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">NYU Dataset</head><p>The NYU dataset <ref type="bibr" target="#b38">[40]</ref> contains over 72k training and 8k test frames of multi-view RGB-D data. The dataset was captured using a structured light-based sensor. Thus, the depth maps show missing values as well as noisy outlines, which makes the dataset very challenging. For our experiments we use only the depth data from a single camera. The dataset has accurate annotations and exhibits a high variability of different poses. The training set contains samples from a single user and the test set samples from two different users. We follow the established evaluation protocol <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b38">40]</ref> and use the 14 joints for calculating the metrics.</p><p>Our results are shown in <ref type="table">Table 1</ref> together with a comparison to current state-of-the-art methods. We compare DeepPrior++ to several related methods, and it significantly outperforms the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average <ref type="formula">3D</ref>   <ref type="bibr" target="#b38">[40]</ref>. We report the average 3D error in mm. Deep-Prior++ significantly performs better than all other methods for this dataset.</p><p>In <ref type="figure">Figure 4</ref> we compare our method with other discriminative approaches. Although Supancic et al. <ref type="bibr" target="#b28">[30]</ref> report a very accurate results for a fraction of the frames, our approach significantly performs better for the majority of the frames.</p><p>In <ref type="figure">Figure 5</ref> we compare state-of-the-art methods using a different evaluation protocol, i.e. we follow the protocol of <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b35">37]</ref>, who evaluate the first 2400 frames of the test set. Also for this protocol, we significantly outperform the stateof-the-art method of Taylor et al. <ref type="bibr" target="#b35">[37]</ref>. Note that <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b35">37]</ref> require a, possibly user-specific, 3D hand model, whereas our method only uses training data without any 3D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">ICVL Dataset</head><p>The ICVL dataset <ref type="bibr" target="#b32">[34]</ref>  Zhou et al. <ref type="bibr" target="#b46">[48]</ref> Guo et al. <ref type="bibr" target="#b4">[6]</ref> Xu et al. <ref type="bibr" target="#b42">[44]</ref> Tompson et al. <ref type="bibr" target="#b38">[40]</ref> Supancic et al. <ref type="bibr" target="#b28">[30]</ref> Figure 4: Comparison with state-of-the-art discriminative methods on the NYU dataset <ref type="bibr" target="#b38">[40]</ref>. We plot the fraction of frames where all joints are within a maximum distance from the ground truth. A larger area under the curve indicates better results. Our proposed approach performs best among other discriminative methods. (Best viewed in color) Tang et al. <ref type="bibr" target="#b33">[35]</ref> Tan et al. <ref type="bibr" target="#b31">[33]</ref> Tompson et al. <ref type="bibr" target="#b38">[40]</ref> Taylor et al. <ref type="bibr" target="#b35">[37]</ref> Tagliasacchi et al. <ref type="bibr" target="#b30">[32]</ref> Figure 5: Comparison with state-of-the-art model-based methods on the NYU dataset <ref type="bibr" target="#b38">[40]</ref>. We plot the fraction of frames where the average joint error per frame is within a maximum distance from the ground truth, following the protocol of <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b35">37]</ref>. A larger area under the curve indicates better results. Our proposed approach even outperforms model-based approaches on this dataset, with more than 90% of the frames with an error smaller than 10mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Best viewed in color)</head><p>and sharp outlines with little noise. Although the authors provide different artificially rotated training samples, we start from the genuine 22k frames only and apply the data augmentation as described in Section 4.1. However, the pose variability of this dataset is limited compared to other datasets <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b38">40]</ref>, and annotations are rather inaccurate as discussed in <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref>. We show a comparison to different state-of-the-art methods in <ref type="table">Table 2</ref>. Again, our method shows state-of-the-art accuracy. However, the gap to other methods is much smaller. This may be attributed to the fact that the dataset is much easier, with smaller pose variations <ref type="bibr" target="#b28">[30]</ref>, and due to errors in the annotations for the evaluation <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average <ref type="formula">3D</ref>   <ref type="table">Table 2</ref>: Comparison with state-of-the-art on the ICVL dataset <ref type="bibr" target="#b32">[34]</ref>. We report the average 3D error in mm.</p><p>In <ref type="figure">Figure 6</ref> we compare DeepPrior++ to other methods on the ICVL dataset <ref type="bibr" target="#b32">[34]</ref>. Our approach performs similar to the works of Guo et al. <ref type="bibr" target="#b4">[6]</ref>, Wan et al. <ref type="bibr" target="#b40">[42]</ref>, and Tang et al. <ref type="bibr" target="#b33">[35]</ref>, all achieving state-of-the-art accuracy on this dataset. This might be an indication that the performance on the dataset is saturating, and the remaining error is due to the annotation uncertainty. This empirical finding is similar to the discussion in <ref type="bibr" target="#b28">[30]</ref>. Although Tang et al. <ref type="bibr" target="#b33">[35]</ref> performs slightly better in some parts of the curve in <ref type="figure">Figure 6</ref>, our approach performs significantly better on the NYU dataset, as shown in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">MSRA Dataset</head><p>The MSRA dataset <ref type="bibr" target="#b26">[28]</ref> contains about 76k depth frames. It was captured using a time-of-flight camera. The dataset comprises sequences from 9 different subjects. We follow the common evaluation protocol <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b39">41]</ref> and perform a leave-one-out cross-validation: We train on 8 different subjects and evaluate on the remaining subject. We repeat this procedure for each subject and report the average errors over the different runs.</p><p>A comparison of the average 3D error is shown in <ref type="table">Table 3</ref>. Again, DeepPrior++ outperforms the existing methods by a large margin of 3mm. In <ref type="figure">Figure 7</ref>, DeepPrior++ also outperforms all other methods on the plotted metric, Oberweger et al. <ref type="bibr" target="#b16">[18]</ref> Tang et al. <ref type="bibr" target="#b32">[34]</ref> Tang et al. <ref type="bibr" target="#b33">[35]</ref> Zhou et al. <ref type="bibr" target="#b46">[48]</ref> Guo et al. <ref type="bibr" target="#b4">[6]</ref> Figure 6: Comparison with state-of-the-art on the ICVL dataset <ref type="bibr" target="#b32">[34]</ref>. We plot the fraction of frames where all joints are within a maximum distance from the ground truth. Several works show a similar error curve, which can be an indicator for saturating performance for this dataset. (Best viewed in color) which shows that it is also able to handle different users' hands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average 3D error</p><p>Ge et al. <ref type="bibr" target="#b3">[5]</ref> 13.2mm Sun et al. <ref type="bibr" target="#b26">[28]</ref> (HPR) <ref type="bibr" target="#b13">15</ref>.2mm Wan et al. <ref type="bibr" target="#b39">[41]</ref>  <ref type="table">(CrossingNets)</ref> 12.2mm Yang et al. <ref type="bibr" target="#b43">[45]</ref> 13.7mm This work (DeepPrior++) 9.5mm <ref type="table">Table 3</ref>: Comparison with state-of-the-art on the MSRA dataset <ref type="bibr" target="#b26">[28]</ref>. We report the average 3D error in mm. Deep-Prior++ significantly performs better than all other methods for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Experiments</head><p>We performed additional experiments to show the contributions of our modifications. We evaluate the modifications on the NYU dataset <ref type="bibr" target="#b38">[40]</ref>, since it has the most accurate annotations, with diverse poses, and two different users for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Training Data Augmentation</head><p>In order to evaluate the contribution of the training procedure, we tested the different data augmentation schemes. Wan et al. <ref type="bibr" target="#b39">[41]</ref> Sun et al. <ref type="bibr" target="#b26">[28]</ref> Figure 7: Comparison with state-of-the-art on the MSRA dataset <ref type="bibr" target="#b26">[28]</ref>. We plot the fraction of frames where all joints are within a maximum distance from the ground truth. A larger area under the curve indicates better results. Our approach significantly outperforms current state-of-the-art discriminative approaches on this dataset. (Best viewed in color)</p><p>The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. Using data augmentation results in an increase in accuracy over 7mm. Most importantly, augmenting the hand translation accounts for errors in the hand detection part, and augmenting the rotation accounts for rotated hand poses, thus effectively enlarging the training poses. Although augmenting the scale only does not help as much as augmenting translation or rotation on the NYU dataset, it can help in cases where the size of the users' hand is not accurately determined, i.e. a new user in a practical application. Interestingly, computing the prior from augmented the 3D hand poses is very important as well. If the data is augmented, but the prior is computed from the original 3D hand poses, the accuracy is worse compared to no data augmentation, since the prior is not expressive enough to capture the variances of the augmented hand poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Hand Localization</head><p>Further, we evaluate the influence of the hand localization on the final 3D joint error. For this experiment, we use the ResNet architecture and all data augmentation. The results are shown in <ref type="table">Table 5</ref>. The highest accuracy can be achieved using the ground truth location of the hand, which is not feasible in practice, since real detectors do not provide perfect hand localization. This indicates, that there is still room for   <ref type="table">Table 5</ref>: Impact of hand localization accuracy on NYU dataset <ref type="bibr" target="#b38">[40]</ref>. The ground truth localization gives the lowest 3D pose error, but this localization is not applicable in practice. Our refinement of the commonly used center of mass localization (CoM) improves the accuracy by over 1mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Network Architecture</head><p>We evaluate the impact of the different network architectures in <ref type="table">Table 6</ref>. We use the refined hand localization and all data augmentation for training both networks. The improved training procedure and better localization already improve the results for the original architecture by more than 3mm (19.8mm from <ref type="bibr" target="#b16">[18]</ref>). Using the proposed ResNet architecture, the accuracy can be improved by another 4mm on average, due to the higher capacity of the model. We also evaluated the original architecture, but changed the convolutional layers such that they use the same number of filters as the ResNet architecture, but this architecture is still inferior to the ResNet.</p><p>DeepPrior <ref type="bibr" target="#b16">[18]</ref> DeepPrior++ <ref type="figure">Figure 8</ref>: Qualitative comparison between DeepPrior and DeepPrior++ on the NYU dataset <ref type="bibr" target="#b38">[40]</ref>. We show the inferred 3D joint locations projected on the depth images. Ground truth is shown in blue, the predicted poses in red. The results provided by DeepPrior++ are significantly better than the results from the original DeepPrior, especially on complex poses. (Best viewed in color)</p><p>The ResNet architecture is slower than the original implementation, however it is still able to run at over 30fps on a single GPU, making it applicable to realtime applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>Average 3D error fps</p><p>Original <ref type="bibr" target="#b16">[18]</ref> 16.6mm 100 Original with more filters 13.7mm 80 ResNet 12.3mm 30 <ref type="table">Table 6</ref>: Impact of network architecture on the NYU dataset <ref type="bibr" target="#b38">[40]</ref>. The more recent ResNet architecture performs significantly better than the original network architecture, even when using the same number of filters as ResNet for the Original architecture (Original with more filters). Most importantly, we can still maintain realtime performance with 30fps in our hand tracking application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Qualitative Evaluation</head><p>We show several qualitative results in <ref type="figure">Figure 8</ref>, where we compare to the original DeepPrior <ref type="bibr" target="#b16">[18]</ref>. In general, Deep-Prior++ provides significantly better results compared to the original DeepPrior, especially on highly articulated poses. This can be attributed to the data augmentation and better localization, but also to the more powerful CNN structure, which enables the CNN to learn highly accurate poses for complex articulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>Since the publication of DeepPrior, other works on pose estimation introduced a pose prior in a Deep Learning framework, showing the importance of such prior:</p><p>• <ref type="bibr" target="#b20">[22]</ref> proposed to replace the linear transformation computed by the PCA by an encoder. This encoder is trained first, together with a decoder, to predict a compact representation of the pose. As the decoder has a more complex form, it brings some improvement in accuracy.</p><p>• <ref type="bibr" target="#b37">[39]</ref> considers human pose estimation and also uses an auto-encoder, but to compute a pose embedding of larger dimensions than the original pose, which appears to significantly improves the accuracy in the case of body pose estimation.</p><p>• <ref type="bibr" target="#b47">[49]</ref> learns a pose prior for estimating the 3D hand joint locations from 2D heatmaps by factorizing the prior into canonical coordinates and a relative motion, while our prior learned with PCA does not distinguish between the two.</p><p>Maybe a high-level conclusion of the work presented in this paper is that our community should be careful when comparing approaches: By paying attention to its different steps, we were able to make DeepPrior++ perform significantly better than the original DeepPrior and performs similarly or better than more recent works, while the key ideas are the same for the two methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The network architecture for the original Deep-Prior. C denotes a convolutional layer with the number of filters and the filter size inscribed, FC a fully-connected layer with the number of neurons, and P a max-pooling layer with the pooling size. The shown Multi-layer Network can be an arbitrary Neural Network, with an additional layer for the prior. DeepPrior introduces a pose prior by precomputing the weights of the last layer from a PCA applied to the 3D hand pose data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our ResNet architecture. C denotes a convolutional layer with the number of filters and the filter size inscribed, FC a fully-connected layer with the number of neurons, D a Dropout layer with the probability of dropping a neuron, R a residual module with the number of filters and filter size, and P a max-pooling layer with the pooling region size. The hand crop from the depth image is fed to the ResNet that predicts the final 3D hand pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>comprises a training set of over 180k depth frames showing various hand poses. The test set contains two sequences with each approximately 700 frames. The dataset is recorded using a time-of-flight camera and has 16 annotated joints. The depth images have a high quality with hardly any missing depth values,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>with mean below distance / % This work Oberweger et al.<ref type="bibr" target="#b17">[19]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effects of the new training procedure on the NYU dataset<ref type="bibr" target="#b38">[40]</ref>. By using different data augmentation methods, the accuracy can be significantly increased. In the first row we do not use any data augmentation. In the last row we apply augmentation on the training data, but not for computing the pose prior, showing the importance of having a good pose prior. improvement by using a more accurate 3D hand localization method.Starting with the very simple center of mass localization and by refining the estimated center of mass localization, this step decreases the 3D localization error by almost 20mm. This in turn improves further the final average 3D pose error by over 1mm.</figDesc><table><row><cell>Localization</cell><cell cols="2">Avg. 3D pose error Loc. 3D error</cell></row><row><cell>CoM Refined CoM Ground truth</cell><cell>13.8mm 12.3mm 10.8mm</cell><cell>28.1mm 8.6mm 0.0mm</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment: This work was partially funded by the Christian Doppler Laboratory for Semantic 3D Computer Vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>[1] D. Bouchacourt, M. P. Kumar, and S. Nowozin. DISCO Nets: Dissimilarity Coefficient Networks. In Advances in Neural Information Processing Systems, 2016.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Hand3D: Hand Pose Estimation Using 3D Neural Network. In arXiv Preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision-Based Hand Pose Estimation: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Twombly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-Task, Multi-Domain Learning: Application to Semantic Segmentation and Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fourure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tremeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">251</biblScope>
			<biblScope unit="page" from="68" to="80" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust 3D Hand Pose Estimation in Single Depth Images: from Single-View CNN to Multi-View CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Region Ensemble Network: Improving Convolutional Network for Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real Time Hand Pose Estimation Using Depth Sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kıraç</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hand Pose Estimation and Hand Shape Classification Using Multi-Layered Randomized Decision Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kıraç</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Guided Optimisation through Classification and Regression for Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krejov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="138" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D Hand Pose Estimation Using Randomized Decision Forest with Segmentation Index Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-To-End Global to Local CNN Learning for Hand Pose Recovery in Depth Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamics Based 3D Skeletal Hand Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orsten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Graphics Interface Conference</title>
		<meeting>of Graphics Interface Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hands Deep in Deep Learning for Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVWW</title>
		<meeting>of CVWW</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training a Feedback Loop for Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Full DOF Tracking of a Hand Interacting with an Object by Modeling Occlusions and Physical Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Realtime and Robust Hand Tracking from Depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Framework for Articulated Hand Pose Estimation and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SCIA</title>
		<meeting>of SCIA</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">OctNet: Learning Deep 3D Representations at High Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deephand: Robust Hand Pose Estimation by Completing a Matrix Imputed with Deep Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast and robust hand tracking using detection-guided optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oulasvirta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cascaded Hand Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Convolutional Network Cascade for Facial Point Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Depth-Based Hand Pose Estimation: Data, Methods, and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust Articulatedicp for Realtime Hand Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schrder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="101" to="114" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fits Like a Glove: Rapid and Reliable Hand Shape Personalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Real-Time Articulated Hand Pose Estimation Using Semi-Supervised Transductive Regression Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient and Precise Interactive Hand Tracking through Joint, Continuous Optimization of Pose and Correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bordeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Corish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Topalian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">143</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Vitruvian Manifold: Inferring Dense Correspondences for One-Shot Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Crossing Nets: Dual Generative Models with a Shared Latent Space for Hand Pose. Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hand Pose Estimation from Local Surface Normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient Hand Pose Estimation from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking, and Action Recognition on Lie Groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Govindarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hand Pose Regression via a Classification-Guided Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatial Attention Deep Net with Partial PSO for Hierarchical Hybrid Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multivariate Regression with Grossly Corrupted Observations: A Robust Approach and Its Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Model-Based Deep Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to Estimate 3D Hand Pose from Single RGB Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
