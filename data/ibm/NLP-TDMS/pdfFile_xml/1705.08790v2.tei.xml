<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
							<email>maxim.berman@esat.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><surname>Rannen</surname></persName>
							<email>amal.rannen@esat.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triki</forename><surname>Matthew</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Blaschko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. ESAT</orgName>
								<orgName type="department" key="dep2">Center for Processing Speech and Images KU</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted as a conference paper at CVPR 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Jaccard index, also referred to as the intersectionover-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance -which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lovász extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We consider the task of semantic image segmentation, where each pixel i of a given image has to be classified into an object class c ∈ C. Most of the deep network based segmentation methods rely on logistic regression, optimizing the cross-entropy loss <ref type="bibr" target="#b10">[11]</ref> </p><formula xml:id="formula_0">loss(f ) = − 1 p p i=1 log f i (y * i ),<label>(1)</label></formula><p>with p the number of pixels in the image or minibatch considered, y * i ∈ C the ground truth class of pixel i, f i (y * i ) the network probability estimate of the ground truth probability of pixel i, and f a vector of all network outputs f i (c). This supposes that the unnormalized scores F i (c) of the network have been mapped to probabilities through a softmax unit</p><formula xml:id="formula_1">f i (c) = e Fi(c)</formula><p>c ∈C e Fi(c ) ∀i ∈ <ref type="bibr">[1, p]</ref>, ∀c ∈ C.</p><p>(2)</p><p>Loss (1) generalizes the logistic loss and leads to smooth optimization. During testing, the decision function commonly used consists in picking the class of maximum score: the predicted class for a given pixel i isỹ i = arg max c∈C F i (c).</p><p>The measure of the cross-entropy loss on a validation set is often a poor indicator of the quality of the segmentation. A better performance measure commonly used for evaluating segmentation masks is the Jaccard index, also called the intersection-over-union (IoU) score. Given a vector of ground truth labels y * and a vector of predicted labelsỹ, the Jaccard index of class c is defined as <ref type="bibr" target="#b14">[15]</ref> J c (y * ,ỹ) = |{y * = c} ∩ {ỹ = c}| |{y * = c} ∪ {ỹ = c}| ,</p><p>which gives the ratio in [0, 1] of the intersection between the ground truth mask and the evaluated mask over their union, with the convention that 0/0 = 1. A corresponding loss function to be employed in empirical risk minimization is ∆ Jc (y * ,ỹ) = 1 − J c (y * ,ỹ).</p><p>For multilabel datasets, the Jaccard index is commonly averaged across classes, yielding the mean IoU (mIoU). We develop here a method for optimizing the performance of a discriminatively trained segmentation system with respect to the Jaccard index. We show that a piecewise linear convex surrogate to the Jaccard loss based on the Lovász extension of submodular set functions yields a consistent improvement of predicted segmentation masks as measured by the Jaccard index.</p><p>Although the Jaccard index is often computed globally, over every pixel of the evaluated segmentation dataset <ref type="bibr" target="#b7">[8]</ref>, it can also be computed independently for each image. Using the per-image Jaccard index is known to have better perceptual accuracy by reducing the bias towards large instances of the object classes in the dataset <ref type="bibr" target="#b5">[6]</ref>. Due to these favorable properties, and the empirical risk minimization principle of optimizing the loss of interest at training time <ref type="bibr" target="#b29">[29]</ref>, optimization of the Jaccard loss during training has been frequently considered in the literature. However, in contrast to the present work, existing methods all have significant shortcomings that do not allow plug-and-play application to a wide range of learning architectures.</p><p>[21] provides a Bayesian framework for optimization of the Jaccard index. The author proposes an approximate algorithm using parametric linear programming to optimize a statistical approximation to the objective. <ref type="bibr" target="#b0">[1]</ref> optimize IoU by selecting among a few candidate segmentations, instead of directly optimizing the model with respect to the loss. <ref type="bibr" target="#b2">[3]</ref> optimize the Jaccard loss in a structured output SVM, but are only able to do so with a branch-and-bound optimization over bounding boxes and not full segmentations.</p><p>Alternative approaches train binary classifiers, but on data that are sampled to capture high Jaccard index. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref> use IoU and related overlap measures to define training sets for binary classifiers in a complex multi-stage training. Such sampling-based approaches clearly induce suboptimality in the empirical risk approximation and do not lend themselves to convenient modular application in a deep learning setting.</p><p>Still other recent high-impact research has highlighted the need for optimization of the Jaccard index, but resort to binary training as a proxy, presumably for lack of a convenient and flexible method of directly optimizing the loss of interest. <ref type="bibr" target="#b19">[19]</ref> train with logistic loss and test with the Jaccard index. The paper introducing the highly influential OverFeat network specifically addresses the shortcoming in the discussion section <ref type="bibr" target="#b25">[25]</ref>: "We are using 2 loss, rather than directly optimizing the intersection-over-union (IoU) criterion on which performance is measured. Swapping the loss to this should be possible...." However, this is left to future work. In this paper, we develop the necessary plug-and-play loss layer to enable flexible direct minimization of the Jaccard loss in a deep learning setting, while demonstrating its applicability for training state-of-the-art image segmentation networks.</p><p>Our approach is based on the recent development of general strategies for generating convex surrogates to submodular loss functions, including the Lovász hinge <ref type="bibr" target="#b30">[30]</ref>. Based on the result that the Jaccard loss is submodular, this strategy is directly applicable. We moreover generalize this approach to a multiclass setting by considering a regression-based variant, using a softmax activation layer to naturally map network probability estimates to the Lovász extension of the Jaccard loss. In this work, we (i) apply the Lovász hinge with Jaccard loss to the problem of binary image segmentation (Sec. 2.1), (ii) propose a surrogate for the multi-class setting, the Lovász-Softmax loss (Sec. 2.2), (iii) design a batch-based IoU surrogate that acts as an efficient proxy to the dataset IoU measure (Sec. 3.1), (iv) analyze and compare the proper-ties of different IoU-based measures, and (v) demonstrate a substantial and consistent improvement in performance measured by the Jaccard index in state-of-the-art deep learning based segmentation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Optimization surrogates for submodular loss functions</head><p>In order to optimize the Jaccard index in a continuous optimization framework, we consider smooth extensions of this discrete loss. The extensions are based on submodular analysis of set functions, where the set function maps from a set of mispredictions to the set of real numbers <ref type="bibr" target="#b30">[30,</ref><ref type="bibr">Equation (6)</ref>].</p><p>For a segmentation outputỹ and ground truth y * , we define the set of mispredicted pixels for class c as</p><formula xml:id="formula_4">M c (y * ,ỹ) = {y * = c,ỹ = c} ∪ {y * = c,ỹ = c}. (5)</formula><p>For a fixed ground truth y * , the Jaccard loss in Eq. (4) can be rewritten as a function of the set of mispredictions</p><formula xml:id="formula_5">∆ Jc : M c ∈ {0, 1} p → |M c | |{y * = c} ∪ M c | .<label>(6)</label></formula><p>Note that for ease of notation, we naturally identify subsets of pixels with their indicator vector in the discrete hypercube {0, 1} p . In a continuous optimization setting, we want to assign a loss to any vector of errors m ∈ R p + , and not only to discrete vectors of mispredictions in {0, 1} p . A natural candidate for this loss is the convex closure of function <ref type="bibr" target="#b5">(6)</ref> in R p . In general, computing the convex closure of set functions is NP-hard. However, the Jaccard set function <ref type="bibr" target="#b5">(6)</ref> has been shown to be submodular <ref type="bibr" target="#b31">[31,</ref><ref type="bibr">Proposition 11]</ref>.</p><formula xml:id="formula_6">Definition 1 [10]. A set function ∆ : {0, 1} p → R is sub- modular if for all A, B ∈ {0, 1} p ∆(A) + ∆(B) ≥ ∆(A ∪ B) + ∆(A ∩ B).<label>(7)</label></formula><p>The convex closure of submodular set functions is tight and computable in polynomial time <ref type="bibr" target="#b20">[20]</ref>; it corresponds to its Lovász extension.</p><formula xml:id="formula_7">Definition 2 [2, Def. 3.1]. The Lovász extension of a set function ∆ : {0, 1} p → R such that ∆(0) = 0 is defined by ∆ : m ∈ R p → p i=1 m i g i (m)<label>(8)</label></formula><p>with g i (m) = ∆({π 1 , . . . , π i }) − ∆({π 1 , . . . , π i−1 }), (9) π being a permutation ordering the components of m in decreasing order, i.e. x π1 ≥ x π2 . . . ≥ x πp .</p><p>Let ∆ be a set function encoding a submodular loss such as the Jaccard loss defined in Equation <ref type="bibr" target="#b5">(6)</ref>. By submodularity ∆ is the tight convex closure of ∆ <ref type="bibr" target="#b20">[20]</ref>. ∆ is piecewise linear and interpolates the values of ∆ in R p \ {0, 1} p , while having the same values as ∆ on {0, 1} p , i.e. on any set of mispredictions (Equation <ref type="formula">(5)</ref>). Intuitively, if m is a vector of all pixel errors, ∆(m) is a sum weighting these errors according to the interpolated discrete loss. By its convexity and continuity, ∆ is a natural surrogate for the minimization of ∆ with first-order continuous optimization, such as in neural networks. The elementary operations involved to compute ∆ (sort, dot product, . . . ) are differentiable and implemented on GPU in current deep learning frameworks. The vector g(m) of which the components are defined in Equation (9) directly corresponds to the derivative of ∆ with respect to m.</p><p>In the following, we consider two different settings in which we construct surrogate losses by using the Lovász extension and specifying the vector of errors m that we use:</p><p>1. The foreground-background segmentation problem, which leads to the Lovász hinge, as described in <ref type="bibr" target="#b31">[31]</ref>;</p><p>2. The multiclass segmentation problem, which leads to the Lovász-Softmax loss, incorporating the softmax operation in the Lovász extension.  In the binary case, we consider the optimization of the Jaccard index for the foreground class ∆ J1 . We use a maxmargin classifier: for an image x, we define • y * i ∈ {−1, 1} the ground truth label of pixel i, • F i (x) the i-th element of the output scores F of the model, such that the predicted labelỹ i = sign(F i (x)),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Foreground-background segmentation</head><p>• m i = max(1 − F i (x) y * i , 0) the hinge loss associated with the prediction of pixel i.</p><p>In this setting, the vector of hinge losses m ∈ R + is the vectors of errors discussed before. With ∆ J1 the Lovász extension to ∆ J1 , the resulting loss surrogate</p><formula xml:id="formula_8">loss(F ) = ∆ J1 (m(F ))<label>(10)</label></formula><p>is the Lovász hinge applied to the Jaccard loss, as described in <ref type="bibr" target="#b30">[30]</ref>. It is piecewise linear in the output scores F as a composition of piecewise linear functions. Moreover, by choice of the hinge loss for the vector m, the Lovász hinge reduces to the standard hinge loss <ref type="bibr" target="#b28">[28]</ref> in the case of a single prediction, or when using the Hamming distance instead of the Jaccard loss as a basis for the construction. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the extension of the Jaccard loss in the case of the prediction of two pixels, illustrating the convexity and the tightness of the surrogate.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multiclass semantic segmentation</head><formula xml:id="formula_9">d i = F i (y * i ) − F i (1 − y * i ) for i = 1, 2.</formula><p>In a segmentation setting with more than two classes, we propose a surrogate based on a logistic output instead of using a max-margin setting. Specifically we map the output scores of the model to probability distributions using a softmax unit as is done traditionally in the case of the cross-entropy loss.</p><p>We use the class probabilities f i (c) ∈ [0, 1] defined in Equation <ref type="formula">(2)</ref> to construct a vector of pixel errors m(c) for class c ∈ C defined by</p><formula xml:id="formula_10">m i (c) = 1 − f i (c) if c = y * i , f i (c) otherwise.<label>(11)</label></formula><p>We use the vector of errors m(c) ∈ [0, 1] p to construct the loss surrogate to ∆ Jc , the Jaccard index for class c:</p><formula xml:id="formula_11">loss(f (c)) = ∆ Jc (m(c))<label>(12)</label></formula><p>When considering the class-averaged mIoU metric, common in semantic segmentation, we average the class-specific surrogates; hence we define the Lovász-Softmax loss as</p><formula xml:id="formula_12">loss(f ) = 1 |C| c∈C ∆ Jc (m(c))<label>(13)</label></formula><p>which is piecewise linear in f , the normalized network outputs. <ref type="figure" target="#fig_3">Figure 2</ref> show this loss as a function of the unnormalized vector outputs F for a prediction of two pixels. In the limit of large scores (confident outputs), the probability vectors at each pixel (f i (c)) c∈C are close to an indicator vector, and we recover the values of the discrete Jaccard index for the corresponding discrete labeling with respect to the ground truth, as seen on the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Optimization of intersection over union</head><p>Naïve computation of the Lovász extension (Equation <ref type="formula" target="#formula_7">(8)</ref>) applied to ∆ Jc can be achieved by sorting the elements of m in O(p log p) time and doing O(p) calls to ∆ Jc . However, if we compute ∆ Jc by Equation <ref type="formula" target="#formula_2">(3)</ref>, each call will cost O(n). As π is known in advance, we may simply keep track of the cumulative number of false positives and negatives in {π 1 , . . . , π i } for increasing i yielding an amortized O(1) cost per evaluation of ∆ Jc (cf. [31, Equation <ref type="formula" target="#formula_2">(43)]</ref>). This computation also yields the gradient g(m) at the same computational cost. This is a powerful result implying that a tight surrogate function for the Jaccard loss is available and computable in time O(p log p). The algorithm for computing the gradient of the loss surface resulting from this procedure is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Gradient of the Jaccard loss extension ∆ Jc</head><p>Inputs: vector of errors m(c) ∈ R p + class foreground pixels δ = {y * = c} ∈ {0, 1} p Output: g(m) gradient of ∆ Jc (Equation (9)) 1: π ← decreasing sort permutation for</p><formula xml:id="formula_13">m 2: δ π ← (δ πi ) i∈[1,p] 3: intersection ← sum(δ) − cumulative sum(δ π ) 4: union ← sum(δ) + cumulative sum(1 − δ π ) 5: g ← 1 − intersection/union 6: if p &gt; 1 then 7: g[2 : p] ← g[2 : p] − g[1 : p − 1] 8: end if 9: return g π −1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image-mIoU vs. dataset-mIoU</head><p>The official metric of the semantic segmentation task in Pascal VOC <ref type="bibr" target="#b6">[7]</ref> and numerous other popular competitions is the dataset-mIoU,</p><formula xml:id="formula_14">dataset-mIoU = 1 |C| c∈C J c (y * ,ỹ),<label>(14)</label></formula><p>where y * andỹ contain the ground truth and predicted labels of all pixels in the testing dataset. The Lovász-Softmax loss considers an ensemble of pixel predictions for the computation of the surrogate to the Jaccard loss. In a stochastic gradient descent setting, only a small numbers of pixel predictions are taken into account in one optimization step. Therefore, the Lovász-Softmax loss cannot directly optimize the dataset-mIoU. We can compute this loss over individual images, optimizing for the expected image-mIoU, or over each minibatch, optimizing for the expected batch-mIoU. However, it is not true in general that</p><formula xml:id="formula_15">E intersection union ≈ E(intersection) E(union) ,<label>(15)</label></formula><p>and we found in our experiments that optimizing the image-mIoU or batch-mIoU generally degrades the dataset-mIoU compared with optimizing the standard cross-entropy loss.</p><p>The main difference between the dataset and image-mIoU measures resides in the absent classes. When the network wrongly predicts a single pixel belonging to a class that is absent from an image, the image intersection over union loss corresponding to that class changes from 0 to 1. By contrast, a single pixel misprediction does not substantially affect the dataset-mIoU metric.</p><p>Given this insight, we propose as an heuristic for optimizing the dataset-mIoU to compute the batch Lovász-Softmax surrogate by taking the average in Equation <ref type="formula" target="#formula_0">(13)</ref> only over the classes present in the batch's ground truth. As a result, the loss is more stable to single predictions in absent classes, mimicking the dataset-mIoU. As outlined in our experiments, the optimization of the Lovász-Softmax restricted to classes present in each batch, effectively translates into gains for the dataset-mIoU metric.</p><p>We propose an additional trick for the optimization of the dataset-mIoU. Since the mIoU gives equal importance to each class, and to make the expectation of the batch-mIoU closer to the dataset-mIoU, it seems important to ensure that we feed the network with samples from all classes during training. In order to enforce this requirement, we sample the patches from the training by cycling over every classes, such that each class is visited at least once every |C| patches. This method is referred to as equibatch in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic experiment</head><p>We demonstrate the relevance of using the Jaccard loss for binary segmentation with a synthetic binary image segmentation experiment. We generate N = 10 binary images of size 50 × 50 representing circles of various radius, and extract for each pixel i a single feature using a unit variance Gaussian perturbation of the ground truth,  = 1/2 for the foreground and −1/2 for the background, as illustrated in <ref type="figure" target="#fig_5">Figure 3a</ref>.</p><p>We consider a model classifying pixels in the foreground class for f p &gt; −b, and we learn the bias term b. An exhaustive search, illustrated in <ref type="figure" target="#fig_5">Figure 3b</ref>, shows that among the losses considered, only the Lovász hinge efficiently captures the absolute minimum of the Jaccard loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Binary segmentation on Pascal VOC</head><p>We base our Pascal VOC experiments on the DeeplabV2single-scale semantic segmentation network <ref type="bibr" target="#b16">[17]</ref>. The network uses a Resnet-101 <ref type="bibr" target="#b13">[14]</ref> based architecture, re-purposed for image segmentation, notably using dilated (or atrous) convolutions. We use the initialization weights provided by the authors. These weights were pre-trained on MS-COCO <ref type="bibr" target="#b17">[18]</ref> using cross-entropy loss and weight decay. We further fine-tune these weights on a segmentation dataset consisting of Pascal VOC 2012 training images <ref type="bibr" target="#b7">[8]</ref> and the extra images provided by <ref type="bibr" target="#b11">[12]</ref>, as is common in recent semantic image segmentation applications.</p><p>For our binary segmentation experiments, we perform an initial fine-tuning of the weights using cross-entropy loss alone jointly on the 21 classes of Pascal VOC (including the background class); this constitutes our basis network. We then turn to binary segmentation by selecting one particular class and finetune the output of the network for the selected class. In order to consider a realistic binary segmentation setting, for each class, we sample the validation set such that half of the images contain at least one foreground pixel. The training is done on random crops of size 321 × 321 extracted from the training set, with random scale and horizontal flipping. Training batches are randomly sampled from the training set such that half of the selected images contain the foreground class on average.</p><p>Our experiments revolve around the choice of the training loss during fine-tuning to binary segmentation. We do a fine-tuning of 2 epoch iterations, with an initial learning rate of 5 · 10 −4 , reduced to 1 · 10 −4 after 1 epoch. <ref type="table" target="#tab_0">Table 1</ref> shows the average of the losses considered after a training with different loss objectives. Evidently, training with a particular loss leads generally to a better objective value of this loss on the validation set. Moreover, we see that the Lovász hinge acts as a  good surrogate of the discrete image-IoU, leading to a better validation accuracy for this measure. <ref type="figure" target="#fig_6">Figure 4</ref> shows example binary segmentation mask outputs. We notice that the Jaccard loss tends to fill gaps in segmentation, recover small objects, and lead to a more sensible segmentation globally, than other losses considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance of the surrogate</head><p>Comparison to prior work <ref type="bibr" target="#b23">[23]</ref> propose separately approximating I </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-class segmentation on Pascal VOC</head><p>We again useDeeplab-resnet-v2. This time, we exactly replicate the training procedure of the authors and following the same learning rate schedule, simply swapping out the loss for our multiclass surrogate, the Lovász-Softmax loss as described in Equation <ref type="formula" target="#formula_0">(13)</ref>, with the mean being restricted to the classes present in a given batch.</p><p>As in the reference implementation, we use a stochastic gradient descent optimizer with momentum 0.9 and weight decay 5 · 10 −4 ; the learning rate at training iteration k is</p><formula xml:id="formula_16">lr (k) = lr base 1 − k max iter power<label>(16)</label></formula><p>where power = 0.9 and lr base = 2.5 · 10 −4 . We experiment either with 20K iterations of batches of size 10 as in the reference paper, or with 30K iterations. We train the network with patches of size 321 × 321, with random flipping and rescaling. The 1449 validation images of Pascal VOC are  We train Deeplab-resnet at a single input scale, which fits the memory constraints of a single GPU. We optionally evaluate the learned weights in a multiscale setting by taking the mean of the probabilities given by the network at scales 1, 0.75, and 0.5, and also include the Gaussian CRF post-processing step used by Deeplab-v2. In this evaluation setting, we found that the baseline performance of the network trained with cross-entropy reaches 76.44% dataset-mIoU on the test set of Pascal VOC. <ref type="table" target="#tab_1">Tables 2 and 3</ref> present the scores obtained after training the network with cross-entropy or Lovász-Softmax loss, with and without equibatch, under various evaluation regimes. For a given training and evaluation setting, our loss achieves higher mIoU. <ref type="figure" target="#fig_8">Figure 5</ref> shows some example outputs. <ref type="figure" target="#fig_9">Figure 6a</ref> shows the evolution of the validation mIoU over the course of the training. We notice that the performance gain manifests itself especially in the last epochs of the optimization. Therefore, we also experiment with the same training setting with 30K iterations, to further benefit from the effects of the loss at these smaller learning rates. In agreement with our intuition, we see in <ref type="table" target="#tab_1">Table 2</ref> that training with our surrogate benefits from a larger number of iterations, in contrast to the original training with cross-entropy.</p><p>The CRF post-processing step of Deeplab appears to bring complementary improvements to the use of our mIoU surrogate. While using equibatch (batches with cyclic sampling from each class) does significantly help the crossentropy loss with respect to the dataset-mIoU, its effect on the performance with Lovász-softmax seems marginal. This may be linked with the fact that our loss ignores classes absent from the minibatch ground truth, and therefore relies less on the order of appearance of the classes across batches.  We found however that using equibatch facilitates the convergence of the training, as it helps the network to consider all classes during the course of the optimization. This is especially important in the early stages of the optimization, where a class absent for too long can end up being dropped by the classifier in favor of the other classes.  <ref type="figure" target="#fig_9">Figure 6b</ref> shows the joint evolution of the dataset-mIoU, and the batch-mIoU computed over present classes, during training. The correlation between these two measures justifies our choice of restricting the Lovász-Softmax to present classes as a proxy for optimizing the dataset-mIoU. As highlighted by <ref type="figure" target="#fig_9">Figure 6c</ref>, the image-mIoU is a poor surrogate for the dataset-mIoU, as discussed in Section 3.1: optimizing one measure is generally detrimental to the other. <ref type="figure" target="#fig_10">Figure 7</ref> illustrates some qualitative differences between segmentations predicted by the network optimized for batch-mIoU and the network optimized for image-mIoU. The biggest difference between batch-mIoU and image-mIoU is the penalty associated with predicting a class that is absent from the ground truth. Accordingly, we notice that optimizing for image-mIoU tends to produce more sparse outputs, and output less extraneous classes, sometimes at the price of not including classes that are harder to detect. Comparison to prior work Instead of changing the learning, Nowozin <ref type="bibr" target="#b21">[21]</ref> designs a test-time decision function for mIoU based on the assumption of independent classifiers with calibrated probabilities. We applied this method on the Softmax output probabilities of the best model trained with cross-entropy loss (cross-entropy + equibatch), and compare with the outputs from Lovász-Softmax (Lovász + equibatch 30K). Since <ref type="bibr" target="#b21">[21]</ref> performs a local optimization (batches), we randomly select 20 batches of 21 images with every class represented, optimize the decision function, and compare the optimized mIoU of the batch with the mIoU of the selected batch in our output. The baseline has an average mIoU of 68.7±1.2, our method significantly improves it to 72.5±1.2, while <ref type="bibr" target="#b21">[21]</ref> significantly degrades it to 65.1 ± 1.4. We believe this comes from the miscalibration of the neural network's probabilities, which adversely affects the assumptions of the decision function, as discussed in <ref type="bibr" target="#b21">[21,</ref><ref type="bibr">Sec. 5</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Cityscapes segmentation with ENet</head><p>We experiment with ENet, a segmentation architecture optimized for speed <ref type="bibr" target="#b22">[22]</ref>, on the Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref>. We fine-tune the weights provided by the authors, obtained after convergence of weighted cross-entropy loss, a loss that biases the cross-entropy loss to account for class inbalance in the training set. We do not need such a reweighing as our method inherently captures the class balancing of the mIoU.</p><p>We finetune ENet using an Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with the same learning rate and schedule as in Equation <ref type="formula" target="#formula_0">(16)</ref>.</p><p>(a) Initial ENet outputs <ref type="bibr" target="#b22">[22]</ref> (b) Ground truth masks (c) ENet + Lovász-Softmax fine-tuning  Consistent with <ref type="bibr" target="#b22">[22]</ref>, we use images of size 512 × 1024 with no data augmentation. We snapshot every 1K iterations and report the test performance of snapshot 9K with batches of size 10, which corresponds to the highest validation score. <ref type="figure" target="#fig_12">Fig. 9</ref> shows that our fine-tuning leads to a higher validation mIoU, while further training with weighted crossentropy barely affects the performance -as expected. Higher batch sizes generally lead to more improvement thanks to a better approximation of the dataset IoU. Equibatch training did not make a difference in our experiments, which can be explained by the fact that the dataset is more uniform than Pascal VOC in terms of class representation. Note that we optimize for the mIoU measure, named Class IoU in Cityscapes. Accordingly, we observe a substantial gain in performance in Cityscapes IoU metrics, with the Class IoU increasing from 58.29% to 63.06%. Reweighting the different classes in the average of the Lovász-Softmax loss (Equation (13)) could allow us to target IoU-based measures which are weighted differently, such as CityScapes' iIoU metrics. <ref type="figure" target="#fig_11">Figure 8</ref> presents some example output masks; we find that our fine-tuning generally reduces false positives and leads to finer details. Of course, our improved segmentation accuracy does not impact the high inference speed for which ENet is designed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusions</head><p>In this work, we have demonstrated a versatile approach for optimizing the Jaccard loss for image segmentation. Our proposed method can be flexibly applied to a large number of function classes for segmentation, and we have demonstrated their effectiveness on state-of-the-art deep network architectures, substantially improving accuracies on semantic segmentation datasets simply by optimizing the correct loss during training. Qualitatively, we see greatly improved segmentation quality, in particular on small objects, while large objects tend to have consistent but smaller improvement in accuracy.</p><p>This work shows that submodular measures such as the Jaccard index can be readily optimized in a continuous optimization setting. Further work includes the application of the approach to different tasks and losses exhibiting submodularity, and a derivation of specialized optimization routines given the piecewise-linear nature of the Lovász extension.</p><p>The code associated with this publication, with replication of the experiments and implementations of the Lovász-Softmax loss, is released on https://github.com/ bermanmaxim/LovaszSoftmax.</p><p>Acknowledgements. This work is partially funded by Internal Funds KU Leuven and FP7-MC-CIG 334380. We acknowledge support from the Research Foundation -Flanders (FWO) through project number G0A2716N. The authors thank J. Yu, X. Jia and Y. Huang for valuable comments and discussions.  <ref type="figure">Figure A</ref>.1 shows segmentations obtained for binary foreground-background segmentation on Pascal VOC under different training losses, after finetuning a base multi-class classification network for a specific class. We see that the Lovász hinge for the Jaccard loss tends to fill gaps in segmentation, recover small objects, and lead to a more sensible segmentation globally. <ref type="table" target="#tab_5">Table A</ref>.1 presents detailed scores for this binary segmentation task. We notice a clear improvement of the per image-IoU by optimizing with the Jaccard loss. Moreover, the results are in agreement with the intuition that the best performance for a given loss on the validation set is achieved when training with that loss. In some limited cases (boat, bottle) the performance of the base multi-class network is actually higher than the fine-tuned versions. Our understanding of this phenomenon is that the context is particularly important for these specific classes, and the absence of label for the other classes during finetuning impedes the predictive ability of the network. Additionally, <ref type="figure">Figure A.</ref>2 presents an instance of convergence curves of this binary network, under the different losses considered.</p><p>Comparison to prior work <ref type="bibr" target="#b23">[23]</ref> propose separately approximating the intersection</p><formula xml:id="formula_17">I p i=1 F i [y * i = 1], (A.1)</formula><p>using the Iverson bracket notation, and the union</p><formula xml:id="formula_18">U n i=1 (p i + [y * i = 1]) − I (A.2)</formula><p>for optimizing the binary IoU I/U . We compared the validation image mIoU under the loss of <ref type="bibr" target="#b23">[23]</ref> and the binary Lovász hinge, for all the categories of binarized Pascal VOC, in the setting of section 4.2. We chose for <ref type="bibr" target="#b23">[23]</ref> the bestscoring among 3 learning rates. As seen in <ref type="table" target="#tab_5">Table A</ref>.2 the proxy loss in <ref type="bibr" target="#b23">[23]</ref> does not reach the performance of our method. Since <ref type="bibr" target="#b23">[23]</ref> uses the same approximation "batch-IoU dataset-IoU", these observations extend to the binary dataset-IoU measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Supplementary experiment: IBSR brain segmentation</head><p>Data and Model In order to test the Lovász-Softmax loss on a different type of images, we consider the publicly available dataset provided by the Internet Brain Segmentation Repository (IBSR) <ref type="bibr" target="#b24">[24]</ref>. This dataset is composed of Magnetic Resonance (MR) image data of 18 different patients annotated for segmentation. For this segmentation task, we used a model based on Deeplab <ref type="bibr" target="#b16">[17]</ref> adapted to IBSR by Shakeri et al. <ref type="bibr" target="#b26">[26]</ref>. Our evaluation follows the same procedure as in the cited paper: a subset of 8 subcortical structures is first selected: left and right thalamus, caudate, putamen, and pallidum, then 3 folds composed of respectively 11, 1, and 6 train, validation, and test volumes are used for training and testing. <ref type="table" target="#tab_5">Table B</ref>.1 details the model architecture to which we add batch normalization layers between the convolutional layers and their ReLU activations.</p><p>Settings Similarly to <ref type="bibr" target="#b26">[26]</ref>, we consider the dataset composed of the 256 axial brain slices of each volume rather than using the 3D structure of the data. This dataset is composed of 256×128 grayscale images. Moreover, we discard the images that contain only the background class during training. For each fold, the training data is then limited to ≈ 800-900 slices. Training is done with stochastic gradient descent and a learning rate schedule to exponentially decrease from 10 −1 to  batch-mIoU for present classes variant). As we are interested on showing the effect of the loss, we do not apply the CRF post-processing proposed in <ref type="bibr" target="#b26">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The mean Jaccard index and DICE over the 3 folds for each of the four classes (right + left) of interest along with the mean scores across all classes are given in <ref type="table" target="#tab_1">Table B.2,</ref> showing an improvement when using the Lovász-Softmax loss. Some qualitative results are shown in <ref type="figure">Figure B</ref>.1, highlighting the improvements in detecting some fine subcortical structures when the Lovász-Softmax loss is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proximal gradient algorithm</head><p>We have developed a specialized optimization procedure for the Lovász Hinge for binary classification with the Jaccard loss, based on a computation of the proximal operator of the Lovász Hinge. We include this algorithm here for completeness but have not used it for the main results of the paper, instead relying on standard stochastic gradient descent with momentum. The proximal gradient algorithm we propose here has been independently proposed by Frerix et al. <ref type="bibr" target="#b8">[9]</ref>.</p><p>Our motivation for the proximal gradient algorithm stems from the piecewise-linearity of our loss function, which might destabilize stochastic gradient descent. Instead we would like to exploit the geometry of the Lovász Hinge. We therefore analyze the applicability of (variants of) the proximal gradient algorithm for optimization of a risk functional based on the Lovász hinge.</p><p>Definition C.1 Proximal operator. The proximal operator of a function f with a regularization parameter λ is</p><formula xml:id="formula_19">prox f,λ (x) = arg min u f (u) + λ 2 u − x 2 (C.1)</formula><p>We consider the problem of minimizing a (sub)differentiable function f .</p><p>Iterative application of the proximal operator with an appropriately decreasing schedule of {λ t } 0≤t≤∞ leads to convergence to a local minimum analogously to gradient descent. Furthermore, it is straightforward to show that, given an appropriately chosen schedule of λ parameters, the proximal gradient algorithm will converge at least as fast as gradient descent.</p><p>Proposition C.1. Given a gradient descent parameter η, x t+1 = x t − η∇f (x t ), there exists a set of descent parameters {λ t } 0≤t≤∞ such that (i) the step size of the proximal operator is equivalent to gradient descent and (ii) prox f,λt (x t ) ≤ x t − η∇f (x t ). </p><formula xml:id="formula_20">training b x h j b x h j b x h j b x h j sheep sofa train tvmonitor training b x h j b x h j b x h j b x h j</formula><p>Proof. Starting with claim (i), we note that the proximal operator is the Lagrangian of the constrained optimization problem arg min u f (u) s.t. x − u 2 ≤ R for some R &gt; 0, and we may therefore consider λ t such that R t = η∇f (x t ) 2 , where {x t } 0≤t≤∞ is the sequence of values visited in gradient descent. Claim (ii) follows directly from the definition of the prox-imal operator as the minimization of f (u) within a ball of radius R t around x t must be at least as small as the value at the gradient descent direction.</p><p>It is straightforward to convert a gradient descent step size schedule to an equivalent proximal gradient schedule of λ t values such that, were the objective linear, the two</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Cross-entropy Lovász-Softmax   algorithms would be equivalent. Indeed, the proximal gradient algorithm applied to a piecewise linear objective only differs from gradient descent at the boundaries between linear pieces, in which case it converges in a strictly smaller number of steps than gradient descent. We optimize a deep neural network architecture by a modified backpropagation algorithm in which the gradient direction with respect to the loss layer is given by the direction of the empirical difference x t − prox f (x t ). We note that this modification to the standard gradient computation is compatible with popular optimization strategies such as Adam <ref type="bibr" target="#b15">[16]</ref>.</p><p>In initial experiments using the true gradient rather than that based on the proximal operator, we found that the use of momentum led to faster empirical convergence than Adam, and we therefore have based our subsequent comparison and empirical results on optimization with momentum.</p><p>We show here that these momentum terms still do not lead in practice to as efficient update directions as those defined by the proximal operator.</p><p>Definition C.2 Momentum <ref type="bibr" target="#b27">[27]</ref>. Gradient descent with momentum is achieved with the following update rules v t+1 =αv t + ∇f (x t )</p><formula xml:id="formula_21">(C.2) x t+1 =x t − ηv t+1 , (C.3)</formula><p>where η is the gradient descent parameter and α ∈ [0, 1] is the momentum coefficient.</p><p>Unrolling this recursion shows that momentum gives an exponentially decaying weighted average of previous gradient values, and setting α = 0 recovers classical gradient descent. <ref type="figure" target="#fig_0">Figure C.1</ref> shows the behavior of gradient descent with momentum on the problem</p><formula xml:id="formula_22">min x∈R 2 max 0, x, ν 0 , x, 0 1 , (C.4)</formula><p>where ν is a positive scalar that allows us to adjust the relative scale of the gradients on either side of the boundary between the pieces. In all cases, the momentum oscillates around piecewise-linear edges, and in <ref type="figure">Figure C</ref>.1c, we see that traversing to a piece of the loss surface with very different slope can lead to multiple steps away from the boundary before returning to a steeper descent direction. By contrast, the proximal algorithm immediately determines the optimal descent direction.  Optimization study We specialize the proximal gradient algorithm to our proposed Jaccard Hinge loss. We compute an approximate value of the proximal point to any initial point on the loss surface by following a greedy minimization path to the proximal objective C.1. This computation is detailed in Algorithm C.1. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(a) GT = [− 1 ,</head><label>1</label><figDesc>−1] (b) GT = [−1, 1] (c) GT = [1, −1](d) GT =<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Lovász hinge in the case of two pixel predictions for the four possible ground truths GT, as a function of the relative margins r i = 1 − F i (x) y * i for i = 1, 2. The red dots indicate the values of the discrete Jaccard index.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Lovász-Softmax for the foreground class, with two classes {−1, 1} and two pixels, for each ground truth labeling GT. The loss is plotted against the difference of unnormalized scores</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>f i ∼ N ( , 1) where (a) Sample label &amp; features (b) Relative losses for varying bias b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Synthetic model studied in 4.1 and loss objectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Binary bicycle masks predicted on a validation image after training the network under various losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>p i=1 F</head><label>i=1</label><figDesc>i [y * i = 1] and U n i=1 (p i + [y * i = 1]) − I for optimization of binary IoU I/U . In our experiments, we were not able to observe a consistent improvement of the IoU using this surrogate, contrary to the Lovász hinge. Details on this comparison are included in the Supplementary Material, Section A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>(a) Input images (b) Ground truth masks (c) Lovász-Softmax + CRF (d) Cross-entropy + CRF Multiclass segmentations after training with the Lovász-Softmax or the cross-entropy loss, and post-processed with Gaussian CRF. The color scheme follows the standard convention of the Pascal VOC dataset<ref type="bibr" target="#b7">[8]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Evolution of some validation measures over the course of the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Details of predicted masks after training with Lovász-Softmax per-batch vs. Lovász-Softmax per-image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>ENet: parts of output masks before and after fine-tuning with Lovász-Softmax (using the Cityscapes color palette).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Convergence of ENet on the validation set under fine-tuning with Lovász-Softmax, with various batch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>10 − 3 Figure A. 1 :</head><label>1031</label><figDesc>over 35 epochs with either the cross-entropy loss as in the original model, or the Lovász-Softmax loss (the I Example binary segmentations trained with different losses and associated IoU scores on Pascal VOC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure A. 2 :</head><label>2</label><figDesc>Evolution of the validation IoU during the course of the optimization with the different losses considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure B. 1 :</head><label>1</label><figDesc>Some examples of segmentation on the ISBR dataset. These examples are taken from two different patients and two different folds, and show an improvement in the segmentation of some fine structures when the Lovász-Softmax loss is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>3 Figure C. 1 :</head><label>31</label><figDesc>(a) ν = 0.7 (b) ν = 1.0 (c) ν = 1.Optimization behavior of the piecewise-linear surface defined in Equation C.4: gradient descent (green, dashed) and momentum (orange, plain) oscillate around the edge, while the proximal algorithm (green) finds the optimal descent direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure C. 2 :</head><label>2</label><figDesc>Jaccard loss optimization with different optimization methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average of mean validation binary losses over the 20 Pascal VOC categories, after a training with cross-entropy, hinge, and Lovász hinge loss. The image-mIoU of the basis network, trained for all categories, is equal to 78.29.</figDesc><table><row><cell cols="4">Training loss → Cross-entropy Hinge Lovász hinge</cell></row><row><cell>Cross-entropy</cell><cell>6.84</cell><cell>6.96</cell><cell>7.91</cell></row><row><cell>Hinge</cell><cell>7.81</cell><cell>6.95</cell><cell>7.11</cell></row><row><cell>Lovász hinge</cell><cell>8.37</cell><cell>7.45</cell><cell>5.44</cell></row><row><cell>Image-IoU</cell><cell>77.14</cell><cell>75.8</cell><cell>80.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>x-loss</cell><cell>74.64</cell><cell>76.23</cell><cell>76.53</cell><cell>76.44</cell></row><row><cell>x-loss + equibatch</cell><cell>75.53</cell><cell>76.70</cell><cell>77.31</cell><cell>78.05</cell></row><row><cell>x-loss + equibatch -30K iterations</cell><cell>74.97</cell><cell>76.24</cell><cell>76.73</cell><cell></cell></row><row><cell>Lovász</cell><cell>76.56</cell><cell>77.24</cell><cell>77.99</cell><cell></cell></row><row><cell>Lovász + equibatch</cell><cell>76.53</cell><cell>77.28</cell><cell>78.49</cell><cell></cell></row><row><cell>Lovász + equibatch -30K iterations</cell><cell>77.41</cell><cell>78.22</cell><cell>79.12</cell><cell>79.00</cell></row><row><cell cols="2">included in the training only for experiments evaluated on</cell><cell></cell><cell></cell><cell></cell></row><row><cell>the official test evaluation server.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Performance of Deeplab-v2 single-scale trained with cross-entropy (x-loss) vs. Lovász-Softmax loss, for different network evaluations: raw single-scale network output, multi-scale, and Gaussian CRF post-processing.validation mIoU (%) test mIoU (%) single-scale multi-scale multi-scale + CRF multi-scale + CRF</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Per-class test IoU (%) corresponding to the best-performing variants in Table 2. airplane cycle bird boat bottle bus car cat chair cow d. table dog horse mbike person plant sheep sofa train tv x-loss 92.95 41.06 87.06 61.23 77.6 91.99 88.11 92.45 32.84 82.48 59.6 90.13 89.83 86.77 85.79 58.06 85.31 52.00 84.47 71.26 x-loss-equi. 93.32 40.29 91.47 63.74 77.03 93.10 86.70 93.37 34.79 87.92 69.74 89.53 90.61 84.70 85.13 59.23 87.71 64.46 82.89 68.57 Lovász-equi 30K 92.63 41.55 87.87 68.41 77.75 94.71 86.71 90.37 38.59 86.24 74.50 89.02 91.69 87.28 86.37 65.92 87.13 65.21 83.69 68.64</figDesc><table><row><cell></cell><cell>80</cell><cell></cell><cell>dataset mIoU</cell><cell>55</cell><cell>74.4</cell><cell>imag e mIoU</cell><cell>95.70</cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell>50</cell><cell>74.2</cell></row><row><cell></cell><cell>65 70</cell><cell>0</cell><cell>5000 training iteration 10000 15000 mIoU-batch-present 20000</cell><cell>40 45</cell><cell cols="2">training iteration 18000 18500 19000 19500 20000 73.8 95.40 95.55 74.0 dat ase t mIo U</cell></row><row><cell>(a) Dataset mIoU on the validation set over</cell><cell cols="4">(b) Validation dataset-mIoU vs. batch-mIoU</cell><cell cols="2">(c) Validation dataset-mIoU and image-</cell></row><row><cell>the course of the Lovász-Softmax or cross-</cell><cell cols="4">restricted to present classes during training</cell><cell cols="2">mIoU during training with Lovász-Softmax</cell></row><row><cell>entropy optimization.</cell><cell cols="3">with Lovász-Softmax.</cell><cell></cell><cell cols="2">optimizing for image-mIoU.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Cityscapes results with Lovász-Softmax finetuning</figDesc><table><row><cell></cell><cell cols="4">Class IoU Class iIoU Cat. IoU Cat. iIoU</cell></row><row><cell>ENet 1</cell><cell>58.29</cell><cell>34.36</cell><cell>80.40</cell><cell>63.99</cell></row><row><cell>Finetuned 2</cell><cell>63.06</cell><cell>34.06</cell><cell>83.58</cell><cell>61.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</figDesc><table><row><cell>Supplementary Material</cell></row><row><cell>Maxim Berman Amal Rannen Triki Matthew B. Blaschko</cell></row><row><cell>Dept. ESAT, Center for Processing Speech and Images</cell></row><row><cell>KU Leuven, Belgium</cell></row><row><cell>{maxim.berman,amal.rannen,matthew.blaschko}@esat.kuleuven.be</cell></row><row><cell>A. Detailed results for Section 4.2: binary seg-</cell></row><row><cell>mentation on Pascal VOC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A .</head><label>A</label><figDesc>1: Losses measured on our validation set of the 20 Pascal VOC categories, after a training with cross-entropy loss (x), hinge-loss (h), and Lovász-hinge (j). b indicates the performance of the base network, trained for all categories. 59.6 83.4 84.0 82.6 86.3 66.7 70.6 70.0 73.8 83.8 82.1 81.7 87.6</figDesc><table><row><cell></cell><cell></cell><cell cols="2">aeroplane</cell><cell></cell><cell></cell><cell cols="2">bicycle</cell><cell></cell><cell></cell><cell>bird</cell><cell></cell><cell></cell><cell></cell><cell>boat</cell><cell></cell><cell></cell></row><row><cell>training</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell></row><row><cell>x-entropy, ·10 −2</cell><cell></cell><cell>2.8</cell><cell>3.3</cell><cell>4.1</cell><cell></cell><cell cols="3">12.3 11.0 11.3</cell><cell></cell><cell>3.4</cell><cell>4.0</cell><cell>4.6</cell><cell></cell><cell>6.4</cell><cell>6.4</cell><cell>6.9</cell></row><row><cell>hinge, ·10 −2</cell><cell></cell><cell>2.9</cell><cell>2.6</cell><cell>2.8</cell><cell></cell><cell cols="3">14.8 12.1 11.5</cell><cell></cell><cell>3.6</cell><cell>3.3</cell><cell>3.1</cell><cell></cell><cell>7.4</cell><cell>6.6</cell><cell>6.6</cell></row><row><cell>Jacc-Hinge, ·10 −1</cell><cell></cell><cell>3.8</cell><cell>3.6</cell><cell>2.8</cell><cell></cell><cell cols="3">13.8 12.0 9.2</cell><cell></cell><cell>6.2</cell><cell>5.8</cell><cell>4.1</cell><cell></cell><cell>7.4</cell><cell>7.4</cell><cell>5.2</cell></row><row><cell>Image-IoU, %</cell><cell cols="16">86.2 88.6 87.7 89.6 63.2 61.2 58.7 66.3 84.5 82.1 81.3 86.9 80.3 75.8 73.2 79.9</cell></row><row><cell></cell><cell></cell><cell cols="2">bottle</cell><cell></cell><cell></cell><cell>bus</cell><cell></cell><cell></cell><cell></cell><cell>car</cell><cell></cell><cell></cell><cell></cell><cell>cat</cell><cell></cell><cell></cell></row><row><cell>training</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell></row><row><cell>x-entropy, ·10 −2</cell><cell></cell><cell>5.8</cell><cell>5.9</cell><cell>7.3</cell><cell></cell><cell>3.7</cell><cell>4.3</cell><cell>5.1</cell><cell></cell><cell>4.0</cell><cell>4.4</cell><cell>5.6</cell><cell></cell><cell>4.9</cell><cell>5.2</cell><cell>5.9</cell></row><row><cell>hinge, ·10 −2</cell><cell></cell><cell>6.6</cell><cell>5.6</cell><cell>4.5</cell><cell></cell><cell>3.9</cell><cell>3.4</cell><cell>3.9</cell><cell></cell><cell>4.4</cell><cell>4.0</cell><cell>3.5</cell><cell></cell><cell>5.4</cell><cell>4.9</cell><cell>5.1</cell></row><row><cell>Jacc-Hinge, ·10 −1</cell><cell></cell><cell cols="3">14.8 11.8 8.0</cell><cell></cell><cell>3.6</cell><cell>3.1</cell><cell>2.4</cell><cell></cell><cell>9.8</cell><cell>8.9</cell><cell>5.4</cell><cell></cell><cell>4.8</cell><cell>4.4</cell><cell>3.3</cell></row><row><cell>Image-IoU, %</cell><cell cols="16">71.9 70.1 68.0 70.5 90.7 90.2 90.4 91.2 76.3 77.0 75.5 80.5 88.7 86.0 86.5 89.8</cell></row><row><cell></cell><cell></cell><cell cols="2">chair</cell><cell></cell><cell></cell><cell>cow</cell><cell></cell><cell></cell><cell></cell><cell cols="2">diningtable</cell><cell></cell><cell></cell><cell>dog</cell><cell></cell><cell></cell></row><row><cell>training</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell><cell>b</cell><cell>x</cell><cell>h</cell><cell>j</cell></row><row><cell>x-entropy, ·10 −2</cell><cell></cell><cell cols="3">11.4 11.1 13.1</cell><cell></cell><cell>6.1</cell><cell>6.5</cell><cell>7.7</cell><cell></cell><cell cols="3">14.1 12.7 12.9</cell><cell></cell><cell>5.7</cell><cell>6.0</cell><cell>6.3</cell></row><row><cell>hinge, ·10 −2</cell><cell></cell><cell cols="3">13.3 11.8 11.0</cell><cell></cell><cell>6.9</cell><cell>6.2</cell><cell>7.6</cell><cell></cell><cell cols="3">16.7 14.5 13.7</cell><cell></cell><cell>6.3</cell><cell>5.8</cell><cell>5.8</cell></row><row><cell>Jacc-Hinge, ·10 −1</cell><cell></cell><cell cols="3">16.6 14.4 9.8</cell><cell></cell><cell>5.6</cell><cell>5.1</cell><cell>4.1</cell><cell></cell><cell cols="3">12.5 10.7 7.9</cell><cell></cell><cell>5.6</cell><cell>5.0</cell><cell>3.4</cell></row><row><cell>Image-IoU, %</cell><cell cols="3">59.3 54.0 51.2 horse</cell><cell></cell><cell></cell><cell cols="2">motorbike</cell><cell></cell><cell></cell><cell cols="2">person</cell><cell></cell><cell></cell><cell cols="2">potted-plant</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table B .</head><label>B</label><figDesc></figDesc><table /><note>1: Layers used for the brain image segmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table B</head><label>B</label><figDesc>.2: Test results on IBSR brain segmentation task -Average on 3 folds</figDesc><table><row><cell></cell><cell></cell><cell>Thalamus Proper</cell><cell cols="4">Caudate Putamen Pallidum Mean</cell></row><row><cell>Cross</cell><cell>Jaccard</cell><cell>72.74</cell><cell>52.31</cell><cell>61.55</cell><cell>54.04</cell><cell>60.16</cell></row><row><cell>Entropy</cell><cell>DICE</cell><cell>84.17</cell><cell>68.33</cell><cell>76.07</cell><cell>70.02</cell><cell>74.65</cell></row><row><cell>Lovász</cell><cell>Jaccard</cell><cell>73.56</cell><cell>54.44</cell><cell>62.57</cell><cell>55.74</cell><cell>61.55</cell></row><row><cell>Softmax</cell><cell>DICE</cell><cell>84.74</cell><cell>70.25</cell><cell>76.89</cell><cell>71.50</cell><cell>75.84</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://cityscapes-dataset.com/method-details/?submissionID=132 2 https://cityscapes-dataset.com/method-details/?submissionID=993</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the choice of the optimization in terms of empirical convergence rates on the validation data. We evaluate the use of varying optimization strategies for the last layer of the network in <ref type="figure">Figure C.</ref>2. Experimentally, we find that the proximal gradient algorithm converges better than stochastic gradient descent alone, and has similar or better performance to stochastic gradient descent with momentum, which it can easily be combined with.</p><p>V</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing expected intersectionover-union with candidate-constrained CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning with submodular functions: A convex optimization perspective. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="145" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to localize objects with structured output regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>D. Forsyth, P. Torr, and A. Zisserman</editor>
		<imprint>
			<biblScope unit="volume">5302</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2" to="15" />
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, Part VI</title>
		<editor>K. Daniilidis, P. Maragos, and N. Paragios</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="168" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What is a good evaluation measure for semantic segmentation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Proximal backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Möllenhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>II</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Submodular Functions and Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fujishige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Discrete Mathematics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
			<publisher>Elsevier Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV, Part VII</title>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Étude comparative de la distribution florale dans une portion des Alpes et des Jura</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin de la Société Vaudoise des Sciences Naturelles</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="547" to="579" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang-Chieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Submodular functions and convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming The State of the Art</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1983" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimal decisions from probabilistic models: The intersection-over-union case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ENet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimizing intersection-over-union in deep neural networks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<meeting><address><addrLine>I, III</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image similarity and tissue overlaps as surrogates for image registration accuracy: Widely used but unreliable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rohlfing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="163" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sub-cortical brain structure segmentation using F-CNN&apos;s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Biomedical Imaging</title>
		<meeting><address><addrLine>I, II</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="269" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>III-1139-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning submodular losses with the Lovász hinge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The Lovász hinge: A convex surrogate for submodular losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.07797</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Per-class test IoU (%) corresponding to the results by the best learning rate for [23] compared to the results of the Lovász hinge. airplane cycle bird boat bottle bus car cat chair cow d. table dog horse mbike person plant sheep sofa train tv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Table</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>23] 79.9 54.7 75.5 72.5 68.7 86.2 73.3 78.4 56.6 75.4 72.2 76.9 68.8 79.4 71.7 62.1 76.5 69.9 77.8 77.1</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lovász-Hinge</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">89</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
