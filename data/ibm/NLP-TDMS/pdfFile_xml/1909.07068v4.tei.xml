<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pose Neural Fabrics Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Wankou</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhen</forename><surname>Cui</surname></persName>
						</author>
						<title level="a" type="main">Pose Neural Fabrics Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human pose estimation</term>
					<term>neural architecture search</term>
					<term>cell-based neural fabrics</term>
					<term>micro and macro search space</term>
					<term>prior knowledge</term>
					<term>part-specific NAS</term>
					<term>vector representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS) technologies have emerged in many domains to jointly learn the architectures and weights of the neural network. However, most existing NAS works claim they are task-specific and focus only on optimizing a single architecture to replace a human-designed neural network, in fact, their search processes are almost independent of domain knowledge of the tasks. In this paper, we propose Pose Neural Fabrics Search (PoseNFS). We explore a new solution for NAS and human pose estimation task: part-specific neural architecture search, which can be seen as a variant of multi-task learning. Firstly, we design a new neural architecture search space, Cellbased Neural Fabric (CNF), to learn micro as well as macro neural architecture using a differentiable search strategy. Then, we view locating human keypoints as multiple disentangled prediction sub-tasks, and then use prior knowledge of body structure as guidance to search for multiple part-specific neural architectures for different human parts. After search, all these part-specific CNFs have distinct micro and macro architecture parameters. The results show that such knowledge-guided NASbased architectures have obvious performance improvements to a hand-designed part-based baseline model. The experiments on MPII and MS-COCO datasets demonstrate that PoseNFS 1 can achieve comparable performance to some efficient and state-ofthe-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>N EURAL Architecture Search (NAS), the process of jointly learning the architecture and weights of the neural network <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b8">[9]</ref>, can play a potential role at designing efficient network architectures automatically. Current NAS methods mainly take image classification as basic task, and only search for a micro "cell" to build a chain-like structure. However, when applying NAS to dense (pixel-wise) prediction tasks such as semantic segmentation and human pose estimation, the micro search space is no longer able to generate more complex architectures. Therefore, it become a necessity to artificially design a macro search space allowing identifying a hierarchical structure upon cells for these tasks. In addition, most existing NAS works such as <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref> optimize a single architecture in the search space, and finally obtain a so-called task-specific architecture to replace a humandesigned architecture in the pipeline. Such practice, in fact, decouples the automating architecture engineering from the characteristics of tasks, failing to take advantage of significant <ref type="figure">Fig. 1</ref>. A schematic diagram of searched architectures: five part-specific CNFs (Cell-based Neural Fabrics) are associated with different body parts according to the prior body structure. They take as input the shared feature pyramid produced from a common CNN backbone or a derived CNF. The weighted operations combinations in the cells are also distinct in the partspecific CNFs. domain knowledge to guide the search process and achieve the expected targets. For human pose estimation task, we argue that the prior knowledge of human body structure can help discover specific neural architectures in a part-based and automatic way. Thus, we propose a new paradigm: part specific neural architecture search, to search multiple neural architectures for different human parts with guide of prior knowledge, as shown in <ref type="figure">Fig. 1</ref>.</p><p>Naturally, the first step to introduce NAS into human pose estimation is to construct an architecture search space that can identify multi-scale, stacked or cascaded neural network. To this end, we propose a general parameterized Cell-based Neural Fabric (CNF), a scalable topology structure to encode micro and macro architecture parameters into cells. The discrete search space is relaxed into continuous search space to make it searchable by gradient descent. This design is motivated by Convolutional Neural Fabrics <ref type="bibr" target="#b11">[12]</ref> and DARTS <ref type="bibr" target="#b0">[1]</ref>, it can be described as a neural fabric architecture woven by cells, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>In addition, there exists an inconsistency gap between the derived child network (sub-architecture) and the converged parent network (super-network) in DARTS <ref type="bibr" target="#b0">[1]</ref>. Many works attempt to eliminate this bias such as SNAS-series works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. In our work, we avoid it in a direct and simple way. We do not re-discretize the continuous architecture after searching, which means that all operations are densely preserved with macro and micro architecture parameters in both searching arXiv:1909.07068v4 [cs.CV] 5 Dec 2020 The neural fabric is woven by cells in different scales and layer depths. Black arrow represents identity transformation; purple arrow represents reducing spatial size and doubling the channels of feature maps; red arrow represents increasing spatial size and halving the channels of feature maps. and evaluation stage. In order to verify such a setting, we test the performances of architectures randomly sampled from the continuous parameter space. Furthermore, we explore a simple yet effective method, gradient-based synchronous optimization, as the major search strategy to reduce the cost of time and computational budgets for the search process.</p><p>Designing the neural search space or search strategy is just the beginning for NAS, we believe domain knowledge can help NAS go further. For human pose estimation task, the special human body structure information is significant prior knowledge to be exploited. Current ConvNet-based human pose estimation methods <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b19">[20]</ref> usually use FCNlike architecture to predict keypoints heatmaps. The global spatial relationship between all keypoints is implicitly learned by shared convolutional blocks and a single linear prediction head. Nevertheless, such learning methods may ignore the explicit knowledge: the spatial distributions of keypoints might be highly correlated in the short-range while weakly in the long-range. Tang et al. <ref type="bibr" target="#b20">[21]</ref> have found that some pairs of body parts are weakly related by analyzing their mutual information, and a sharing mechanism using the same network to learn shared feature for all keypoints may cause negative transfer between uncorrelated parts. They therefore view human pose estimation as homogeneous multi-task learning (MTL) <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, and design a structure-shared part-based branching network (PBN) to estimate related parts. Here we ask: since human pose estimation can be viewed as multiple prediction tasks, the internal characteristics of these tasks would be different, are structure-shared branching networks really better for multiple sub-tasks? Whether can NAS search for more specific neural architectures to localize these disentangled human parts?</p><p>As a matter of fact, such problems can be converted into searching multiple task-specific neural architectures for multiple sub-tasks. There also has been a new trend to explore Multi-task Neural Architecture Search such as <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>, due to that MTL could improve generalization by leveraging the domain-specific information contained in the training signals of related tasks <ref type="bibr" target="#b21">[22]</ref>. In this work, provided with the proposed search space, we can search multiple architectures to achieve multiple learning objectives in a single shot, by exploiting simple prior knowledge as guidance. As a result, part-specific CNFs with distinct micro and macro architecture parameters are achieved to adapt to the characteristics of different subtasks. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, highly correlated keypoints are grouped into the corresponding part representations that are predicted by searchable CNFs (multi-head predictions). Then each location of keypoint is inferred by the associated part representation. Such NAS-based part-specific architectures extend the carefully designed message passing mechanism, handdesigned branch or information fusion module <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> to fuse feature information between related joints. Besides, we replace scalar value by a vector in each pixel position of heatmaps, because the scalar value representing the existing probability of keypoints is still inadequate to encode local feature. We use the length ( 2 norm) of the vector to represent the existing score of the keypoint and vector space to capture more local component information of the body part. In summary, our main contributions are:</p><p>• We propose a novel micro and macro architecture search space: parameterized Cell-based Neural Fabric (CNF). • With simple prior knowledge as guidance, our method automatically searches part-specific neural architectures to localize disentangled body parts, which extends the traditional part-based methods. • Such part-specific neural architecture search can be seen as a variant of multi-task learning. It is a novel NAS paradigm as Multi-Task Neural Architecture Search for human pose estimation task. • The experiments show that NAS-based part-specific architectures have obvious performance improvements to a hand-designed part-based baseline model. And our models achieve comparable performances to some efficient and state-of-the-art methods.</p><p>II. RELATED WORK a) Neural Architecture Search: Our work is motivated by convolutional neural fabrics <ref type="bibr" target="#b11">[12]</ref> and neural architecture search methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Liu et al. <ref type="bibr" target="#b0">[1]</ref> propose the continuous relaxation for architecture representation to search architectures using differentiable strategy. Chen et al. <ref type="bibr" target="#b3">[4]</ref> adopt random search to search Dense Prediction Cell architecture for dense image prediction. Ghiasi et al. <ref type="bibr" target="#b2">[3]</ref> use Reinforce Learning to explore more connection possibilities in different scales of feature pyramid network. Xie et al. <ref type="bibr" target="#b7">[8]</ref> explore randomly wired networks for image classification and proposed the concept of network generator. Liu et al. <ref type="bibr" target="#b8">[9]</ref> propose Auto-DeepLab, searching for a hierarchical neural network as backbone to replace original human-designed network in a common pipeline (DeepLab) for semantic segmentation. It aims to search a cell structure and a better path in multiple branches. In contrast, our architecture space construction method is one-step, unlike the two-step construction scheme for architecture search space in AutoDeepLab, the whole architecture at macro and micro level is totally parameterized as each cell's parameters and not pruned into sparsely connected architecture. Our work also can be viewed as Multi-task Neural Architecture Search. There are similar works like [24]- <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Pyramid</head><p>Input Image b) Human pose estimation and part-based model: Topdown <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b28">[29]</ref> and bottom-up <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref> human pose estimation approaches have been proven extremely successful in learning global or long-range dependencies relationships of body pose. However, parts occlusions, viewpoint variations, crowed scene, and background interference etc. are still tough problems. Compositional structure models or part-based models <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b43">[44]</ref> attempt to overcome aforementioned problems by representing the human body as a hierarchy of parts and subparts. Early part-based models <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b44">[45]</ref>, pictorial structure models <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref> and some of the current models <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b45">[46]</ref>- <ref type="bibr" target="#b47">[48]</ref> are usually based on hand-crafted feature (e.g. HOG, SIFT), hand-designed convolutional neural networks or handdesigned information fusion modules. Our method also exploits the compositionality of body pose to separately predict related keypoints, but further develops it by employing NAS to learn distinct CNFs for different keypoints groups.</p><formula xml:id="formula_0">⃗ ! ⃗ " # ℎ #</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part representation</head><p>c) Vector Representation: The vector in pixel method is motivated by embedding and vector representation methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b50">[51]</ref>. Newell et al. <ref type="bibr" target="#b32">[33]</ref> propose associative embedding to group body keypoints. Papandreou et al. <ref type="bibr" target="#b48">[49]</ref> use geometric embedding representation to predict offset vectors of keypoints. Cao et al. <ref type="bibr" target="#b13">[14]</ref> use part affinity vector field to supervise the part prediction. In addition, Hinton et al. <ref type="bibr" target="#b49">[50]</ref> use matrix with extra scalar to represent an entity. Sabour et al. <ref type="bibr" target="#b50">[51]</ref> propose Activity Vector, its length can represent existing probability and its orientation represents the instantiation parameters. In this work, we view each type of keypoint as a type of entity in the image and use activity vectors to locate keypoints to estimate human pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. POSE NEURAL FABRICS SEARCH A. Overview</head><p>Based on top-down method, 2D human pose estimation aims to locate all K keypoints coordinates of body joints(e.g.,  <ref type="figure">Fig. 4</ref>. An overview of inner structure of a cell in scale s and l layer. To simply present the inner structure of cell, we set the number of hidden nodes H as 2. Actually, its number of hidden nodes can be 1 or more than 2, each hidden node h H is densely connected from its previous nodes {h 0 , h 1 , ..., h H }.</p><p>shoulder, wrist, knee, etc) in a single pose. Let S = {(x i , y i )|x, y ∈ R + , i = 1, 2, ..., K} denote the set consisting of all keypoints coordinates. Considering human body skeleton structures, we disentangle the whole body pose into P part representations as P prediction sub-tasks. P subnetworks are constructed from CNFs sharing backbone to predict related keypoints subset s (s ⊆ S) whose element is associated with the corresponding part. The vector in pixel method is introduced to capture keypoint's feature in a relaxed vector space and the prediction of specified keypoint's location is determined by the location of vector v whose length is the largest. The entire framework is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>In Section III-B, III-C, III-F, we demonstrate how to design and employ parameterized CNFs as the choice of subnetworks and backbone. Then, we describe how to randomly sample architectures and optimize the models by synchronous optimization. In Section III-E, we describe what prior structures of human body are employed to guide neural architecture search. In Section III-D, we demonstrate how to utilize the vector representation to estimate keypoints' locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Architecture Search Space</head><p>Micro structure. Cell is a repeatable unit across different layers and scales of the whole architecture. Illustrated in  <ref type="figure">Fig. 4</ref>, it receives outputs from previous layer's cells as its single input node I and it has H nodes as it's hidden states. Each hidden node h j is connected by a directed edge with each element of candidate nodes set {h 0 , h 1 , h 2 , ..., h j−1 } (h 0 = I, j = 1, 2, ..., H). Continuous Relaxation <ref type="bibr" target="#b0">[1]</ref> method is adopted to represent each directed edge with mixed operations. For each h j is computed by:</p><formula xml:id="formula_1">h j = j−1 i=0 o∈O exp α (i,j) o o ∈O exp α (i,j) o o(h i ),<label>(1)</label></formula><p>where O is the set of candidate operations. We choose 6 types of basic operations o, consisting of: zero, skip connection, 3×3 depthwise separable convolution, 3 × 3 dilated convolution with 2 rate, 3 × 3 average pooling, 3 × 3 max pooling. α </p><formula xml:id="formula_2">(i,j) o |∀o ∈ O, ∀j ∈ {1, 2, ..., H} , ∀i ∈ {0, 1, ..., j − 1} , α s,l ∈ R O × H(H+1) 2</formula><p>. All hidden nodes {h 1 , h 2 , ..., h H } are concatenated together and reduced in channels by a 1 × 1 conv to achieve an independent output node O s,l .</p><p>Macro Structure. For Cell s,l , it receives the sum of outputs from cells in the previous layer: O 2s,l−1 , O s 2 ,l−1 and O s,l−1 . They are associated with macro architecture parameters β s,l = β l 2s , β l s , β l s 2 ∈ R 3 , which are normalized to control different information reception level from previous cells in different scales. All β s,l in all cells of the fabric construct the macro continuous search space. For each Cell s,l , its I is computed by:</p><formula xml:id="formula_3">I = h 0 = (O,β)∈Z s,l exp (β) β ∈β s,l exp (β ) T (O),<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">Z s,l = (O 2s,l−1 , β l 2s ), (O s,l−1 , β l s ), (O s 2 ,l−1 , β l s 2</formula><p>) and T (·) is scale transformation operation. In particular, T (O 2s,l−1 ) is downsampling operation via Conv-BN-ReLU mode with 2 stride (meanwhile doubling the channels of data node), T (O s 2 ,l−1 ) is upsampling operation via bilinear interpolation (meanwhile halving the channels of data node by 1×1 conv) and T (O s,l−1 ) means identity transformation.</p><p>Parametric Form. In summary, we parameterize the form of cell in the l-th layer and s-th scale of CNF in such a pattern:</p><formula xml:id="formula_5">O s,l = Cell s,l O 2s,l−1 , O s,l−1 , O s 2 ,l−1 ; w s,l , α s,l , β s,l , (3)</formula><p>where w s,l represents the weights of all operations in each cell, α s,l and β s,l encode architecture search space inside the cell. The hyperparameter of each cell is θ s,l = (H, C, O). H is the number of hidden nodes in each cell. C is the channel factor for each node to control the model capacity, i.e., the number of channels of its data node is C × 1 s . O is the set of candidate operations. This form also can be seen as a scalable topology structure or a network generator <ref type="bibr" target="#b7">[8]</ref>, which can map from a parameter space Θ to a space of neural network architectures N , formulated as g : Θ −→ N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Constructing Subnetworks or Backbone</head><p>Benefit from its local homogeneous connectivity pattern as shown in the left of <ref type="figure" target="#fig_3">Fig. 5</ref>, cell-based fabric is very flexible and easy to extend into different layers and scales for high and low resource use cases. It is determined by a group of hyperparameters Θ = (L, 1/1, 1/2, ..., 1/2 b , H, C, O) where L is total layers and 1/2 b is the smallest scale. Illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>, fabric backbone can be constructed by reserving the first m layers and discarding the latter L − m layers to produce feature pyramid in multiple scales. Likewise, fabric subnetwork can be constructed by reserving the latter n layers and discarding the first L−n layers to receive feature pyramid from backbone. Note that our backbone is not restricted to the proposed architecture, and we do not use 3×3 average pooling and 3×3 max pooling as candidate operations for subnetworks as they are empirically more suitable for extracting low-level visual feature rather high-level feature.</p><p>Following common practice for pose estimation, the smallest scale is set to 1/32. We use a two-layer convolutional stem structure to firstly reduce the resolution to 1/4 scale, and consecutively weave the whole CNF 2 . In order to achieve a higher resolution feature map to locate keypoints' coordinates, we just use the final cell's output in 1/4 scale as the part representation.</p><p>Finally, we use P subnetworks (multi-head) sharing backbone to produce P part representations, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Thus, the local information and pairwise relationships between highly correlated keypoints are combined in each part representation and predicted by each subnetwork. The global information and constraint relationships among all parts are implicitly learned by the shared feature and predicted by the backbone. The longrange and short-range constraint relationships of the human pose are enforced by the whole architecture in an end-to-end learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Body Part Representation with Vector in Pixel</head><p>Based on the top-down method of pose estimation, we estimate human pose with single person proposal. Given a input image I ∈ R H×W ×3 centered at a person proposal, there will be P part representations T 1 , T 2 , ..., T P to be predicted. Let T p ∈ R J×h ×w ×d denote its p-th body part representation, where J is the number of keypoints belonging to this part (see the assignment in Tab I), i.e. a part what we mean here may associate several keypoints. h and w are the height and width of part representation; in our down-sampling setting,</p><formula xml:id="formula_6">h /H = w /W = 1/4. d is the dimension of vector in each pixel position. For the i-th keypoints of T p , vector in position (x, y) is denoted as v i,x,y = T i p (x, y) ∈ R d , simplified</formula><p>as v. Note that the dimension d of vector is set to 8 by default and choice for dimension is discussed in Section IV .</p><p>We relax scalar value into a latent vector as keypoint entity in each image pixel, expecting it to implicitly capture the redundant and local feature information around the keypoint position, please see more explanation in Appendix A.</p><p>Besides inherent to the characteristics of encoding locations, v can represent existing probability of keypoints by using Squashing Function f S (·) <ref type="bibr" target="#b50">[51]</ref> to normalize its 2 norm to [0, 1). Formally, for i-th keypoint of p-th part, the non-linear Squash function will compute the squashed vector v s in each</p><formula xml:id="formula_7">position (x, y) of T i p by: v s = v 2 1 + v 2 v v ,<label>(4)</label></formula><formula xml:id="formula_8">v s = f S ( v) = v 2 1 + v 2 ,<label>(5)</label></formula><p>where v s exactly represents i-th keypoint's existing score in position (x, y). In inference, the position (x,ȳ) of the longest (max-norm) v will be regarded as keypoint location. Predicted score maps are collected from all part presentations. Commonly, groundtruth score map H gt k is generated from the groundtruth of k−th keypoint's position by applying 2D Gaussian with deviation of σ where the peak value equals 1 and σ controls the spread of the peak. Train loss L train is computed by Mean Square Error (MSE) between the predicted score maps and groundtruth score maps for all P part representations.</p><p>Note that the i-th keypoint of the part T p is some type of all keypoints, and several parts may contain the same type of keypoint, e.g. elbow maybe fall into the upper arm part and the lower arm part. Thus we make a indicator function 1 (k, p, i) equal 1 if the local index i-th keypoint of the part T p is the global index k-th type of all keypoints otherwise 0. And the final position (x,ȳ) will be inferred by the prediction summed from these parts (same effect by averaging). We finally formulate the training loss as:</p><formula xml:id="formula_9">L train = 1 K K k=1 x y Ĥ k (x, y) − H gt k (x, y) 2 2 , (6) H k (x, y) = p,i f S T i p (x, y) · 1 (k, p, i) ,<label>(7)</label></formula><p>whereĤ k is the prediction map for each keypoint, (x, y) is each location of T i p ,Ĥ k and H gt k . In practice, we will mask out the keypoints without position annotation in the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Prior Knowledge of human body structure</head><p>The intuition behind this work is that using part-specific neural networks to separately capture each pairwise spatial relationship in local part is possibly better than using a shared neural network to model the global relationship. Therefore, we consider the human body structures and adopt four types of grouping strategy to make comparison. Specially, P = 1 means that we model long-range dependencies relationships of pose and global relationship of all joints is learned by a shared neural network. P = 3 means that body pose is predefined into 3 parts associated with 3 CNFs: head part, upper limb part and lower limb part. P = 8 means that body pose is predefined into 8 parts associated with 8 CNFs: head-shoulder, left upper arm, left lower arm, right upper arm, right lower arm, thigh, left lower leg and right lower leg. In addition, we also adopt the data-driven grouping strategy of <ref type="bibr" target="#b20">[21]</ref> by setting P = 5: head-shoulder, left lower arm, right lower arm, thigh, lower limb part. In Tab I, all skeleton keypoints are associated with the corresponding part according to the prior body structure. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Optimization</head><p>One-shot Search and Part-specific architectures for Different Parts. Given a hyperparameter Θ for a CNF, the weights w o = {w s,l } and architecture α o = {α s,l } , β o = {β s,l } are optimized. Following the principle of one-shot architecture search <ref type="bibr" target="#b5">[6]</ref>, we assume that α s,l share same weights across a single CNF and β s,l is cell-wise in each CNF. Supposing that there are P CNFs, their weights of operations are w = {w 0 , ..., w P } and the total architecture parameters are α = {α 1 , ..., α P } , β = {β 1 , ..., β P }. We search for part-specific CNF to adapt each prediction sub-task, which means that the architecture parameters α 1 , ..., α P are totally distinct and so are β 1 , ..., β P . In the Section IV-B0b, we make contrastive experiments to study the differences on different keypoints groups strategies. Random Sampling. Random search can be seen as a powerful baseline for neural architecture search or hyperparameter optimization <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. It is conducted in <ref type="bibr" target="#b0">[1]</ref> as well and has a competitive result compared with the gradient-based method. In this work, we randomly initialize values of α, β by standard normal distribution and make them fixed in the whole training process. From another point of view, this also can be viewed as a stochastic network generator like <ref type="bibr" target="#b7">[8]</ref>. The sampled architecture parameters make no assumption about the structures, and only the weights of neural networks are optimized. Therefore, we conduct it to validate the design of CNF, each random experiment also can represent the performance of CNF without any neural architecture search strategies.</p><p>Synchronous Optimization. In DARTS <ref type="bibr" target="#b0">[1]</ref>, the architecture search problem is regarded as a bilevel optimization problem. An extra subset val of original train set is held out serving as performance validation to produce the gradient w.r.t. architecture parameters α, β excluding the weights w. However, the training of the second-order gradient-based method is still time-consuming and restricted by GPU memory due to the high resolution intermediate representation for pose estimation. Moreover, the training for parent continuous network and the derived net are inconsistent in DARTS, final pruned network needs to be trained again with all training samples. There are works attempting to eliminate this problem, such as SNAS <ref type="bibr" target="#b9">[10]</ref> introducing Gumbel-Softmax trick. Instead, we explore a more simple way as our major optimization strategy, benefit from the parametric form of cell. The α and β are registered to model's parameters, synchronously optimized with the weights w, i.e. the w, α and β are updated by ∇ w,α,β L train in a single step of gradient descent. Without extra validation phase, the final continuous α, β and the weights w have seen all training samples, thus it do not need to be pruned into discrete architecture by argmax operation and trained again from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A series of experiments have been conducted on two datasets MPII Human Pose Dataset <ref type="bibr" target="#b51">[52]</ref> and COCO Keypoint <ref type="bibr" target="#b52">[53]</ref>. In Section IV-A and Section IV-B, we show the implementation details and ablation study for the effectiveness of the purposed optimization strategies, part-based pose representation and the vector in pixel method. Then we compare our model with a part-based baseline model and an efficient light-weight model, in Section IV-C. Finally, in Section IV-D, we present the results compared with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>As for most subnetworks in ablation study, we set C=10 and total final layers is 3 (discarding first 3 layers of CNF architecture with L = 6), and the total number of cells is 6 as a basic configuration. Backbone with fabric can be constructed as described in Section III-C. To make fair comparison with methods using model pretrained on ImageNet <ref type="bibr" target="#b55">[56]</ref>, we take Fabric-1, Fabric-2, Fabric-3, pretrained Mobilenet-V2 <ref type="bibr" target="#b56">[57]</ref>, ResNet-50 <ref type="bibr" target="#b57">[58]</ref> and HRNet-W32-Stem-Stage3 <ref type="bibr" target="#b19">[20]</ref> feature blocks (5.8M, 6.8M, 10.5M, 1.3M, 23.5M, 8.1M parameters respectively) as choices for backbone to provide feature pyramid to subnetworks, see configuration details in Appendix B-A and B-B.</p><p>We implement our work by PyTorch <ref type="bibr" target="#b58">[59]</ref> and each experiment is conducted on a single NVIDIA Titan Xp GPU. Training epoch is 200 and batchsize is set to 24 (not fixed). We use Adam <ref type="bibr" target="#b59">[60]</ref> optimizer to update the weights and architecture parameters with 0.001 initial learning rate, decay at epoch 90, 120, 150 with 0.25 factor by default. Data augmentation strategies are used with random rotation range in [−45 • , 45 • ], random scale range in [0.7, 1.3] and random flipping with 0.5 probability. Flip test is used in inference. In practice, one quarter offset from the peak to the secondary peak is introduced to reduce the quantization error. Strategies mentioned above are adopted in all ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>Dataset and Evaluation. We conduct ablation study on MPII Human Pose Dataset <ref type="bibr" target="#b51">[52]</ref> which is a benchmark for evaluation of pose estimation. The dataset consist of around 25K images containing over 40K people with annotated body joints. All models in ablation study experiments are trained on a subset MPII training set and evaluate on a held validation set of 2958 images following <ref type="bibr" target="#b18">[19]</ref>. The standard PCKh metric (head-normalized probability of correct keypoint) is used for MPII. PCKh@0.5 means that a predicted joint is correct if its position is within 50% of the length groudtruth head box from its groundtruth location. Evaluation procedure reports the PCKh@0.5 of head, shoulder, elbow, wrist, hip, knee, ankle, mean PCKh@0.5 and mean PCKh@0.1. a) Optimization Strategies: For random search strategy, we conduct 5 experiments with different pseudo random seeds, each experiment costs 0.8 days for a single GPU. Result shows that completely random architecture parameters can perform well. As shown in Tab II, synchronous optimization is effective as well as random search under the same configuration and search time. We observe that the best result of random initialization for architecture surpasses the synchronous optimization, this actually reveals two points: 1) the search space design has more significant impact on the performance of neural network; 2) the parameter initialization becomes important when not leveraging NAS to the proposed CNF structure. In addition, we implement the first-order gradientbased optimization method according to the official code 4 of DARTS <ref type="bibr" target="#b0">[1]</ref> for comparison. We hold out half of MPII training data as validation for performance estimation of architecture. Another Adam optimizer is used to update α, β with 0.003 learning rate and 0.001 weight decay, discrete architecture is not derived from continuous architecture for full training. Note that we can not theoretically prove the found architecture is the optimal one, but we find that there is almost no performance difference between the synchronous optimization and the firstorder gradient-based optimization proposed by DARTS, which 4 https://github.com/quark0/darts/blob/master/cnn/architect.py  demonstrates its effectiveness compared with the first-order optimization.</p><p>b) Body Part Representation Modes: We study these four modes predefined by the prior knowledge and results are shown in Tab III. We choose MobileNet-v2 <ref type="bibr" target="#b56">[57]</ref> feature blocks as backbone. We find that multiple part presentations predicted by part-specific CNFs surpasses global whole-body representation predicted by a shared CNF. 8 part representations mode achieve 1% accuracy increase than whole-body representation and mode with P = 3 is a trade-off between performance and model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>c) Dimension Choices for Vector in Pixel:</head><p>We study the effect of choice for dimension d of the vector on performance by setting d with 1, 4, 8, 16. d = 1 represents the common heatmap regression approach without vector in pixel. We find that 8-dim vector has a better performance shown in Tab IV. To validate the generalization of 8-dim vector representation method, we apply it to SimpleBaseline 5 <ref type="bibr" target="#b18">[19]</ref> and HRNet 6 <ref type="bibr" target="#b19">[20]</ref>. We find that this 8-dim vector representation is effective in these two frameworks. It gains 0.23% increase in PCKh@0.5 and 0.88% increase in PCKh@0.1 than Simple-Baseline official results with little increase of complexity and 1.06% increase in PCKh@0.1 than HRNet official results. Although we find that there is no obvious boost on PCKh@0.5 metric and a little in PCKh@0.1 metric, from <ref type="figure" target="#fig_5">Fig. 6</ref> we observe that the losses of training with vector in pixel method converge faster, which implies the fitting between training data and label is more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with Baseline Model and Efficient Model a) Combination and Comparison with Baseline Model:</head><p>[21] conducts part-specific feature learning for pose estimation as well. It uses a part-based branch network (PBN) that has a shared representation extracted by a Hourglass Network and is stacked by 8 times. It represents our main competitor. For the sake of fairness, we follow the <ref type="bibr" target="#b20">[21]</ref> to construct the Branchnet serving as the subnetworks but not stack the whole module repeatedly. We choose the ImageNet pretrained HRNet-W32 <ref type="bibr" target="#b19">[20]</ref> as backbone by replacing blocks of the last stage, which only retains 8.1M parameters. And then we make a comparison between NAS-based CNFs and hand-designed Branchnets. By controlling their parameters amount to be similar, we find that using NAS to learn part-specific CNFs has achieved significantly higher AP (89.9 vs. 89.0) and less FLOPs (8.3G vs. 23.7G) than using a fixed structure for each part, as shown in Tab VI. With 16.4M model size and 9.4 GFLOPs, our model achieves 90.1/39.4AP accuracy in PCKh@0.5/PCKh@0.1 metrics, in contrast to 90.3/37.7AP reported from <ref type="bibr" target="#b19">[20]</ref> by HRNet-W32 (28.5M, 9.5G FLOPs) in single-scale testing. We can see that with obviously fewer parameters (↓58%) compared with the stage4 block of HRNet, the searched CNFs can maintain the high performance at PCKh@0.5 metric and improve PCKh@0.1 metric performance significantly.</p><p>To validate whether the prior knowledge is significant to help NAS search part-specific neural architectures, we make a contrastive experiment by randomly partitioning keypoints into three groups: 1) l-shouder, l-ankle, l-elbow, pelvis, r-wrist, head-top; 2) upper-neck, r-knee, r-elbow, r-hip, l-wrist, throax; 3) l-hip, r-shoulder, l-knee, r-ankle. Such partitioned groups can be seen as wrong knowledge of human body structure or chaotic training signal. We use the same architecture configurations as Ours-c model, with 12.9M parameters and 9.3 GFLOPs. The best accuracy in the training process is only 81.0 AP and the model collapses at 120 epoch with 41.7 AP, as shown in Tab. VI. This result indicate that the guidance of correct prior knowledge can help search higherperformances neural architectures, but incorrect targets also make NAS degenerate. We believe that the intervention of appropriate domain knowledge would be significant for NAS to search task-specific architectures.</p><p>b) Comparison with the Efficient Model: Fast human pose distillation (FPD) <ref type="bibr" target="#b62">[63]</ref> is an ideal efficient model to compare. Though it uses knowledge distillation technique rather than NAS, but to obtain the light-weight models is the same goal for both methods. Specifically, it first trains a large Teacher network, Houglass Network, to achieve a high accuracy performance on the task-dataset. Then a small Student model also taking Hourglass network as backbone is trained by knowledge distillation. We show the trade-off  AP AP 50 AP 75 AP M AP L AR AR 50 AR 75 AR M AR L #Params #FLOPs CMU-Pose <ref type="bibr" target="#b13">[14]</ref> 0.618 0.849 0.675 0.571 0.682 -------Mask-RCNN <ref type="bibr" target="#b28">[29]</ref> 0.631 0.873 0.687 0.578 0.714 -------Associative Embedding <ref type="bibr" target="#b32">[33]</ref> 0 curves between AP and FLOPs in the (a) of <ref type="figure" target="#fig_7">Fig. 7</ref>. The results show that both methods can achieve competitive performance with less computing complexity for MPII dataset. Our smaller models have a slight advantage over the smaller models of FPD.</p><p>Pruning Useless Structures. In practical, we find that some α, β architecture parameters of the final searched architectures might be zero values. That is, cells whose outputs are multiplied by the zero β parameter have no computing contributions for the cells in the next layers; operations whose outputs are multiplied by the zero α parameter have no computing contributions for the next hidden node. To further reduce the parameters and computing complexity, we use empty cells and zero operations without parameters to replace those useless cells and operations. By this way, the architectures only retain the structures associated with non-zero architecture parameters. This method does not affect the precision and also does not need to retrain the architecture. Note that our NAS method cannot ensure there are always existing zero values in the micro or macro architecture parameters. When we cancel the learning rate decay of optimizing the architecture parameters so that they remain sensitive to be optimized in the late stage of the search process, we find that some architecture parameters converge to zero values, i.e. some types of operation or cells have no contributions to the forward computing in some searched architectures. We expect that future works can introduce sparse constraint to optimize the architecture parameters, to guarantee the architecture could be pruned by this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with the state-of-the-art a) Testing on MPII Single Person Pose Estimation:</head><p>To further evaluate our pose estimation method on test set of MPII <ref type="bibr" target="#b51">[52]</ref>, we train the model on all samples of MPII train set with early-stopping strategy. All input images are resized to 384 × 384 pixels, data augmentation is the same as mentioned above. We use pretrained MobileNet-v2 and HRNet-W32-Stem∼stage3 feature blocks as backbone. The   <ref type="bibr" target="#b64">[65]</ref>. It is worth noting that our method works well in some heavily partial occluded hard samples (such as left two images in first row and the fourth in second row).</p><p>hyperparameters are: P = 3/5, H = 2/1, d = 8/8, C = 10/16 and O includes zero, skip connection, 3 × 3 separable conv, 3 × 3 dilated conv with 2 rate. The optimization method is synchronous optimization. As shown in Tab V, we achieve a comparable result (91.0 vs. 91.1 AP and 9.4G vs. 9.0G FLOPs) compared with FPD <ref type="bibr" target="#b62">[63]</ref> that exploits knowledge distillation to achieve a lightweight and efficient model. b) COCO Keypoint Detection Task: MS-COCO <ref type="bibr" target="#b52">[53]</ref> dataset contains more than 200k images and 250k person instances with keypoints label. We use COCO train2017 as our training set, it consists of 57k images and 150k person instances. Val2017 set contains 5k images and test-dev2017 consists of 20k images. It is worth mentioning that some invisible keypoints are labeled on train set and statistics show that around 11.3 % of annotated keypoints are invisible according to train2017 annotations. Object keypoint similarity (OKS) is the standard evaluation metric for keypoints locating accuracy. More detailed information is available in COCO official website 7 .</p><p>COCO keypoint detection task involves detecting bodies and localizing their keypoints. Based on top-down method, we focus on single pose estimation, therefore we use the detected bounding boxes detected by Faster-RCNN [65] with 60.9 AP persons detection results on COCO test-dev2017 dataset. We respectively use pretrained MobileNet-v2 <ref type="bibr" target="#b56">[57]</ref>, ResNet-50 <ref type="bibr" target="#b57">[58]</ref> and HRNet-W32-Stem∼stage3 <ref type="bibr" target="#b19">[20]</ref> feature blocks as backbone and train two models only on train2017 set. The hyperparameters are: P = 3/3/5, H = 1/1/1, d = 8/8/8 and O includes zero, skip connection, 3 × 3 separable conv, 3 × 3 dilated conv with 2 rate. C = 16/10/16 for ours-1/ours-2/ours-3 models. The optimization method is synchronous optimization, we cancel the learning rate decay for searching the architecture parameters of ours-3 model, and prune it as described in SectionIV-C0b. The input size is 384 × 288 pixels and the OKS-NMS algorithm <ref type="bibr" target="#b14">[15]</ref> is utilized to suppress redundant detected bounding boxes . We report average precision (AP) and average recall (AR) on COCO test-dev2017 set. With fewer parameters and low computational complexity, we can achieve comparable results with some state-of-the-art methods without using any extra data, ensemble models or other training tricks, 76.2 AP (gt bbox) and 73.0 AP (detected bbox) on COCO validation set and 72.3 AP on COCO test-dev2017 set, as shown in Tab VII. This model runs at ∼20 FPS on a single GPU. The tendency of the trade-off curves between the average precision (AP) and computational cost (FLOPs) shown in the (b) of <ref type="figure" target="#fig_7">Fig. 7</ref>. can illustrate our model can achieve comparable performances with many other stateof-the-art methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Prediction results on some partial occluded hard samples can be seen in <ref type="figure" target="#fig_10">Fig. 9</ref>.</p><p>In fact, our searched models are still over-parameterized, the sizes of them also have a potential space to be further reduced by pruning and other model compressing techniques. Largecapacity models (e.g. &gt;30M, &gt;20GFLOPs) for COCO dataset are not searched, limited by its huge resource consumption caused by the gradient-based NAS search strategy. More advanced neural search strategies can be explored to overcome this problem in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we presented a new paradigm -part-specific neural architecture search for human pose estimation, in which we made the first attempt to exploit prior knowledge of human body structure to search part-specific neural architectures automatically. Such paradigm develops the part-based method and gives a new example as Multi-task Neural Architecture Search. Experiment results showed that our light-weight models achieved comparable results on MPII dataset with fewer parameters and lower computational complexity than some stateof-the-art methods. For more challenging COCO keypoint detection task, our light-weight model attained comparable results to some state-of-the-art methods with fewer parameters. In addition, we empirically demonstrate the effectiveness of representing the human body keypoints as vector entities at image locations. We hope that these ideas may be helpful to adequately leverage NAS to practical application for human pose estimation task or other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A EXPLANATION FOR THE RELATIONSHIP BETWEEN</head><p>EXPECTED v AND SUPERVISION LEVEL p In the section III-D, we use Squash Function to normalize v to v s whose length ranges in [0, 1),</p><formula xml:id="formula_10">v s = v 2 1 + v 2 v v ,<label>(8)</label></formula><formula xml:id="formula_11">v s = v 2 1 + v 2 ,<label>(9)</label></formula><formula xml:id="formula_12">v = 1 1 − v s − 1,<label>(10)</label></formula><p>where v s is supervised by numerical value p in each pixel position from groundtruth score maps. Ideal value of v s , denoted as v * , equals to c ∈ [0, 1). Therefore, v is supervised by 1 1−c − 1. Intuition and Explanation. The extra advantage of vector in pixel is that ambiguity between image feature and groundtruth position can be reduced in some cases of occlusion. In supervised learning, the difficulty of fitting label is usually not under consideration, hard or easy samples of the same category receive the same level of supervision. This issue occurs in keypoints localization because image appearance varies dramatically in some partially occluded and non-occluded areas. Once the area within the keypoint groundtruth position is occluded, the image feature around the keypoint will be disturbed, (e.g. the first image in the <ref type="figure" target="#fig_10">Fig. 9</ref>, the man's ankle is occluded by a dog, but his ankle's position is labeled), as a result it becomes hard to force the network to predict high confidence to match the strong supervision. In such case, our method can handle it as v s replaces v under supervision (element value of each dimension of vector has no explicit property and is unsupervised) and the expected length for v in groundtruth keypoint pixel is not directly supervised by numerical value c from groundtruth score but supervised by 1 1−c − 1 ∈ [0, +∞) where c ∈ [0, 1). In a slight abuse of notation, we write v * as the expected length of v, which provides a relatively loose range space for v, even if under strong supervision.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B MOBLIENET-V2 BACKBONE ARCHITECTURE DETAILS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>A schematic map of the structure of Cell-based Neural Fabric (CNF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Pose neural fabrics search framework. Left: Given an input image, feature pyramid representing multi-scale feature maps will be produced from backbone network. Then there will be P CNFs receiving the same feature pyramid and predicting P part representations, here two CNFs shown for simplification. The final cell in the highest scale produces the part representation. Dashed lines mean unused connections and cells. Mid: The whole body is divided into multiple parts associated with keypoints. Right: For instance, right lower arm part representation is associated with the wrist and elbow keypoints. v i and v j mean two d-dim vectors respectively for wrist and elbow keypoints at location (x, y) of part representation feature map, and it's shape is 2 × w × h × d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>An overview of the CNF neural search space. Left: The homogeneous local connectivity between cells in a neural fabric. Right: Examples of constructing a CNF-backbone (red box) or a CNF-subnetwork (blue box) from CNF. Dashed lines mean unused connections and cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weight for each operation o ∈ O in edge h i → h j . For a specified Cell s,l in scale s and layer l in CNF, the continuous search space at micro level is: α s,l = α</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Train losses of experiments w and w/o vector in pixel method. Detailed configurations are described in Tab IV. The sudden drop at epoch 90 is caused by learning rate decay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>(a): The mAP of PCKh@0.5 metric vs. model inference complexity (GFLOPs) on MPII val set. (b): The AP of OKS@0.5:0.95 c vs. model inference complexity (GFLOPs) on COCO test-dev2017 dataset. Model parameters and FLOPs of detecting persons in COCO dataset are not included. The areas of the circles are linearly relative with the amounts of models' parameters. TABLE VII COMPARISONS OF PERFORMANCE, MODEL PARAMETERS AND INFERENCE COMPLEXITY ON COCO TEST-DEV SET. MODEL PARAMETERS AND FLOPS OF DETECTING PERSONS ARE NOT INCLUDED. THE BACKBONES OF OURS-1, OURS-2 AND OURS-3 MODELS ARE MOBILENET-V2 (1.3M), RESNET-50 (23.5M) AND HRNET-W32-STEM∼STAGE3 (8.1M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>0.904 0.777 0.667 0.782 0.766 0.941 0.829 0.715 0.836 27.5M 11.4G Ours-3 0.723 0.909 0.795 0.684 0.792 0.779 0.945 0.844 0.731 0.845 15.8M 14.8G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative pose estimation results on MPII val set for single person pose estimation. We show the cropped image regions containing human body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Qualitative pose estimation results on COCO val2017 set. Estimation is conducted on bounding boxes detected by Faster-RCNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>A. CNF Backbone Architecture Details B. MoblieNet-V2 Backbone Architecture Details</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ACCORDING</head><label>I</label><figDesc>TO THE PRIOR KNOWLEDGE OF HUMAN BODY STRUCTURE, THERE ARE DIFFERENT GROUPING TYPES OF BODY PART REPRESENTATIONS.</figDesc><table><row><cell>Representation Mode</cell><cell>Group Name</cell><cell cols="2">Index MPII COCO</cell></row><row><cell>P = 1</cell><cell>all keypoints</cell><cell>0-15</cell><cell>0-16</cell></row><row><cell></cell><cell>head part</cell><cell>0-2</cell><cell>0-4</cell></row><row><cell>P = 3</cell><cell>upper limb part</cell><cell>3-8</cell><cell>5-10</cell></row><row><cell></cell><cell>lower limb part</cell><cell>9-15</cell><cell>11-16</cell></row><row><cell></cell><cell>head-shoulder</cell><cell>0-4</cell><cell>0-6</cell></row><row><cell></cell><cell>left lower arm</cell><cell>5,7</cell><cell>7,9</cell></row><row><cell>P = 5</cell><cell>right lower arm</cell><cell>6,8</cell><cell>8,10</cell></row><row><cell></cell><cell>thigh</cell><cell>9,10,15</cell><cell>11,12</cell></row><row><cell></cell><cell>lower limb part</cell><cell>11-14</cell><cell>13-16</cell></row><row><cell></cell><cell>head-shoulder</cell><cell>0-4</cell><cell>0-6</cell></row><row><cell></cell><cell>left upper arm</cell><cell>3,5</cell><cell>5,7</cell></row><row><cell></cell><cell>left lower arm</cell><cell>5,7</cell><cell>7,9</cell></row><row><cell>P = 8</cell><cell>right upper arm right lower arm</cell><cell>4,6 6,8</cell><cell>6,8 8,10</cell></row><row><cell></cell><cell>thigh</cell><cell>9-12,15</cell><cell>11-14</cell></row><row><cell></cell><cell>left lower leg</cell><cell>11,13</cell><cell>13,15</cell></row><row><cell></cell><cell>right lower leg</cell><cell>12,14</cell><cell>14,16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II OPTIMIZATION</head><label>II</label><figDesc>STRATEGIES (SEARCH METHOD). WE CHOOSE MOBILENET-V2 AS THE BACKBONE OF MODEL. P = 3, H = 1, C = 10, d = 8, EACH SUBNETWORK HAS SIX CELLS AND TOTAL PARAMETERS OF MODEL IS 3.3M, THE MADDS OF MODEL INFERENCE COMPLEXITY FOR SINGLE INPUT SAMPLE IS 1.2 GFLOPS. WE CHOOSE MOBILENET-V2 AS THE BACKBONE OF MODEL. H = 1, C = 10, d = 8, EACH SUBNETWORK HAS SIX CELLS.</figDesc><table><row><cell>Search Method</cell><cell>Search Time</cell><cell>Mean</cell><cell>Mean</cell></row><row><cell>(search strategy)</cell><cell cols="3">(GPU days) (PCKh@0.5) (PCKh@0.1)</cell></row><row><cell>Random</cell><cell>0.8</cell><cell>87.1±0.2</cell><cell>35.2±0.4</cell></row><row><cell>First-order gradient-based</cell><cell>0.9</cell><cell>87.1</cell><cell>34.7</cell></row><row><cell>Synchronous gradient-based</cell><cell>0.8</cell><cell>87.0</cell><cell>34.6</cell></row><row><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="4">BODY PART REPRESENTATION MODES. Representation Mode Mean(PCKh@0.5) Mean(PCKh@0.1)</cell></row><row><cell>P = 1</cell><cell>86.4</cell><cell></cell><cell>33.4</cell></row><row><cell>P = 3</cell><cell>87.0</cell><cell></cell><cell>34.6</cell></row><row><cell>P = 5</cell><cell>87.1</cell><cell></cell><cell>35.1</cell></row><row><cell>P = 8</cell><cell>87.3</cell><cell></cell><cell>35.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV DIMENSION</head><label>IV</label><figDesc>CHOICES FOR VECTOR IN PIXEL. WE CHOOSE MOBILENET-V2 AS THE BACKBONE OF MODEL. P = 3, H = 1, C = 10, EACH SUBNETWORK HAS SIX CELLS AND TOTAL PARAMETERS OF MODEL IS 3.3M, THE MADDS (FLOPS) OF MODEL INFERENCE COMPLEXITY FOR SINGLE INPUT SAMPLE IS 1.2 GFLOPS. † IS THE RESULT ACHIEVED BY RUNNING THE OFFICIAL CODE ON OUR MACHINE, THE OFFICIAL IS 90.3.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0010</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>w/o Vector in Pixel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0009</cell><cell></cell><cell></cell><cell></cell><cell>w/ Vector in Pixel (d=4)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average train loss per epoch</cell><cell>0.0005 0.0006 0.0007 0.0008</cell><cell></cell><cell></cell><cell></cell><cell>w/ Vector in Pixel (d=8) w/ Vector in Pixel (d=16)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0004</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>25</cell><cell>50</cell><cell>75</cell><cell>100 Epoch</cell><cell>125</cell><cell>150</cell><cell>175</cell><cell>200</cell></row><row><cell>Dimension</cell><cell cols="4">Mean(PCKh@0.5) Mean(PCKh@0.1) #Params #Madds(FLOPs)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>d = 1(scalar)</cell><cell>86.8</cell><cell>33.5</cell><cell>3.3M</cell><cell>1.1G</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>d = 4</cell><cell>86.9</cell><cell>34.9</cell><cell>3.3M</cell><cell>1.1G</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>d = 8</cell><cell>87.0</cell><cell>34.6</cell><cell>3.3M</cell><cell>1.2G</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>d = 16</cell><cell>86.8</cell><cell>34.9</cell><cell>3.3M</cell><cell>1.2G</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimpleBaseline [19]</cell><cell>88.5</cell><cell>33.9</cell><cell>34.0M</cell><cell>12.0G</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ vector(8-dim)</cell><cell>88.7</cell><cell>34.2</cell><cell>34.0M</cell><cell>12.1G</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HRNet [20]</cell><cell>90.1 †</cell><cell>37.7</cell><cell>28.5M</cell><cell>9.5G</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ vector(8-dim)</cell><cell>90.2</cell><cell>38.1</cell><cell>28.5M</cell><cell>9.6G</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V COMPARISONS</head><label>V</label><figDesc>OF PERFORMANCE, MODEL PARAMETERS AND INFERENCE COMPLEXITY ON MPII TEST SET. THE BACKBONES OF OURS-A AND OURS-B MODELS ARE MOBILENET-V2 (1.3M) AND HRNET-W32-STEM∼STAGE3 (8.1M). BASELINE MODELS ON MPII VALIDATION SET. WE CHOOSE HRNET-W32-STEM∼STAGE3 AS THE BACKBONE, WHICH OUTPUTS SHARED FEATURE PYRAMIDS WITH FOUR LEVELS FO FEATURE MAPS. INPUT SIZE IS 256 × 256. OUR-C-CNF: P = 3, H = 2, C = 12. OUR-B-CNF: P = 5, H = 1, C = 16. O INCLUDES ZERO, SKIP CONNECTION, 3 × 3 SEPARABLE CONV, 3 × 3 DILATED CONV WITH 2 RATE. HERE WE USE A 1 × 1 CONVOLUTION TO FIRST REDUCE THE FEATURE DIMENSION TO W = 400/296 AND THEN D = 8/8 SUBSEQUENT RESIDUAL BLOCKS FOR THE BRANCHNETS OF BASELINE-1 AND BASELINE-2. † IS THE RESULT ACHIEVED BY RUNNING THE OFFICIAL CODE ON OUR MACHINE, THE OFFICIAL RESULT IS 90.3 AP. ‡ REPRESENTS THE METRIC MEAN PCKH@0.5 OVER TEN HARD JOINTS, REPORTED IN [21].</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="6">Head Shoulder Elbow Wrist Hip Knee Ankle Total #Params #FLOPs</cell></row><row><cell cols="2">Tompson et al. [46]</cell><cell>95.8</cell><cell>90.3</cell><cell cols="2">80.5 74.3 77.6 69.7 62.8 79.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Belagiannis &amp; Zisserman [34]</cell><cell>97.7</cell><cell>95.0</cell><cell cols="2">88.2 83.0 87.9 82.6 78.4 88.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Wei et al. [13]</cell><cell>97.8</cell><cell>95.0</cell><cell cols="2">88.7 84.0 88.4 82.8 79.4 88.5</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Insafutdinov et al. [30]</cell><cell>96.8</cell><cell>95.2</cell><cell cols="4">89.3 84.4 88.4 83.4 78.0 88.5 42.6M 41.2G</cell></row><row><cell cols="2">Bulat&amp;Tzimiropoulos [61]</cell><cell>97.9</cell><cell>95.1</cell><cell cols="2">89.9 85.3 89.4 85.7 81.7 89.7</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Newell et al. [62]</cell><cell>98.2</cell><cell>96.3</cell><cell cols="4">91.2 87.1 90.1 87.4 83.6 90.9 25.1M 19.1G</cell></row><row><cell cols="2">Xiao et al. [19]</cell><cell>98.5</cell><cell>96.6</cell><cell cols="4">91.9 87.6 91.1 88.1 84.1 91.5 68.6M 20.9G</cell></row><row><cell cols="2">Tang et al. [35]</cell><cell>98.4</cell><cell>96.9</cell><cell cols="4">92.6 88.7 91.8 89.4 86.2 92.3 15.5M 33.6G</cell></row><row><cell cols="3">FPD (Knowledge Distillation) [63] 98.3</cell><cell>96.4</cell><cell cols="2">91.5 87.4 90.9 87.1 83.7 91.1</cell><cell>3M</cell><cell>9G</cell></row><row><cell></cell><cell>Ours-a</cell><cell>97.9</cell><cell>95.6</cell><cell cols="3">90.7 86.5 89.8 86.0 81.5 90.2 5.2M</cell><cell>4.6G</cell></row><row><cell></cell><cell>Ours-b</cell><cell>98.2</cell><cell>95.9</cell><cell cols="3">91.5 87.6 90.1 87.3 83.2 91.0 16.4M</cell><cell>9.4G</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell>COMPARISON WITH Method</cell><cell>Backbone</cell><cell></cell><cell>Head</cell><cell></cell><cell cols="3">NAS Part-specific Mean@0.5 Mean@0.1 #Params #FLOPs</cell></row><row><cell>SimpleBaseline [19]</cell><cell>ResNet-152 (52.2M)</cell><cell></cell><cell cols="2">DeConvs (16.4M)</cell><cell>89.6</cell><cell>35.0</cell><cell>68.6M</cell><cell>20.9G</cell></row><row><cell>HRNet-W32 [20]</cell><cell>Stem∼stage3 (8.1M)</cell><cell></cell><cell cols="2">Stage4(19.7M)</cell><cell>90.1 †</cell><cell>37.7</cell><cell>27.8M</cell><cell>9.5G</cell></row><row><cell cols="5">Stacked PBNs [21] Stacked Hourglass Network Stacked Branchnets</cell><cell>88.14 ‡</cell><cell>-</cell><cell>26.7M</cell><cell>-</cell></row><row><cell>Baseline-1</cell><cell>Stem∼stage3 (8.1M)</cell><cell cols="3">Branchnet × 3 (4.7M)</cell><cell>89.0</cell><cell>38.2</cell><cell>12.8M</cell><cell>23.7G</cell></row><row><cell>Baseline-2</cell><cell>Stem∼stage3 (8.1M)</cell><cell cols="3">Branchnet × 5 (4.5M)</cell><cell>89.3</cell><cell>38.7</cell><cell>12.6M</cell><cell>22.8G</cell></row><row><cell>Random partition</cell><cell>Stem∼stage3 (8.1M)</cell><cell></cell><cell cols="2">CNF × 3 (4.8M)</cell><cell>81.0</cell><cell>30.5</cell><cell>12.9M</cell><cell>8.3G</cell></row><row><cell>Ours-c</cell><cell>Stem∼stage3 (8.1M)</cell><cell></cell><cell cols="2">CNF × 3 (4.8M)</cell><cell>89.9</cell><cell>39.5</cell><cell>12.9M</cell><cell>8.3G</cell></row><row><cell>Ours-b</cell><cell>Stem∼stage3 (8.1M)</cell><cell></cell><cell cols="2">CNF × 5 (8.3M)</cell><cell>90.1</cell><cell>39.4</cell><cell>16.4M</cell><cell>9.4G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VIII THE</head><label>VIII</label><figDesc>DETAILED CONFIGURATIONS FOR FABRIC-1,2,3. THE LAYERS RESERVED MEANS THE NUMBER OF LAYERS RESERVED BY DISCARDING THE IN THE LATTER LAYERS OF CNF</figDesc><table><row><cell>-</cell><cell cols="3">Fabric-1 Fabric-2 Fabric-3</cell></row><row><cell>L</cell><cell>7</cell><cell>8</cell><cell>8</cell></row><row><cell>C</cell><cell>10</cell><cell>10</cell><cell>12</cell></row><row><cell>H</cell><cell>2</cell><cell>1</cell><cell>1</cell></row><row><cell>Layers reserved</cell><cell>3</cell><cell>5</cell><cell>5</cell></row><row><cell>Number of Cells</cell><cell>9</cell><cell>17</cell><cell>17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IX GIVEN</head><label>IX</label><figDesc>A H × W × 3 RGB IMAGE, THE LAYER OF MOBILENET-V2 BACKBONE. EACH LINE DESCRIBES A SEQUENCE OF 1 OR MORE IDENTICAL (MODULO STRIDE) LAYERS, REPEATED N TIMES. ALL LAYERS</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For cells in first layer, they only receive the stem's output. And for cells in 1/32 scale or 1/4 scale of its current layer, it may have only have two outputs from previous cells. In this case, we will copy one of candidate inputs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For MPII<ref type="bibr" target="#b51">[52]</ref> dataset, we set indices of head top, upper neck, thorax, l-shoulder, r-shoulder, l-elbow, r-elbow, l-wrist, r-wrist, l-hip, r-hip, l-knee, r-knee, l-ankle, r-ankle, pelvis keypoints to 0-15 orderly. For COCO<ref type="bibr" target="#b52">[53]</ref> dataset, we set indices of nose, l-eye, r-eye, l-ear, r-ear, l-shoulder, r-shoulder, l-elbow, r-elbow, l-wrist, r-wrist, l-hip, r-hip, l-knee, r-knee, l-ankle and r-ankle to 0-16 orderly.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/microsoft/human-pose-estimation.pytorch 6 https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">http://cocodataset.org/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Neural architecture search with reinforcement learning. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8699" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02985</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09926</idno>
		<title level="m">Snas: stochastic neural architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dsnas: Direct neural architecture search without parameter retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural fabrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4053" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1281" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Does learning specific features for related parts help human pose estimation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich Caruana. Multitask learning</title>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning and Data Mining</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evolutionary architecture search for deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Feature partitioning for efficient multi-task architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04339</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mtlnas: Task-agnostic neural architecture search towards general-purpose multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoping</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11543" to="11552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Heterogeneous multitask learning for human pose estimation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="482" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
	<note>Mykhaylo Andriluka, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6457" to="6465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="417" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="190" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Compositionality, mdl priors, and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="838" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attribute andor grammar for joint parsing of human pose, parts and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1555" to="1569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Articulated part-based model for joint object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tracking human pose using max-margin markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5274" to="5287" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hierarchical contextual refinement networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="924" to="936" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1212" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiscale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipeng</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="713" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Matrix capsules with EM routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frosst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Random search for hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012-02" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fast human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3512" to="3521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
