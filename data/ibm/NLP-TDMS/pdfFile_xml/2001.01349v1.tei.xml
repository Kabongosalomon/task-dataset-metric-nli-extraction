<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning and Memorizing Representative Prototypes for 3D Point Cloud Semantic and Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning and Memorizing Representative Prototypes for 3D Point Cloud Semantic and Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D point cloud semantic and instance segmentation is crucial and fundamental for 3D scene understanding. Due to the complex structure, point sets are distributed off balance and diversely, which appears as both category imbalance and pattern imbalance. As a result, deep networks can easily forget the non-dominant cases during the learning process, resulting in unsatisfactory performance. Although re-weighting can reduce the influence of the well-classified examples, they cannot handle the non-dominant patterns during the dynamic training. In this paper, we propose a memory-augmented network to learn and memorize the representative prototypes that cover diverse samples universally. Specifically, a memory module is introduced to alleviate the forgetting issue by recording the patterns seen in mini-batch training. The learned memory items consistently reflect the interpretable and meaningful information for both dominant and non-dominant categories and cases. The distorted observations and rare cases can thus be augmented by retrieving the stored prototypes, leading to better performances and generalization. Exhaustive experiments on the benchmarks, i.e. S3DIS and ScanNetV2, reflect the superiority of our method on both effectiveness and efficiency. Not only the overall accuracy but also nondominant classes have improved substantially.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent development of rapid and practical 3D sensors has provided easier ways to acquire 3D point cloud data, one of the widely used types of geometric data due to its simplicity <ref type="bibr" target="#b22">[23]</ref>. 3D scene understanding is critically important and fundamental for various applications, such as robotics, autonomous driving, and virtual reality. The core tasks include semantic segmentation and instance segmentation on point clouds, i.e. assigning semantic labels and instance indication label for each point, respectively. Com- * The first two authors contribute equally. † Corresponding author: chunhua.shen@adelaide.edu.au.  paring to the studies on 2D images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5]</ref>, semantic and instance on 3D point clouds lags far behind and have just started recently <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Based on the pioneering works PointNet <ref type="bibr" target="#b22">[23]</ref> and Point-Net++ <ref type="bibr" target="#b24">[25]</ref>, directly processing point sets becomes simpler, more memory-efficient and flexible than handling the volumetric grids with 3D convolution <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b18">19]</ref>. Some following approaches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref> propose to handle semantic and instance segmentation in an end-to-end network jointly for fine-grained description of the scene. Specifically, discriminative instance embeddings are learned to measure the instance-level clustering patterns of the points <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Although existing methods have achieved some impressive results, we still can observe performance bottlenecks on different benchmark datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, especially on the non-dominant classes with less samples (see <ref type="figure">Figure 5</ref>). Suffering from the catastrophic forgetting issue <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref>, deep networks can forget the non-dominant rare cases easily while learning on a dataset distributed off balance and diversely. On point cloud data, imbalance issue usually appears as the category imbalance and pattern imbalance, which is severer than that on 2D images <ref type="bibr" target="#b40">[40]</ref>. Firstly, discrepancy among the proportions of different categories are significant. In an indoor scene (see <ref type="figure" target="#fig_1">Figure 1</ref> and 6), most points belong to the background (e.g. ground, ceiling, and wall), whereas the proportions of the objects (e.g. chairs, desks and monitors) are much smaller. For example, in S3DIS <ref type="bibr" target="#b0">[1]</ref>, the total amount of ceiling points is 50 times larger than chair. Secondly, the patterns of the points are imbalanced and distributed diversely, which are often caused by the complex geometric informations, such as positions, shapes and relative relationships among instances. Some rare instance cases only have limited examples across the whole dataset. For example, chairs are usually placed neatly in a conference room, while can also be placed in arbitrary positions (e.g., stacking and back-to-back) in an office room, as is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. Conventional methods <ref type="bibr" target="#b40">[40]</ref> ignore this issue or simply resort to the focal loss <ref type="bibr" target="#b16">[17]</ref>, by down weighing the well learned samples during training. However, they cannot directly handle the non-dominant patterns, which can be easily overwhelmed and forgotten.</p><p>To address the above issues, we propose to learn and memorize the discriminative and representative prototypes covering all the samples, which is implemented as a memory-augmented network, referred to as MPNet. The proposed MPNet includes two branches for predicting point-level semantic labels and obtaining per-point embedding for instance grouping, respectively. As shown in <ref type="figure">Figure</ref> 2, the two branches access a shared compact memory via two separate memory readers, which dynamically calibrate the per-point features with the memory items and associate the two tasks via the shared memory. Given an input, MPNet retrieves the most relevant items in the memory for the extracted per-point features and feeds only retrieved features to the following segmentation tasks. Thus, driven by the task-specific training objectives, the compact memory is pushed to record the representative prototypes seen in mini-batches and associate them with newly seen patterns, alleviating the forgetting issues and strengthening the generalization.</p><p>In the proposed MPNet, the memory is maintained as a dictionary of the representative features and a semantic summarization, as shown in <ref type="figure" target="#fig_3">Figure 2</ref>. Since the memory is trained to represent all the instances compactly, the learned prototypes can express a shared understanding of various instances. We observe that the learned memory items (i.e. dictionary bases) can reflect interpretable and meaningful informations, such as position and structure (see <ref type="figure">Figure 3</ref>). Benefiting from the associative memory, the rare cases and distorted observations can be augmented by retrieving the stored prototypes, leading to better robustness and generalization. Additionally, different from previous methods relying on either pairwise relations computing <ref type="bibr" target="#b34">[35]</ref> or KNN based feature aggregation <ref type="bibr" target="#b36">[36]</ref>, the proposed MPNet is free from complex and time-consuming operations, which is more efficient.</p><p>The main contributions are summarized as:</p><p>• We propose a memory-augmented network for point cloud instance segmentation (i.e. MPNet), which is trained to explicitly record the prototypes of the perpoint features in a compact memory. The proposed MPNet is more effective and efficient than previous methods. • The learned prototypes can consistently represent interpretable and meaningful concepts of various instances, including dominant and non-dominant cases. • Our proposed MPNet can boost the performance by a large margin with limited consumptions on computation and memory. State-of-the-art performance is achieved, showing the superiority on both effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Deep Learning for 3D Point Cloud Existing methods for extracting features for 3D point cloud can be roughly categorized into three groups, including voxel-based <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b18">19]</ref>, multi-view based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref> and point-based <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">39]</ref> are the pioneering works to transfer irregular points to regular volumetric grids, aiming to efficiently extract feature representation with 3D convolution. To reduce irrelevant operation on void places and save runtime memory usage, many works are proposed <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10]</ref>. Multi-view based methods extract features in both 2D and 3D domain. <ref type="bibr" target="#b29">[30]</ref> is one of the pioneering multi-view based method, which apply view-pooling over the 2D predictions. 3D-SIS <ref type="bibr" target="#b12">[13]</ref>, proposed by Hou et al. , combine features from 2D and 3D via explicit spatial mapping in an endto-end trainable network. PointNet <ref type="bibr" target="#b22">[23]</ref> is the first deeplearning-based work to operate directly on point sets, which uses shared MLP (multi-layer perceptron) to extract perpoint feature. PointNet++ <ref type="bibr" target="#b24">[25]</ref> improves the performance by extracting a hierarchical representation. Many following works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b15">16]</ref> have been proposed to get a better representation of local context. Due to its simplicity, we select PointNet++ as our backbone and leave the choices of other backbones for future work. Instance Segmentation on Point Cloud Deep-learningbased instance segmentation for 3D point cloud is rarely studied until huge application potential has been discovered recently. SGPN <ref type="bibr" target="#b34">[35]</ref> is the first deep learning based method working on this field. It first splits the whole scene into separate blocks. For every single block, per-point grouping candidates are proposed by predicting a similarity matrix that reflects affinity between each pair of points. A block merging algorithm is conducted for post-processing by taking segmentation results of the overlapped area into consideration. However, huge memory is needed for storing the pair-wise matrix, which makes it memory-consuming for post-processing. In order to solve this, Wang et al. proposed ASIS <ref type="bibr" target="#b36">[36]</ref>, which utilized a discriminative loss func-  tion <ref type="bibr" target="#b1">[2]</ref> to encourage points belonging to the same instance are mapped to a metric space with close distances. Moreover, in order to make the two tasks take advantage of each other, convolution and KNN search are applied for mutual feature aggregation of the two tasks, making it inefficient and time-consuming. Memory Networks Memory based approaches have been discussed for solving various problems. NTM <ref type="bibr" target="#b10">[11]</ref> is proposed to improve the generalization ability of the network by introducing an attention-based memory module. Gong et al. <ref type="bibr" target="#b8">[9]</ref> proposed a memory augmented auto-encoder for detecting anomaly. Anomaly is detected by represented the input with prototypical elements of the normal data maintained in a memory module. Prototypical Network <ref type="bibr" target="#b28">[29]</ref> maintains a category-wise templates for the problem of fewshot classification. Liu <ref type="bibr" target="#b17">[18]</ref> proposed an OLTR algorithm to solve the open-ended and long-tail problem by associating a memory feature that can be transfered to both head and tail classes adaptively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of the Proposed MPNet</head><p>We propose to tackle the imbalance issue in point cloud semantic and instance segmentation by learning and memorizing prototypes of the cases seen during training. The discriminative and representative prototypes are stored in a memory module and can be accessed via specific readers. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, the proposed memory-augmented network (i.e. MPNet) adopts an encoder-decoder architecture, which is free from the specific design of the encoder and decoder. In the proposed MPNet, we use PointNet++ <ref type="bibr" target="#b24">[25]</ref> to implement the encoder for per-point feature extraction. Two parallel decoders for instance segmentation and semantic segmentation are built upon the shared encoder. As described in the following, the memory is implemented as a dictionary to record the prototypes as bases. For the both branches, given a per-point feature, two specifically designed memory readers are applied to generate addressing weights to access the memory, respectively, via softattention. The retrieved items from the memory are then applied for the following semantic labeling and instance grouping tasks. For any input sample, the relevant memory items are retrieved for the two tasks and also updated via prorogations driven by the task objectives and specifically designed instance regularizer (described in Section 3.3).</p><p>Given a set of input points {p i } P i=1 with p i ∈ R K , we can formulate the input of the network as a matrix P ∈ R P ×K , where K denotes the input feature dimension and P denotes the total number of input points. Input features of each points may consist of both geometry and appearance information, i.e. 3D coordinate (x, y, z) and RGB values. The two branches produces features F seg ∈ R N ×D and F ins ∈ R N ×D , respectively, where D denotes the dimension of features. Instead of directly using F seg and F ins to perform semantic and instance segmentation tasks, respectively, MPNet applies them as queries to retrieve the prototypes in the memory and then obtain alternative features F seg and F ins , which are delivered to the following semantic classifier and instance embedding module. The memory is randomly initialized and updated during training. The two branches access the memory with specifically designed read heads,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory Representation for Prototypes</head><p>The prototypes memory is designed as a matrix M ∈ R N ×D , where N is a hyper-parameter that defines the number of memory slots and D is the feature dimension that is identical with the outputs from the two branches. The N memory slots are used to restore the prototypes shared by all the instances across all categories. To easily represent the semantic characteristics, we define a semantic memory C ∈ R C×D of M, where C denotes the number of categories for the semantic segmentation task and each row of C represents the summary of a class. Although the memory slots in prototypes memory M are shared to represent universal concepts of the all instances, to generate semantic summary C from M, we equally associate the N memory slots in M with C categories and thus define N = N c × C, where N c is denoted as the number per-category prototypes. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, the i-th row in C, i.e. c i , can be seen as a average of the i-th subsegment in M, i.e. rows in M from (i − 1) × N c + 1 to i × N c . Specifically, we obtain c i by averaging the submatrix M i :</p><formula xml:id="formula_0">c i = 1 N c i×Nc j=(i−1)×Nc+1 m j ,<label>(1)</label></formula><p>where m j denotes the j-th row vector of M.</p><p>Given the query features F ins and F seg , the instance grouping branch directly addresses the prototypes memory M and the semantic labeling branch accesses the semantic summary C, with two specifically designed readers. M can be seen as an dictionary to restore the representative bases shared by all instances, since the instances cross different categories can share some common basic components and characteristics. As the semantic memory C is a re-parameterization of M, the two tasks are naturally associated together, without computation-consuming operations as <ref type="bibr" target="#b36">[36]</ref>. Due to the supervision from both tasks, the learned and memorized prototypes are discriminative not only for grouping instances but also for semantic classification.  <ref type="figure">Figure 3</ref> -Visualization of the memory representation. The goal is to find out part instance segmentation within an object, i.e. four chair legs in each example are four different instances. Even with various external shapes appearances, the instance memory successfully capture consistent representation for both dominant cases and rare cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Memory-augmented Instance Embedding</head><p>Memory reader for instance segmentation Given the i-th instance query feature f ins,i from F ins , an attention-based reader is proposed to address the most relevant prototypes from M. The soft addressing weights w is calculated as follows:</p><formula xml:id="formula_1">w ij = exp(d(f ins,i , m j )) N j=1 exp(d(f ins,i , m j )) ,<label>(2)</label></formula><p>where m j is the j-th row vector of M and d(·, ·) is function for measuring similarity of the i-th query item and the jth prototype item. In MPNet, we utilize cosine distance. The alternated i-th instance feature f ins,i from F ins can be calculated through: f ins,i = N j=1 w ij m j . To have a better understanding of the learned memory prototypes, we select the category of 'Chair' in PartNet <ref type="bibr" target="#b20">[21]</ref> for training and visualization, as shown in <ref type="figure">Figure 3</ref>. Each chair is a testing sample and the goal is to find out instance part of object. For example, the four chair legs from a chair are noted as different instances. We select two memory prototypes m i and m j , and different colors in a chair refer to the addressing weights of the i-th and j-th columns in w (see Eq. <ref type="formula" target="#formula_1">(2)</ref>). For each memory item, the points that are addressing it have consistent geometric meaning, as shown in <ref type="figure">Figure 3</ref>. The consistency of the learned prototypes allows it to capture discriminative representation for both dominant and rare cases. Instance-aware regularization The prototypes memory are updated via propagation driven by the training objectives. To make it effective, specifically designed regularization term R ins is proposed, defined as follows:</p><formula xml:id="formula_2">R ins = 1 K K k=1 1 N k N k n=1 G( f ins,n ) − GT k 2 ,<label>(3)</label></formula><p>where K is the instance number, N k is the point number of k-th instance, G(·) is a simple MLP that predicts geometric centroid of the k-th instance. GT k is the corresponding geometrical ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Memory-augmented Semantic Labeling</head><p>Memory reader for semantic segmentation Similar to the instance reader, the segmentation reader outputs an addressing weights for retrieving the most relevant categories. Given the i-th segmentation query f seg,i from F seg and j-th memory item c j , the feature for semantic segmentation is calibrated by:</p><formula xml:id="formula_3">f seg,i = α T C = C j=1 α ij c j .</formula><p>where c j is the j-th centroid for segmentation and α ij is the similarity coefficients between f seg,i and c j , similar to Eq. (2). Semantic memory regularization To force the centroids of different classes, i.e. the semantic summarization C, to keep a separable distance, R seg is proposed to regularize the large margin of the inter-class and the compactness of innerclass. Given the i-th calibrated feature f seg,i and its semantic label y i , the regularization term R seg is calculated as:</p><formula xml:id="formula_4">R seg = max(0, j=yi f seg,i − c j − j =yi f seg,i − c j + m),<label>(4)</label></formula><p>where m is the relaxation margin, which is set to 5 in all our experiments. Each c j performs like an anchor point and pull the features with identical semantic labels close to it and push the features with different semantic labels away from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Functions</head><p>Classification loss We use cross entropy loss L CE for the semantic segmentation task. Instead of using softmax for normalization, we found squashing function proposed by <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18]</ref> provides a little higher performance on the accuracy of semantic segmentation. To summarize, classification loss is defined as:</p><formula xml:id="formula_5">L CE = 1 P P n=1 CE( f c( f seg,i ) 2 1 + f c( f seg,i ) 2 · f c( f seg,i ) f c( f seg,i ) , y n ),<label>(5)</label></formula><p>where CE refers to the cross entropy loss and P is the total number of examples. f c(·) is a fully convolution operation that projects the calibrated f seg,i to the classification space. Instance discriminative loss Similar to <ref type="bibr" target="#b36">[36]</ref>, given the retrieved instance features { f ins,i } P i=1 , a simple layer perceptron is utilized to project the feature to the embedding space</p><formula xml:id="formula_6">{e ins,i ∈ R c } P i=1</formula><p>with feature dimension c (we set c = 5 in all our experiments). The loss is formulated as follows:</p><formula xml:id="formula_7">L dis = 1 K K k=1 1 N k N k n=1 [ e ins,n − µ k − σ v ] 2 + + 1 K(K − 1) K i=1 K j=1 i =j 2σ d − µ i − µ j 2 + ,<label>(6)</label></formula><p>where K is the instance amount and N k is the number of k-th instance and µ k is the average embedding of the kth instance, which is calculated by µ k = 1 N k N k n=1 e ins,n . σ v and σ d in Eq. (6) are respectively the margins for the variance and distance loss terms as defined in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">36]</ref>. Training objective As all operations are differentiable, prototypes memory module can be updated through backpropagation in an end-to-end manner. By combining the four losses discussed above, the training objective is formulated as:</p><formula xml:id="formula_8">L = L CE + L dis + R seg + λR ins ,<label>(7)</label></formula><p>we found R ins is sensitive to the learning rate and we set it 0.1 and maintain the others to 1.0 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To validate the effectiveness of our proposed method, both qualitative and quantitative experiments are conducted on two public datasets: Stanford 3D Indoor Semantic Dataset (S3DIS) <ref type="bibr" target="#b0">[1]</ref> and ScanNetV2 <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>S3DIS dataset <ref type="bibr" target="#b0">[1]</ref> covers more than 6000 m 2 and is collected in 6 large-scale indoor areas. It includes 272 rooms and more than 215million points, each of which contains both instance and semantic annotations out of 13 classes. ScanNetV2 is another large-scale dataset for point cloud instance segmentation, which consists of 1613 indoor scans from 40 categories. The dataset is split into 1201, 312 and 100 for training, validating and testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>Following <ref type="bibr" target="#b36">[36]</ref> on the S3DIS dataset, the performance on Area-5 and k-fold cross-validation are reported in our experiments. For semantic segmentation, we present the overall accuracy (oAcc), which measures point-level accuracy, mean class accuracy (mAcc), which calculates average category-level accuracy and mean intersection-over-union (mIoU), which provides a measure for matching predicted segmentation results and the ground truth across all categories. For instance segmentation, four evaluation metrics are calculated, namely, mConv, mW Conv, mP rec and mRec. mConv is defined as the mean instance-wise matching IoU score between ground truth and prediction. Instead of treating every instance equally, mW Conv is weighted by the size of each instance object. Moreover, traditional mP rec and mRec represents mean precision and mean recall with IoU threshold 0.5, which are widely used in the 2D image object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>For the S3DIS and ScanNetV2, similar to PointNet <ref type="bibr" target="#b22">[23]</ref>, each room is divided into 1m × 1m blocks with a stride of 0.5m. 4096 points are randomly sampled as input from each block during the training process. The feature for each point is consist of both color and geometric information, i.e. R, G, B, X, Y, Z . . . . Without special notation, all experiments are conducted using vanilla PointNet++ <ref type="bibr" target="#b24">[25]</ref> as backbone (without introducing any multi-scale grouping operation). We use ADAM optimizer with initial learning rate of 1e-2, momentum of 0.9 and batch size of 16. The learning rate is divided by 2 for every 3 × 10 5 iterations. The hyperparameters for metric learning are selected to be the same with <ref type="bibr" target="#b36">[36]</ref>, namely, σ v = 0.5, σ d = 1.5. The number for memory slots is set to 150 per-category. The loss weight for L is set to 0.01, which has a significant influence to the final performance. The whole network is trained end-to-end for 100 epochs in total.</p><p>During inference time, blocks within each room are merged in a snack pattern by utilizing the segmentation and instance results of the overlapped region. Detailed settings of the algorithm are identical with <ref type="bibr" target="#b34">[35]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we describe the influence of each integration of aforementioned components. All the results are tested on S3DIS Area-5 for fair comparison. We first build a strong baseline which is similar to ASIS vanilla <ref type="bibr" target="#b36">[36]</ref>. The framework has two independent decoders which are responsible for semantic segmentation and instance metric grouping, respectively. Using Pointnet++ as backbone, the baseline model achieves 52.3 mP rec and 41.4 mRec on S3DIS Area-5, which is 16.3 and 12.7 higher than SGPN <ref type="bibr" target="#b34">[35]</ref>, respectively. Built upon the strong baseline, our MPNet surpass it by a large margin via memorizing representative prototypes. In the following section, provide detailed analysis on different parts. Focal Loss. The discrepancy among different categories are significant in 3D point cloud. Focal loss <ref type="bibr" target="#b16">[17]</ref> has been widely used in different kinds of vision tasks due to the imbalance of data distribution. It addresses the problem by down-weighting the well-classified samples. However, it only alleviates the category imbalance to some extent and fail to solve the diverse distributed patterns. As shown in <ref type="table" target="#tab_1">Table 1</ref>, focal loss can only improve the mean precision and mean recall by 2.9 and 2.4, respectively. Compared with Focal Loss, our method is more powerful to solve both data imbalance and pattern imbalance by recording and memorizing the prototypical patterns. Prototypes Memory M and C. The representative and consistent prototypes are maintained in a memory module M, which is shared to represent universal concepts of all instances. Besides, a semantic memory C is served as a prototypes summary to efficiently represent the semantic characteristics. As shown in <ref type="table" target="#tab_1">Table 1</ref>, using instance memory M alone can boost mP re from 52.3% to 58.9% and mRec from 41.4% to 47.0%. On the other hand, using segmentation memory C can bring another 1.3% and 0.5% improvement with the metric of mP rec and oAcc. As two tasks are highly correlated due to the shared encoder backbone, utilizing M can also brings about 1.5% improvement for se-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Cloud ASIS Ours</head><p>Common Scenes Rare Scenes <ref type="figure">Figure 4</ref> -Barnes-Hut t-SNE <ref type="bibr" target="#b32">[33]</ref> visualization of our instance embedding on S3DIS Area-5 set (Best viewed when zoom in). The embedding feature is projected to 1-D and the distances is normalized to unit length so that the gap of gray-scale between different instances reflects the distances in the embedding space. mantic segmentation in terms of oAcc. Regularization Loss. To effectively learn representative and discriminative prototypes, regularization losses are proposed in Eq. (4) and Eq. (3). The first one is to keep largemargin between different categories from memory C. The second one is designed for forcing the calibrated instance <ref type="table">Table 3</ref> -Comparison per-class performance of our proposed method with state-of-the-art on S3DIS semantic segmentation task, tested on all areas. Our result utilize vanilla pointnet++ <ref type="bibr" target="#b24">[25]</ref> without multi-scale group. Even with a simple baseline, the proposed method surpassed the graph based method by more than 1% with mIOU. Instance mPrecision</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance mRecall</head><p>Focal Loss <ref type="figure">Figure 5</ref> -The comparison of improvements between our proposed method and baseline model with focal loss <ref type="bibr" target="#b16">[17]</ref> and ASIS <ref type="bibr" target="#b36">[36]</ref>. Both mean precision and mean recall of instance are reported. embeddings to have identical geometric output. As shown in <ref type="table" target="#tab_1">Table 1</ref>, these regularizations can boost the mP re and mRec for about 1.7 and 1.8, respectively. The Impact of Memory Size. We study the influence of the memory size to the final performance. We set three values of N c with 100, 150, 200 as the number of per-category prototypes. The mP rec on S3DIS Area-5 are 60.4, 62.7 and 62.5 respectively. The results show that the performance increases as N c grows, and become stable after 200. In all our experiments, N c is set to 150. Visualization of the Memory Representation. Given the input features, the most relevant prototypes are retrieved to calibrate the features. In <ref type="figure">Figure 4</ref>, we visualize the embedding features with and without the memory module. Both common and rare scenes, i.e. office and lobby, are selected, according to the amount of training samples. The embeddings are projected to 1-D with the help pf Barnes-Hut t-SNE <ref type="bibr" target="#b32">[33]</ref>. In both situations, our MPNet generate more discriminative embedding features, which is critical for separate different instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the State-of-the-art</head><p>Performance on non-dominant cases. We first compare the performance of our proposed MPNet with state-of-theart method ASIS <ref type="bibr" target="#b36">[36]</ref> on non-dominant cases. We first sort the 13 categories on S3DIS according to the total amount of training samples, and split the dataset into three levels: dominant cases (the first 4 classes), mid-dominant cases (the mid 5 classes) and non-dominant cases (the last 4 classes). The amount proportions of the three levels are 79.17%, 16.95% and 3.88%, respectively. As shown in <ref type="figure">Figure 5</ref>, we report the improvement with two metrics: mP rec, mRec. Our method can not only boost the performance on dominant cases, but surpass focal loss and ASIS <ref type="bibr" target="#b36">[36]</ref> by a large margin on non-dominant cases. Performance on S3DIS. We first compare the instance segmentation performance on both Area-5 and 6-fold. The results are presented in <ref type="table" target="#tab_2">Table 2</ref>. Our proposed MPNet achieve promising results and surpass the previous state-of-the-art approaches substantially by a large margin. The large improvement is mainly beneficial from the strong ability of the proposed prototypes memory. Qualitative results is show in <ref type="figure">Figure 6</ref>. In addition to instance segmentation, we also report the results of semantic segmentation and compare it with other methods. The performance is tested on all areas (6-fold), as shown in <ref type="table">Table 3</ref>. Although based on a simple PointNet++, we achieve even better quantitative results than other methods which are based on graph neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">37]</ref>. Performance on ScanNetV2. In addition to S3DIS, we conduct experiments on ScanNetV2 <ref type="bibr" target="#b2">[3]</ref>. The instance segmentation results are reported in <ref type="table" target="#tab_4">Table 4</ref>, which is tested on the validation set. To make fair comparison, we select the methods that are based on PointNet or PointNet++. Our proposed MPNet outperforms previous methods over all overlap thresholds and dominant in many categories. Speed Analysis. We compare the inference speed with other two methods: SGPN <ref type="bibr" target="#b34">[35]</ref> and ASIS <ref type="bibr" target="#b36">[36]</ref>. The whole evaluation process includes two parts: network forward and instance grouping. The first part is to get per-point semantic labeling and instance embedding. The second part utilizes a grouping algorithm to find out instance groups. SGPN, which is based on PointNet, predicts a pair-wise affinity matrix to group points into instance clusters. Due to the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point Cloud</head><p>Ground Truth Ours ASIS 1 2 3 <ref type="figure">Figure 6</ref> -Qualitative results of our method on S3DIS dataset. From left to right are: input point cloud, instance segmentation ground truth, the results of our method and the results of <ref type="bibr" target="#b36">[36]</ref>. Note that different instance are shown with different colors, and the same instance are not necessarily have the same color in ground truth and prediction presentation.  large size of input point cloud, a huge memory is required. Meanwhile, as lots of heuristic parameters are introduced, the whole time for post-processing is much slower than our proposed method. Different from SGPN, ASIS utilize mean-shift for clustering embeddings to instance groups. Meanwhile, ASIS applies KNN for fusing semantic context from a fixed number of neighboring points, which is used on every input point. This operation is extremely timeconsuming and fail to take fully advantage of computational resources. Compared with the above two approaches, our proposed MPNet is free from complex and time-consuming operations, showing the superiority in both effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a memory-augmented network to handle both category and pattern imbalance in point cloud instance segmentation. A memory module is introduced to alleviate the forgetting issue during the training process. The performance on the benchmarks shows the superiority of our method in both effectiveness and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix</head><p>In this supplementary material, we provide more detailed experimental results, including:</p><p>• Both qualitative and quantitative results on the "Chair" category in PartNet <ref type="bibr" target="#b20">[21]</ref>;</p><p>• More visualization of our approach on S3DIS <ref type="bibr" target="#b0">[1]</ref> and ScanNetV2 [3].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Results on PartNet dataset [21]</head><p>In <ref type="figure">Figure 3</ref> in the main paper, to better understand the learned memory prototypes, we do visualization relying on the category of "Chair" in PartNet <ref type="bibr" target="#b20">[21]</ref>. PartNet <ref type="bibr" target="#b20">[21]</ref> is a consistent dataset of 3D objects with fine-grained and hierarchical 3D part annotations. In this section, we report the quantitative results in <ref type="table">Table 6</ref>. Level-1 refers to the coarsest annotation and Level-3 refers to the most fine-grained annotation as defined in <ref type="bibr" target="#b20">[21]</ref>. For fair comparison, all results are evaluated with the same backbone PointNet++ <ref type="bibr" target="#b24">[25]</ref>. Our method outperforms the previous methods by a large margin, showing the flexibility of our method to handle various types of input data. Moreover, visualization examples of the results are shown in <ref type="figure">Figure 7</ref>, indicating that our method can handle both rare and common cases well. <ref type="table">Table 6</ref> -Comparison of the per-level performance of our method with the state-of-the-art methods on "Chair" category in PartNet <ref type="bibr" target="#b20">[21]</ref>. The performance is evaluated using part-category mAP, with IoU threshold of 0.5. All the results are achieved with the same backbone: PointNet++ <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Year Level- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">More Visualization Results</head><p>In the main paper, we illustrate the quantitative results on S3DIS <ref type="bibr" target="#b0">[1]</ref> and ScanNetV2 <ref type="bibr" target="#b2">[3]</ref> datasets in <ref type="table" target="#tab_2">Table 2</ref> and 4, respectively. Visualization examples of both semantic and instance segmentation results on S3DIS and Scan-NetV2 datasets are shown in <ref type="figure">Figure 8</ref> in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse Fine</head><p>Ins GT Ins Pred Ins GT Ins Pred Seg GT Seg Pred Seg GT Seg Pred Common Rare Semantic Coarse Fine <ref type="figure">Figure 7</ref> -Visualization of the performance of on PartNet <ref type="bibr" target="#b20">[21]</ref>. Both coarse and fine-grained results are provided. Note that different instance are shown with different colors, and the same instance are not necessarily have the same color in ground truth and prediction presentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Point Cloud</head><p>Segmentation GT Segmentation Pred Instance GT Instance Pred <ref type="figure">Figure 8</ref> -Visualization of the performance of on S3DIS <ref type="bibr" target="#b0">[1]</ref> and ScanNetV2 <ref type="bibr" target="#b2">[3]</ref>. Both instance and semantic segmentation results are provided. Note that different instance are shown with different colors, and the same instance are not necessarily have the same color in ground truth and prediction presentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Input Point Sets (b) Instance Ground Truth (d) Results With Memory Module (c) Results Without Memory Module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 -</head><label>1</label><figDesc>Comparison of instance segmentation results between the proposed method with and without memory module. The performance of our method shows strong robustness against non-dominant cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 -</head><label>2</label><figDesc>The framework of our proposed MPNet, which contains two parallel branches with a shared encoder. A memory module is proposed to memorize representative prototypes that are shared by all samples. The maintained memory module is shared with all instances across different categories. Both distorted and rare cases can be augmented by retrieving the stored prototypes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 -</head><label>1</label><figDesc>Ablation study on the S3DIS dataset Area-5 set with vanilla Pointnet++ as backbone. FL refers to focal loss. InsMem means the memory is updated by instance information. SegMem means the memory is updated by semantic segmentation supervision. Regul refers to the regularizations used in learning the prototypes memory. Both instance segmentation and semantic segmentation results are provided.</figDesc><table><row><cell cols="2">Method FL InsMem SegMem Regul mPre mRec oAcc</cell></row><row><cell>Baseline</cell><cell>52.3 41.4 86.2</cell></row><row><cell></cell><cell>55.2 43.0 86.9</cell></row><row><cell></cell><cell>58.9 47.0 87.7</cell></row><row><cell></cell><cell>60.2 47.2 88.1</cell></row><row><cell>Ours</cell><cell>62.5 49.0 88.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 -</head><label>2</label><figDesc>Instance Segmentation results on S3DIS dataset.</figDesc><table><row><cell>Both Area-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>OA mIOU ceiling floor wall beam column window door table chair sofa bookcase board clutter</figDesc><table><row><cell cols="3">PointNet [23]</cell><cell>78.5</cell><cell cols="2">47.6</cell><cell cols="2">88.0</cell><cell cols="3">88.7 69.3 42.4</cell><cell>23.1</cell><cell>47.5</cell><cell cols="3">51.6 54.1 42.0</cell><cell>9.6</cell><cell>38.2</cell><cell>29.4</cell><cell>35.2</cell></row><row><cell cols="2">MS+CU [7]</cell><cell></cell><cell>79.2</cell><cell cols="2">47.8</cell><cell cols="2">88.6</cell><cell cols="3">95.8 67.3 36.9</cell><cell>24.9</cell><cell>48.6</cell><cell cols="4">52.3 51.9 45.1 10.6</cell><cell>36.8</cell><cell>24.7</cell><cell>37.5</cell></row><row><cell cols="2">G+RCU [7]</cell><cell></cell><cell>81.1</cell><cell cols="2">49.7</cell><cell cols="2">90.3</cell><cell cols="3">92.1 67.9 44.7</cell><cell>24.2</cell><cell>52.3</cell><cell cols="3">51.2 58.1 47.4</cell><cell>6.9</cell><cell>39.0</cell><cell>30.0</cell><cell>41.9</cell></row><row><cell cols="3">PointNet++ [25]</cell><cell>-</cell><cell cols="2">53.2</cell><cell cols="2">90.2</cell><cell cols="3">91.7 73.1 42.7</cell><cell>21.2</cell><cell>49.7</cell><cell cols="4">42.3 62.7 59.0 19.6</cell><cell>45.8</cell><cell>48.2</cell><cell>45.6</cell></row><row><cell cols="3">PointNeighbor [8]</cell><cell>-</cell><cell cols="2">58.3</cell><cell cols="2">92.1</cell><cell cols="3">90.4 78.5 37.8</cell><cell>35.7</cell><cell>51.2</cell><cell cols="4">65.4 64.0 61.6 25.6</cell><cell>51.6</cell><cell>49.9</cell><cell>53.7</cell></row><row><cell cols="3">DGCNN [37]</cell><cell>84.1</cell><cell cols="2">56.1</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">ResGCN-28 [15] 85.9</cell><cell cols="2">60.0</cell><cell cols="2">93.1</cell><cell cols="3">95.3 78.2 33.9</cell><cell>37.4</cell><cell>56.1</cell><cell cols="4">68.2 64.9 61.0 34.6</cell><cell>51.5</cell><cell>51.1</cell><cell>54.4</cell></row><row><cell cols="3">Ours PointNet++</cell><cell>86.8</cell><cell cols="2">61.3</cell><cell cols="2">94.0</cell><cell cols="3">94.1 76.6 53.4</cell><cell>33.6</cell><cell>54.2</cell><cell cols="4">62.7 70.2 60.2 36.6</cell><cell>53.4</cell><cell>54.3</cell><cell>53.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ASIS</cell><cell></cell><cell></cell><cell cols="2">Our Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell>+15.2</cell><cell></cell><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>14</cell><cell></cell><cell cols="3">Dominant cases (79.17%)</cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>12</cell><cell></cell><cell cols="3">Mid-dominant cases (16.95%)</cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Non-dominant cases (3.88%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell></cell><cell cols="2">+9.3</cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+8.6</cell><cell>+8.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>+6.3</cell><cell></cell><cell>+6.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+4.9</cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell>+3.8</cell><cell>+4.2</cell><cell>4</cell><cell>+3.5</cell><cell>+4.0</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+2.2</cell><cell>+2.0</cell><cell>2</cell><cell>+1.5</cell><cell>+2.6</cell><cell>+2.0</cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+0.2</cell><cell>+0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Instance</cell><cell cols="2">Instance</cell><cell></cell><cell>Instance</cell><cell>Instance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">mPrecision</cell><cell cols="2">mRecall</cell><cell></cell><cell>mPrecision</cell><cell>mRecall</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 -</head><label>4</label><figDesc>Instance segmentation results on ScannetV2 benchmark (validation set). Both results of mAP@0.25 and mAP@0.5 are reported. All methods except<ref type="bibr" target="#b7">[8]</ref> are based on PointNet or PointNet++ (3D-BEVIS<ref type="bibr" target="#b5">[6]</ref> is multi-view based method).</figDesc><table><row><cell>Method</cell><cell>Year</cell><cell>mAP @0.25</cell><cell>mAP @0.5</cell><cell cols="11">bathtub bed shelf cabinet chair counter curtain desk door other picture refrig shCur sink sofa table toilet window</cell></row><row><cell cols="3">MaskRCNN [12] 2017 26.1</cell><cell>5.8</cell><cell>33.3</cell><cell>0.2 0.0</cell><cell>5.3</cell><cell>0.2</cell><cell>0.2</cell><cell>2.1</cell><cell>0.0 4.5 2.4</cell><cell>23.8</cell><cell>6.5</cell><cell>0.0</cell><cell>1.4 10.7 2.0 11.0</cell><cell>0.6</cell></row><row><cell>SGPN [35]</cell><cell cols="3">2019 35.1 14.3</cell><cell cols="2">20.8 39.0 16.9</cell><cell>6.5</cell><cell>27.5</cell><cell>2.9</cell><cell>6.9</cell><cell>0.0 8.7 4.3</cell><cell>1.4</cell><cell>2.7</cell><cell cols="2">0.0 11.2 35.1 16.8 43.8</cell><cell>13.8</cell></row><row><cell cols="2">3D-BEVIS [6] 2019</cell><cell>-</cell><cell>24.8</cell><cell cols="2">66.7 56.6 7.6</cell><cell>3.5</cell><cell>39.4</cell><cell>2.7</cell><cell>3.5</cell><cell>9.8 9.9 3.0</cell><cell>2.5</cell><cell>9.8</cell><cell cols="2">37.5 12.6 60.4 18.1 85.4</cell><cell>17.1</cell></row><row><cell cols="4">R-PointNet [41] 2019 40.0 23.5</cell><cell cols="4">51.3 52.3 12.5 15.2 61.8</cell><cell>0.0</cell><cell>1.5</cell><cell cols="2">7.6 29.0 11.7 14.7</cell><cell>25.0</cell><cell cols="2">3.7 14.0 34.5 18.1 53.0</cell><cell>16.1</cell></row><row><cell>ASIS [36]</cell><cell cols="3">2019 41.5 24.0</cell><cell cols="2">29.9 50.5 0.0</cell><cell cols="2">16.7 57.7</cell><cell>0.0</cell><cell>18.4</cell><cell>7.8 14.8 12.9</cell><cell>1.8</cell><cell cols="3">12.4 38.0 10.2 36.9 37.4 71.7</cell><cell>14.5</cell></row><row><cell>Ours</cell><cell>-</cell><cell cols="2">49.3 31.0</cell><cell cols="2">69.4 59.8 2.7</cell><cell cols="2">23.7 71.1</cell><cell>4.5</cell><cell>8.4</cell><cell>18.3 11.6 17.3</cell><cell>4.8</cell><cell cols="3">21.8 57.0 13.4 27.7 41.8 87.3</cell><cell>18.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 -</head><label>5</label><figDesc>Inferencing time comparison on S3DIS Area-5 set. Forward time is network running time on GPU, whereas Postprocessing time is the BlockMerging algorithm introduced in<ref type="bibr" target="#b34">[35]</ref>. ASIS is 45% slower than our method in the forward process due to the usage of KNN, which is extremely time consuming. Reported time is running on a single 1080ti GPU with 4096 input points.</figDesc><table><row><cell cols="2">Method Backbone</cell><cell cols="3">Inference Time (ms) mPre mRec Overall Forward Post</cell></row><row><cell cols="2">SGPN[35] PointNet</cell><cell>730</cell><cell>22</cell><cell>708 36.0 28.7</cell></row><row><cell cols="3">ASIS[36] PointNet2 183</cell><cell>58</cell><cell>125 55.3 42.4</cell></row><row><cell>Ours</cell><cell cols="2">PointNet2 165</cell><cell>40</cell><cell>125 62.5 49.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D Semantic Parsing of Large-Scale Indoor Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic Instance Segmentation with a Discriminative Loss Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Instance-aware semantic Segmentation via Multi-task Network Cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">3D-BEVIS: Bird&apos;s-Eye-View Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathrin</forename><surname>Elich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02199</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alexander Hermans, and B. Leibe. Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis. Workshops</title>
		<meeting>IEEE Int. Conf. Comp. Vis. Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Know What Your Neighbors Do: 3D Semantic Segmentation of Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01151</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Neural Turing Machines . arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08650</idno>
		<title level="m">3D Instance Segmentation via Multi-task Metric Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DeepGCNs: Can GCNs Go as Deep as CNNs?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PointCNN: Convolution On X-Transformed Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-Scale Long-Tailed Recognition in an Open World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Intelligent Robots Syst</title>
		<meeting>IEEE Int. Conf. Intelligent Robots Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint semanticinstance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc</forename><forename type="middle">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Volumetric and Multi-View CNNs for Object Classification on 3D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Point-Net++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Hough Voting for 3D Object Detection in Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litany</forename><surname>Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05009</idno>
		<title level="m">Learning Deep 3D Representations at High Resolutions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic Routing Between Capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prototypical Networks for Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view Convolutional Neural Networks for 3D Shape Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and Deformable Convolution for Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An empirical study of example forgetting during deep neural network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariya</forename><surname>Toneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Tachet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05159</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using Tree-Based Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph Attention Convolution for Point Cloud Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaolin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf</title>
		<meeting>IEEE Conf</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Comp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Associatively Segmenting Instances and Semantics in Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic Graph CNN for Learning on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. On. Graphic</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PointConv: Deep Convolutional Networks on 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">GSPN: Generative Shape Proposal Network for 3d</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Instance Segmentation in Point Cloud</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
