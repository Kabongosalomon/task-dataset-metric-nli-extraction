<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Auto-Encoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Böhm</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Berkeley Center for Cosmological Physics Department of Physics</orgName>
								<orgName type="laboratory">Lawrence Berkeley National Laboratory</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Uroš Seljak Berkeley Center for Cosmological Physics Department of Physics</orgName>
								<orgName type="laboratory">Lawrence Berkeley National Laboratory</orgName>
								<orgName type="institution">University of California Berkeley</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Auto-Encoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the probabilistic auto-encoder (PAE), a generative model with a lower dimensional latent space that is based on an auto-encoder which is interpreted probabilistically after training using a normalizing flow. The PAE is fast and easy to train, achieves small reconstruction errors, high sample quality and good performance in downstream tasks. Compared to a VAE and its common variants, the PAE trains faster, reaches a lower reconstruction error and produces state of the art sample quality without requiring special tuning parameters or training procedures. We further demonstrate that the PAE is a powerful model for performing the downstream tasks of outlier detection and probabilistic image reconstruction: 1) We identify a PAE-based outlier detection metric which achieves state of the art results and outperforms other likelihood based estimators. 2) We perform high dimensional data inpainting and denoising with uncertainty quantification by means of posterior analysis in the PAE latent space. Most generative models are specifically tuned to excel in one or two applications. With the PAE we introduce an easy-to-train, simple, but at the same time powerful model that performs well and reliably in many tasks without requiring special fine-tuning or training procedures. We make all PAE codes publicly available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep generative models are powerful machine learning models designed to perform tasks such as realistic artificial data generation and data likelihood evaluation. Many generative models are laid out for one specific task. For example, generative adversarial networks <ref type="bibr" target="#b13">(Goodfellow et al., 2014)</ref> produce high quality samples, but do not allow for a direct evaluation of the data likelihood. Their samples often fail to cover the entire data space. A probabilistic model that covers the data space are variational auto-encoders (VAEs) <ref type="bibr" target="#b21">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b38">Rezende et al., 2014)</ref>, which are trained to maximize a lower bound to the data likelihood (ELBO). VAEs allow for expressive architectures but require fine-tuning additional parameters <ref type="bibr" target="#b17">(Higgins et al., 2017)</ref>, architectural tweaks or special training procedures to reach good sample quality <ref type="bibr" target="#b1">(Alemi et al., 2018b;</ref><ref type="bibr" target="#b37">Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b24">Larsen et al., 2016)</ref>. Both GANs and VAEs use lower dimensional latent spaces. Normalizing flows (NFs) <ref type="bibr" target="#b39">(Rippel &amp; Adams, 2013;</ref><ref type="bibr" target="#b9">Dinh et al., 2014</ref><ref type="bibr" target="#b10">Dinh et al., , 2016</ref><ref type="bibr" target="#b19">Kingma &amp; Dhariwal, 2018a;</ref><ref type="bibr" target="#b14">Grathwohl et al., 2018;</ref><ref type="bibr" target="#b20">Kingma &amp; Dhariwal, 2018b;</ref><ref type="bibr" target="#b12">Durkan et al., 2019;</ref><ref type="bibr" target="#b34">Papamakarios et al., 2019)</ref> model the probability density p(x) of high-dimensional data directly. They introduce a bijective mapping from x to an underlying latent representation z. The latter is enforced to follow a tractable prior distribution p(z). NFs do not reduce the dimensionality and achieve high data likelihoods on many standard data sets <ref type="bibr" target="#b15">(Gritsenko et al., 2019)</ref>. The GLOW model <ref type="bibr" target="#b20">(Kingma &amp; Dhariwal, 2018b)</ref> achieves visually impressive results in artificial image generation, but requires the user to make a trade-off between image qualities and diversities. This trade-off is chosen by setting a temperature parameter. In addition, the data likelihood of NFs has been shown to be a poor out-of-distribution (OoD) metric <ref type="bibr" target="#b33">(Nalisnick et al., 2018)</ref>. This finding similarly applies to the ELBO of VAEs <ref type="bibr" target="#b33">(Nalisnick et al., 2018)</ref>.</p><p>The failure of VAEs and NFs in OoD detection shows that maximizing the data likelihood does not necessarily correspond to optimizing a model for a specific downstream task. For example, a phenomenon which is well known in linear methods such as probabilistic PCA <ref type="bibr" target="#b41">(Tipping &amp; Bishop, 1999)</ref> are singular latent space variables. They have vanishing effect on the data distribution, yet dominate the likelihood: the contribution of the smallest eigenvalues, λ i , to the log likelihood of a Gaussian, linear model is ∆ ln p(x) = −0.5[1+ln(2πλ i )], which diverges for λ i → 0. This suggests that maximizing the likelihood under a model can be dominated by these eigenvalues, while they may have no relevance for tasks such as generative sampling or outlier detection. In linear methods this problem is addressed with shrinkage regularization <ref type="bibr" target="#b26">(Ledoit &amp; Wolf, 2004</ref> or dimensionality reduction (low rank PCA) <ref type="bibr" target="#b41">(Tipping &amp; Bishop, 1999)</ref>.</p><p>Here we propose a new composite generative model, the Probabilistic auto-encoder (PAE), which is not trained to maximize the data likelihood, but achieves state of the art results in many downstream tasks. Specifically, we show that the PAE produces high quality samples, that a PAE-based density estimator outperforms other generative model-based outlier detectors, and that the PAE can be used for probabilistic data denoising and inpainting. The PAE maps the data to a lower dimensional latent space with an auto-encoder (AE) and performs density estimation in the latent space of the AE with a normalizing flow. In the limit of no dimensionality reduction, the PAE becomes a normalizing flow. The PAE can thus be interpreted as a regularized normalizing flow, in which the effect of singular dimensions is suppressed by the AE compression, in analogy to a low rank probabilistic PCA regularizing the density estimation in the linear case. An advantage of the PAE over many other generative models is that it requires little hyperparameter tuning, no special training procedures or architectural tweaks to achieve good results. It is thus particularly useful for non-ML experts, in particular scientists, who are looking for an easy-to-use and reliable generative model that also performs well in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PAE training, sampling and density estimation</head><p>To construct the PAE we start by training a standard auto-encoder on the N -dimensional data x. The latent space z is of dimensionality K&lt;N . Encoder f and decoder g are deep neural networks with trainable parameters φ and θ, respectively.</p><formula xml:id="formula_0">f φ : R N → R K , x → f φ (x), g θ : R K → R N , z → g θ (z).</formula><p>(1)</p><p>The training objective of the AE is the reconstruction error</p><formula xml:id="formula_1">L AE = E p(x) [x − g θ (f φ (x))] 2 .<label>(2)</label></formula><p>The auto-encoder by itself is not a probabilistic model. To construct the PAE, we interpret it probabilistically after training. The latent space prior, p(z), of the PAE is found by performing a density estimation on the AE-encoded training data. Formally, this can be be derived from</p><formula xml:id="formula_2">p(z) = dx p(x) p φ (z|x) = E p(x) [p φ (z|x)] = E p(x) δ D (z − f φ (x)) ,<label>(3)</label></formula><p>where in the last step we have identified the AE posterior, p φ (z|x), as a Dirac delta distribution. In our experiments we learn p(z) using a normalizing flow (NF), but other density estimators could be used instead. Note that the NF training is performed after we have trained the auto-encoder. We do not retrain the generator or decoder of the AE in this step. The AE latent space is of relatively low dimensionality, K N , which allows for computationally tractable density estimation: the NF models require little tuning in terms of architecture and are fast to train. The performance of different density estimators can be compared at this step.</p><p>The NF is a bijective mapping from the latent space of the AE-encoder, z, to the latent space of the NF, u, which is parametrized by deep networks with parameters, γ, The training objective of the NF is the negative log likelihood of the encoded samples,</p><formula xml:id="formula_3">b γ : R K → R K , z → u=b γ (z).<label>(4)</label></formula><formula xml:id="formula_4">L NF = − ln p γ (z)= − ln p(u) − ln det ∂b −1 γ (u) ∂u .<label>(5)</label></formula><p>where the NF prior is p(u)=N (0, 1) and the determinant of the NF Jacobian is given by</p><formula xml:id="formula_5">|det J γ | = det ∂b γ (z) ∂z = det ∂b −1 γ (u) ∂u .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sampling from the PAE</head><p>To sample from p(x) we draw a sample, u∼N (0, 1), from the NF latent distribution and pass it through both the NF and AE generators ( <ref type="figure" target="#fig_0">Figure 1</ref>),</p><formula xml:id="formula_6">x = g θ (b −1 γ (u)).<label>(7)</label></formula><p>It is often argued that AEs are problematic because they can map very different images into very nearby latent space points. This is cured by the NF, which maps well separated samples, u, to nearby samples in the AE latent space z. It makes sure that many samples land in regions of high p(z) and that the entire AE latent space is covered. This approach achieves an excellent sample quality and diversity, as well as continuous interpolations between data points. We note that a lower dimensional latent space is usually not a hindrance to high sample quality, as long as its dimensionality is chosen high enough to achieve reconstructions with the desired quality.</p><p>The PAE is trained to first achieve optimal reconstruction quality. The NF then allows to draw samples of that same quality. This is different from a VAE which needs to balance the reconstruction error (the likelihood term) with the sample quality (the KL term). If the former dominates the loss during training, the encoded distribution and prior do not match well. In this case, samples from the prior can land outside of the encoded domain resulting in a low sample quality. If the KL term dominates, some latent variables will not encode sufficient information and the reconstruction quality is low (a problem known as posterior collapse). A number of solutions have been proposed to overcome these problems. They generally add complexity in terms of model design and training procedure to the VAE. The differences between PAE and modified VAEs are discussed in more detail in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A PAE-based density estimator for outlier detection</head><p>The PAE is not trained to maximize the data likelihood p(x). While we have shown in Section 2.1 that this is not required to obtain a probabilistic generative model, an estimate of p(x) can be useful for certain downstream tasks such as out-of-distribution detection.</p><p>In latent space models a p(x) estimate is obtained by marginalization over the latent space</p><formula xml:id="formula_7">p θ,γ (x) = dz p γ (z)p θ (x|z).<label>(8)</label></formula><p>The prior, p γ (z), is modelled by the NF (Eq. 5), but Eq. 8 further requires defining an implicit likelihood, p(x|z). For this we make the Gaussian ansatz</p><formula xml:id="formula_8">p θ (x|z)=[2π i σ 2 i ] − N 2 exp − 1 2 N i=1 [x i −g θi (z)] 2 σ 2 i ,<label>(9)</label></formula><p>where σ i is the mean reconstruction error of the AE in dimension i, which we measure on the validation data. We denote the noise covariance of the implicit likelihood, which is the diagonal matrix with values σ i , as σ.</p><p>The integral in Eq.8 is generally not tractable and needs to be solved approximately. In VAEs it is approximated with variational inference (VI). Here, we follow a different approach and approximate it with Laplace's method: We approximate the exponent in the integrand of Eq.8 by its expansion to second order around the maximum and perform the remaining Gaussian integral. For a function A :</p><formula xml:id="formula_9">R K → R, z → A(z), A ∈ C 2 with unique maximum A(z ) the Laplace approximation to the integral dz exp [−A(z)] is dz e −A(z) ≈ e −A(z ) dz e − 1 2 (z−z ) T Σ −1 (z−z ) = e −A(z ) (2π) K det Σ −1 ,<label>(10)</label></formula><p>with the covariance given by the inverse Hessian at the maximum</p><formula xml:id="formula_10">Σ=H −1 , H ij =∂ i ∂ j A(z ).</formula><p>The PAE density estimate for a data point under the Laplace approximation is thus,</p><formula xml:id="formula_11">ln p(x) ≈ ln p θ (x|z ) + ln p γ (z ) − N 2 ln 2π + 1 2 ln det Σ z .<label>(11)</label></formula><p>Evaluating Eq. 11 proceeds in two steps:</p><p>1. Find the maximum of the posterior (MAP), z , by minimizing</p><formula xml:id="formula_12">L MAP = arg min z {− ln p θ (x|z) − ln p γ (z)}.<label>(12)</label></formula><p>The encoded value, f φ (x), can be used as initial value. The minimization is generally computationally tractable and fast since it is performed in the lower dimensional latent space. We have found that using the amortized f φ (x) in lieu of the MAP already achieves excellent outlier detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Obtain the Hessian at the MAP value</head><formula xml:id="formula_13">Σ −1 z = J T γ J γ + ∂g θ ∂z (z ) T σ −2 ∂g θ ∂z (z ) ,<label>(13)</label></formula><p>where we have used the Gauss-Newton approximation to the Hessian, meaning that we have dropped all terms with higher derivatives.</p><p>Given the MAP position and the Hessian we can then estimate p(x) under the Laplace approximation,</p><formula xml:id="formula_14">ln p(x) ≈ − 1 2 [x − g θ (z )] T σ −2 [x − g θ (z )] − 1 2 b γ (z ) T b γ (z ) + ln |det J γ | − [ln det σ 2 − ln det Σ z + N ln 2π]/2.<label>(14)</label></formula><p>We found in our experiments that the Hessian tends to suffer from numerical noise, which decreases the accuracy of the density estimation. To obtain a more reliable OoD metric, we drop the Hessian term,</p><formula xml:id="formula_15">lnp(x) ≈ − 1 2 [x − g θ (z )] T σ −2 [x − g θ (z )] − 1 2 b γ (z ) T b γ (z ) + ln |det J γ | .<label>(15)</label></formula><p>The metric in Eq. 15 has been derived from a normalized probability density estimator but can no longer be interpreted as one. Intuitively, it can be interpreted as the conjunction of two outlier metrics: a (weighted) reconstruction error (first term of Eq. 15) and a density estimator in the latent space of the auto-encoder (second and third term of Eq. 15).</p><p>We will show that in high latent space dimensions we can also drop the reconstruction error (first term) from Eq. 15: in this case the density estimator becomes a regularized NF density estimator, where regularization has been achieved by dimensionality reduction. We find that Eq. 15 is an excellent outlier detection metric for high-dimensional data x, outperforming AE-based outlier detection (AE-based outlier detection uses the reconstruction error, which corresponds to the first term of Eq. 15 if all σ i are equal.), the ELBO, IWAE <ref type="bibr" target="#b5">(Burda et al., 2016)</ref>, and the data likelihood of normalizing flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PAE-based denoising and inpainting with uncertainty quantification</head><p>Another important downstream task of generative models is image denoising and inpainting. This is often performed with point estimators and the important task of uncertainty quantification (UQ) is neglected. The probabilistic PAE framework can be used to address both with posterior inference <ref type="bibr" target="#b2">(Böhm et al., 2019)</ref>. For Gaussian noise, n, with noise covariance matrix σ noise (typically a diagonal matrix) and a pixel-wise mask M , the log posterior of a corrupted data point,</p><formula xml:id="formula_16">x = M x + n, under the PAE is ln p θ,γ (u|x, M , σ noise ) = ln p θ,γ (x|u, M , σ noise ) + ln p(u) − const.<label>(16)</label></formula><p>The prior is p(u)=N (0, 1) and the implicit likelihood is given by</p><formula xml:id="formula_17">p θ,γ (x|u, M , σ noise ) = N M g θ (b −1 γ (u)) , σ 2 + σ 2 noise .<label>(17)</label></formula><p>The covariance of this Gaussian likelihood is composed of the PAE reconstruction error,σ 2 i , and the noise level in the corrupted data, σ 2 noise,i . For sufficiently high latent space dimensionalities the latter dominates, σ i σ noise,i , ensuring that the likelihood is well approximated by a Gaussian. Note that we have replaced x by its generative process, g θ (b −1 γ (u)), thus bringing the inference problem to the low dimensional latent space of the PAE. Posterior analysis is computationally more tractable in this lower dimensional space.</p><p>To denoise and inpaint a corrupted image we perform latent space posterior analysis. A point estimate is given by the MAP, u , the maximum of Eq. 16, which forward modeled into data space,</p><formula xml:id="formula_18">x recon = g θ (b −1 γ (u ))</formula><p>, yields the most likely underlying image. For a full posterior analysis any technique, such as MAP+Lapace, VI or MCMC sampling could be used. Given the multi-modal posterior of some of our examples we fit a full rank Gaussian mixture model to Eq. 16 following <ref type="bibr" target="#b40">Seljak &amp; Yu (2019)</ref>. We can then sample from this model to obtain other solutions that are compatible with the data, thereby obtaining an uncertainty estimate of the reconstruction. In Sec. 4 we show that this procedure recovers high quality images from badly corrupted inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>The PAE can be viewed as a generalization of probabilistic PCA <ref type="bibr" target="#b41">(Tipping &amp; Bishop, 1999</ref>) and reduces to it for linear, Gaussian models. The PAE replaces the low rank PCA components by the low dimensional AE latent space. Both approaches treat the reconstruction error that arises from the compression as Gaussian noise (Eq. 9). The low rank PCA representation removes components with very small eigenvalues that would otherwise dominate the likelihood. A Gaussian prior is imposed on the PCA weights to make the PCA probabilistic. The PAE imposes a normalizing flow prior on the latent space.</p><p>The PAE is also a form of regularized normalizing flow, since an NF can be viewed as a nonlinear generalization of a full rank probabilistic PCA. The PAE regularizes the NF by reducing its dimensionality. A similar reinterpretation of an NF as a VAE has been discussed in <ref type="bibr" target="#b15">Gritsenko et al. (2019)</ref>. The PAE uses an AE as a dimensionality reduction tool, which for powerful architectures and high latent dimensions has a small reconstruction error, yet regularizes the sensitivity of the NF to low variance components.</p><p>The PAE uses an NF prior, which differs from a VAE with an NF posterior <ref type="bibr" target="#b37">(Rezende &amp; Mohamed, 2015)</ref>: for the latter the training relies on the standard ELBO optimization. The PAE uses an NF to learn the prior after the AE training is complete. VI and ELBO are never used. This separation allows for fast, easy and stable training of both the AE and the NF.</p><p>Other approaches that try to improve the VAE sample quality include β-VAEs <ref type="bibr" target="#b17">(Higgins et al., 2017)</ref> and 2-Stage-VAEs <ref type="bibr" target="#b8">(Dai &amp; Wipf, 2019)</ref>. β-VAEs need to balance reconstruction error and sample quality and require an additional costly hyperparameter tuning (Eq. 18). 2-stage-VAEs combine two consecutive VAE's, one for the purpose of data compression, where the KL term in the ELBO is suppressed (small β in Eq. 18), and a second one for latent space density estimation. This is similar to the PAE, and achieves similarly high quality samples. Our approach shows that VI and ELBO are not essential to obtain these results: in the first stage the KL term of the ELBO (Eq. 18) can be completely dropped and no sampling from q is needed (the VAE becomes an AE), and in the second stage the VAE can be replaced by a powerful NF, which is useful for downstream tasks such as OoD detection.  <ref type="table" target="#tab_0">Table 1</ref>: the PAE reaches better FIDs than the β-VAE without requiring parameter-tuning. <ref type="bibr">Xiao et al. (2019)</ref> propose a similar conjunction of AE and normalizing flow (the generative latent flow) and achieve high sample quality, but do not explore the probabilistic interpretation for downstream tasks. Generative moment matching networks <ref type="bibr" target="#b28">(Li et al., 2015)</ref> have also been proposed to be used in a 2-stage PAE-like set up, consisting of an auto-encoder and a mapping of a Gaussian to the encoded distribution. The second stage is non-invertible and trained with a moment-matching objective. GMMNs have been shown to produce high quality samples on MNIST and the Toronto face dataset. Wasserstein auto-encoders <ref type="bibr" target="#b42">(Tolstikhin et al., 2018</ref>) employ yet another training objective to match the encoded distribution to a given prior. WAEs achieve high sample quality with comparable FID scores to the PAE, but do not provide a density estimate that is required for other tasks.</p><p>Other approaches that can be interpreted as regularized normalizing flows and that address cases where the data is confined to a lower dimensional manifold include M (e) -flows <ref type="bibr" target="#b4">(Brehmer &amp; Cranmer, 2020)</ref> and relaxed injective probability flows <ref type="bibr" target="#b22">(Kumar et al., 2020)</ref>. Instead of separating the tasks of compression and density estimation of the manifold, these models regularize the flow itself.</p><p>Out-of-distribution detection with generative models has recently attracted a lot of attention, triggered by the finding that state-of-the art generative models such as VAE, GLOW and MAF fail in this task on a number of standard datasets <ref type="bibr" target="#b33">(Nalisnick et al., 2018)</ref>. <ref type="bibr" target="#b33">(Nalisnick et al., 2018)</ref> proposed a fix that uses likelihood ratios as an OoD metric instead of the likelihood itself. While achieving good OoD accuracy, their method requires fine-tuning a noise parameter, which in turn requires some kind of correctly labeled in-and out-of-distribution data sets. We demonstrate in our experiments (Section 4.2) that the PAE density estimate achieves high OoD accuracy without fine-tuning or requiring a labeled outlier training set.</p><p>There is a long list of works that have addressed the problem of image inputation and denoising with generative models, <ref type="bibr" target="#b38">(Rezende et al., 2014;</ref><ref type="bibr">Mattei &amp; Frellsen, 2018a,b;</ref><ref type="bibr" target="#b11">Dong et al., 2015;</ref><ref type="bibr" target="#b18">Jin et al., 2016;</ref><ref type="bibr" target="#b35">Putzky &amp; Welling, 2017;</ref><ref type="bibr" target="#b43">Ulyanov et al., 2017;</ref><ref type="bibr" target="#b3">Bora et al., 2018)</ref>. Most of them, however, do not address the crucial task of uncertainty estimation. A single point estimate is useless in many applications if its precision cannot be quantified. Our approach has two further advantages that many other methods do not share: 1) It only requires a single training of the generative model and can be applied to different corruption types without retraining. 2) It uses a robust Bayesian framework that comes with well established theoretical guarantees and well known behavior in reconstruction and uncertainty estimation, while it limits the use of the generative model to the task of dimensionality reduction, where its performance can be easily tested and verified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We train PAEs on the MNIST <ref type="bibr" target="#b25">(Lecun et al., 1998)</ref>, Fashion-MNIST <ref type="bibr" target="#b44">(Xiao et al., 2017)</ref> and Celeb-A <ref type="bibr" target="#b29">(Liu et al., 2015)</ref> training data sets. We preprocess these data sets with standard procedures: we dequantize and rescale pixel values to the interval [-0.5,0.5]. The celeb-A samples are cropped to the central 128x128 pixels and then downsampled to 64x64 pixels. The first building block of the PAE is an auto-encoder: The AE encoder and decoder networks are based on the infoGAN architecture <ref type="bibr" target="#b6">(Chen et al., 2016)</ref>. We train the AEs until convergence (at least 500 epochs) using an initial learning rate of 1e-3 which we reduce to 1e-4 after 400 epochs. The second building block of the PAE is a normalizing flow. This NF is trained on the AE latent space (Eq. 4). Depending on the latent space dimensionality we use different architectures for the NF. For K&lt;=32, we find that a realNVP architecture <ref type="bibr" target="#b10">(Dinh et al., 2016)</ref> with random permutations is sufficient to achieve samples of the same quality as the reconstructions. For higher K, we switch to more expressive neural spline flows <ref type="bibr" target="#b12">(Durkan et al., 2019)</ref> and trainable permutations <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018a)</ref>. All NFs are trained for at least 100 epochs until convergence. For comparison we also train (β)-VAEs using the same architecture and training procedure as for the AE. The VAEs are trained with a Gaussian likelihood (allowing for a direct comparison with the PAE), a standard normal distribution as prior and using the mean field approximation for the posterior. In β-VAEs <ref type="bibr" target="#b17">(Higgins et al., 2017)</ref> the training objective is modified by two tunable parameters, β and C,</p><formula xml:id="formula_19">L β−VAE = E q φ (z|x) [p θ (x|z)] − β|D KL (q φ (z|x)||p(z)) − C|.<label>(18)</label></formula><p>We iterate over several parameter combinations until we achieve optimal sample quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sample and reconstruction quality</head><p>We compare the sample and reconstruction quality of the PAE with an equivalent VAE. We quantify sample qualities with the Frechet-Inception Distance (FID) <ref type="bibr" target="#b16">(Heusel et al., 2017)</ref> to the test data. We also quote FIDs for reconstructions, not only because they quantify reconstruction quality, but also because they constitute a lower bound to the sample FID that is achievable with a model. The reconstruction quality is further quantified with the mean reconstruction error on the test data (averaged over all pixels).</p><p>In <ref type="figure">Fig. 2</ref> we show F-MNIST samples of different models at latent space dimensionality K=32. The samples of the VAE without parameter-tuning are very similar to samples from an AE (to sample from the AE, we draw latent space points from N (0, 1) and pass them through the AE generator). The low VAE sample quality is a consequence of the likelihood term dominating the ELBO in this setting. This can be cured by fine-tuning the relative weighting of likelihood and KL terms (Eq. 18). We run 5 different parameter combinations (changing the parameters based on the result of the previous combination) and show results for the combination with the highest FID score. In contrast to the β-VAE, the PAE does not require any iterative parameter search to achieve high quality samples. In <ref type="table" target="#tab_0">Table 1</ref> we list the corresponding FID scores.</p><p>While the β-VAE balances a trade-off between reconstruction error and sample quality, the PAE is trained to reach an optimal reconstruction error. Matching the latent space distribution of the AE with an NF then allows to achieve samples with similar quality as the reconstructions. Because of this, the PAE reaches lower reconstruction errors <ref type="table" target="#tab_0">(Table 1)</ref> and higher sample quality than the β-VAE. At the same time it does so at lower computational (and human) cost: Several β-VAE models have to be trained to find the optimal parameter combination, while the PAE only needs to be trained once. We show PAE samples for F-MNIST and PAE samples and interpolations for celeb-A in <ref type="figure" target="#fig_1">Figs. 3 and 4</ref>. The interpolations are obtained by mapping two random images from the test data set into the latent space, fitting a linear interpolation between the latent points and mapping equidistant points along this line back into the image space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Downstream tasks: Out of Distribution (OoD) detection, denoising, inpainting</head><p>In Section 2.2 we derived an approximate, PAE-based density estimator which we apply as an outlier detector. OoD detection with generative models has recently attracted a lot of attention, since their probability estimates have been shown to be poor outlier-detectors: <ref type="bibr" target="#b33">Nalisnick et al. (2018)</ref> found that different generative models, including VAEs and NFs, can assign consistently higher log probabilities to OoD data than training data. One particular combination of data sets for which this  has been observed is Fashion-MNIST and MNIST, where a model trained on the former assigns higher probability to the latter. We use this pair to demonstrate the discriminating power of the PAE outlier detector. We compare our results to other recently suggested outlier detectors: <ref type="bibr" target="#b7">Choi et al. (2018)</ref> propose the use of the Watanabe-Akaike information criterion (WAIC) and compare it to several other techniques. Here, we also quote their results using the variational information bottleneck OoD detector (VIB) <ref type="bibr" target="#b0">(Alemi et al., 2018a)</ref>, since it performed best in their experiments on the data pairs we consider here, and the importance weighted auto-encoder (IWAE) <ref type="bibr" target="#b5">(Burda et al., 2016)</ref>. Different to our PAE-based detector, the VIB OoD-detector requires a labeled training data set. <ref type="bibr" target="#b36">Ren et al. (2019)</ref> propose to use the likelihood ratio between two models: one trained on in-distribution data, the other trained on perturbed in-distribution data. Their method requires hyper-parameter tuning (the amount of perturbation) for which some OoD data needs to be used.</p><p>The PAE-based outlier detector, Eq. 15 combines two terms, a likelihood term measuring the reconstruction error, and a latent space density term measuring the probability of the encoded data point in latent space. We find that the reconstruction error is more informative about outliers at low latent space dimensionalities (K&lt;16), while it quickly becomes irrelevant for K&gt;16. At higher K, the reconstruction error becomes very small (as illustrated in <ref type="figure">Fig. 5</ref>), while the latent space density becomes very informative about outliers. If we allowed for an additional hyper-parameter, we could find an optimal weighting between likelihood and latent space term for each K. However, the aim of the PAE is to omit hyper-parameter tuning, and so we quote results for the two extreme settings: 1) OoD detection with only the latent space density at K=64. 2) OoD detection with ony the AE reconstruction error at K=4. We compare our results for these settings to other methods in terms of the Area Under Receiver Operating Characteristic (AUROC) curve in <ref type="table" target="#tab_1">Table 2</ref>. As OoD data we use MNIST, OMNIGLOT <ref type="bibr" target="#b23">(Lake et al., 2015)</ref> as well as vertically (vflip) and horizontaly (hflip) flipped F-MNIST test data. The in distribution data is the F-MNIST test set. We find that the PAE outlier detector yields consistently high AUROC values without any parameter tuning, outperforming the other methods in MNIST, OMNIGLOT and hflip. The good performance of the PAE-detector is owed to the regularization imposed by dimensionality reduction, as it eliminates the low variance components that dominate the likelihood but are not informative about outliers. The likelihood ratio <ref type="bibr" target="#b36">(Ren et al., 2019)</ref> similarly upweights the more informative part of the data by adding noise, and  <ref type="figure">Figure 5</ref>: MNIST input images (outermost left plot) and their reconstructions with with AEs trained on F-MNIST. As K increases from left to right <ref type="bibr">(4,</ref><ref type="bibr">8,</ref><ref type="bibr">16,</ref><ref type="bibr">32)</ref>, the reconstructions transition from being visually close to F-MNIST (high reconstruction error) to visually close to the actual input (low reconstruction error). On the right we compare OoD detection based on reconstruction error (yellow) to latent space density (blue) as a function of K in terms of the ratio of True Positive (TP) over False Positive (FP) rate at a False Negative rate of 5%. At low K the reconstruction error is a better OoD detector, at higher K the latent space density outperforms it.</p><p>can be viewed as a form of regularization since it also eliminates information from low variance components. However, it requires fine-tuning the type and amount of perturbation on some additional OoD data set. We achieve competitive results without parameter tuning and without requiring any kind of OoD data for calibration.</p><p>Another downstream task that deep probabilistic models are useful for is posterior analysis of incomplete or corrupted data. We address this task with the PAE following Section 2.3. In the left panel of <ref type="figure" target="#fig_3">Figure 6</ref> we show 3 examples of corrupted MNIST test data (left column), reconstructions at the peak of the posterior (MAP) (middle) and and true images (right column). The first example of the masked 3 illustrates why uncertainty quantification in terms of posterior analysis is important: The corrupted data is compatible not only with a 3, but also a 5. We see in the middle panel 2 that the corresponding posterior is complicated, possibly multi-modal. Drawing from this posterior results in the samples at the top of the right panel. The masked samples in the bottom show that they are all compatible with the input data. The samples are mostly 3s, with occasional occurrence of 5 (possibly 2 plot created with the mcmcplot package <ref type="bibr" target="#b32">(Miles, 2018)</ref>  associated with the secondary peak in the latent space posterior). While the digit 5 can be consistent with the data, this is true for only a small fraction of the prior and the posterior reflects this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and conclusion</head><p>We show that an auto-encoder combined with a normalizing flow is a powerful, easy and fast to train probabilistic model (PAE). The PAE can be used for sample generation, outlier detection and probabilistic image inpainting and denoising. When compared to baselines such as (β-)VAEs it outperforms these in terms of reconstruction error, sample quality, computational time, and downstream tasks such as outlier detection, without the need of additional parameter-tuning.</p><p>The PAE can be viewed either as a nonlinear generalization of a low rank probabilistic PCA or as a regularized form of an normalizing flow. The regularization through dimensionality reduction avoids the issues that plague normalizing flows, such as zero or near zero variance pixels, which can flaw density estimation based outlier detection. This allows the PAE to achieve state of the art results in out-of-distribution problems where normalizing flows are known to fail.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic diagram of the PAE (left panel) and an illustration of the sampling procedure from the PAE (right panel). The auto-encoder networks are depicted as gray trapezia, the normalizing flow is represented by black arrows and the latent spaces of the auto-encoder and normalizing flow are shown in red and blue, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>F-MNIST samples from the PAE at K=64 with FID=31.7 (left) and K=128 with FID=28.0 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>PAE performance on Celeb-A at K=64. Samples (left) reach FID=49.2 (reconstructions FID=44.0). Right: Interpolations between samples from the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Left panel: corrupted input data with masks shown in gray (left column), MAP reconstructions (middle column) and underlying truth (right column). Middle panel: posterior density of two latent space variables (out of 8) for the first example of the masked 3. Right panel: Samples drawn from the posterior, the bottom rows show the same samples with the mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>FID scores and reconstruction errors at latent space dimensionality K=32 measured on 10000 samples/test data. The errors on the FIDs are smaller than the differences (order 1).</figDesc><table><row><cell>Model</cell><cell>AE</cell><cell>VAE</cell><cell cols="2">β-VAE PAE</cell></row><row><cell>sample FID score ↓</cell><cell cols="3">102.9 102.4 56.3</cell><cell>35.4</cell></row><row><cell>reconstruction FID score ↓</cell><cell>31.5</cell><cell>32.3</cell><cell>51.3</cell><cell>31.5</cell></row><row><cell cols="4">mean reconstruction error ↓ 0.077 0.084 0.114</cell><cell>0.077</cell></row></table><note>Figure 2: Sample qualities at K=32: From left to right we show samples from an AE, samples from an equivalent VAE without parameter-tuning, samples from a β-VAE with tuned parameters (β=100, C=15), samples from the PAE and reconstructions of test data. Corresponding FID scores are listed in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>OoD detection accuracy quantified by the AUROC (↑) for models trained on F-MNIST.</figDesc><table><row><cell>Outlier</cell><cell cols="6">MNIST OMNIGLOT FMNIST-hflip FMNIST-vflip</cell></row><row><cell>PAE density (this work)</cell><cell>0.997</cell><cell>0.981</cell><cell cols="2">0.698</cell><cell></cell><cell>0.891</cell></row><row><cell cols="2">AE reconstruction error (this work) 0.986</cell><cell>0.916</cell><cell cols="2">0.689</cell><cell></cell><cell>0.880</cell></row><row><cell>likelihood ratios (Ren et al. 2019)</cell><cell>0.996</cell><cell>-</cell><cell cols="2">-</cell><cell></cell><cell>-</cell></row><row><cell>VIB (Choi et al. 2018)</cell><cell>0.941</cell><cell>0.943</cell><cell cols="2">0.667</cell><cell></cell><cell>0.902</cell></row><row><cell>WAIC (Choi et al. 2018)</cell><cell>0.766</cell><cell>0.796</cell><cell cols="2">0.624</cell><cell></cell><cell>0.704</cell></row><row><cell>IWAE (Choi et al. 2018)</cell><cell>0.423</cell><cell>0.568</cell><cell cols="2">0.594</cell><cell></cell><cell>0.668</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FP/TP in % at p=0.05</cell><cell>0 25 50 75 100</cell><cell>10</cell><cell>K</cell><cell>20</cell><cell>30 ln[p(f (x))] ln[p(x|f (x))]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/VMBoehm/PAE Preprint. Under review. arXiv:2006.05479v3 [cs.LG] 22 Oct 2020</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank François Lanusse for useful discussions. This material is based upon work supported by the National <ref type="figure">Science</ref>  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Uncertainty in the variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<idno>abs/1807.00906</idno>
		<ptr target="http://arxiv.org/abs/1807.00906" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fixing a broken ELBO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/alemi18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Dy, J. G. and Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lanusse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Seljak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10046</idno>
		<title level="m">Uncertainty Quantification with Generative Models. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative models from lossy measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ambientgan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hy7fDog0b" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Flows for simultaneous manifold learning and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13913</idno>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Importance weighted autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1509.00519" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<editor>Bengio, Y. and LeCun, Y.</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets. CoRR, abs/1606.03657</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1606.03657" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waic</surname></persName>
		</author>
		<idno>abs/1810.01392</idno>
		<ptr target="http://arxiv.org/abs/1810.01392" />
		<title level="m">Generative ensembles for robust anomaly detection. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Diagnosing and enhancing VAE models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1903.05789" />
		<imprint>
			<date type="published" when="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">NICE: non-linear independent components estimation. CoRR, abs/1410</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.8516" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8516</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Density estimation using real NVP. CoRR, abs/1605.08803</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.08803" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1501.00092</idno>
		<ptr target="http://arxiv.org/abs/1501.00092" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural spline flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bekasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">; H M</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8969-neural-spline-flows" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="7509" to="7520" />
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinberger</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<editor>K. Q.</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">FFJORD: freeform continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno>abs/1810.01367</idno>
		<ptr target="http://arxiv.org/abs/1810.01367" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the relationship between normalising flows and variational-and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HklKEUUY_E" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">; I</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule-converge-to-a-local-nash-equilibrium" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
	<note>Guyon</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Sy2fzU9gl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep convolutional neural network for inverse problems in imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Froustey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
		<idno>abs/1611.03679</idno>
		<ptr target="http://arxiv.org/abs/1611.03679" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions" />
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
	<note>Generative flow with invertible 1x1 convolutions</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes. CoRR, abs/1312</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6114" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Regularized autoencoders via relaxed injective probability flow. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.08927" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno>0036-8075. doi: 10.1126/ science.aab3050</idno>
		<ptr target="https://science.sciencemag.org/content/350/6266/1332" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/larsen16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<editor>Balcan, M. and Weinberger, K. Q.</editor>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A well-conditioned estimator for large-dimensional covariance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ledoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0047-259X(03)00096-4</idno>
		<ptr target="https://doi.org/10.1016/S0047-259X(03)00096-4" />
	</analytic>
	<monogr>
		<title level="j">J. Multivar. Anal</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="411" />
			<date type="published" when="2004-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectrum estimation: A unified framework for covariance matrix estimation and PCA in large dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ledoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmva.2015.04.006</idno>
		<idno>doi: 10.1016/ j.jmva.2015.04.006</idno>
		<ptr target="https://doi.org/10.1016/j.jmva.2015.04.006" />
	</analytic>
	<monogr>
		<title level="j">J. Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="360" to="384" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/li15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Bach, F. R. and Blei, D. M.</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miwae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02633</idno>
		<title level="m">Deep Generative Modelling and Imputation of Incomplete Data. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04826</idno>
		<title level="m">Leveraging the Exact Likelihood of Deep Latent Variable Models. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miles</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1342783</idno>
	</analytic>
	<monogr>
		<title level="j">Zenodo</title>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Do deep generative models know what they don&apos;t know? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Görür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno>abs/1810.09136</idno>
		<ptr target="http://arxiv.org/abs/1810.09136" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Normalizing flows for probabilistic modeling and inference. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1912.02762" />
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Putzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04008</idno>
		<title level="m">Recurrent Inference Machines for Solving Inverse Problems. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Likelihood ratios for out-of-distribution detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.02845" />
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/rezende15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v32/rezende14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">High-dimensional probability estimation with deep density models. CoRR, abs/1302</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1302.5125" />
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Seljak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04454</idno>
		<title level="m">Posterior inference unchained with EL_2O. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<idno type="DOI">https:/rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00196</idno>
		<idno>doi: 10.1111/ 1467-9868.00196</idno>
		<ptr target="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00196" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wasserstein auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkL7n1-0b" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10925</idno>
		<title level="m">Deep Image Prior. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>abs/1708.07747</idno>
		<ptr target="http://arxiv.org/abs/1708.07747" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
