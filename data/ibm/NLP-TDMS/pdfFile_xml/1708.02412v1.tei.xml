<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wasserstein CNN: Learning Invariant Features for NIR-VIS Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-08">Aug 2017. AUGUST 2017 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Ran</forename><forename type="middle">He</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
						</author>
						<title level="a" type="main">Wasserstein CNN: Learning Invariant Features for NIR-VIS Face Recognition</title>
					</analytic>
					<monogr>
						<title level="j" type="main">JOURNAL OF L A T E X CLASS FILES</title>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="2017-08">Aug 2017. AUGUST 2017 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Heterogeneous face recognition</term>
					<term>VIS-NIR face matching</term>
					<term>feature representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heterogeneous face recognition (HFR) aims to match facial images acquired from different sensing modalities with mission-critical applications in forensics, security and commercial sectors. However, HFR is a much more challenging problem than traditional face recognition because of large intra-class variations of heterogeneous face images and limited training samples of cross-modality face image pairs. This paper proposes a novel approach namely Wasserstein CNN (convolutional neural networks, or WCNN for short) to learn invariant features between near-infrared and visual face images (i.e. NIR-VIS face recognition). The low-level layers of WCNN are trained with widely available face images in visual spectrum. The high-level layer is divided into three parts, i.e., NIR layer, VIS layer and NIR-VIS shared layer. The first two layers aim to learn modality-specific features and NIR-VIS shared layer is designed to learn modality-invariant feature subspace. Wasserstein distance is introduced into NIR-VIS shared layer to measure the dissimilarity between heterogeneous feature distributions. So W-CNN learning aims to achieve the minimization of Wasserstein distance between NIR distribution and VIS distribution for invariant deep feature representation of heterogeneous face images. To avoid the overfitting problem on small-scale heterogeneous face data, a correlation prior is introduced on the fully-connected layers of WCNN network to reduce parameter space. This prior is implemented by a low-rank constraint in an end-to-end network. The joint formulation leads to an alternating minimization for deep feature representation at training stage and an efficient computation for heterogeneous data at testing stage. Extensive experiments on three challenging NIR-VIS face recognition databases demonstrate the significant superiority of Wasserstein CNN over state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>U BIQUITOUS face sensors not only facilitate the wide application of face recognition but also generate various heterogeneous sets of facial images <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>. Matching faces across different sensing modalities raises the problem of heterogeneous face recognition (HFR) or cross-modality face recognition. Due to significant difference in sensing processes, heterogeneous images of the same subject have a large appearance variation, which has distinguished HFR from regular visual (VIS) face recognition <ref type="bibr" target="#b2">[3]</ref>. During the last decade, HFR has become increasingly important in many practical security applications and drawn much attention in the computer vision community. Impressive progress has been made in research areas such as near infrared (NIR) vs. VIS <ref type="bibr" target="#b3">[4]</ref>, sketch vs. VIS <ref type="bibr" target="#b4">[5]</ref>, 2D vs. 3D <ref type="bibr" target="#b5">[6]</ref>, different resolutions <ref type="bibr" target="#b6">[7]</ref> and poses <ref type="bibr" target="#b7">[8]</ref>, etc.</p><p>Since NIR imaging technique provides an efficient and straightforward solution to improve face recognition performance in extreme lighting conditions, it has been considered as one of the most prominent alternative sensing modalities in HFR <ref type="bibr" target="#b8">[9]</ref>. Moreover, NIR imaging has been proved to be less sensitive to visible light illumination variations <ref type="bibr" target="#b9">[10]</ref>, and thus is applicable to face recognition at a distance or even at night-time. It has been widely used in face identification or authorization applications, such as security surveillance and E-passport. However, most face galleries only consist of VIS images due to the mass deployment of VIS sensors, while the probe images often come in NIR modalities. Therefore, the demand for robust matching between NIR and VIS face images, also known as the NIR-VIS heterogeneous face recognition problem, has greatly raised and drawn much attention.</p><p>Much research effort has been made to improve the NIR-VIS HFR performance <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b10">[11]</ref>. Traditional NIR-VIS methods generally involve image synthesis, subspace learning and invariant feature extraction <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b1">[2]</ref>. These methods are often based on several processing steps to achieve satisfying accuracy. Recently, inspired by the successful application of convolutional neural networks (CNN) in VIS face recognition <ref type="bibr" target="#b12">[13]</ref> [14] <ref type="bibr" target="#b14">[15]</ref>, several deep models <ref type="bibr" target="#b15">[16]</ref> [17] <ref type="bibr" target="#b8">[9]</ref> attempt to transfer the knowledge learned on a large scale VIS face database to NIR modality. These methods firstly train a basic CNN network on the public CASIA NIR-VIS 2.0 database <ref type="bibr" target="#b3">[4]</ref> and then make the basic network adaptable to both NIR and VIS modalities. Experimental results suggest that deep models have a potential to outperform the traditional NIR-VIS methods.</p><p>However, NIR-VIS HFR still remains a challenging problem for deep models and is largely unsolved mainly due to the following two reasons: 1) The gap between sensing patterns of VIS and NIR modalities. Since NIR and VIS images are captured from different sensing modalities, they have large differences in feature representations. Lacking representative spectral information of NIR images, the deep models trained on VIS data fail to provide satisfying results <ref type="bibr" target="#b15">[16]</ref> [17] <ref type="bibr" target="#b8">[9]</ref>. The debate on the optimal measurement of the difference and approach to close the gap between VIS and NIR modalities remains active, and thus it is still challenging in exploring modality-invariant representations of both NIR and VIS face images via large-scale VIS face data.</p><p>2) The over-fitting on small-scale training set. With the thriving development of Internet, large collection of VIS face images can be gathered more efficiently. However, VIS face images paired with NIR layout can hardly be available online, making paired VIS and NIR images expensive to obtain at large scale. Most existing HFR databases are of small-scale (fewer than 10,000 samples) while having large feature dimensions (at least 100×100 pixels). Consequently, deep models will likely to over-fit to the training set during feature learning <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b8">[9]</ref>. Exploring the optimal method to fit deep models to small-scale NIR-VIS datasets remains a central problem.</p><p>In this paper, the two aforementioned problems are tackled by a novel Wasserstein CNN (WCNN) architecture. WCNN employs one single network structure to map both NIR and VIS images to a compact Euclidean feature space so that the NIR and VIS images in the embedding space directly correspond to face similarity. WCNN is composed of three key components in an end-to-end fashion. First of all, inspired by the observation and results that the appearance of a face is composed of identity information and variation information (e.g., lightings, poses, and expressions) <ref type="bibr" target="#b17">[18]</ref> [4] <ref type="bibr" target="#b18">[19]</ref>, we divide the high-level layer of WCNN into two orthogonal subspaces that contain modality-invariant identity information and modality-variant spectrum information, respectively. Secondly, we focus on the way to evaluate how close the NIR distribution and the VIS distribution are. Wasserstein distance is imposed on the identity subspace to measure the difference between NIR and VIS feature distributions, which reduces the gap between the two modalities. The learned identity subspace is expected to contain the identity invariant information of the two modalities. We further assume that the features of the same subject in the identity subspace follow a Gaussian distribution so that the Wasserstein distance can be efficiently optimized. Lastly, considering that the fully connected layers of WCNN have a large number of parameters and are prone to over-fit on small-scale dataset, we impose a correlation prior on the fully connected layers, which is implemented by a nonconvex low-rank constraint. The advantage of this prior is particularly significant when a training set is small.</p><p>Our convolutional network is first trained on large-scale VIS data. Its convolutional layers and fully connected layer are implemented by the simplest case of maxout operator <ref type="bibr" target="#b19">[20]</ref>. This network makes our learned representation to be robust to intra-class variations of individuals. Then, the low-level layers of this network are fine-tuned to be adaptable to NIR data. Our joint formulation leads to an alternating minimization approach for deep representation at the training time and an efficient computation for heterogeneous data at the testing time. The effectiveness of our WCNN method is extensively evaluated using the most challenging CASIA NIR-VIS 2.0 Database <ref type="bibr" target="#b3">[4]</ref>, Oulu-CASIA NIR-VIS Database <ref type="bibr" target="#b20">[21]</ref> and BUAA NIR-VIS Database <ref type="bibr" target="#b21">[22]</ref>. Our results demonstrate that the proposed WCNN method clearly outperforms the related state-of-the-art NIR-VIS methods, and significantly improve state-of-the-art rank-1 accuracy and verification rate (VR) at a low false acceptance rate (FAR).</p><p>The main contributions of our work are summarized as follows,</p><p>• An effective end-to-end network architecture is developed to learn modality invariant features. This architecture could naturally combine invariant feature extraction and subspace learning into a unified network. Two orthogonal subspaces are embedded to model identity and spectrum information respectively, resulting in one single network to extract both NIR and VIS features.</p><p>• A novel Wasserstein distance is introduced to measure the distribution difference between NIR and VIS modalities. Compared to previous sample-level measures <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b8">[9]</ref>, Wasserstein distance could effectively reduce the gap between the two modalities and results in better feature representation.</p><p>• A correlation prior is imposed on the fully connected layers of deep models to alleviate the over-fitting problem on small scale datasets. This prior makes the proposed WCNN work well on small-scale NIR-VIS dataset and significantly improves verification rate on a low verification rate.</p><p>• Experimental results on the challenging CASIA NIR-VIS 2.0 face database show that WCNN advances the best verification rate (@FAR=0.1%) from 91.0% to 98.4%. Compared with state-of-the-art results <ref type="bibr" target="#b22">[23]</ref>, it further reduces the error rate (1-VR) by 82% only with a compact 128-D feature representation.</p><p>The rest of this paper is organized as follows. We briefly review some related work on NIR-VIS heterogeneous face recognition in Section 2. In Section 3, we present the details of our Wasserstein CNN approach for NIR-VIS face recognition. Section 4 provides experimental results, prior to summary in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The problem of heterogeneous identity matching across different sensing modalities has received increasing attention in biometrics community. Almost all types of biometrics (e.g., face and iris <ref type="bibr" target="#b23">[24]</ref>) have encountered this problem. NIR-VIS HFR has been one of the most extensively researched subject in heterogeneous biometrics. We briefly describe some recent works on this related subject and generally categorize these works into four classes <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b11">[12]</ref> [3]: image synthesis, subspace learning, feature representation and deep learning.</p><p>Image synthesis methods aim to synthesize face images from one modality (or domain) into another so that heterogeneous images can be compared in the same distance space. These methods try to handle the difference of sensing modalities at image preprocessing stage. Image synthesis was firstly used in face photo-sketch synthesis and recognition <ref type="bibr" target="#b24">[25]</ref>. <ref type="bibr" target="#b25">[26]</ref> applied face analogy to transform a face image from one modality to another. <ref type="bibr" target="#b26">[27]</ref> resorted to multiscale Markov random fields to synthesize pseudo-sketch to face photo. Then, <ref type="bibr" target="#b27">[28]</ref> further used hidden Markov model to learn the nonlinear relationship between face photos and sketches. <ref type="bibr" target="#b5">[6]</ref> reconstructed a 3D face model from a single 2D face image using canonical correlation analysis (CCA).</p><p>[29], <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b30">[31]</ref> used coupled or joint dictionary learning to reconstruct face images and then performed face recognition. Recently, a cross-spectral hallucination and low-rank embedding was proposed in <ref type="bibr" target="#b31">[32]</ref> to synthesize a VIS image from a NIR image in a patch way. Although better rank-1 accuracy was claimed in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b31">[32]</ref> does not follow the standard 10-fold testing protocol <ref type="bibr" target="#b3">[4]</ref>. Since image synthesis is an ill-posed problem and a photo-realistic synthesis image is usually difficult to generate, this kind of approaches can only reduce the modality difference to some extent <ref type="bibr" target="#b2">[3]</ref>.</p><p>Feature representation methods try to explore modalityinvariant features that are robust to various sensing conditions. The current methods are almost based on handcrafted local features, such as local binary patterns (LBP), histograms of oriented gradients (HOG), Difference-of-Gaussian (DoG) and SIFT <ref type="bibr" target="#b32">[33]</ref> [34] <ref type="bibr" target="#b34">[35]</ref>. In addition, <ref type="bibr" target="#b35">[36]</ref> applied sparse representation to learn modality-invariant features. <ref type="bibr" target="#b36">[37]</ref> further applied the densely sampled SIFT and multi-block LBP features to represent heterogeneous face images. <ref type="bibr" target="#b9">[10]</ref> combined Log-DoG filtering, local encoding and uniform feature normalization together to find better feature representation. Based on bag of visual words, <ref type="bibr" target="#b37">[38]</ref> proposed a hierarchical hyperlingual-words to capture highlevel semantics across different modalities. <ref type="bibr" target="#b2">[3]</ref> converted face images pixel by pixel into encoded face images with a trained common encoding model, and then applied a discriminant method to match heterogeneous face images. Feature extraction methods reduce the modality difference when converting heterogeneous images to features, and are often applied along with subspace learning methods.</p><p>Subspace learning methods learn mappings to project homogenous data into a common space in which intermodality difference is minimized as much as possible. CCA and partial least squares (PLS) are two representative methods. <ref type="bibr" target="#b38">[39]</ref> proposed a common discriminant feature extraction approach to incorporate both discriminative and local information. <ref type="bibr" target="#b39">[40]</ref> developed a coupled discriminant analysis based on the locality information in kernel space. <ref type="bibr" target="#b40">[41]</ref> proposed a regularized discriminative spectral regression method to map heterogeneous data into a common spectral space. Recently, <ref type="bibr" target="#b41">[42]</ref> took feature selection into consideration during common subspace learning. <ref type="bibr" target="#b42">[43]</ref> proposed prototype random subspace method with kernel similarities for HFR. State-of-the-art NIR-VIS results are often obtained by removing some principal subspace components <ref type="bibr" target="#b18">[19]</ref>. Multiview discriminant analysis <ref type="bibr" target="#b43">[44]</ref> and mutual component analysis <ref type="bibr" target="#b44">[45]</ref> were further developed to reduce the modality difference.</p><p>Deep learning methods mainly resort to CNN to extract deep feature representation of heterogeneous images. These methods are often pre-trained on a large-scale VIS dataset, and then are fine-tuned on NIR face images to learn a modality invariant representation. <ref type="bibr" target="#b16">[17]</ref> used a pre-trained VIS CNN along with different metric learning strategies to improve HFR performance. <ref type="bibr" target="#b15">[16]</ref> employed two types of NIR-VIS triplet loss to reduce intra-class variations and to augment the number of training sample pairs. <ref type="bibr" target="#b8">[9]</ref> trained two networks (named VisNet and NIRNet) with small convolutional filters, and coupled the two networks' output features by creating a Siamese network with contrastive loss. By performing CNN, these methods achieved a verification rate of 91.03% at FAR of 0.1% and rank-1 accuracy of 95.74% on the challenging CASIA NIR-VIS 2.0 database <ref type="bibr" target="#b15">[16]</ref>. However, compared to VIS recognition, the performance of NIR-VIS HFR is still far from satisfying. For example, rank-1 accuracy on the CASIA NIR-VIS 2.0 face database is significantly lower than that on the Labeled Faces in the Wild (LFW) VIS database <ref type="bibr" target="#b45">[46]</ref> (Rank-1 accuracy has been more than 99%). The high performance of VIS recognition benefits from deep learning techniques and large amounts of VIS face images. However, due to the gap and over-fitting problem, NIR-VIS HFR is still challenging for deep learning methods.</p><p>The invariant deep representation method was first proposed in our early work <ref type="bibr" target="#b46">[47]</ref>. Apart from providing more in-depth analysis and more extensive experiments, the major difference between this paper and <ref type="bibr" target="#b46">[47]</ref> is the introduction of the new Wasserstein distance and correlation constraint. Our experiments suggest that the new Wasserstein distance could better measure the feature distribution difference between NIR and VIS face data, leading to further improvement of recognition performance (especially in a lower false acceptance rate). In addition, the correlation constraint on the fully connected layers of WCNN could make learned features more adaptable to small-scale NIR training database, which also improves the performance. Compared with our early work <ref type="bibr" target="#b46">[47]</ref>, our new WCNN method reduces the error rate by 62% at FAR=0.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED WASSERSTEIN CNN</head><p>Benefiting from the development of convolutional neural network (CNN), VIS face recognition has made great progress in recent years <ref type="bibr" target="#b12">[13]</ref> [14] <ref type="bibr" target="#b14">[15]</ref>. This section introduces a new CNN architecture to learn modality invariant deep features for NIR-VIS HFR, named Wasserstein CNN, which consists of three key components as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The first component aims to seek a low-dimensional subspace that contains modality-invariant features. The second one explores the Wasserstein distance to measure the difference between NIR and VIS distributions. The last one imposes correlation prior on the fully connected layers to alleviate over-fitting on small-scale NIR dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Let I V and I N be the VIS and NIR images respectively. The CNN feature extraction process is denoted as</p><formula xml:id="formula_0">X i = Conv(I i , Θ i ) (i ∈ {N, V }),</formula><p>where Conv() is the feature extraction function defined by the ConvNet, X i is the extracted feature vector, and Θ i denotes ConvNet parameters for modality I to be learned. In heterogeneous recognition, one basic assumption is the fact that there is some common concepts between heterogeneous samples. Hence, we assume that NIR and VIS face images share some common low-level features. That is, Θ N = Θ V = Θ and X i = Conv(I i , Θ). As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the output of the last max-pooling layer represents X i ∈ R p , corresponding to the NIR and VIS channel, respectively. These two channels share the same parameter Θ.</p><p>Modality Invariant Subspace: Previous NIR-VIS matching methods often use a trick to alleviate the problem of appreance variation by removing some principal subspaces that are assumed to contain light spectrum information <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b18">[19]</ref>. Observation and results also demonstrate that the appearance of a face is composed of identity information and variation information (e.g., lightings, poses, and expressions) <ref type="bibr" target="#b17">[18]</ref> and removing spectrum information is helpful for NIR-VIS performance <ref type="bibr" target="#b18">[19]</ref>. Inspired by these results, we introduce three mapping matrices (i.e., W, P i ∈ R d×p ) in CNN to model identity invariant information and variant spectrum information. Therefore, the deep feature representation can be defined as</p><formula xml:id="formula_1">f i = f shared f unique = W X i P i X i (i ∈ {N, V }),<label>(1)</label></formula><p>where W X i and P i X i denote the shared feature and the unique feature respectively. Considering the subspace decomposition properties of the matrices W and P i , we further impose an orthogonal constraint to make them to be unrelated to each other, i.e.,</p><formula xml:id="formula_2">P T i W = 0 (i ∈ {N, V }).<label>(2)</label></formula><p>This orthogonal constraint could also reduce parameter space and alleviate over-fitting. Different from previous methods <ref type="bibr" target="#b18">[19]</ref> [45] <ref type="bibr" target="#b37">[38]</ref> [3] that treat feature representation and subspace learning as two independent steps, our architecture is able to naturally combine these two steps in an end-to-end network. The Wasserstein Distance: The gap of sensing mechanism between NIR and VIS images is a major difficulty in HFR. Previous methods often resort to sample-level constraints to reduce this gap. The triplet loss and contrastive loss are imposed on NIR-VIS sample pairs in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b8">[9]</ref> respectively. These methods only consider the relationship between NIR-VIS samples rather than NIR-VIS distributions. Recently, Wasserstein distance proves to play a prominent role of measuring the model distribution and the real distribution in generative adversarial networks (GAN) <ref type="bibr" target="#b47">[48]</ref>  <ref type="bibr" target="#b48">[49]</ref>. Inspired by Wasserstein GAN <ref type="bibr" target="#b47">[48]</ref> and BEGAN <ref type="bibr" target="#b48">[49]</ref>, we make use of Wasserstein distance to measure how close NIR data distribution and VIS data distribution are. Considering that NIR-VIS data are from different subjects and there are large extra-class variations, we impose Wasserstein distance on the distributions of one subject. We further assume the data distributions of one subject follow a Gaussian distribution after non-linear feature mapping. The Gaussian distribution assumption in Wasserstein distance have been shown to be effective in image generation problem <ref type="bibr" target="#b48">[49]</ref> and sequence matching problem <ref type="bibr" target="#b49">[50]</ref>. Experimental results show that this assumption also provides meaningful learning results for HFR.</p><p>Given the two Gaussian distributions X = N (m N , C N ) and Y = N (m N , C N ) corresponding to one subject, where the means m N , m V ∈ R p and the covariances C N , C V ∈ R p×p , the 2-Wasserstein distance between X and Y of one subject could be defined as <ref type="bibr" target="#b48">[49]</ref>:</p><formula xml:id="formula_3">W 2 (X, Y ) 2 = m N −m V 2 2 +trace(C N +C V −2(C 1 2 V C N C 1 2 V ) 1 2 ).<label>(3)</label></formula><p>As in <ref type="bibr" target="#b48">[49]</ref>, we simplify (3) to:</p><formula xml:id="formula_4">W 2 (X, Y ) 2 = 1 2 m N − m V 2 2 + (c N + c V − 2 √ c N c V ) = 1 2 m N − m V 2 2 + σ N − σ V 2 2 ,<label>(4)</label></formula><p>where the σ N and σ V are the standard deviations of X and Y , taking the following forms:</p><formula xml:id="formula_5">σ 1 = 1 n n i=0 (x i − m N ) 2 = 1 n n i=0 x 2 i − m 2 N , σ 2 = 1 n n i=0 (y i − m V ) 2 = 1 n n i=0 y 2 i − m 2 V .<label>(5)</label></formula><p>Their gradients can be computed as</p><formula xml:id="formula_6">∂W 2 ∂x i = 1 n (m N − m V ) + (σ 1 − σ 2 ) ∂(σ N − σ V ) ∂x i ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">∂(σ N − σ V ) ∂x i = 2 n (x i − m N ) σ 2 V + ǫ ,<label>(7)</label></formula><p>and ǫ is a constant. Therefore, the final gradient of X can be denoted as</p><formula xml:id="formula_8">∂W 2 ∂x i = 1 n   (m N − m V ) + 2(σ N − σ V ) (x i − m N ) σ 2 N + ǫ   . (8)</formula><p>Analogously, the gradient of Y can be written as</p><formula xml:id="formula_9">∂W 2 ∂y i = − 1 n   (m N − m V ) + 2(σ N − σ V ) (y i − m V ) σ 2 V + ǫ   .</formula><p>(9) Correlation Prior: One challenge of applying CNN to HFR is the over-fitting problem of CNN on a small-scale training set. In CNN, fully connected layers often take up the majority of the parameters. Since there are both NIR and VIS labels in HFR, the number of class labels in HFR is twice larger than that in VIS face recognition. A large number of class labels also result in fully connected layers of large size. Hence, when the training set is of small-scale, fully connected layers can not be well adjusted and are easy to be over-fitting. The fully connected layer of WCNN is composed of two matrices F N and F V corresponding to NIR and VIS modalities respectively. We expected that M = F N F V are highly correlated so that M T M is a block-diagonal matrix 1 .</p><p>A correlated M will reduce the estimated parameter space and naturally alleviate the over-fitting problem. We make use of the matrix nuclear norm on M, i.e.,</p><formula xml:id="formula_10">M * = tr( √ M T M).<label>(10)</label></formula><p>The matrix nuclear norm requires that M has a low-rank structure and its elements are linearly correlated. Then M T M tends to be a block-diagonal matrix. Given the SVD decomposition of M = U ΣV T , we can obtain:</p><formula xml:id="formula_11">R = M * = tr( √ V ΣU T U ΣV T ) = tr( √ Σ 2 ).<label>(11)</label></formula><p>Since the elements of Σ are non-negative, the gradient of the nuclear norm can be written as:</p><formula xml:id="formula_12">∂R ∂M = ∂tr(Σ) ∂M = U V T .<label>(12)</label></formula><p>Therefore, we can use U V T as the subgradient of nuclear norm. Note that since the fully connected matrices F V and F N are not used in the testing time, the correlation prior only intends to alleviate over-fitting rather than compress a network.</p><p>1. Block-diagonal prior was used in subspace segmentation to make clustering results more accurately <ref type="bibr" target="#b50">[51]</ref>. It requires an affinity matrix to be block-diagonal to characterize sample clusters</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization Method</head><p>The commonly used softmax loss is used to train the whole network, taking the following form,</p><formula xml:id="formula_13">L cls = i∈{N,V } softmax(F i , c, Θ, W, P i ) = − i∈{N,V } ( N j=1 1{y ij = c}logp ij ) s.t. P T i W = 0 (i ∈ {N, V })<label>(13)</label></formula><p>where c is the class label for each sample andp ij is the predicted probability. Moreover, we denote 1{·} as the indicator function so that 1{a true statement} = 1 and 1{a false statement} = 0.</p><p>According to the theory of lagrange multipliers, (16) can be reformulated as an unconstrained problem,</p><formula xml:id="formula_14">L cls = i∈{N,V } softmax(F i , c, Θ, W, P i ) + i∈{N,V } λ i P T i W 2 F ,<label>(14)</label></formula><p>where λ i are the lagrange multipliers and · 2 F denotes the Frobenius norm.</p><p>To decrease the discrepancy between different modalities, we apply Wasserstein distance to measure the two distributions of NIR and VIS images from one subject.</p><formula xml:id="formula_15">L dist = 1 2 m N − m V 2 2 + σ N − σ V 2 2 .<label>(15)</label></formula><p>Specially, under the WCNN training scheme, we employ mini-batch stochastic gradient descent to optimize the objective function, so the statistics of each mini-batch are used to represent the means and standard deviations instead.</p><p>To alleviate over-fitting, we also introduce Eq. <ref type="bibr" target="#b10">(11)</ref>. Then the final objective function takes the following form,</p><formula xml:id="formula_16">L = β 1 L cls + β 2 L dist + β 3 R + i∈{N,V } λ i P T i W 2 F ,<label>(16)</label></formula><p>where β 1 , β 2 and β 3 are the trade-off coefficients for each part. If gradient descent method is used to minimize Eq. <ref type="formula" target="#formula_1">(16)</ref>, we should update the parameters W, P i , F i and Θ.</p><p>For the convolutional parameters Θ, we follow the backpropagation method to update it. The gradients of W , P i and F i can be expressed as</p><formula xml:id="formula_17">∂L ∂W = ∂L cls ∂W + ∂L dist ∂W<label>(17)</label></formula><formula xml:id="formula_18">∂L ∂P i = ∂L cls ∂P i + ∂L dist ∂P i<label>(18)</label></formula><formula xml:id="formula_19">∂L ∂F i = ∂L cls ∂F i + ∂R ∂F i<label>(19)</label></formula><p>Note that the updating gradients for W , P i and F i contain two parts. The first one is used for conventional back-propagation in CNN. The second part of W, P i for subspace learning can be re-organized in</p><formula xml:id="formula_20">∂L ∂W = i∈{N,V } λ i P i P T i W<label>(20)</label></formula><p>Algorithm 1: Training the Wasserstein CNN network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require:</head><p>Training set X i , learning rate γ and lagrange multipliers λ i . Ensure: The CNN parameters Θ and the mapping matrix W . 1: Initialize parameters Θ by pre-trained model and the mapping matrices W, P i , F i by Eq.(26); 2: for t = 1, . . . , T do <ref type="bibr">3:</ref> CNN optimization: <ref type="bibr">4:</ref> Update Θ, W, P i , F i via back-propagation method; <ref type="bibr">5:</ref> Fix Θ: <ref type="bibr">6:</ref> Update W according to Eq.(20); <ref type="bibr">7:</ref> Update P i according to Eq.(21); <ref type="bibr">8:</ref> Update F i according to Eq.(12); 9: end for; <ref type="bibr">10:</ref> Return Θ and W ;</p><formula xml:id="formula_21">∂L ∂P i = λ i W W T P i<label>(21)</label></formula><p>For the low-rank correlation constraint, we can update M = [F N , F V ] T by Eq.(12). Then we update these parameters with a learning rate γ via</p><formula xml:id="formula_22">Θ (t+1) = Θ (t) − γ ∂L ∂Θ (t)<label>(22)</label></formula><formula xml:id="formula_23">W (t+1) = W (t) − γ ∂L ∂W (t)<label>(23)</label></formula><formula xml:id="formula_24">P (t+1) i = P (t) i − γ ∂L ∂P (t) i (24) F (t+1) i = F (t) i − γ ∂L ∂F (t) i<label>(25)</label></formula><p>Since Eq.(16) contains several variables and is nonconvex, we develop an alternating minimization method to minimize Eq. <ref type="bibr" target="#b15">(16)</ref> in an end-to-end CNN optimization scheme. First, we update the parameters by conventional back-propagation to optimize CNN. Then, we fix the CNN parameters and update matrices W, P i , F i by their own gradients. The optimization detail is summarized in Algorithm 1. As in <ref type="bibr" target="#b51">[52]</ref>, the parameters Θ of CNN is initialized by the pre-trained model and the mapping matrices W, P i , F i is initialized by</p><formula xml:id="formula_25">W, P i , F i ∼ U − 1 √ m , 1 √ m<label>(26)</label></formula><p>where U [−a, a] is the uniform distribution in the interval (−a, a) and m is the dimension of original features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Structure</head><p>The basic VIS network architecture (the convolution parameters sharing part in <ref type="figure" target="#fig_0">Fig. 1</ref>) and initial values of Θ are trained on a large-scale VIS dataset <ref type="bibr" target="#b52">[53]</ref>. We employ the light CNN network <ref type="bibr" target="#b53">[54]</ref> as the basic network 2 . The network includes nine convolution layers with four max-pooling layers, followed by the fully connected layer. Softmax is used as the loss function. The training VIS face images are normalized and cropped to 144 × 144 according to five facial points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">https://github.com/AlfredXiangWu/face verification experiment</head><p>To enrich the input data, we randomly cropped the input images into 128×128. The MS-Celeb-1M dataset <ref type="bibr" target="#b52">[53]</ref>, which contains totally 8.5M images for about 100K identities, is employed to train the basic network. Dropout ratio is set to 0.7 for fully connected layer and the learning rate is set to 1e −3 initially and reduced to 1e −5 for 4, 000, 000 iterations. The trained single model for the basic network obtained 98.90% on the LFW dataset. Based on the basic VIS network, we develop a modality invariant convolution neural network for NIR-VIS face recognition. The low-level convolution layers are initialized by the pre-trained basic network. We implement two CNN channels with shared parameters to input NIR and VIS images respectively. Then we define the feature layer (as in <ref type="figure" target="#fig_0">Fig. 1</ref>) that aims to project the low-level features into two orthogonal feature subspaces. In this way, we can leverage the correlated properties of NIR and VIS identities and enforce the domain-specific properties of both modalities. When the summation of Wasserstein distance over all subjects reaches zero, invariant deep features are learned. Finally, the softmax loss functions are separately used for NIR and VIS representation as the supervisory signals. Note that since there is a maxout operator in the feature layer, the final feature dimension is d/2 when W ∈ R d×m . As in VIS training, all NIR and VIS images are cropped and resized to 144 × 144 pixels and a randomly selected 128 × 128 regions are fed into WCNN for NIR-VIS training. The learning rate of the Wasserstein CNN is set to 1e −4 initially and reduced to 1e −6 gradually for around 100, 000 iterations. The tradeoff parameters β 1 , β 2 and β 3 can be set to 1, 1 and 0.001, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>In this section, we systemically evaluate the proposed WCNN approach against traditional methods and deep learning methods on three recently published NIR-VIS face databases: CASIA NIR-VIS 2.0 database, Oulu-CASIA NIR-VIS database and BUAA-VisNir database. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the samples of cropped VIS and NIR facial images in the three databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Protocols</head><p>The CASIA NIR-VIS 2.0 Face Database <ref type="bibr" target="#b3">[4]</ref> is widely used in NIR-VIS heterogeneous face evaluations because it is the largest public and most challenging NIR-VIS database. Its challenge is due to large variations of the same identity, including lighting, expression, pose, and distance. Wearing glasses or not is also considered to generate variations. The database is composed of 725 subjects, each with 1-22 VIS and 5-50 NIR images. Each image is randomly gathered so that there are not one-to-one correlations between NIR and VIS images. The database contains two views of evaluation protocols. View 1 is used for super-parameters adjustment, and View 2 is used for training and testing.</p><p>For a fair comparison with other results, we follow the standard protocol in View 2. There are 10-fold experiments in View 2. Each fold contains a collection of training and testing lists. Nearly equal numbers of identities are included in the training and testing sets, and are kept disjoint from each other. Training on each fold is many-to-many (i.e., images from NIR and VIS are randomly combined). For each training fold, there are approximately 2,500 VIS images and 6,100 NIR images from around 360 subjects. These subjects are mutually exclusive from the 358 subjects in the testing set. That is, the subjects in the training set and testing set are entirely different. The training set in each fold is used for IDR training. For each testing fold, the gallery set always contains a total of 358 subjects, and each subject only has one VIS image. The probe set has over 6,000 NIR images from the same 358 subjects. All the probe set is to be matched against the gallery set, resulting in a similarity matrix of size 358 by around 6, 000. The Oulu-CASIA NIR-VIS database <ref type="bibr" target="#b20">[21]</ref> is composed of 80 subjects with six expression variations (anger, disgust, fear, happiness, sadness, and surprise). 50 subjects are from Oulu University and the remaining 30 subjects are from CASIA. Since the facial images of this database are captured under different environments from two institutes, their illumination conditions are slightly different <ref type="bibr" target="#b37">[38]</ref>. Following the protocols in <ref type="bibr" target="#b37">[38]</ref>, we select a subset of this database for our experiments, including 10 subjects from Oulu University and 30 subjects from CASIA. Eight face images from each expression are randomly selected from both NIR and VIS. As a result, there are totally 96 (48 NIR images and 48 VIS images) images for each subject. 20 subjects are used as training and the remaining 20 subjects are used as testing. All VIS images of the 20 subjects in testing are as the gallery and all their corresponding NIR images are as the probe.</p><p>The BUAA-VisNir face database <ref type="bibr" target="#b21">[22]</ref> is often used for domain adaptation evaluation across imaging sensors. It has 150 subjects with 9 VIS images and 9 NIR images captured simultaneously. The nine images of each subject correspond to nine distinct poses or expressions: neutral-frontal, leftrotation, right-rotation, tilt-up, tilt-down, happiness, anger, sorrow and surprise. The training set and testing set are composed of 900 images of 50 subjects and 1800 images from the remaining 100 subjects respectively. As in <ref type="bibr" target="#b20">[21]</ref>, to avoid that the probe and gallery images are in the same pose and expression, only one VIS image of each subject is selected in the gallery set during testing. Hence, the gallery set and the probe set have 100 VIS images and 900 NIR images respectively. This testing protocol is challenging due to large pose and illumination variations in the probe set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on the CASIA NIR-VIS 2.0 Database</head><p>To verify the performance of IDR, we compare our method with state-of-the-art NIR-VIS recognition methods, including traditional methods and deep learning methods. Since most of methods follow the standard protocol to evaluate their performance on the CASIA NIR-VIS 2.0 database, we directly report their results from the published papers. The traditional methods include kernel coupled spectral regression (KCSR) <ref type="bibr" target="#b54">[55]</ref>, kernel prototype similarities (KPS) <ref type="bibr" target="#b42">[43]</ref>, kernel discriminative spectral regression (KDSR) <ref type="bibr" target="#b40">[41]</ref>, PCA+Sym+HCA <ref type="bibr" target="#b3">[4]</ref>, learning coupled feature spaces (LCFS) <ref type="bibr" target="#b41">[42]</ref>, coupled discriminant face descriptor (C-DFD) <ref type="bibr" target="#b55">[56]</ref>, DSIFT+PCA+LDA <ref type="bibr" target="#b56">[57]</ref>, coupled discriminant feature learning (CDFL) <ref type="bibr" target="#b11">[12]</ref>, Gabor+RBM+Remove 11PCs <ref type="bibr" target="#b18">[19]</ref>, re-construction+UDP <ref type="bibr" target="#b30">[31]</ref>, H2(LBP3) <ref type="bibr" target="#b37">[38]</ref>, common encoding feature discriminant (CEFD) <ref type="bibr" target="#b2">[3]</ref>. The results of LCFS, C-DFD and CDFL are from <ref type="bibr" target="#b11">[12]</ref>, and those of the remaining compared methods are from their published papers. For deep learning methods, we compare the recently proposed TRIVET <ref type="bibr" target="#b15">[16]</ref>, HFR-CNNs <ref type="bibr" target="#b16">[17]</ref> and IDNet <ref type="bibr" target="#b8">[9]</ref>. In addition, the results of two VIS CNN methods are also discussed, including VGG <ref type="bibr" target="#b57">[58]</ref> and SeetaFace <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the rank-1 accuracy and verification rates of different NIR-VIS methods. <ref type="figure" target="#fig_3">Fig. 3 (a)</ref> further plots the receiver operating characteristic (ROC) curves of the pro-  posed method and its three top competitors. For a better illustration, we do not report some ROC curves of other methods if these curves are low. We have the following observations:</p><p>Due to the sensing gap, three VIS deep models can not work well for NIR-VIS HFR. The rank-1 accuracy and VR@FAR=0.1% of VGG and SeetaFace are lower than those of state-of-the-art traditional methods, and significantly worse than those of the deep learning methods trained on NIR-VIS dataset. Compared with VGG and SeetaFace, CEFD and Gabor+RBM can also obtain higher rank-1 accuracy. These results suggest that although large-scale VIS dataset is helpful for VIS face recognition, it has limited benefit for HFR if there is only a small-scale NIR dataset. Hence it is necessary to design suitable deep structures for NIR and VIS modalities. Then deep learning based methods (TRIVET, HFR-CNNs and IDNet) begin to outperform the traditional methods.</p><p>Compared to the traditional methods (CEFD, Ga-bor+RBM and reconstruction+UDP), the improvements of the recently proposed deep learning methods (TRIVET, HFR-CNNs and IDNet) are limited. Particularly, high rank-1 accuracy can not ensure a high verification rate or a better ROC curve. Experimental results clearly show that our WCNN methods yield superior overall performance compared to other NIR-VIS methods. It is worth pointing out that one of the main strengths of WCNN is that it yields consistent improvement over rank-1 accuracy and verification rates. The advantage of WCNN is particularly apparent when FAR is low. Moreover, since we make use of orthogonal subspace to separate spectral information and identity information, the feature dimension of our method is smaller than that of other methods. All of these results suggest that deep learning is effective for the NIR-VIS recognition problem, and a compact and modality invariant feature representation can be learned from a single CNN.</p><p>Compared with our early version IDR <ref type="bibr" target="#b46">[47]</ref>, the WCNN+low-rank method further improves rank-1 accuracy from 97.3% to 98.7% and VR@FAR=0.1% from 95.7% to 98.4%. It further reduces error rate (1-VR) by 62% at FAR=0.1%. Although rank-1 accuracy and VR@FAR=0.1% of WCNN are high, the low-rank constraint could still improve the performance of WCNN. Note that there are 2,148,000 NIR-VIS pairs in the testing. Hence a small im- provement will result in the correct classification of many NIR-VIS pairs. These results highlight the importance of the Wasserstein distance and the low-rank constraint for the problems of sensing gap as well as over-fitting. When these two problems are well treated, deep learning methods could significantly improve NIR-VIS recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on the Oulu-CASIA NIR-VIS Database</head><p>In this subsection, we evaluate the proposed methods on the Oulu-CASIA NIR-VIS Database. Compared to CASIA NIR-VIS 2.0 Database, the training set of the Oulu-CASIA NIR-VIS Database only consists of 20 subjects, which is of relative small-scale. Hence, it is challenging for a deep learning method due to over-fitting. We follow the testing protocol in <ref type="bibr" target="#b37">[38]</ref> and compare WCNN with MPL3 <ref type="bibr" target="#b20">[21]</ref>, KCSR <ref type="bibr" target="#b54">[55]</ref>, KPS <ref type="bibr" target="#b42">[43]</ref>, KDSR <ref type="bibr" target="#b40">[41]</ref>, KDSR <ref type="bibr" target="#b40">[41]</ref>, H2(LBP3) <ref type="bibr" target="#b37">[38]</ref> and TRIVET <ref type="bibr" target="#b15">[16]</ref>. The results of MPL3, KCSR, KPS, KDSR, KDSR and H2(LBP3) are from <ref type="bibr" target="#b37">[38]</ref>. TRIVET is used as the baseline of deep learning methods. <ref type="table" target="#tab_1">Table 2</ref> shows rank-1 accuracy and verification rates of different NIR-VIS matching methods. We observe that the methods can be nearly ordered in ascending rank-1 accuracy as MPL3, KPS, KCSR, KDSR, H2(LBP3), TRIVET, IDR, WCNN and WCNN+low-rank. The four deep learning methods perform significantly better than the five traditional methods in terms of rank-1 accuracy. Although the rank-1 accuracy of TRIVET is higher than that of H2(LBP3), VR@FAR=0.1% of TRIVET is close to that of H2(LBP3). This may be because all VIS images of one subject are from the gallery and all their corresponding NIR images are treated as probe. Since NIR image and VIS image are paired during testing, it is easy for a deep learning method to give a high similarity score for paired data so that the rank-1 accuracy of one deep learning method is high. However, due to the sensing gap, a NIR image feature of one person is potentially similar to the VIS image feature of another person under the same expression. These two features may also have a higher similarity score so that verification rates of all methods are not very high at a low FAR. Due to the small-scale training set of this database, the four deep learning methods can not capture all variations so that their verification rates are lower than those on the CASIA NIR-VIS 2.0 Database. As expected, WCNN methods achieve the highest performance in terms of rank-1 accuracy and verification rates. <ref type="figure" target="#fig_3">Fig. 3 (b)</ref> further plots the ROC curves of the four deep learning methods. The verification rates of all four methods drop dramatically as FAR becomes small. TRIVET obtains the lowest ROC curve. It is interesting to observe that there is only small improvement between the curves of WCNN and IDR. When the low-rank constraint is imposed on IDR, the ROC curve of IDR+low-rank is close to that of WCNN. This means that Wasserstein distance does not contribute too much to ROC curve. This is mainly because the training set of this database is small-scale so that WCNN over-fits on this small-scale training set. When low-rank constraint is imposed on the fully connected layer of WCNN, there is a significant difference between the ROC curves of WCNN and WCNN+low-rank. These results suggest that a suitable constraint on the fully connected layer can alleviate the over-fitting problem on a small training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on the BUAA VisNir Database</head><p>In this subsection, we evaluate the proposed methods on the BUAA VisNir Database. As shown in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>, VIS and NIR images are well aligned and have similar appearance because they are captured simultaneously. These well-aligned NIR and VIS images potentially facilitate deep learning methods to capture intrinsic identity variation and reduce sensing gap. We follow the testing protocol in <ref type="bibr" target="#b37">[38]</ref> to evaluate different NIR-VIS matching methods. The results for the BUAA VisNir database are presented in <ref type="table" target="#tab_2">Table 3</ref> and  <ref type="figure" target="#fig_3">Fig. 3 (c)</ref>. The results of MPL3, KCSR, KPS, KDSR, KDSR and H2(LBP3) are from <ref type="bibr" target="#b37">[38]</ref>.</p><p>We observe that the five deep learning methods perform better than the five traditional methods. The methods can be nearly ordered in ascending rank-1 accuracy as MPL3, KPS, KCSR, KDSR, H2(LBP3), TRIVET, IDR, IDR+lowrank, WCNN and WCNN+low-rank. Our WCNN+low-rank method improves the best rank-1 accuracy from 88.8% to 97.4% and VR@FAR=0.1 from 73.4% to 91.9%. When lowrank constraint and Wasserstein distance are introduced to IDR, IDR's performance is significantly improved. Particularly, the highest performance is achieved when both low-rank constraint and Wasserstein distance are used. This is because deep learning methods are simultaneously degraded by the sensing gap and the over-fitting problems. Our proposed architecture can naturally deal with these two problems in an end-to-end network, resulting in higher performance on this database. From <ref type="figure" target="#fig_3">Fig. 3 (c)</ref>, we observe that the methods can be nearly ordered in ascending ROC curve as TRIVET, IDR, IDR+low-rank, WCNN and WCNN+low-rank. The lowrank constraint significantly improves the ROC curves of IDR and WCNN especially when FAR is low. Since the training set of this database is of small-scale, deep learning may potentially over-fit on the training set. <ref type="figure">Fig. 4</ref> further plots the values of the matrix M T M without ( <ref type="figure">Fig. 4 (a)</ref>) or with ( <ref type="figure">Fig. 4  (b)</ref>) the low-rank constraint on the fully connected layer of WCNN. A lighter color indicates a higher correlation. When the low-rank correlation constraint is used, there is obvious variations on top-right and bottom-left areas of M T M. Note that M is composed of F N and F V . The diagonal elements in the top-right and bottom-left areas have lighter color. This indicates that F N and F V are correlated, which reduces parameter space of the fully connected layer. These results further validate the effectiveness of the low-rank correlation constraint, suggesting the usage of correlation constraints on the fully connected layer to alleviate the over-fitting problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>By naturally combining subspace learning and invariant feature extraction into CNNs, this paper has developed a Wasserstein CNN approach that uses only one network to map both NIR and VIS images to a compact Euclidean space. The high-level layer of WCNN is divided into two orthogonal subspaces that contain modality-invariant identity information and modality-variant light spectrum information, respectively. Wasserstein distance has been used to measure the difference between heterogeneous feature distributions and proven to be effective to reduce the sensing gap. To the best of our knowledge, it is the first attempt in NIR-VIS field to formulate a probability distribution learning for VIS-NIR matching. In addition, low-rank constraint has been studied to alleviate the over-fitting problem on small-scale NIR-VIS face data. An alternating minimization approach has been developed to minimize the joint formulation of WCNN in an end-to-end way. Experimental results on three challenging NIR-VIS face recognition databases show that our WCNN methods significantly outperform state-of-the-art NIR-VIS face recognition methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>An illustration of our proposed Wasserstein CNN architecture. The Wasserstein distance is used to measure the difference between NIR and VIS distributions in the modality invariant subspace (spanned by matrix W ). At the testing time, both NIR and VIS features are exacted from the shared layer of one single neural network and compared in cosine distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) The CASIA NIR-VIS 2.0 database (b) The Oulu-CASIA NIR-VIS database (c) The BUAA-VisNir database Cropped VIS and NIR facial images in the three databases. The first row contains the NIR images from the probe set and the second row contains the VIS images from the gallery set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>ROC curves of different methods on the three NIR-VIS datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Without low-rank constraint (b) With low-rank constraintFig. 4. A correlation illustration of the matrix M T M in the fully connected layer of WCNN. A lighter color indicates a higher correlation. When the low-rank correlation constraint is introduced, there is obvious variations on top-right and bottom-left areas of M T M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Rank-1 accuracy and verification rate on the CASIA 2.0 NIR-VIS face database.</figDesc><table><row><cell>Methods</cell><cell cols="2">Rank-1 FAR=1%</cell><cell>FAR=0.1%</cell><cell>Dim</cell></row><row><cell>KCSR [55]</cell><cell>33.8</cell><cell>28.5</cell><cell>7.6</cell><cell>-</cell></row><row><cell>KPS [43]</cell><cell>28.2</cell><cell>17.4</cell><cell>3.7</cell><cell>-</cell></row><row><cell>KDSR [41]</cell><cell>37.5</cell><cell>33.0</cell><cell>9.3</cell><cell>-</cell></row><row><cell>PCA+Sym+HCA [4]</cell><cell>23.7</cell><cell>-</cell><cell>19.3</cell><cell>-</cell></row><row><cell>LCFS [42] [12]</cell><cell>35.4</cell><cell>35.7</cell><cell>16.7</cell><cell>-</cell></row><row><cell>H2(LBP3) [38]</cell><cell>43.8</cell><cell>36.5</cell><cell>10.1</cell><cell>-</cell></row><row><cell>C-DFD [56] [12]</cell><cell>65.8</cell><cell>61.9</cell><cell>46.2</cell><cell>-</cell></row><row><cell>DSIFT [57]</cell><cell>73.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CDFL [12]</cell><cell>71.5</cell><cell>67.7</cell><cell>55.1</cell><cell>1000</cell></row><row><cell>Gabor+RBM [19]</cell><cell>86.2</cell><cell>-</cell><cell>81.3</cell><cell>-</cell></row><row><cell>Recon.+UDP [31]</cell><cell>78.5</cell><cell>-</cell><cell>85.8</cell><cell>1024</cell></row><row><cell>CEFD [3]</cell><cell>85.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VGG [58]</cell><cell>62.1</cell><cell>70.9</cell><cell>39.7</cell><cell>4096</cell></row><row><cell>SeetaFace [23]</cell><cell>68.0</cell><cell>85.2</cell><cell>58.8</cell><cell>2048</cell></row><row><cell>TRIVET [16]</cell><cell>95.7</cell><cell>98.1</cell><cell>91.0</cell><cell>512</cell></row><row><cell>HFR-CNNs [17]</cell><cell>85.9</cell><cell>-</cell><cell>78.0</cell><cell>-</cell></row><row><cell>IDNet [9]</cell><cell>87.1</cell><cell>-</cell><cell>74.5</cell><cell>320</cell></row><row><cell>IDR [47]</cell><cell>97.3</cell><cell>98.9</cell><cell>95.7</cell><cell>128</cell></row><row><cell>WCNN</cell><cell>98.4</cell><cell>99.4</cell><cell>97.6</cell><cell>128</cell></row><row><cell>WCNN + low-rank</cell><cell>98.7</cell><cell>99.5</cell><cell>98.4</cell><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Rank-1 accuracy and verification rate on the Oulu-CASIA NIR-VIS</cell></row><row><cell></cell><cell>Database.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Rank-1 FAR=1%</cell><cell>FAR=0.1%</cell></row><row><cell>MPL3 [21]</cell><cell>48.9</cell><cell>41.9</cell><cell>11.4</cell></row><row><cell>KCSR [55]</cell><cell>66.0</cell><cell>49.7</cell><cell>26.1</cell></row><row><cell>KPS [43]</cell><cell>62.2</cell><cell>48.3</cell><cell>22.2</cell></row><row><cell>KDSR [41]</cell><cell>66.9</cell><cell>56.1</cell><cell>31.9</cell></row><row><cell>H2(LBP3) [38]</cell><cell>70.8</cell><cell>62.0</cell><cell>33.6</cell></row><row><cell>TRIVET [16]</cell><cell>92.2</cell><cell>67.9</cell><cell>33.6</cell></row><row><cell>IDR</cell><cell>94.3</cell><cell>73.4</cell><cell>46.2</cell></row><row><cell>IDR+low-rank</cell><cell>95.0</cell><cell>73.6</cell><cell>50.3</cell></row><row><cell>WCNN</cell><cell>96.4</cell><cell>75.0</cell><cell>50.9</cell></row><row><cell>WCNN + low-rank</cell><cell>98.0</cell><cell>81.5</cell><cell>54.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Rank-1 accuracy and verification rate on the BUAA NIR-VIS Database.</figDesc><table><row><cell>Methods</cell><cell cols="2">Rank-1 FAR=1%</cell><cell>FAR=0.1%</cell></row><row><cell>MPL3 [21]</cell><cell>53.2</cell><cell>58.1</cell><cell>33.3</cell></row><row><cell>KCSR [55]</cell><cell>81.4</cell><cell>83.8</cell><cell>66.7</cell></row><row><cell>KPS [43]</cell><cell>66.6</cell><cell>60.2</cell><cell>41.7</cell></row><row><cell>KDSR [41]</cell><cell>83.0</cell><cell>86.8</cell><cell>69.5</cell></row><row><cell>H2(LBP3) [38]</cell><cell>88.8</cell><cell>88.8</cell><cell>73.4</cell></row><row><cell>TRIVET [16]</cell><cell>93.9</cell><cell>93.0</cell><cell>80.9</cell></row><row><cell>IDR [47]</cell><cell>94.3</cell><cell>93.4</cell><cell>84.7</cell></row><row><cell>IDR + low-rank</cell><cell>94.8</cell><cell>94.5</cell><cell>86.0</cell></row><row><cell>WCNN</cell><cell>95.4</cell><cell>93.9</cell><cell>86.9</cell></row><row><cell>WCNN + low-rank</cell><cell>97.4</cell><cell>96.0</cell><cell>91.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Editorial: Special issue on ubiquitous biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on heterogeneous face recognition: Sketch, infra-red, 3d and low-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="28" to="48" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition: A common encoding feature discriminant approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2079" to="2089" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The casia nir-vis 2.0 face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="348" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face photo recognition using sketch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Face shape recovery from a single image using cca mapping between tensor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multidimensional scaling for matching low-resolution face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2019" to="2030" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seeing the forest from the trees: A holistic approach to near-infrared heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Perception Beyond the Visible Spectrum</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matching NIR face to VIS face using transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="501" to="514" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recent advances on crossdomain face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="147" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coupled discriminative feature learning for heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="640" to="652" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transferring deep representation for nir-vis heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shared representation learning for heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning mappings for face synthesis from near infrared to visual light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The BUAA-VisNir face database instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>IRIP- TR-12-FR-001</idno>
		<imprint>
			<date type="published" when="2012-07" />
			<pubPlace>Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Beihang University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Viplfacenet: An open source deep face recognition sdk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Coupled feature selection for cross-sensor iris recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Biometrics: Theory, Applications and Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="687" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An analysis-by-synthesis method for heterogeneous face biometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face photo-sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1955" to="1967" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face sketch synthesis algorithm based on e-hmm and selective ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="487" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-coupled dictionary learning with applications to image super-resolution and photosketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2216" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coupled dictionary and feature space learning with applications to cross-domain image synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2496" to="2503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">NIR-VIS heterogeneous face recognition via cross-spectral joint dictionary learning and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Not afraid of the dark: Nirvis face recognition via cross-spectral hallucination and low-rank embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition from local structures of normalized appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition: Matching NIR to visible light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1513" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluation of face recognition system in heterogeneous environments (visible vs NIR)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Windridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2160" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning modality-invariant features for heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1683" to="1686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matching forensic sketches to mug shot photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="639" to="646" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-modality feature learning through generic hierarchical hyperlingual-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="451" to="463" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inter-modality face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Coupled discriminant analysis for heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1707" to="1716" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Regularized discriminative spectral regression method for heterogeneous face matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="353" to="362" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning coupled feature spaces for cross-modal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2088" to="2095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition using kernel prototype similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1410" to="1422" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-view discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="188" to="194" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mutual component analysis for heterogeneous face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning invariant deep representation for nir-vis face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2000" to="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875v2</idno>
	</analytic>
	<monogr>
		<title level="j">Wasserstein GAN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Order-preserving wasserstein distance for sequence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation with block-diagonal prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">MS-Celeb-1M: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/1607.08221</idno>
		<ptr target="http://arxiv.org/abs/1607.08221" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A light CNN for deep face representation with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<idno>CoRR abs/1511.02683</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Coupled spectral regression for matching heterogeneous faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1123" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning discriminant face descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="302" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On effectiveness of histogram of oriented gradient features for visible to near infrared face matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Dhamecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1788" to="1793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
