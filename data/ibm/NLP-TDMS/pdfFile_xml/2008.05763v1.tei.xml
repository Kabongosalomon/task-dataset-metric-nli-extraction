<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Powers of layers for image-to-image translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Powers of layers for image-to-image translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a simple architecture to address unpaired image-to-image translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. We start from an image autoencoder architecture with fixed weights. For each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. A specific training schedule is required to alleviate the exponentiation effect of the iterations. At test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. This is useful, for instance, when the type or amount of noise to suppress is not known in advance. Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. The performance of our model is comparable or better than CycleGAN with significantly fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Generative adversarial networks (GANs) [13] is a framework where two networks, a generator and a discriminator, are learned together in a zero-sum game fashion. The generator learns to produce more and more realistic images wrt. the training dataset with real images. The discriminator learns to discriminate better and better between real data and generated images. GANs are used in many tasks such as domain adaptation, style transfer, inpainting and talking head generation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural networks define arbitrarily complex functions involved in discriminative or generative tasks by stacking layers, as supported by the universal approximation theorem <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12]</ref>. More precisely, the theorem states that stacking a number of basic blocks can approximate any function with arbitrary precision, provided it has enough hidden units, with mild conditions on the non-linear basic blocks.</p><p>Studies on non-linear complex holomorphic functions involved in escape-time fractals showed that iterating simple non-linear functions can also construct arbitrarily complex landscapes <ref type="bibr" target="#b1">[2]</ref>. These functions are complex in the sense that their iso-surfaces are made arbitrarily large by increasing the number of iterations. Yet there is no control on the actual shape of the resulting function. This is why generative fractals remain mathematical curiosities or at best tools to construct intriguing landscapes.</p><p>Our objective is to combine the expressive power of both constructions, and study the optimization of a function that iterates a single building block in the latent space of an auto-encoder. We focus on image translation tasks, that can be trained from either paired or unpaired data. In the paired case, pairs of corresponding input and output images are provided during training. It offers a direct supervision, so the best results are usually obtained with these methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>In this paper we focus on the unpaired case: only two corpora of images are provided, one for the input domain A and the other for output domain B. Therefore we do not have access to any parallel data <ref type="bibr" target="#b7">[8]</ref>, which is a realistic scenario in many applications, e.g., image restoration. We train a function f AB : A → B, such that the output b * = F (a) for a ∈ A is indiscernible from images of B.</p><p>Our transformation is performed by a single residual block that is composed a variable number of times. We obtain this compositional property thanks to a progressive learning scheme that ensures that the output is valid for a large range of iterations. As a result, we can modulate the strength of the transformation by varying the number of times the transformation is composed. This is of particular interest in image translation tasks such as denoising, where the noise level is unknown at training time, and style transfer, where the user may want to select the best rendering. This "Powers of layers" (PoL) mechanism is illustrated in <ref type="figure">Figure 1</ref> in the category transfer context (horse to zebra).</p><p>Our architecture is very simple and only the weights of the residual block differ depending on the task, which makes it suitable to address a large number of tasks with a limited number of parameters. This proposal is in sharp contrast with the trend of current state-of-the-art works to specialize the architecture and to increase its complexity and number of parameters <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b6">7]</ref>. Despite its simplicity, our proof of concept exhibits similar or better performance than a vanilla CycleGAN architecture, all things being equal otherwise, for the original set of image-to-image translation tasks proposed in their papers, as well as for denoising, deblurring and deblocking. With significantly fewer parameters and a versatile architecture, we report competitive results confirmed by objective and psycho-visual metrics, illustrated by visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminator Encoder Decoder</head><p>Stopping criterion CONV CONV NORM res AB RELU <ref type="figure">Figure 1</ref>: Illustration of Powers of layers for a category transfer task. The encoder and decoder are directly borrowed from a vanilla auto-encoder and are not learnable. At inference time, we apply a variable number of compositions, producing different images depending on how many times we compose the residual block in the embedding space. Depending on the task, we either modulate the transformation and choose the result, or let a discriminator determine when to stop iterating.</p><p>Unpaired image-to-image translation considers the tasks of transforming an image from a domain A into an image in a domain B. The training set comprises a sample of images from domains A and B, but no pairs of corresponding images. A classical approach is to train two generators (A → B and B → A) and two discriminators, one for each domain. When there is a shared latent space between the domains, a possible choice is to use a variational auto encoder like in CoGAN <ref type="bibr" target="#b28">[29]</ref>. CycleGAN <ref type="bibr" target="#b47">[48]</ref>, DualGAN <ref type="bibr" target="#b42">[43]</ref> and subsequent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref> augment the adversarial loss induced by the discriminators with a cycle consistency constraint to preserve semantic information throughout the domain changes. All these variants have architectures roughly similar to CycleGAN: an encoder, a decoder and residual blocks operating on the latent space. They also incorporate elements of other networks such as StyleGAN <ref type="bibr" target="#b22">[23]</ref>. In our work, we build upon a simplified form of the CycleGAN architecture that generalizes over tasks easily. High resolution images with GANs. Generating high-resolution images is challenging. Until recently, GANs architectures were designed to produce low-resolution images. Indeed, the memory usage at training time depends heavily on the size of the images. The general approach is to produce the target image in a scale pyramid, which outputs results of much finer quality <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21]</ref>. Generating high resolution images makes it possible to get closer to the format of real pictures used in CGI <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7]</ref> or medical imaging <ref type="bibr" target="#b44">[45]</ref>. Transformation modulation is an interpolation between two image domains. It is a byproduct of some approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35]</ref>. For instance, a linear interpolation in latent space <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref> morphs between two images. Nevertheless, one important limitation is that the starting and ending points must both be known, which is not the case in unpaired learning. Other approaches such as the Fader networks <ref type="bibr" target="#b24">[25]</ref> or StyleGan2 <ref type="bibr" target="#b39">[40]</ref> act on scalar or boolean attributes that are disentangled in the latent space (eg., age for face images, wear glasses or not, etc). Nevertheless, this results in complex models, for which dataset size and the variability of images strongly impacts the performance: they fail to modulate the transform with small datasets or with large variabilities. A comparison of PoL with the Fader network is provided in Appendix C and shows that our approach is more effective. Progressive learning and inference time modulation are performed in multi-scale methods such as SinGAN <ref type="bibr" target="#b35">[36]</ref> and ProgressiveGAN <ref type="bibr" target="#b21">[22]</ref>. Progressive learning obtains excellent results for high resolution images where it is more difficult to use classical approaches. The training is performed in several steps during which the size of both the images and the network are increased.</p><p>The inference time of some architectures can be modulated by stopping the forward pass at some layer <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9]</ref>. This differs from our approach, where the number of residual block compositions ("powers") can be chosen, to shorten the inference. A by-product is a reduction of the number of network parameters. Weight sharing is a way of looking at our method, because the same layer is applied several times within the same network. Recurrent Neural Networks (RNN) are the most classical example weight sharing in a recursive architecture. Besides RNNs, weight sharing is mainly used for model compression, sequential data and ordinary differential equations (ODE) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6]</ref>. A few works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b45">46]</ref> apply weight sharing to unfold a ResNet and evaluate its performance in classification tasks. The optimization is inherently difficult, so they use independent batch normalization for each shared layer. With PoL we observe the same optimization issues, that we solve by a progressive training strategy, see Section 3.3. Recent work <ref type="bibr" target="#b19">[20]</ref> are interested in the composition of the same block by considering the parallel with the fixed point theorem, nevertheless their application remains to rather simple problems compared to our unpaired image-to-image translation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Power of layers</head><p>We adopt the same context as CycleGAN <ref type="bibr" target="#b47">[48]</ref> and focus on unpaired image to image translation: the objective is to transform an image from domain A into an image from domain B. In our case the domains can be noise levels, painting styles, blur, JPEG artifacts, or simply object classes that appear in the image. The training is unpaired: we do not have pairs of corresponding images at training time. CycleGAN is simple, adaptable to different tasks and allows a direct comparison in Sections 4 and 5.</p><p>We learn two generators and two discriminators. The generator G AB : I → I transforms an element of A into an element of B, and G BA goes the other way round, I being the fixed-resolution image space. The discriminators D A : I → [0, 1] (resp. D B ) predicts whether an element belongs to domain A (resp B). We use the same losses as commonly used in unpaired image-to-image translation:</p><formula xml:id="formula_0">L Total = λ Adv L Adv + λ Cyc L Cyc + λ Id L Id ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">L Adv (G AB , D B ) = E b∼B [log D B (b)] + E a∼A [log(1 − D B (G AB (a)))], L Cyc (G AB , G BA ) = E b∼B [ G AB (G BA (b)) − b 1 ] + E a∼A [ G BA (G AB (a)) − a 1 ], L Id (G AB , G BA ) = E b∼B [ G AB (b) − b 2 ] + E a∼A [ G BA (a) − a 2 ].</formula><p>The Adversarial loss L Adv (G AB , D B ) verifies that the generated images are in the correct domain. The Cycle Consistency loss L Cyc (G AB , G BA ) ensures a round-trip through the two generators reconstructs the initial image, and the identity loss L Id (G AB , G BA ) penalizes the generators transforming images that are already in their target domain. We keep the same linear combination coefficients as in CycleGAN <ref type="bibr" target="#b47">[48]</ref>: λ Adv = 1, λ Cyc = 10, λ Id = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network architecture</head><p>We start from the CycleGAN architecture <ref type="bibr" target="#b47">[48]</ref>. The encoder and decoder consist of 2 layers and a residual block. The embedding space E of our model is 256 × 64 × 64: its spatial resolution is 1/4 the input image resolution of 256 × 256 and it has 256 channels. All translation operations take place in the fixed embedding space E. The encoder Enc : I → E produces the embedding and consists of two convolutions. The decoder Dec : E → I turns the embedding back to image space and consists of two transposed convolutions.</p><p>Note that we will provide the implementation for the sake of reproducibility. Pre-training of the auto-encoder. We train the encoder and decoder of our model on a reconstruction task with 6M images randomly drawn from the YFCC100M dataset <ref type="bibr" target="#b36">[37]</ref> during one epoch, using an 2 reconstruction loss in pixel space. We use the Adam optimizer with a learning rate of 16 × 10 −4 . Our data-augmentation consists of an image resizing, a random crop and a random horizontal flip. Both the encoder and decoder weights are fixed for all the other tasks, only the residual block is adapted (and the discriminator in case we use it for the stopping criterion). The embedding transformer -single block. The transformation between domains is applied is based on a residual block f AB , similar to the feed-forward network used in transformers <ref type="bibr" target="#b37">[38]</ref>. It writes:</p><formula xml:id="formula_2">f AB (x) = x + res AB (x), ∀x ∈ A.<label>(2)</label></formula><p>There is a dimensionality expansion factor K between the two convolutions in the residual block (see <ref type="figure">Figure 1</ref>). Adjusting K changes the model's capacity. We adopt the now standard choice of the original transformer paper (K = 4). The full generator writes</p><formula xml:id="formula_3">G AB (x) = Dec(f AB (Enc(x))), ∀x ∈ A.<label>(3)</label></formula><p>The other direction, with f BA and res BA , is defined accordingly. Powers of layers. We start from the architecture above and augment its representation capacity. There are two standard ways of doing this: (1) augmenting the capacity of the f AB block by increasing K; (2) increasing the depth of the network by chaining several instances of f AB , since the intermediate representations are compatible. In contrast to these fixed architectures, PoL iterates the f AB block n ≥ 1 times, which amounts to sharing the weights of a deeper network:</p><formula xml:id="formula_4">G AB (x) = Dec(f n AB (Enc(x))), ∀x ∈ A.<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization in a residuals blocks weight sharing context</head><p>In the following, we drop the AB suffix from f AB , since powers of layers operates in the same way on f AB and f BA . Thus,</p><formula xml:id="formula_5">f : E → E is f (x) = x + res(x).</formula><p>The parameters of f are collected in a vector w. The embedding x ∈ E is 3D activation map, but for the sake of the mathematical derivation we linearize it to a vector. The partial derivatives of f are ∂f ∂x = ∂res ∂x + Id and ∂f ∂w = ∂res ∂w . We compose the f function n times as  <ref type="table">Table 1</ref>: Denoising: PSNR on Urban-100 <ref type="bibr" target="#b16">[17]</ref>. Comparison between Power of layers (POL) and independent (ind) blocks for different maximum number of composition / residual block. Best value for each column are in Bold. We could not fit more than 16 independent blocks in memory in our experiments. We provide standard deviation and additional results in Appendix A.</p><formula xml:id="formula_6">∂f n ∂x (x) = 0 i=n−1 ∂f ∂x (f i (x)) and ∂f n ∂w (x) = 1 i=n−1 ∂f ∂x f i (x) ∂f ∂w (x).<label>(5)</label></formula><p>The stability of the SGD optimization depends on the magnitude and conditioning of the matrix M n defined as:</p><formula xml:id="formula_7">M n = 1 i=n−1 ∂f ∂x f i (x) = 1 i=n−1 ∂res ∂x f i (x) + Id ,<label>(6)</label></formula><p>which is sensitive to initialization during the first optimization epochs. Indeed, the length of the SGD steps on w depends on the eigenvalues of M n . When simplifying the basic residual block to a linear transformation L ∈ R d×d (i.e., ignoring the normalization and the ReLU non-linearity), we have M n = (L + Id) n−1 . The eigenvalues of M n are (λ i + 1) n−1 , where λ 1 , ..., λ d are the eigenvalues of L. At initialization, the components of L are sampled from a random uniform distribution. To reduce the magnitude of λ i , one option is to make the entries of L small. However, to decrease (λ i + 1) n−1 sufficiently, λ i must be so small that it introduces floating-point cancellations when the residual block is added back to the shortcut connection. This is why we prefer to adjust n, as detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Progressive training</head><p>We adopt a progressive learning schedule in a "warm up" phase: we start the optimization with a single block and add one iteration at every epoch until we reach the required n blocks. This is possible because the blocks operate in the same embedding space E and because their weights are shared, so all blocks are still in the same learning schedule. In addition, this approach allows the discriminator to improve progressively during the training. For example, in the case of the transformation horse → zebra, a slightly whitened horse fools the discriminator at the beginning of the training, but a stronger stripes texture is required later on. Training for modulation. If the network is trained with a fixed number of compositions, the intermediate states do not correspond to modulations of the transformation that "look right" (see Appendix A). Therefore, in addition to this scheduled number of iterations during the first n epochs of warm up, we also randomize the number of iterations in subsequent epochs. This forces the generator to also produce acceptable intermediate states, and enables modulating the transform. Stopping criterion at inference time. Each image of domain A is more or less close to domain B. For example, when denoising, the noise level can vary so the denoising strength should adapt to the input. Similarly for horse→zebra: a white horse is closer to a zebra than a brown horse. Therefore, at inference time, we can adjust the number of compositions as well. In particular, for each test image, we select n that best deceives the discriminator, thus effectively adapting the processing to the relative distance to the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>In this section we study the impact of the main training and design choices and on the performance of powers of layers. Appendices A and B provide complementary analysis for training-and inference-time choices, respectively.</p><p>For this preliminary analysis, we focus on denoising tasks for which the performance is easily measurable. We add three types of noise to images: Gaussian noise, Gaussian blur and JPEG artifacts. The noise intensity is quantified by the noise standard deviation, the blur radius and the JPEG quality factor, respectively. We generate transformed images and measure how well our method recovers the initial image.</p><p>Note that, in the literature, these tasks are best addressed by providing (original, noisy) pairs if images. Our objective is to remain in a completely unpaired setting during the training phase. It corresponds to the case where parallel data is not available (like for the restoration of ancient movies), and also better reflects the situation where the noise strength is not known in advance. Therefore, the original image is solely employed to measure the performance. This provides a more reliable signal than more classical unpaired image-to-image translation evaluations. Experimental protocol. To train, we sample 800 domain A images from the high-resolution Div2K dataset <ref type="bibr" target="#b0">[1]</ref>. In the baseline training procedure, the warm up phase starts from a single block and increases the number of compositions at every epoch, until we reach epoch n tr . Then we keep the number of compositions fixed.</p><p>We test on the Urban-100 <ref type="bibr" target="#b16">[17]</ref> dataset. Unless specified otherwise, we set the number of compositions to n te = n tr and measure the Peak Signal to Noise Ratio (PSNR) of our model on the dataset images, degraded with the same intensity as at training time. For the JPEG case we use the Naturalness Image Quality Evaluator metric <ref type="bibr" target="#b29">[30]</ref> (NIQE, lower=better) instead, because it is more sensitive to JPEG artifacts. NIQE is a perceptual metric that does not take the original image into account. Block composition or independent successive blocks? <ref type="table">Table 1</ref> compares PoL's composition of the same block versus using independent blocks with distinct weights. In spite of the much larger capacity offered by independent blocks, the noise reduction operated by Power of layers is stronger. Our interpretation is that the model is easier to train. Analysis of the progressive training strategy. <ref type="table">Table 1</ref> also evaluates the impact of the maximum number of compositions n tr . Having several compositions clearly helps. Since we choose the number of compositions n te at inference time (see next paragraph), it may be relevant to vary n tr at training time to minimize the discrepancy between the train-and test-time settings.</p><p>For this, we tried different intervals to randomly draw the maximum number of compositions for each epoch, after the warmup phase. If n te is fixed, the optimal choice is n tr = n te . However, if we use an adaptive n te , the best range is n tr ∈ [ <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>], and the adaptive case with randomised training gives the best performance for denoising and debluring. Appendix A reports results obtained with different n tr ranges. Stopping criterion. We consider two cases: either we use a fixed n te , or we use the discriminator to evaluate the transformation quality: it selects the value n te maximizing the target discriminator error for a given image. <ref type="figure" target="#fig_0">Figure 2</ref> shows that setting a fixed n tr causes the discriminator to select n te = n tr as the best iteration at inference time. By selecting the best n te for each image we obtain on average a PSNR improvement of +1.36dB for a Gaussian noise of standard deviation 30, compared to fixing n te . In Appendix B, we compare it with the best possible stopping criterion: an Oracle that selects n te directly on the PSNR. Our adaptive strategy significantly tightens the gap to this upper bound. Comparison with CycleGAN. We use CycleGAN as a baseline. The differences between CycleGAN and powers of layers are (1) we use a single encoder and decoder trained in advance and common to all tasks; (2) CycleGAN has 9 residual blocks, PoL iterates a single residual block an arbitrary number of times. The inference time of PoL depends on the number of compositions n te but the number of parameters does not:</p><formula xml:id="formula_8">encoder + decoder residual block discriminators total PoL 1× 1.7M 2 × K × 1.1M 2×2.7M 15.9M CycleGAN 2× 11.4M 2×2.7M</formula><p>28.2M <ref type="figure">Figure 3</ref> compares the performance obtained by the two methods on denoising tasks with varying noise intensities. PoL gives better results than CycleGAN in terms of objective metrics, and overall the images produced by our method look as realistic and/or accurate. Training with few images. <ref type="figure">Figure 3</ref> also compares our method with CycleGAN when training in a data-starving scenario. Whatever the number of training images, PoL outperforms CycleGAN, but the gap is particularly important with very few images. This is expected for two reasons. Firstly, our approach has fewer parameters to learn than CycleGAN. Secondly, it only requires to learn the transformation because the encoder and decoder are pre-learned as a vanilla auto-encoder, while CycleGAN needs to learn how to encode and decode images. Parametrization: remarks. Beyond the settings inherited from CycleGAN, the main training parameters of Powers of layers are the maximum number of compositions n tr and the range from which they are randomly sampled. The number of compositions at inference time n te is also important but the discriminator criterion can be used to set it automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We now run experiments on two image generation applications. We refer to Section 3.1 for the architecture and training protocol.</p><p>In Appendix C we also give a comparison with the Fader network for the capacity to modulate a transformation, and more visual examples in Appendix D.</p><p>Unpaired image-to-image translation. We report results for 6 of the 8 unpaired image-to-image translation tasks introduced in the CycleGAN <ref type="bibr" target="#b47">[48]</ref> paper (the two remaining ones lead to the same conclusions) and we used the datasets from the website <ref type="bibr" target="#b46">[47]</ref>. We compare the Frechet Inception Distance (FID) of these two approaches in <ref type="figure">Figure 4</ref>. The FID measures the similarity between two datasets of images, we use it to compare the target dataset with the transformed dataset. It is a noisy measure for which Impact of the noise intensity applied to Urban-100 images Impact of the amount of training data PSNR (higher=better) NIQE  <ref type="figure">Figure 3</ref>: Top: Comparison between Powers of layers (PoL) and CycleGAN (CLG) to denoise images of Urban-100 <ref type="bibr" target="#b16">[17]</ref>. We provide standard deviation and additional results in Appendix C. Bottom: Visual comparison between our method (manual and discriminator choice) and CycleGAN for deblocking, denoising and debluring.</p><note type="other">(lower=better) PSNR NIQE Noise Noisy Denoising Sigma Blur Debluring JPEG JPEG Deblocking # train Denoise</note><p>only large deviations are significant. Yet the results and visualization show that our method has results comparable to those of CycleGAN, achieved with much fewer parameters.</p><p>High resolution experiments. PoL is fully convolutional architecture, therefore it is technically possible to apply models trained in a low resolution to high resolution images. However, the results are not always convincing, as shown in <ref type="figure">Figure 5</ref> (right) where the model trained on low resolution images does not create stripes at the "right" scale on zebras. To circumvent this problem, CycleGAN trains on patches taken from high resolution images. This works for transformations affecting the whole image (painting↔photo), but this is not applicable in the case where only a part of the image is affected (horse→zebra). In contrast, our proposal can adapt the memory used by changing its number of compositions, so we can apply it to very large images without running out of memory. <ref type="figure">Figure 5</ref> (left) depicts results obtained with our method trained on high resolution images.</p><p>Combining transformation. The different blocks associated with different transformations operate in the same embedding space for different tasks. Hence we can compose transformations, each being realized by one residual block. We train Transform #1 in the usual way, then freeze its residual block. Transform #2 is trained on the output of #1. Visual results are in <ref type="figure">Figure 6</ref>. The composition in the embedding space gives better results than decoding/encoding to image space mid-way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Powers of layers consists in iterating a residual block to learns a complex transformation with no direct supervision.  In this supplementary material we report additional analyses, results and examples that complement our paper. In Appendix A we consider the training phase, which supports the importance of our progressive training strategy compared to one with a fixed number of iterations. Appendix B considers the inference-time choices, in particular possible strategies to select the number of iterations. Appendix C provides additional comparison to CycleGAN and makes a comparison with the Fader network. Finally we present additional visual results for high resolution images and illustrate the progressive evaluation of results along iterations in the appendix D.</p><p>A Analysis of our progressive training strategy <ref type="figure" target="#fig_2">Figure 7</ref> shows the different training strategies we explore. The degree of freedom that we can adjust is the number of compositions. It can be set independently per training mini-batch. Progressive training versus fixed training. <ref type="figure">Figure 8</ref> illustrates the modulation of the horse→zebra transformation. This is effective only with our progressive learning, which forces the network to produce acceptable intermediate states.</p><p>If the network is trained with a fixed number of compositions, the intermediate states do not correspond to modulations of the transformation. The output is satisfactory only when the n te = n tr . In all other cases we observe artifacts in images, which therefore do not qualify as natural images. The generated images do not look right, and the source and target discriminators would not accept them as real images.</p><p>In contrast, with our progressive training, each number of iterations produces a satisfactory output. Iterating the residual block gradually transforms the horse into a zebra. Warm-up phase. <ref type="figure">Figure 9</ref> compares the performance obtained during the first three epochs of learning with and without progressive training, with different maximum number of compositions n tr . We compare this way of stabilizing the training with another classical approach: reducing the ranges of initialisation of the residual blocks. <ref type="figure">Figure 9</ref> shows that changing the initialization improves the performance during the first epoch, but that a warm-up phase with progressive training is more effective to improve the optimization stability. <ref type="figure" target="#fig_2">Figure 7</ref> shows the evolution of the number of compositions for different training strategies. Block composition or independent successive blocks? <ref type="table" target="#tab_3">Table 2</ref> provides results that complement <ref type="table">Table 1</ref> in the main paper. It compares the performance with (1) the composition of the same block or (2) using independent residual blocks. In particular, we report standard deviations that assess the statistical significance of our improvement. Choice of the maximum number of compositions. <ref type="table" target="#tab_4">Table 3</ref> compares the PSNR obtained for denoising and deblurring and NIQE for deblocking task on Urban-100 <ref type="bibr" target="#b16">[17]</ref>. We report more results and standard deviations compared to the main paper. Note that these tasks work well with a relatively low maximum of iterations, in contrast to style transfer image-to-image translations, which require more complex functions.   Composition step. <ref type="table" target="#tab_6">Table 4</ref> compares different choices for the number of steps of augmentation associated with the number of compositions n tr . As we can see, taking too large steps tends to affect performance, it is better to ramp up the number of compositions quickly during the warm-up phase.</p><p>Comparison between randomised and fixed number of compositions n tr . <ref type="figure" target="#fig_4">Figure 10</ref> compares the trajectories of PSNRs as a function of the number of Powers-of-layers composition. We get a better average performance if we randomly draw the maximum number of composition. The different positions of the maxima in the adaptive case also suggests that it is necessary to adjust the amount of transformation to each image, as discussed below.  <ref type="figure">Figure 8</ref>: Comparison between our progressive training approach and a non progressive approach. We represent the images obtained by varying the number of iterations at inference time n te in the network that transforms from domain A (horse) into domain B (zebra). The first image (n te =0) is the original image. Since our method was learned with n tr =30 compositions the last two images are extrapolations. Our progressive training is key to ensure that all outputs look like natural images and therefore that we can modulate transformation strength at inference time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSNR (dB)</head><p>Fixed, Init 0.01 Fixed, Init 0.1 Fixed, Init 1.0 Progressive, Init 1.0 <ref type="figure">Figure 9</ref>: Difference in PSNR between fixed learning and progressive learning during the first training epochs, evaluated on a denoising task. Green: n tr =30 orange: n tr =16 red: n tr =4     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Analysis of choices at inference time</head><p>Adjusting n te at inference time. Each image is more or less distant from the target domain, so we explore adapting the transformation to each image rather than applying a fixed transformation. For example, depending on the amount of noise, we may want to adjust the strength of the denoising. By modulating n te we can adapt the transformation to each image. <ref type="figure" target="#fig_6">Figure 11</ref> shows that the more noisy the input image is, the more we should compose to best denoise with Powers-of-Layers. Stopping criterion: fixed, discriminator, versus an Oracle. At inference time the number of composition n te applied to each image can be set using several strategies. We can choose to apply a constant number of composition or use the discriminator to choose the n te for which it gets the best response. <ref type="table" target="#tab_7">Table 5</ref> compares the performance of two strategies at test time for different random ranges at training time, and compare it to the upper bound achieved by an Oracle (i.e,. the performance attained when the optimal number of iteration is known for each image).</p><p>With n tr =30, we have chosen different ranges of the form [[d × 30, 30]] for d ∈ {100%, 75%, 66%, 50%, 33%, 25%, 0%}. The optimal range for debluring and denoising is with d = 66% C Additional comparisons with CycleGAN and the Fader Network <ref type="table">Table 6</ref> compares the results obtained with PoL and CycleGAN for different noise levels. <ref type="table" target="#tab_9">Table 7</ref> compares the results obtained with PoL and CycleGAN for different amounts of data. These numbers are the same as <ref type="figure">Figure 3</ref>, with standard deviations. In most cases, whether with different amounts of data or different noise, our method is better than CycleGAN. This is mainly due to its smaller number of parameters and the flexibility brought by the adaptive criterion. Experiments on transformation adjustment As baseline we use the Fader network <ref type="bibr" target="#b24">[25]</ref> for transformation adjustment. The Fader Network is a neural network composed of an encoder and a decoder, for which it it is possible to modulate a transformation. This is done by removing the factors of variations related to this transformation in the latent space resulting from the encoder, and in turn by choosing the factors to be added to the embedding going into the decoder.</p><p>To interpolate between domain A and domain B, the Fader network has a latent representation where the attributes relative to each domain have been disentangled. The Fader network has been applied to faces, for instance to add glasses on a face, to <ref type="table">Table 6</ref>: Comparison between our approach (PoL) and CycleGAN. We three tasks, all computed with the Urban-100 dataset: PSNR (higher is better) with different amount of Gaussian noise and Gaussian blur, and NIQE (lower is better) measured for different JPEG compression quality.  age a person, etc. We observe experimentally with smaller datasets, where the variability from one image to another is larger than with faces, that the Fader's results are not as good.</p><p>In contrast, our approach, like CycleGAN, does not have limitations incurred by a latent space disentanglement because it exploits a cyclic loss. <ref type="figure" target="#fig_0">Figure 12</ref> shows the results obtained with the Fader network and with our method on the Horse→Zebra transformation adjustment. The Fader network is unable to significantly transform the source when the network is too shallow (3 layers) and destroys the image when it is deep (6 layers). In contrast, Powers-of-layers convincingly hybridizes a horse and a zebra. In terms of FID for the Horse to Zebra task, the Fader network is significantly worse: it obtains a FID greater than 163.0 in the both case against 53.0 for our method (lower is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional results: visualizations of transform modulation and high resolution</head><p>Progressive results <ref type="figure">Figure 13</ref> shows the progressive transformations obtained on different tasks. It shows that progressive transformations are realistic for most tasks. Results in high resolution. <ref type="figure">Figure 14</ref> shows the high-resolution results of the section 5, along with the original images.  <ref type="figure" target="#fig_0">Figure 12</ref>: Visual comparison between Fader networks <ref type="bibr" target="#b24">[25]</ref> and our power-of-layers on the task Horse to Zebra.  <ref type="figure">Figure 13</ref>: Illustration of the different results obtained along the iterations of the recurrent block, during the transformation of a horse into a zebra. The first image is the original image, then each image corresponds to 5 additional compositions of our method. Since our method was learned with a maximum of 30 compositions the last two images are extrapolations. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Response of the discriminators as a function of the number of compositions for the transformation Gaussian noise→natural image. We plot the average response and the standard deviation over examples (gray). Higher=the discriminator classifies the image into its target domain. Top: training with a fixed number of compositions, n tr =30. Bottom: training with randomised n tr ∈ [<ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30]</ref>].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Left: Different visual results with high resolution images. See the original images in Appendix D. Right: Comparisons of the generations obtained by models trained either with high-resolution or low-resolution images, applied to a high-resolution test image. Composition of transformations in the embedding/image space. Supplementary material for "Powers of layers for image-to-image translation" Hugo Touvron, Matthijs Douze, Matthieu Cord, Hervé Jégou</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Number of compositions at training time for different training strategies. The number of compositions is adjusted per batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Evolution of the average PSNR as well as for different individual images according to the number of compositions n te . The maximum number of compositions used for training is n tr = 30 and the Gaussian Noise standard deviation is 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Evolution of the optimal number of composition according to the noise. The maximum number of compositions used for training is 30 and the Gaussian Noise standard deviation is 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>On various tasks, power of layers gives similar performance to CycleGAN with fewer parameters. The flexibility offered by the common embedding space can be used to modulate the strength of a transformation or to compose several transformations. While in most examples the discriminator is only used for training, Powers of layers can also exploit it to adjust the transformation to the input image at inference time.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>horse→zebra</cell><cell>summer→winter</cell><cell>Monet→photo</cell><cell>orange→apple</cell></row><row><cell>Domain Summer → Winter Summer ← Winter Horse → Zebra</cell><cell>CycleGAN 48.8 48.4 89.7</cell><cell>POL 46.1 44.4 53.0</cell><cell>Original image</cell></row><row><cell>Horse ← Zebra</cell><cell>110.5</cell><cell>112.3</cell><cell></cell></row><row><cell>Van-Gogh → Picture</cell><cell>163.4</cell><cell>134.4</cell><cell></cell></row><row><cell>Van-Gogh ← Picture Cezanne → Picture Cezanne ← Picture Monet → Picture</cell><cell>151.4 127.4 145.5 60.3</cell><cell>152.7 138.8 147.6 70.3</cell><cell>CycleGAN</cell></row><row><cell>Monet ← Picture</cell><cell>61.8</cell><cell>82.1</cell><cell></cell></row><row><cell>Apple → Orange Apple ← Orange</cell><cell>88.9 116.7</cell><cell>83.2 113.2</cell><cell>Powers of layers</cell></row><row><cell cols="5">Figure 4: Left: Frechet Inception Distance (FID) obtained on image-to-image translation tasks (lower is better). We compare</cell></row><row><cell cols="5">CycleGAN and our Powers of layers method. Right: example results.</cell></row><row><cell>Photo → Van Gogh</cell><cell></cell><cell cols="2">Photo → Monet</cell><cell>Horse → Zebra</cell><cell>Trained at 512 × 512</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Trained at 256 × 256</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>PSNR on Urban-100. Comparison between our choice (PoL) and independent blocks. ±0.<ref type="bibr" target="#b14">15</ref> 23.26 ±0.15 18.61 ±0.13 18.61 ±0.13 2 23.28 ±0.07 23.21 ±0.05 18.48 ±0.25 18.64 ±0.21 4 24.41 ±0.27 23.16 ±0.09 19.19 ±0.04 19.17 ±0.51 8 23.91 ±0.23 22.27 ±0.02 18.95 ±0.14 19.33 ±0.29 12 23.91 ±0.10 22.48 ±0.32 19.70 ±0.21 18.76 ±0.09 16 23.88 ±0.14 22.54 ±0.48 19.00 ±0.30 18.11 ±0.41</figDesc><table><row><cell>Number</cell><cell cols="2">Gaussian noise (std 30)</cell><cell cols="2">Gaussian blur (sigma 4)</cell></row><row><cell>of blocks</cell><cell>POL</cell><cell>independent</cell><cell>POL</cell><cell>independent</cell></row><row><cell>1</cell><cell>23.26</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison between different maximum number of compositions. We report the most adapted metric on Urban-100: PSNR for the Gaussian noise and blur, NIQE for deblocking.</figDesc><table><row><cell></cell><cell cols="2">PSNR (higher=better)</cell><cell>NIQE (lower=better)</cell></row><row><cell cols="3">ntr Gaussian noise (std 30) Gaussian blur (σ=4)</cell><cell>JPEG (quality=25)</cell></row><row><cell>1</cell><cell>23.26 ±0.15</cell><cell>18.61 ±0.13</cell><cell>10.17 ±0.55</cell></row><row><cell>2</cell><cell>23.28 ±0.07</cell><cell>18.48 ±0.25</cell><cell>10.78 ±0.39</cell></row><row><cell>3</cell><cell>23.89 ±0.07</cell><cell>19.13 ±0.08</cell><cell>10.57 ±0.21</cell></row><row><cell>4</cell><cell>24.41 ±0.27</cell><cell>19.19 ±0.04</cell><cell>10.43 ±0.25</cell></row><row><cell>5</cell><cell>23.79 ±0.59</cell><cell>19.06 ±0.21</cell><cell>10.65 ±0.52</cell></row><row><cell>6</cell><cell>23.80 ±0.31</cell><cell>19.09 ±0.08</cell><cell>10.42 ±0.19</cell></row><row><cell>7</cell><cell>23.64 ±0.27</cell><cell>19.12 ±0.13</cell><cell>10.93 ±0.61</cell></row><row><cell>8</cell><cell>23.91 ±0.23</cell><cell>18.95 ±0.14</cell><cell>10.34 ±0.61</cell></row><row><cell>12</cell><cell>23.91 ±0.10</cell><cell>19.70 ±0.21</cell><cell>9.74 ±0.41</cell></row><row><cell>16</cell><cell>23.88 ±0.14</cell><cell>19.00 ±0.30</cell><cell>8.51 ±0.19</cell></row><row><cell>17</cell><cell>23.97 ±0.17</cell><cell>18.83 ±0.21</cell><cell>8.36 ±0.32</cell></row><row><cell>18</cell><cell>24.17 ±0.18</cell><cell>18.97 ±0.13</cell><cell>7.49 ±0.63</cell></row><row><cell>24</cell><cell>23.83 ±0.20</cell><cell>18.56 ±0.16</cell><cell>7.22 ±0.45</cell></row><row><cell>27</cell><cell>23.62 ±0.02</cell><cell>18.48 ±0.36</cell><cell>7.25 ±0.36</cell></row><row><cell>30</cell><cell>23.45 ±0.10</cell><cell>19.09 ±0.31</cell><cell>8.15 ±0.64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>PSNR on Urban-100 [17] -Gaussian noise (std=30). Comparison between augmentation steps during the warm-up phase. The augmentation step is to the number of epochs performed with the same number of compositions (1 epoch corresponds to 800 backward passes). ±0.27 24.44 ±0.11 23.62 ±0.16 23.93 ±0.10 23.22 ±0.07 16 23.88 ±0.14 23.97 ±0.22 23.57 ±0.06 23.52 ±0.1 23.19 ±0.02 30 23.45 ±0.10 23.77 ±0.14 23.95 ±0.17 23.14 ±0.04 23.17 ±0.04</figDesc><table><row><cell>ntr</cell><cell></cell><cell>augmentation step</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell></row><row><cell>4 24.41 fixed ntr = 30</cell><cell></cell><cell></cell><cell></cell><cell>Random ntr ∈ [[20, 30]]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Effect of setting the number of compositions n tr randomly on the PSNR on Urban-100 with two types of noise. We compare (Constant) a fixed n te = 30 and (Adaptive) value n te maximizing the target discriminator error for each image. Oracle: n te minimizing PSNR for each image.</figDesc><table><row><cell></cell><cell cols="3">Gaussian Noise (std=30)</cell><cell></cell><cell>Gaussian Blur (σ=4)</cell><cell></cell></row><row><cell>Random range</cell><cell>Constant</cell><cell>Adaptive</cell><cell>Oracle</cell><cell>Constant</cell><cell>Adaptive</cell><cell>Oracle</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>±0.<ref type="bibr" target="#b18">19</ref> 27.37 ±0.<ref type="bibr" target="#b25">26</ref> <ref type="bibr" target="#b1">2</ref> 21.58 20.37 ±0.<ref type="bibr" target="#b25">26</ref> 22.14 ±0.21 15 9.01 7.89 ±1.14 7.90 ±0.32 30 19.2 21.93 ±0.04 23.68 ±0.17 4 19.20 18.55 ±0.37 19.22 ±0.37 25 8.94 7.45 ±0.63 7.10 ±0.58 50 15.2 21.57 ±0.04 22.52 ±0.37 8 17.48 16.09 ±0.14 17.53 ±0.34 30 8.90 7.46 ±0.32 6.76 ±0.83 70 12.7 21.02 ±0.17 20.94 ±0.24 16 16.13 16.11 ±0.31 16.16 ±0.14 50 8.99 6.96 ±0.55 6.56 ±0.48 100 10.4 20.00 ±0.12 19.42 ±0.22 24 15.50 12.88 ±0.24 15.48 ±0.07 70 8.94 7.11 ±0.21 6.81 ±0.39</figDesc><table><row><cell>Noise</cell><cell>Noisy</cell><cell>Denoising</cell><cell></cell><cell>Sigma</cell><cell>Blur</cell><cell>Debluring</cell><cell></cell><cell>JPEG</cell><cell>JPEG</cell><cell>Deblocking</cell></row><row><cell>(std)</cell><cell cols="2">images CycleGAN</cell><cell>PoL</cell><cell>Blur</cell><cell>images</cell><cell>CycleGAN</cell><cell>PoL</cell><cell cols="2">quality images CycleGAN</cell><cell>PoL</cell></row><row><cell>15</cell><cell>24.9</cell><cell>22.37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison between CycleGAN and Power of layer on Urban-100<ref type="bibr" target="#b16">[17]</ref> with different amount of training data. We use PSNR to compare methods for Gaussian noise and Gaussian blur. ±0.<ref type="bibr" target="#b23">24</ref> 22.27 ±0.<ref type="bibr" target="#b15">16</ref> 14.36 ±0.<ref type="bibr" target="#b11">12</ref> 18.68 ±0.27 5 16.57 ±0.04 23.13 ±0.18 16.72 ±0.15 18.76 ±0.23 10 20.88 ±0.36 23.19 ±0.58 17.03 ±0.07 18.82 ±0.56 100 21.03 ±0.76 23.39 ±0.33 18.17 ±0.15 18.97 ±0.23 400 21.78 ±0.12 23.60 ±0.32 18.21 ±0.11 19.01 ±0.31 800 21.93 ±0.04 23.88 ±0.14 18.55 ±0.37 19.22 ±0.37</figDesc><table><row><cell>Number of data</cell><cell cols="2">Denoising (std=30)</cell><cell cols="2">Debluring (sigma=4)</cell></row><row><cell cols="2">training images CycleGAN</cell><cell>PoL</cell><cell>CycleGAN</cell><cell>PoL</cell></row><row><cell>1</cell><cell>14.50</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The science of fractal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinz-Otto</forename><surname>Mandelbrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Peitgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saupe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast image processing with fully-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2497" to="2506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<title level="m">Stargan v2: Diverse image synthesis for multiple domains. Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatially adaptive computation time for residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometry-consistent generative adversarial networks for one-sided unsupervised domain mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross domain model compression by structurally weight sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangqian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Residual connections encourage iterative inference. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younahan</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02868</idno>
		<title level="m">Differentiable fixed-point iteration layer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Karnewar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu Sesha</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Msg-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06048</idno>
		<title level="m">Multi-scale gradient gan for stable image synthesis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04958</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fader networks: Manipulating images by sliding attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Che</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Coco-gan: Generation by parts via conditional coordinating. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Universal approximation depth and errors of narrow belief networks with discrete units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Montúfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1386" to="1407" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Model compression via distillation and quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Polino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><forename type="middle">Rott</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Yfcc100m: the new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Stylegan2 distillation for feed-forward image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Viazovetskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Ivashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Kashin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogério</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<title level="m">Blockdrop: Dynamic inference paths in residual networks. Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2868" to="2876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Few-shot adversarial learning of realistic neural talking head models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandra</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08233</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Muckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Parente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hersh</forename><surname>Katsnelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Chandarana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">G</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Pinkerton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nafissa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Yakubova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">K</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><forename type="middle">W</forename><surname>Sodickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08839</idno>
		<title level="m">fastmri: An open dataset and benchmarks for accelerated mri</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recurrent convolutions: A model compression point of view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheolkon</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshops: Compact Deep Neural Network Representation with Industrial Applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<ptr target="https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/" />
		<imprint>
			<biblScope unit="page" from="2019" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>23.45 ±0.10 23.46 ±0.40 23.81 ±0.48 19.09 ±0.31 18.87 ±0.41 19.49 ±0.08</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>7, 30. 15, 30. 20, 30. 22, 30. 30, 30</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
