<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Recurrent Neural Networks for Acoustic Modelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
							<email>williamchan@cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
							<email>lane@cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Language Technologies Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Recurrent Neural Networks for Acoustic Modelling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Deep Neural Networks</term>
					<term>Recurrent Neural Net- works</term>
					<term>Long-Short Term Memory</term>
					<term>Asynchronous Stochastic Gradient Descent</term>
					<term>Automatic Speech Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel deep Recurrent Neural Network (RNN) model for acoustic modelling in Automatic Speech Recognition (ASR). We term our contribution as a TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with Time Convolution (TC), followed by a Bidirectional Long-Short Term Memory (BLSTM), and a final DNN. The first DNN acts as a feature processor to our model, the BLSTM then generates a context from the sequence acoustic signal, and the final DNN takes the context and models the posterior probabilities of the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ) eval92 task or more than 8% relative improvement over the baseline DNN models</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs) have yielded many state-of-the-art results in acoustic modelling for Automatic Speech Recognition (ASR) tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. DNNs and CNNs often accept some spectral feature (e.g., log-Mel filter banks) with a context window (e.g., +/-10 frames) as inputs and trained via supervised backpropagation with softmax targets learning the Hidden Markov Model (HMM) acoustic states.</p><p>DNNs do not make much prior assumptions about the input feature space, and consequently the model architecture is blind to temporal and frequency structural localities. CNNs are able to directly model local structural localities through the usage of convolutional filters. CNN filters connect to only a subset region of the feature space and are tied and shared across the entire input feature, giving the model translational invariance <ref type="bibr" target="#b2">[3]</ref>. Additionally, pooling is often added, which yields rotational invariance <ref type="bibr" target="#b1">[2]</ref>. The inherent structure of CNNs yields a model much more robust to small shifts and permutations.</p><p>Speech is fundamentally a sequence of time signals. CNNs (with time convolution) can capture some of this time locality through the convolution filters, however CNNs may not be able to directly capture longer temporal signal patterns. For example, temporal patterns may span 10 or more frames, however the convolution filter width may only be 5 frames wide. The CNN model must then rely on the higher level fully connected layers to model these long term dependencies. Additionally, one size may not fit all, the frame width of phones and temporal patterns are of varying lengths. Optimizing the convolution filter size is a expensive procedure and corpora dependent <ref type="bibr" target="#b3">[4]</ref>.</p><p>Recently, Recurrent Neural Networks (RNNs) have been introduced demonstrating power modelling capabilities for sequences <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. RNNs incorporate feedback cycles in the network architecture. RNNs include a temporal memory component (for example, in LSTMs the cell state <ref type="bibr" target="#b8">[9]</ref>), which allows the model to store temporal contextual information directly in the model. This relieves us from explicitly defining the size of temporal contexts (e.g., the time convolution filter size in CNNs), and allows the model to learn this directly. In fact in <ref type="bibr" target="#b7">[8]</ref>, the whole speech sequence can be accumulated in the temporal context.</p><p>There exist many implementations of RNNs <ref type="bibr" target="#b9">[10]</ref>. LSTM and Gated Recurrent Units (GRUs) <ref type="bibr" target="#b9">[10]</ref> are particular implementations of RNNs that are easy to train and do not suffer from the vanishing or exploding gradient problems when performing Backpropagation Through Time (BPTT) <ref type="bibr" target="#b10">[11]</ref>. LSTMs have the capability to remember sequences with long range temporal dependencies <ref type="bibr" target="#b8">[9]</ref> and have been applied successfully to many applications include image captioning <ref type="bibr" target="#b11">[12]</ref>, end-to-end speech recognition <ref type="bibr" target="#b12">[13]</ref> and machine translation <ref type="bibr" target="#b13">[14]</ref>.</p><p>LSTMs process sequential signals in one direction. One natural extension is bidirectional LSTMs (BLSTMs), which is composed of two LSTMs. The forward LSTM process the sequence as usual (e.g., reads the input sequence in the forward direction), the second processes the input sequence in backward order. The outputs of the two sequences can then be concatenated. BLSTMs have two distinct advantages over LSTMs, the first advantage being the forward and backward passes of the sequence yields differing temporal dependencies, the model can capture both sets of the signal dependencies. The second advantage is the higher level sequence layers (e.g., stacked BLSTMs) using the BLSTM outputs can access information from both input directions.</p><p>LSTMs and GRUs (and their bidirectional variants) have recently been successfully applied to acoustic modelling and ASR <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. In <ref type="bibr" target="#b4">[5]</ref> TIMIT phone sequences were trained end-toend from unsegmented sequence data using a LSTM transducer. LSTMs can be combined with Connectionist Temporal Classification (CTC) and implicitly perform sequence training over the speech signal on TIMIT <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr" target="#b14">[15]</ref> used GRUs and generated an explicit alignment model between the TIMIT speech sequence data to the phone sequence. In <ref type="bibr" target="#b7">[8]</ref> a commercial speech system is trained using a LSTM acoustic model, here the the entire speech sequence is used as the context for classifying context dependent phones. <ref type="bibr" target="#b15">[16]</ref> extend from <ref type="bibr" target="#b7">[8]</ref> and applied sequence training on top of LSTMs. Our contribution in this paper is a novel deep RNN acoustic model which is easy to train and archives an 8% relative improvement over DNNs for the Wall Street Journal (WSJ) corpus. length sequences (as opposed to variable length whole sequences <ref type="bibr" target="#b7">[8]</ref>) of a context window. The advantage of our model is we can easily use BLSTMs online (e.g., we don't need to wait to see the end of the sequence to generate the backward direction pass of the LSTM). The disadvantage is however the amount of temporal information stored in the model is limited to the context width (e.g., similar to DNNs and CNNs). However, in offline decoding, we can also compute all the acoustic states in parallel (e.g., one big minibatch) versus the O(T ) iterations needed by <ref type="bibr" target="#b7">[8]</ref> due to the iterative dependency of the LSTM memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>The model begins with a fixed window context of acoustic features (e.g., fMLLR) similar to a standard DNN or CNN acoustic model <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b2">3]</ref>. Within the context window, an overlapping time window of features, or Time Convolution (TC) of features is fed in at each timestep. A similar approach was used by <ref type="bibr" target="#b17">[18]</ref>, however they used a stride of 2 for the sake of reducing computational cost, however, our motivation is time convolution rather than performance and we use a stride of 1. The model processes these features with independent columns of DNNs over the context window timesteps. We refer this as the TC-DNN component of the model. The objective of the TC-DNN component is to project the original acoustic feature into a high dimensional feature space which can then be easily modelled or consumed by the LSTM. <ref type="bibr" target="#b18">[19]</ref> refers to this as a Deep Input-to-Hidden Function.</p><p>The transformed high dimensional acoustic signal is then fed into a BLSTM. The BLSTM models the time sequential component of the signal. Our LSTM implementation is similar to <ref type="bibr" target="#b11">[12]</ref> and described in the equations below:</p><formula xml:id="formula_0">it = φ(Wxixt + W hi ht−1) (1) ft = φ(W xf xt + W hf ht−1) (2) ct = ft cst−1 + it tanh(Wxcxt + W hc ht−1) (3) ot = φ(Wxoxt + W ho ht−1) (4) ht = ot tanh(ct)<label>(5)</label></formula><p>We do not use bias, nor peephole connections; on initial experimentation, we observed negligible difference, hence we omitted them in this work. Additionally, we did not apply any gradient clipping or gradient projection, we did however apply a cell activation clipping of 3 to prevent saturation in the sigmoid non-linearities. We found the cell activation clipping to help remove convergence problems and exploding gradients. We also do not use a recurrent projection layer <ref type="bibr" target="#b7">[8]</ref>. We found our LSTM implementation to train very easily without exploding gradients, even with high learning rates.</p><p>The BLSTM scans our input acoustic window of time width T emitted by the first DNN and outputs two fixed value vector (one for each direction), which is then concatenated:</p><formula xml:id="formula_1">c = h f T h b 1<label>(6)</label></formula><p>We refer c as the context of the acoustic signal generated by the BLSTM. Context c compresses the relevant acoustic infor-mation needed to classify the phones from the feature context (e.g., the window of fMLLR features). The context is further manipulated and projected by a second DNN. The second DNN adds additional non-linear transformations before being finally fed to the softmax layer to model the context dependent state posteriors. <ref type="bibr" target="#b18">[19]</ref> refers this as the Deep Hidden-to-Output Function. The model is trained supervised with backpropagation minimizing the cross entropy loss. <ref type="figure" target="#fig_0">Figure 1</ref> gives a visualization of our entire model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Optimization</head><p>We found our LSTM models to be very easy to train and converge. We initialize our LSTM layers with a uniform distribution U(−0.01, 0.01), and our DNN layers with a Gaussian distribution N (0, 0.001). We clip our LSTM cell activations to 3, we did not need to apply any gradient clipping or gradient projection.</p><p>We train our model with Stochastic Gradient Descent (SGD) using a minibatch size of 128, we found using larger minibatches (e.g., 256) to give slightly worse WERs. We used a simple geometric decay schedule, we start with a learning rate of 0.1 and multiply it by a factor of 0.5 every epoch. We have a learning rate floor of 0.00001 (e.g., the learning rate does not decay beyond this value). We experimented with both classical and Nesterov momentum, however we found momentum to harm the final WER convergence slightly, hence we use no momentum. We apply the same optimization hyperparameters for all our experiments, it is possible using a slightly different decay schedule will yield better results. Our best model took 17 epochs to converge or around 51 hours in wall clock time with a NVIDIA Tesla K20 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We experiment with the WSJ dataset. We use si284 with approximately 81 hours of speech as the training set, dev93 as our development set and eval92 as our test set. We observe the WER of our development set after every epoch, we stop training once the development set no longer improves. We report the converged dev93 and the corresponding eval92 WERs. We use the same fMLLR features generated from the Kaldi s5 recipe <ref type="bibr" target="#b19">[20]</ref>, and our decoding setup is exactly the same as the s5 recipe (e.g., large dictionary and trigram pruned language model). We use the tri4b GMM alignments as our training targets and there are a total of 3431 acoustic states. The GMM tri4b baseline achieved a dev and test WER of 9.39 and 5.39 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">DNN</head><p>Two baseline DNN systems are presented, the first is the Kaldi s5 WSJ recipe with sigmoid DNN model which pretrains with a Deep Belief Network <ref type="bibr" target="#b20">[21]</ref>, it achieved a WER of 3.81.</p><p>We also built a ReLU DNN which requires no pretraining. The ReLU DNN consisted of 4 layers of 2048 ReLU neurons followed by softmax and trained with geometrically decayed SGD. We also experimented with deeper and wider networks, however we found this 5 layer architecture to be the best. Our ReLU DNN is much easier to train (e.g., no expensive pretraining) and achieves a WER of 3.79 matching the WER of the pretrained Sigmoid DNN. The ReLU DNN results suggest that pretraining may not be necessary given sufficient supervised data and is competitive for the acoustic modelling task. <ref type="table" target="#tab_0">Table 1</ref> summarizes the WERs for our DNN baseline systems.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">TC-DNN-BLSTM-DNN</head><p>We experimented next with a DNN-BLSTM model. Our DNN-BLSTM model does not have time convolution at its input, and lacks the second DNN non-linearities for context projection. The two layer 2048 neuron ReLU DNN in front of the BLSTM acts as a signal processor, projecting the original acoustic signal (e.g., each fMLLR vector) into a new high dimensional space which can be more easily digested by the LSTM. The BLSTM module uses 128 bidirectional cells. Compared to the 128 bidirectional cell BLSTM model, the model improves from 5.19 WER to 3.92 WER or 24% relatively. The results of this experiment suggest the fMLLR features may not be the best features for BLSTM models (to consume directly at least); but rather We then experimented with a DNN-BLSTM-DNN model (without time convolution). Each DNN has two layers of 2048 ReLU neurons, and the BLSTM layer had 128 cells per direction. We combine both the benefits of a learnt signal processing DNN and the context projection. Compared to a 128 bidirectional cell BLSTM model, our WER drops from 5.19 to 3.76 or 28% relatively. Compared to a 1024 bidirectional cell BLSTM model, we essentially redistributed our parameters from a wide shallow network to a deeper network. We achieve a 11% relative improvement compared to a single layer 1024 bidirectional cell BLSTM, suggesting the deeper models are much more expressive and powerful.</p><p>Finally, our TC-DNN-BLSTM-DNN model combines the DNN-BLSTM-DNN with input time convolution. Our model further improves from 3.76 WER without time convolution to 3.47 WER with time convolution. Compared to the DNN models, we achieve 0.32 absolute WER reduction or 8% relatively. To the best of our knowledge, this is the best WSJ eval92 performance without sequence training <ref type="bibr" target="#b21">[22]</ref>. We hypothesize the time convolution gives a richer signal representation to the DNN signal processor and consequently the BLSTM model to consume. The time convolution also relieves the LSTM computation power to learning long term dependencies, rather than short term dependencies. <ref type="table" target="#tab_2">Table 3</ref> summarizes the experiments for this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Distributed Optimization</head><p>All results presented in the previous sections of this paper were trained with a single GPU with SGD. To reduce the time required to train an individual model we also experimented with distributed Asynchronous Stochastic Gradient Descent (ASGD) across multiple GPUs. Our implementation is similar to <ref type="bibr" target="#b22">[23]</ref>, we have 4 GPUs (NVIDIA Tesla K20) in our system, 1 GPU is dedicated as a parameter server and we have 3 GPU compute shards (e.g., the independent SGD learners). We do not apply  any stale gradient decay <ref type="bibr" target="#b22">[23]</ref> or warm starting <ref type="bibr" target="#b23">[24]</ref>. We use the exact same learning rate schedule, minibatch size and hyperparameters as our TC-DNN-BLSTM-DNN SGD baseline. <ref type="bibr" target="#b7">[8]</ref> applied distributed ASGD optimization, however they applied it on a cluster of CPUs rather than GPUs. Additionally, <ref type="bibr" target="#b7">[8]</ref> did not compare if there was a WER differential between SGD versus ASGD.</p><p>Our baseline TC-DNN-BLSTM-DNN SGD system took 17 epochs or 51 wall clock hours to converge to a dev and test WER of 6.58 and 3.47. Our distributed implementation converges in 14 epochs and 16.8 wall clock hours, achieves a dev and test WER of 6.57 and 3.72. The distributed optimization is able to match the dev WER, however the test WER is significantly worse. It is unclear whether this WER differential is due to the asynchronicity characteristic of the optimizer or due to the small datasets, we suspect with larger datasets the gap between the ASGD and SGD will shrink. The conclusion we draw is that ASGD can converge much quicker and faster, however there may be a impact to final WER performance. <ref type="table" target="#tab_3">Table 4</ref> and <ref type="figure" target="#fig_1">Figure  2</ref> summarizes our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we presented a novel TC-DNN-BLSTM-DNN acoustic model architecture. On the WSJ eval92 task, we report a 3.47 WER or more than 8% relative improvement over the DNN baseline of 3.79 WER. Our model is easy to optimize and implement, and does not suffer from exploding gradients even with high learning rates. We also found that pretraining may not be necessary for DNNs, the DBN pretrained DNN achieved a 3.81 WER compared to our ReLU DNN without pretraining of 3.79 WER. We also experimented with ASGD with our TC-DNN-BLSTM-DNN model, we were able to match the SGD dev WER, however the WER on the evaluation set was significantly lower at 3.72. In future work, we seek to apply sequence training on top of our acoustic model to further improve the model accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our model architecture can be summarized as a TC-DNN-BLSTM-DNN acoustic model. Our model deals with fixed arXiv:1504.01482v1 [cs.LG] 7 Apr 2015 TC-DNN-BLSTM-DNN Architecture. The model contains 3 parts, a signal processing DNN which takes in the original fMLLR acoustic features and projects them to a high dimensional space, a BLSTM which models the sequential signal and produces a context, and a final DNN which takes the context generated by the BLSTM and estimates the likelihoods across acoustic states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>SGD vs x3 ASGD WER convergence comparison, each point represents one epoch of the respective optimizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>WERs for Wall Street Journal. The ReLU DNN requires no pretraining and matches the WER of the Kaldi s5 recipe which uses DBN pretraining.</figDesc><table><row><cell>Model</cell><cell cols="2">dev93 WER eval92 WER</cell></row><row><cell>GMM Kaldi tri4b</cell><cell>9.39</cell><cell>5.39</cell></row><row><cell>DNN Kaldi s5</cell><cell>6.68</cell><cell>3.81</cell></row><row><cell>DNN ReLU</cell><cell>6.84</cell><cell>3.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>BLSTM WERs for Wall Street Journal. Larger recurrent models tend to perform better without overfitting. The deep BLSTM models do not yield any substantial gains over their single layer counterparts.We experimented with single layer and two layer deep BLSTM models. The cell size reported is per direction (e.g., total cells are doubled). The BLSTM models take longer to train and underperform compared to the ReLU DNN model. The large BLSTM models tend to outperform the smaller ones, suggesting overfitting is not an issue. However, there is limited incremental gain in WER performance with additional cells. Our best single layer BLSTM with 1024 bidirectional cells achieved only 4.06 WER compared to 3.79 from our ReLU DNN model.Deep BLSTM models<ref type="bibr" target="#b6">[7]</ref> may give additional model performance, since the upper layers can access information from the shallow layers in both directions and additional layers of non-linearities are available. Our deep BLSTM models contain two layers, the cell size reported is per direction per layer (e.g., total cells are quadrupled). Our deep BLSTM experiments give mixed results. For the same number of cells per layer, the deep model performs slightly better. However, if we fixed the number of parameters, the single layer BLSTM model performs slightly better, the single layer of 1024 bidirectional cells achieved a WER of 4.16 while the deep two layer BLSTM model with 512 bidirectional cells per layer achieved a WER 4.25.Table 2summarizes our BLSTM experiment WERs.</figDesc><table><row><cell cols="4">Cell Size Layers dev93 WER eval92 WER</cell></row><row><cell>128</cell><cell>1</cell><cell>8.19</cell><cell>5.19</cell></row><row><cell>256</cell><cell>1</cell><cell>7.94</cell><cell>4.66</cell></row><row><cell>512</cell><cell>1</cell><cell>7.43</cell><cell>4.36</cell></row><row><cell>768</cell><cell>1</cell><cell>7.36</cell><cell>4.16</cell></row><row><cell>1024</cell><cell>1</cell><cell>7.23</cell><cell>4.06</cell></row><row><cell>256</cell><cell>2</cell><cell>7.54</cell><cell>4.36</cell></row><row><cell>512</cell><cell>2</cell><cell>7.40</cell><cell>4.25</cell></row><row><cell>4.2. Deep BLSTM</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation effects of our TC-DNN-BLSTM-DNN model. The DNNs and Time Convolution are used for signal and context projections. We show that all components are critical to obtain the best performing model.</figDesc><table><row><cell>Model</cell><cell cols="2">dev93 WER eval92 WER</cell></row><row><cell>DNN-BLSTM</cell><cell>7.40</cell><cell>3.92</cell></row><row><cell>BLSTM-DNN</cell><cell>6.90</cell><cell>3.84</cell></row><row><cell>DNN-BLSTM-DNN</cell><cell>7.19</cell><cell>3.76</cell></row><row><cell>TC-DNN-BLSTM-DNN</cell><cell>6.58</cell><cell>3.47</cell></row><row><cell cols="3">learnt features (through the DNN feature processor) can yield</cell></row><row><cell cols="2">better features for the BLSTM model to consume.</cell><cell></cell></row><row><cell cols="3">The next experiment we ran was a BLSTM-DNN model.</cell></row><row><cell cols="3">Here, the BLSTM accepts the original acoustic feature without</cell></row><row><cell cols="3">modification and emits a context. The context is passed through</cell></row><row><cell cols="3">to a two layer 2048 neuron ReLU DNN which provides addi-</cell></row><row><cell cols="3">tional layers of non-linear projections before classification by</cell></row><row><cell cols="3">the softmax layer. Once again, the BLSTM module uses only</cell></row><row><cell cols="3">128 bidirectional cells. The model improves from 5.19 WER to</cell></row><row><cell cols="3">3.84 WER or 26% relatively when compared to the original 128</cell></row><row><cell cols="3">bidirectional cell BLSTM model which does not have the con-</cell></row><row><cell cols="3">text non-linearities. The result of this experiment suggest the</cell></row><row><cell cols="3">LSTM context should not be used directly for softmax phone</cell></row><row><cell cols="3">classification, but rather additional layers of non-linearities are</cell></row><row><cell cols="2">needed to achieve the best performance.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Effects of distributed optimization for our TC-DNN-BLSTM-DNN model. The ASGD experiments uses 3 independent SGD shards.</figDesc><table><row><cell cols="8">Model Epochs Time (hrs) dev93 WER eval92 WER</cell></row><row><cell cols="2">SGD</cell><cell>17</cell><cell></cell><cell>51.5</cell><cell>6.58</cell><cell>3.47</cell></row><row><cell cols="2">ASGD</cell><cell>14</cell><cell></cell><cell>16.8</cell><cell>6.57</cell><cell>3.72</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>WER vs. TIme</cell><cell></cell><cell></cell></row><row><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WER</cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 2</cell><cell>10</cell><cell>20 SGD dev93</cell><cell>30 Time (Hours) SGD eval92</cell><cell>40 3x ASGD dev93</cell><cell>50 3x ASGD eval92</cell><cell>60</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Application of Pretrained Deep Neural Networks to Large Vocabulary Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improvements to Deep Convolutional Neural Networks for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Aravkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Acoustic Modeling in Low Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequence Transduction with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning: Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech Recognition with Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid Speech Recognition with Bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems: Workshop Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<title level="m">Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards End-to-End Speech Recognition with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems: Workshop Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<title level="m">Sequence Discriminative Distributed Training of Long Short-Term Memory Recurrent Neural Networks,&quot; in INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Acoustic Modeling in Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep Speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How to Construct Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Kaldi Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequencediscriminative training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed Asynchronous Optimization of Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large Scale Distributed Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
