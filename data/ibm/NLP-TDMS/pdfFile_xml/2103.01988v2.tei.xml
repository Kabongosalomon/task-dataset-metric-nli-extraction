<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Pretraining of Visual Features in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Inria *</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Pai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Pretraining of Visual Features in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, self-supervised learning methods like MoCo [22], SimCLR [8], BYOL [20]  and SwAV <ref type="bibr" target="#b6">[7]</ref> have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that selfsupervised models are good few-shot learners achieving 77.9% top-1 with access to only 10% of ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A recent trend shows that well-tailored model pretraining approaches (weakly-supervised, semi-supervised, self-supervised) can drastically improve the performance on downstream tasks for most deep learning applications. It has been observed for Natural Language Processing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39]</ref>, Speech Recognition <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> and Computer Vision <ref type="bibr" target="#b34">[35]</ref>. There are two key ingredients that have contributed towards this success. The first is pretraining on massive datasets: the GPT-3 <ref type="bibr" target="#b3">[4]</ref> language model is pretrained on 300B words, while the speech model Wav2vec2.0 <ref type="bibr" target="#b1">[2]</ref> is learned on 53K hours of audio <ref type="bibr" target="#b27">[28]</ref>. The second ingredient is the use of models with massive capacity, even reaching hundreds of billions of parameters for the largest NLP models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>. SEER SwAV SimCLRv2 ViT <ref type="figure">Figure 1</ref>: Performance of large pretrained models on ImageNet. We pretrain our SEER models n an uncurated and random images. They are RegNet architectures <ref type="bibr" target="#b39">[40]</ref> trained with the SwAV self-supervised method <ref type="bibr" target="#b6">[7]</ref> We compare with the original models trained in Caron et al. <ref type="bibr" target="#b6">[7]</ref> as well as the pretraining on curated data from SimCLRv2 <ref type="bibr" target="#b8">[9]</ref> and ViT <ref type="bibr" target="#b13">[14]</ref>. The network architectures are different. We report the top-1 accuracy after finetuning on ImageNet.</p><p>While the benefit of pretraining has been demonstrated in computer vision, it has been in the limited scope of curated datasets originally collected for supervised or weakly supervised learning. These datasets represent only a limited fraction of the general distribution of internet scale images <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. Prior attempts to train self-supervised models on uncurated data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref> have only used a few millions of images for which using small model architectures is sufficient. This still leaves an open question -can we achieve good performance by pretraining on an extremely large collection of random, uncurated and unlabeled images? Answering this question has important implications. Practically, it may lead to strategies for pretrain-ing that use unlabeled data to achieve state-of-the-art performance on transfer learning, and to create systems that continually learn in a self-supervised manner from an unending data stream.</p><p>In this work, we address this question by pretraining high capacity models on billions of random internet images, i.e., completely unconstrained images from the internet. We do not rely on image meta-data or any form of weak/manual annotations to filter data or train the model. Training powerful image representations on unlabeled data has recently been made possible with advances in self-supervised learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>. The most recent self-supervised models pretrained on ImageNet <ref type="bibr" target="#b43">[44]</ref> even surpass supervised pretrained models on multiple downstream tasks <ref type="bibr" target="#b21">[22]</ref>. Recent developments <ref type="bibr" target="#b6">[7]</ref> have also shown their potential when trained on random internet images. Further, these methods are amenable to online learning, making them perfect candidates for training large models on unlimited data.</p><p>For our analysis, we focus on the RegNet family of architectures <ref type="bibr" target="#b39">[40]</ref> and in particular the architecture with 700M parameters. The RegNet architectures are particularly well suited for this task for two reasons. First, they offer an excellent trade-off of efficiency and performance. Second, they are very flexible for scaling the number of parameters. We train these models online on a dataset of 2B random internet images using the SwAV self-supervised approach <ref type="bibr" target="#b6">[7]</ref>. We use SwAV for this study for its fast convergence to good performance in the online setting with large batch size. To make this study tractable at this scale, we leverage several existing tools to reduce the memory usage of our models, including mixed precision and gradient checkpointing.</p><p>The main finding of our study is that our SElf-supERvised ("SEER") pretrained models are not only good for initializing training on curated dataset like ImageNet, they are also excellent few shot learners, achieving 75.1% with only 10% of ImageNet. Our model also achieves better performance than supervised model trained on ImageNet on several downstream tasks, confirming the benefits of selfsupervised pretraining, even when performed on uncurated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work explores the limits of training large architectures on large uncurated datasets with self-supervised learning. We build on prior work from different areas: selfsupervised learning, training at scale and large convolutional network architectures.</p><p>Unsupervised pretraining of visual features. Selfsupervised learning has a long history in computer vision with methods based on autoencoders <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b50">51]</ref>, clustering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11]</ref> or instance-level discrimination <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b51">52]</ref>. Recently, methods based on contrastive learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38]</ref> have shown that unsupervised pretraining produces features that surpass the supervised feature representations on many downstream tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37]</ref>. These methods discriminate either between each instance feature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37]</ref> or between their cluster assignments <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref>. Most works on unsupervised pretraining focus on supervised datasets like ImageNet <ref type="bibr" target="#b43">[44]</ref> or curated datasets collected by filtering images related to pre-defined labels <ref type="bibr" target="#b46">[47]</ref>. The key takeaway from these works is that supervised labels are not required as long as you trained on the filtered data. Some works have explored unsupervised training in the wild for images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref> and videos <ref type="bibr" target="#b35">[36]</ref>. These studies were conducted at a small scale, and there are now evidences that self-supervised pretraining benefits greatly from large archtiectures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref>. Our work builds upon these findings to explore if we can learn good visual representations by training large architectures on large collection of random, uncurated and unlabeled images.</p><p>Learning of visual features at scale. Benefiting from the advances in distributed training <ref type="bibr" target="#b17">[18]</ref>, several works have shown the advantages of pretraining on large curated image datasets with weak-supervised learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref>, semisupervised learning <ref type="bibr" target="#b53">[54]</ref> or supervised training on hundreds of millions of filtered images <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b46">47]</ref>. Of particular interest, Mahajan et al. <ref type="bibr" target="#b34">[35]</ref> show that the pretraining on billions of images significantly improves the performance of large architectures compared to training them from scratch. Most works on training at large data scale rely on a data filtering step to only keep the images associated with targeted concepts. This filtering either uses hastags that are synsets of ImageNet classes <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b53">54]</ref>, or the predictions from a pretrained object classifier <ref type="bibr" target="#b46">[47]</ref>. As opposed to this line of work, we are interested in learning features that cover any available image, and hence, we do not curate our training dataset to match a pre-defined set of concepts.</p><p>Scaling architectures for image recognition. Many works have shown the benefits of training large architectures on the quality of the resulting visual features <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53]</ref>. Training large architectures is especially important when pretraining on a large dataset, where a model with limited capacity will underfit <ref type="bibr" target="#b34">[35]</ref>. This becomes further more important when the pretraining is performed with contrastive learning, where the network has to learn to discriminate between each instance of the dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref> in order to learn good visual representations. For instance, Kolesnikov et al. <ref type="bibr" target="#b29">[30]</ref> have demonstrated the importance of training wider networks for the quality of visual features learned with self-supervision. More recently, Chen et al. <ref type="bibr" target="#b8">[9]</ref> have achieved impressive performance with deeper and wider configurations of ResNet <ref type="bibr" target="#b23">[24]</ref>. However, scaling architectures for image recognition goes beyond simply changing the width and the depth of a model, and a large amount of literature is dedicated to building scale efficient models with large capacity <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b48">49]</ref>. Of particular interest, the RegNets <ref type="bibr" target="#b39">[40]</ref> achieve competitive performance on standard image benchmarks while offering an efficient runtime and memory usage making them a candidate for training at scale. In our work, we show the benefits of this model family for large scale self-supervised pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we provide a brief overview of the components used in this work to pretrain visual features in the wild. We describe the self-supervised method, SwAV <ref type="bibr" target="#b6">[7]</ref>, and the family of convnet architectures, RegNet <ref type="bibr" target="#b39">[40]</ref>. We then discuss several technical strategies required to train large models on billions of images with self-supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Supervised Pretraining</head><p>We pretrain our model with an online self-supervised approach called SwAV that we briefly summarize in this section. We refer to Caron et al. <ref type="bibr" target="#b6">[7]</ref> for more details.</p><p>SwAV is an online clustering method to train convnets without annotations. It works by training an embedding that yields consistent cluster assignments between multiple views of the same image. By mining clusters invariant to data augmentations, the system learns semantic representations. In practice, SwAV works by comparing the features of different views of the same image using their intermediate cluster assignments. If these features capture the same information, it should be possible to predict the assignment of one from the feature of another view. More precisely, we consider a set of K clusters, each associated with a learnable d-dimensional prototype vector v k . Given a batch of B images, each image i is transformed into two views: x i1 and x i2 . All views are then featurized with a convnet, resulting in two sets of features (f 11 , . . . , f B1 ) and (f 12 , . . . , f B2 ). Each set of features is assigned independently to the cluster prototypes using an Optimal Transport solver. This solver enforces that the features are uniformly split across clusters, avoiding trivial solutions where all representations are mapped to an unique prototype. The resulting assignments are then swapped between the two sets: the cluster assignment y i1 of the view x i1 has to be predicted from the feature representation f i2 of the view x i2 , and vice-versa. Formally, the convnet and prototypes weights are trained to minimize the following loss for all examples i:</p><formula xml:id="formula_0">L(f i1 , f i2 ) = (f i1 , y i2 ) + (f i2 , y i1 ).</formula><p>The cluster prediction loss (f , y) is the cross entropy between the cluster assignment and a softmax of the dot products of f and all prototypes v k :</p><formula xml:id="formula_1">(f , y) = − k y (k) log p (k)</formula><p>where:</p><formula xml:id="formula_2">p (k) = exp 1 τ f t v k k exp 1 τ f t v k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scale efficient model family: RegNetY</head><p>Scaling data and model capacity jointly requires using architectures that are efficient in terms of both memory and runtime. RegNets are a family of models designed for this purpose and we briefly describe them in this section. We refer to Radosavovic et al. <ref type="bibr" target="#b39">[40]</ref> for more details.</p><p>RegNets are a family of architectures defined by a design space of convnets consisting of 4 stages, with each stage containing a series of identical blocks, while keeping the structure of their blocks fixed -namely the residual bottleneck block of He et al. <ref type="bibr" target="#b23">[24]</ref>. In this work, we focus on the RegNetY architectures, that add a Squeeze-and-excitation op <ref type="bibr" target="#b25">[26]</ref> to the standard RegNets to further improve their performance. The RegNetY model family is parameterized by 5 parameters, allowing the search of a good instance with a certain number of FLOPs with reasonable resources. The models we used were all searched on ImageNet using the same procedure as Radosavovic et al. <ref type="bibr" target="#b39">[40]</ref>. We believe our results can further be improved by searching for RegNetYs directly on our self-supervised pre-training task.</p><p>The RegNetY-256GF architecture. Our model of focus is the RegNetY-256GF architecture. Its parametrization is given by the scaling rules of RegNets <ref type="bibr" target="#b39">[40]</ref>:</p><formula xml:id="formula_3">w 0 = 640, w a = 230.83, w m = 2.53, group width = 373</formula><p>It has 4 stages with stage depths (2, 7, 17, 1) and stage widths (528, 1056, 2904, 7392), leading to a total of 695.5M parameters. It takes 6125ms for a single training iteration over 8, 704 images on 512 V100 32GB NVIDIA GPUs. Training this model on 1 billion images requires 114, 890 training iterations for a batch size of 8, 704 images, summing to 8 days of training over 512 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization and Training at Scale</head><p>In this work, we propose several adjustments to the training of self-supervised methods to adapt it to a large scale.</p><p>Learning rate schedule. We explore two learning rate schedules: the cosine wave <ref type="bibr" target="#b33">[34]</ref> and a simpler fixed learning rate schedule. The cosine wave adapts to the number of updates and we focus on this scheduling for fair comparison between different models. However, it is not adapted to online large scale training because it uses the total of updates for scheduling and it also weighs images differently depending on when they are seen during training. For this reason, we also explore a fixed learning rate schedule. In this scheduling, we keep the learning rate fixed until the loss is non-decreasing, then we divide the learning rate by 2. Our observation is that this schedule works as well in practice and allows for more flexible training. However, we train our largest model, the RegNetY-256GF with cosine learning rate schedule since we use only 1B images.</p><p>Reducing memory consumption per GPU. We reduce the amount of GPU memory required during training with gradient checkpointing <ref type="bibr" target="#b9">[10]</ref> and mixed precision. We use O1 optimization level from NVIDIA Apex library 1 to perform operations like GEMMs and convolutions in 16-bits floating-point precision. We use PyTorch's gradient checkpointing implementation which trades compute for memory. It discards intermediate activations during the forward pass, and recomputes them during the backward pass. In our experiments, using gradient checkpointing, we observe negligible compute overhead in memory-bound settings.</p><p>Optimizing Training speed. Enabling mixed-precision for memory optimization has additional benefits, as modern accelerators take full advantage of the FP16 reduced size by increasing throughput when compared to FP32. This improves memory-bandwidth bottleneck and speeds up training. We also use the optimized SyncBatchNorm implementation with kernels through CUDA/C++ extensions from NVIDIA Apex library. For synchronizing BatchNorm layer across GPUs, we create process groups instead of performing global sync which is slow. Finally, our dataloader pre-fetches more training batches leading to higher data throughput than the default PyTorch dataloader.</p><p>Large scale Pretraining data. For our billion scale pretraining, we consider a dataloader that directly samples random, public, and non-EU images from Instagram. As we train online and in the wild, we do not apply any curation or pre-processing on the images, such as hashtag filtering or de-duplication. This dataset is not static and gets refreshed every 90 days, however, we can confirm that the refreshment doesn't degrade the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details.</head><p>We pretrain a RegNetY-256GF with SwAV, using 6 crops per image of resolutions 2 × 224 + 4 × 96. We follow the same data augmentation as in Caron et al. <ref type="bibr" target="#b6">[7]</ref>. During pretraining, we use a 3-layer multi-layer perceptron (MLP) projection head of dimensions 10444 × 8192, 8192 × 8192 and 8192 × 256. We do not use BatchNorm layers in the head. We use 16K prototypes, temperature τ set to 0.1, the Sinkhorn regularization parameter to 0.05 and perform 10 iterations of Sinkhorn algorithm. We synchronize Batch-Norm stats across gpus and create process groups of size 64 for synchronization. We use a weight decay of 10 −5 , LARS optimizer <ref type="bibr" target="#b54">[55]</ref> and O1 mixed-precision optimization from Apex library. We also apply activation checkpointing <ref type="bibr" target="#b9">[10]</ref>. We train our model with stochastic gradient descent using 1 https://github.com/NVIDIA/apex a large batch size of 8192 different images distributed over 512 NVIDIA V100 32GB GPUs, resulting in 16 different images per GPU. The learning rate is linearly ramped up from 0.15 to 9.6 for the first 8K training updates. After warmup, we follow a cosine learning rate schedule and decay the learning rate to final value 0.0096. Overall, we train on 1B images for a total of 122K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Main Results</head><p>We study the quality of the features generated by our self-supervised pretraining on a variety of downstream tasks and benchmarks. We also consider a low-shot setting with limited access to images and their labels for the downstream task, as well as, standard evaluation using the entire data available for the downstream task. We also compare with prior work trained on large curated datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Finetuning Large Pretrained Models</head><p>In this section, we measure the quality of models pretrained in the wild by transferring them to the ImageNet object classification benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setting.</head><p>We pretrain 6 Reg-Net architectures of different capacities, namely RegNetY-{8,16,32,64,128,256}GF, on 1B random, public and non-EU Instagram images with SwAV. We finetune these models on the task of image classification on ImageNet, using the standard 1.28M training images with labels and evaluate on 50k images in the standard validation set. We apply the same data augmentation as in SwAV <ref type="bibr" target="#b6">[7]</ref>. We finetune for 35 epochs with SGD, batch size of 256, learning rate of 0.0125 reduced by factor of 10 after 30 epochs, weight decay of 10 −4 and momentum of 0.9. We report top-1 accuracy on validation set using the 224×224 center crop.</p><p>Comparision with other self-supervised pretraining.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we compare our largest pretrained model, a RegNetY-256GF, with existing self-supervised pretrained models. We achieve 84.2% top-1 accuracy on Im-ageNet, surpassing by +1%, the best existing pretrained model from SimCLRv2 <ref type="bibr" target="#b8">[9]</ref>. In the <ref type="figure">Figure 1</ref>, we show the same comparison with different model capacities. The conclusion remains unchanged regardless of the model capacity, showing that combining RegNet with SwAV is a good candidate for pretraining. Impact of the model capacity. In <ref type="figure" target="#fig_1">Figure 2</ref>, we show the impact of model capacity on the performance of pretraining compared to training from scratch. While model capacity benefits both initializations, it has a more significant impact on pretrained models when scaled to hundreds of millions of parameters. A reason is that training these architecture from scratch could overfit on ImageNet which is a relatively  We show the impact of finetuning pretrained RegNetY-{8,16,32,64,128}GF compared to training them from scratch on ImageNet. The pretrained RegNet models are trained with our self-supervised approach on 1B random IG images. We report top-1 accuracy on the validation set. small dataset. We confirm that the log-scale performance gain from increasing model capacity also appears in the case where the pretraining data is uncurated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Low-shot learning</head><p>In this section, we are interested in evaluating the performance of our pretrained model in the low-shot setting, i.e., with a fraction of data on the downstream task.</p><p>Experimental setting. We consider two datasets for lowshot learning, namely ImageNet <ref type="bibr" target="#b43">[44]</ref> and Places205 <ref type="bibr" target="#b55">[56]</ref>. We assume a limited access to the dataset during transfer learning, both in terms of labels and images. This setting differs from the standard setting used in self-supervised learning where the entire datasets is accessible and only the access to labels is limited <ref type="bibr" target="#b24">[25]</ref>. For the rest, we follow their experimental setting for finetuning the features.</p><p>Results on Places205. In <ref type="figure">Figure 3</ref>, we show the im-  <ref type="table">Table 2</ref>: Low-shot learning on ImageNet. We compare our approach with semi-supervised approaches and self-supervised pretraining on lowshot learning. Our model is finetuned on either 1% or 10% of ImageNet, and does not access the rest of ImageNet images. As opposed to our method, the other methods use all the images from ImageNet during pretraining or finetuning.</p><p>pact of pretraining on different fractions of the Places205 dataset <ref type="bibr" target="#b55">[56]</ref>. We compare to pretraining on ImageNet with supervision with the same RegNetY-128GF architecture. A surprising result is that we observe a stable gain of 2.5% in top-1 accuracy, regardless of the fraction of training data available to finetune on Places205. The difference between self-supervised and supervised pretraining may be explained by the difference in the nature of training data: features learned from images in the wild may be more suitable to classify scene. Additionally, the non-uniform distribution of underlying concepts in the wild may also provide an advantage to our pretraining on a unbalanced dataset like Places205.</p><p>Results on ImageNet. In <ref type="table">Table 2</ref>, we show the performance of our self-supervised pretrained model on low-shot learning. For completeness, we report performance of existing 1% 5% 10% 20% 50% 100% Fraction of the train set  <ref type="figure">Figure 3</ref>: Low-shot learning on Places. We compare the impact of dfferent pretraining when transferring to Places205 with different fraction of the train set available for finetuning. We report Top-1 accuracy and use a RegNetY-128GF architectures for our pretraining and the supervised pretraining on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours -IG Supervised -INet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>40M</head><p>100M  <ref type="figure">Figure 4</ref>: Impact of capacity on low-shot learning. We report the relative improvement in top-1 accuracy when finetuning pretrained RegNets with different capacities on a fraction of ImageNet. Note that we only access to 1% and 10% of the images and their labels. We also report the relative improvement for a pretrained model finetuned on the full ImageNet dataset ("100%"). For reference, we report the relative improvement of RegNets trained with supervision on the full ImageNet dataset ("Sup.").</p><p>semi-supervised and self-supervised methods. We note that all of these methods use the entire set of 1.2M images from ImageNet for pretraining and only restrict the access to the labels, while we only see 1% and 10% of the images. This greatly favors these approaches since the network has seen more images from the same distribution during pretraining as the fraction used for transfer. Nonetheless, our approach achieves a top-1 accuracy of 77.9% with only 10% of Ima-geNet, which is competitive with these methods (2% gap). On 1% of the data, i.e, 10K images, the gap increases significantly but note that the other methods are using the full ImageNet from pretraining.  <ref type="table">Table 3</ref>: Linear Evaluation on downstream classification tasks. We compare the features from different pretrainings with a linear evaluation on top of frozen features. We report accuracy on the following downstream tasks: iNaturalist ("iNat.") <ref type="bibr" target="#b49">[50]</ref>, OpenImages ("OpIm.") <ref type="bibr" target="#b30">[31]</ref>, Places205 <ref type="bibr" target="#b55">[56]</ref> and Pascal VOC2007 <ref type="bibr" target="#b15">[16]</ref>. Impact of the model capacity. In <ref type="figure">Figure 4</ref>, we explore the impact of model capacity in the different low-shot settings -1%, 10% and 100% of ImageNet. A first observation is that increasing model capacity gives a higher relative improvement as we decrease the access to both labels and images. This result extends the observation of Chen et al. <ref type="bibr" target="#b8">[9]</ref> on the low-shot setting. Interestingly, the relative gains are comparable in both settings (+20% in 1% of the data), even though low-shot learning is strictly harder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer to Other Benchmarks</head><p>In these experiments, we further evaluate our pretrained features by transferring them to other downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear evaluation of image classification.</head><p>In <ref type="table">Table 3</ref>, we compare the features from our pretrained RegNetY-128GF and RegNetY-256GF with features from the same architecture pretrained on ImageNet with and without supervision. To assess features quality, we freeze the model weights and learn a linear classifier on top of the features using the training set of each downstream task. We consider the following benchmarks: iNaturalist <ref type="bibr" target="#b49">[50]</ref>, Open-Images <ref type="bibr" target="#b30">[31]</ref>, Places205 <ref type="bibr" target="#b55">[56]</ref> and Pascal VOC <ref type="bibr" target="#b15">[16]</ref>. We observe that self-supervised features transfer better than supervised features regardless of the pretraining data.</p><p>Detection and segmentation. In <ref type="table">Table 4</ref>, we evalaute pretrained features on detection and segmentation. We train a Mask-RCNN model <ref type="bibr" target="#b22">[23]</ref> on the COCO benchmark <ref type="bibr" target="#b32">[33]</ref> with pretrained RegNetY-64GF and RegNetY-128GF as backbones. For both downstream tasks and architectures, our self-supervised pretraining outperforms supervised pretraining by 1.5 − 2 AP points. However, the gap in performances between different architectures is small (0.1−0.5 AP) compared to what we observed on ImageNet.  <ref type="table">Table 4</ref>: Detection and Segmentation on COCO. We compare the performance of Mask-RCNN models <ref type="bibr" target="#b22">[23]</ref> initialized with different pretrained RegNet architectures as backbone on the detection and segmentation tasks of COCO <ref type="bibr" target="#b32">[33]</ref>. We consider two architectures, RegNetY-64gf and RegNetY-128gf, that we either pretrained with supervision on Ima-geNet or without supervision on 1B IG images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparing to Weakly-Supervised Pretraining</head><p>Many online images have some metadata, e.g., hashtags or geo-localization, that can be leveraged during pretraining. In particular, Mahajan et al. <ref type="bibr" target="#b34">[35]</ref> show that pretraining by predicting a curated set of hashtags can greatly improve the quality of the resulting visual features. Their approach requires to filter images and only works in the presence of textual metadata. In <ref type="table">Table 5</ref>, we compare our self-supervised pretraining on random images to theirs on the same architecture, a ResNeXt101-32x8d, with finetuning. For completeness, we also report their best number with their largest architecture. First, we observe that both pretrainings improve top-1 accuracy over a model trained from scratch, showing in general the benefits of pretraining. Our approach is also in the same ballpark as theirs even though we do not rely on data curation nor supervision. Note that, when the features are frozen, their approach maintains high performance on ImageNet, with 81.6% top-1 accuracy while our model performance drops significantly -around 65% top-1. This result is not surprising: they pretrain on data that follows the same concepts as ImageNet classes and thus the learned features are more aligned with the target distribution. Since we pretrain our model on random images, we require a full-finetuning step of 35 epochs to adapt to the target distribution. This experiment shows that the benefits of pretraining with finetuning exist even if the features come from a different image distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Studies</head><p>These ablation studies focus on the model architecture, how its performance scales with capacity, and the specificities of our pretraining data and our self-supervised method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Impact of the Model Architecture</head><p>Experimental setting.</p><p>We consider several Reg-NetY architectures with growing capacity, namely the RegNetY-{8,16,32,64,128}GF. We also consider ResNet-{50, 101} <ref type="bibr" target="#b23">[24]</ref>   <ref type="table">Table 5</ref>: Comparision with weakly-supervised pretraining on curated data. We compare pretraining a ResNeXt101-32dx8d with selfsupervision on random images with pretraining on filtered images labeled with hashtags that are similar to ImageNet classes <ref type="bibr" target="#b34">[35]</ref>. We report top-1 accuracy on ImageNet with finetuning. For completeness, we also report the best performance reported with larger architectures.  a growing number of parameters, namely RX101-32x{4, 8}d <ref type="bibr" target="#b52">[53]</ref>. We refer to the appendix for details about the different architectures. Every model is pretrained for 1 epoch on 1B random, public and non-EU Instagram (IG) images with SwAV using 3K prototypes. We use same hyperparameters for pretraining all the ablation models.</p><p>Impact of the architecture. In <ref type="figure" target="#fig_4">Figure 5</ref>, we measure how changing the architecture affects the quality of pretrained features with a linear evaluation on ImageNet. This evaluation does not favor models that perform well when trained from scratch on ImageNet, and hence, we directly probe the pretrained features. For ResNets and ResNeXts, we observe that the features from the penultimate layer work better in this setting and we report those for fair comparison with RegNets. Overall, RegNets surpass the other architectures, justifying our choice of architecture for our main model. Finally, we observe, regardless of the architecture, increasing model capacity significantly improves the quality of the features with a logarithmic gain in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Scaling the Training Data</head><p>Pretraining on a larger dataset can improve the quality of the learned visual features for two reasons: more parameter updates and more unique images. In this section, we disentangle these two effects.</p><p>Increasing the number of updates. On the left panel of <ref type="figure" target="#fig_5">Figure 6</ref>, we show the performance as we train a RegNetY-128GF model online on 1B images. We observe that the performance steadily increases with the number of updates as expected and the performance does not saturate even after a number of updates corresponding to 1B images.</p><p>Increasing the number of unique images. On the right panel of <ref type="figure" target="#fig_5">Figure 6</ref>, we report the performance of two models, RegNetY-8GF and RegNetY-16GF when trained for the same number of updates but with a different number of unique images. We train the models for a number of updates that corresponds to 1 epoch over 1B unique images, or 32 epochs for 32M unique images, with a single half-cosine wave learning rate. An interesting observation is that, with this learning rate schedule, the minimum number of unique images required to obtain good performance is greater than the size of ImageNet by only an order of magnitude.</p><p>Overall, these experiments show that the number of updates matters more than seeing the same images multiple times. There is thus no need to fix the pretraining dataset, and instead validating their continual online pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Scaling the self-supervised model head</head><p>In this section, we study the impact of growing the size of the self-supervised model head during pretraining.</p><p>In <ref type="table">Table 6</ref>, we compare RegNetY-8GF architectures trained with different capacity self-supervised heads. We  <ref type="table">Table 6</ref>: Adapting SwaV to IG data. We show the impact of scaling the head of our self-supervised loss, by increasing the size of the MLP and the number of prototypes. We report top-1 accuracy on ImageNet with a linear evaluation on top of the features from the last ("res5") and penultimate blocks ("res4") of a RegNetY-8GF architectures pretrained on 1B random public and non-EU IG images.</p><p>report top-1 accuracy on ImageNet obtained with a linear classifier trained on frozen features. The models are trained on 1B images with a cosine wave learning rate schedule. In particular, we show the impact of a larger MLP and more prototype vectors. We adjust the head from 2-layer MLP of dimensions (2016 × 2016 and 2016 × 256) to 3-layer MLP of dimensions (2016×4096, 4096×4096, 4096×256), and increase the number of prototypes from 3K to 16K. We observe that simply increasing the number of the parameters in the head, and the number of clusters significantly improves the performance of the resulting model (+3%) with the same model, hence same feature size. The reason is that 1B random images contain much more concepts that the original SwAV classifier can memorize in its small head, hence the information about the clusters leaks to the features, degrading their performance. Increasing the head reduces this effect at a minimal compute and storage cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We show that pretraining features on random images with no annotation achieves competitive performance on any downstream task. This result confirm that the recent progress of self-supervised learning is not specific to curated training set, like ImageNet, and could benefit a large range of applications associated with uncurated data. Our work benefits from the scalability of modern self-supervised learning methods in terms of data, and modern efficient high-capacity architectures. In particular, the scalability of RegNets have played a key role in pushing the limits of selfsupervised pretraining, and in the future, we plan to search for larger RegNet architectures suited for this task. For pretraining the above RegNetY architectures, we follow the same pretraining hyperparams as ResNet and ResNeXt training with two differences. We use crops per image of resolutions 2 × 224 + 4 × 96. However, we confirm that the crop resolutions didn't impact the model performance on ImageNet linear classification task on which we show our ablations in Section 5. Infact, using the bigger resolution crops leads to more GPU memory requirement with no impact on model performance on transfer task. The only other difference is the dimensions of 3-layer MLP in the head. Each RegNetY architecture has difference output channel and hence we adapt <ref type="bibr">3-</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Finetuning pretrained RegNets on ImageNet versus Scratch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Comparison across architectures. We pretrain different ResNets, ResNeXts and RegNetYs for 1 epoch on 1B IG images with SwAV. We report top-1 accuracy on ImageNet of a linear classifier trained on frozen features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>(left) Impact of number of updates. We compare the quality of a RegNetY-128GF after different number of updates of an online pretraining on 1B images. For both studies, we report the relative improvement in top-1 accuracy for a linear evaluation of frozen features on ImageNet. (right) Impact of number of unique images. We compare the impact of the size of the training set for a RegNetY-8GF and a RegNetY-16GF pretrained for the same number of updates. The number of updates corresponds to 1 epoch for 1B images, 32 epochs for 32M images and 1K for 1M images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>RegNetY- 32GF .</head><label>32GF</label><figDesc>The model has depth = 20 and RegNet parameters: w 0 = 232, w a = 115.89, w m = 2.53, group width = 232 RegNetY-64GF. The model has depth = 20 and RegNet parameters: w 0 = 352, w a = 147.48, w m = 2.4, group width = 328 RegNetY-128GF. The model has depth = 27 and RegNet parameters: w 0 = 456, w a = 160.83, w m = 2.52, group width = 264 RegNetY-256GF. The model has depth = 27 and RegNet parameters: w 0 = 640, w a = 230.83, w m = 2.53, group width = 373</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Finetuning of models pretrained with self-supervision. We compare with existing features pretrained with no supervision. After pretraining, the models are finetuned on ImageNet and we report top-1 accuracy. We give the details of the architectures and datasets used for pretraining. Numbers are taken from the respective papers. DeepCluster and SwAV are pretrained on uncurated dataset, while SimCLRv2 is pretrained on ImageNet only, and ViT is pretrained on a curated dataset of 300M images.</figDesc><table><row><cell>Method</cell><cell>Data</cell><cell cols="2">#images Arch.</cell><cell cols="2">#param. Top-1</cell></row><row><cell cols="3">DeeperCluster [6] YFCC100M 96M</cell><cell>VGG16</cell><cell>138M</cell><cell>74.9</cell></row><row><cell>ViT [14]</cell><cell>JFT</cell><cell>300M</cell><cell>ViT-B/16</cell><cell>91M</cell><cell>79.9</cell></row><row><cell>SwAV [7]</cell><cell>IG</cell><cell>1B</cell><cell>RX101-32x16d</cell><cell>182M</cell><cell>82.0</cell></row><row><cell>SimCLRv2 [9]</cell><cell>ImageNet</cell><cell>1.2M</cell><cell>RN152w3+SK</cell><cell>795M</cell><cell>83.1</cell></row><row><cell>SEER</cell><cell>IG</cell><cell>1B</cell><cell>RG128</cell><cell>693M</cell><cell>83.8</cell></row><row><cell>SEER</cell><cell>IG</cell><cell>1B</cell><cell>RG256</cell><cell>1.3B</cell><cell>84.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>layer MLP according to the architecture. More concretely, the head dimensions are: RegNetY-8GF has [2016 × 4096, 4096 × 4096 and 4096 × 256], RegNetY-16GF has [3024 × 4096, 4096 × 4096 and 4096×256], RegNetY-32GF has [3712×4096, 4096×4096 and 4096 × 256], RegNetY-64GF has [4920 × 8192, 8192 × 8192 and 8192 × 256], RegNetY-128GF has [7392 × 8192, 8192 × 8192 and 8192 × 256]</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/NVIDIA/apex</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Model architectures.</head><p>We describe below the model architecture settings used for ablation studies. In order to compare the architectures fairly, we follow the same hyperparameters for pre-training. We describe next the setup used for pretraining of ResNet-{50,101}, ResNeXt101-32x{4,8}d and RegNetY-{8,16,32,64,128}GF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Pretraining of ResNet and ResNeXt.</head><p>We pretrain standard ResNet-{50,101} from He et al. <ref type="bibr" target="#b23">[24]</ref> and standard RX101-32x{4,8}d from Xie et al. <ref type="bibr" target="#b52">[53]</ref> with SwAV, using 8 crops per image of resolutions 2×224+ 6×96. We follow the same data augmentation as in Caron et al. <ref type="bibr" target="#b6">[7]</ref>. During pretraining, we use a 2-layer multi-layer perceptron (MLP) projection head of dimensions 2048 × 2048 and 2048 × 256. We do not use BatchNorm layers in the head. We use 3K prototypes, temperature τ set to 0.1, the Sinkhorn regularization parameter to 0.05 and perform 5 iterations of Sinkhorn algorithm. We synchronize Batch-Norm stats across gpus and create process groups of size 32 for synchronization. We use a weight decay of 10 −6 , LARS optimizer <ref type="bibr" target="#b54">[55]</ref> and O1 mixed-precision optimization from Apex library 2 . We train our model with stochastic gradient descent using a large batch size of 8192 different images distributed over 256 NVIDIA V100 32GB GPUs, resulting in 32 different images per GPU. The learning rate is linearly ramped up from 0.3 to 9.6 for the first 6K training updates. After warmup, we follow a half cosine wave learning rate schedule and decay the learning rate from 9.6 to final value 0.0096. Overall, we train on 1B images for a total of 122K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Pretraining of RegNet architectures.</head><p>We train 5 different RegNet architectures namely the RegNetY-{8,16,32,64,128}GF of different capacity. RegNet architectures are generated by following the scaling rules described in Radosavovic et al. <ref type="bibr" target="#b39">[40]</ref>. We first share the parametrization used for each of the RegNet architecture below. We then describe how we train these architectures with SwAV for our ablation study in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RegNetY</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Milking cowmask for semi-supervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12022</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2020. 1</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS), 2020. 1</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Libri-light: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamad</forename><forename type="middle">Hassan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamad</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Jie</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining transfers well across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>the International Conference on Acoustics, Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08237,2020.3</idno>
		<title level="m">Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution discrepancy: Fixefficientnet</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
