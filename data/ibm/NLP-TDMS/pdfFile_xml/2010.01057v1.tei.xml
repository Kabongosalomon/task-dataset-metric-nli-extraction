<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Studio Ousia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN AIP</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
							<email>shindo@is.naist.jp</email>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN AIP</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
							<email>takeda@nii.ac.jp</email>
							<affiliation key="aff4">
								<orgName type="institution">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN AIP</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT (Devlin et al., 2019). The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https: //github.com/studio-ousia/luke.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many natural language tasks involve entities, e.g., relation classification, entity typing, named entity recognition (NER), and question answering (QA). Key to solving such entity-related tasks is a model to learn the effective representations of entities. Conventional entity representations assign each entity a fixed embedding vector that stores information regarding the entity in a knowledge base (KB) <ref type="bibr" target="#b12">(Bordes et al., 2013;</ref><ref type="bibr" target="#b37">Trouillon et al., 2016;</ref><ref type="bibr" target="#b43">Yamada et al., 2016</ref><ref type="bibr" target="#b44">Yamada et al., , 2017</ref>. Although these models capture the rich information in the KB, they require entity linking to represent entities in a text, and cannot represent entities that do not exist in the KB.</p><p>By contrast, contextualized word representations (CWRs) based on the transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>, such as BERT <ref type="bibr" target="#b17">(Devlin et al., 2019)</ref>, and RoBERTa , provide effective general-purpose word representations trained with unsupervised pretraining tasks based on language modeling. Many recent studies have solved entity-related tasks using the contextualized representations of entities computed based on CWRs <ref type="bibr" target="#b30">Peters et al., 2019;</ref>. However, the architecture of CWRs is not well suited to representing entities for the following two reasons: (1) Because CWRs do not output the span-level representations of entities, they typically need to learn how to compute such representations based on a downstream dataset that is typically small. (2) Many entity-related tasks, e.g., relation classification and QA, involve reasoning about the relationships between entities. Although the transformer can capture the complex relationships between words by relating them to each other multiple times using the self-attention mechanism <ref type="bibr" target="#b33">Reif et al., 2019)</ref>, it is difficult to perform such reasoning between entities because many entities are split into multiple tokens in the model. Furthermore, the word-based pretraining task of CWRs is not suitable for learning the representations of entities because predicting a masked word given other words in the entity, e.g., predicting "Rings" given "The Lord of the [MASK]", is clearly easier than predicting the entire entity.</p><p>In this paper, we propose new pretrained contextualized representations of words and entities by developing LUKE (Language Understanding with Knowledge-based Embeddings). LUKE is based on a transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>   <ref type="figure">Figure 1</ref>: Architecture of LUKE using the input sentence "Beyonc√© lives in Los Angeles." LUKE outputs contextualized representation for each word and entity in the text. The model is trained to predict randomly masked words (e.g., lives and Angeles in the figure) and entities (e.g., Los Angeles in the figure). Downstream tasks are solved using its output representations with linear classifiers.</p><p>obtained from Wikipedia. An important difference between LUKE and existing CWRs is that it treats not only words, but also entities as independent tokens, and computes intermediate and output representations for all tokens using the transformer (see <ref type="figure">Figure 1</ref>). Since entities are treated as tokens, LUKE can directly model the relationships between entities. LUKE is trained using a new pretraining task, a straightforward extension of BERT's masked language model (MLM) <ref type="bibr" target="#b17">(Devlin et al., 2019)</ref>. The task involves randomly masking entities by replacing them with [MASK] entities, and trains the model by predicting the originals of these masked entities. We use RoBERTa as base pre-trained model, and conduct pretraining of the model by simultaneously optimizing the objectives of the MLM and our proposed task. When applied to downstream tasks, the resulting model can compute representations of arbitrary entities in the text using [MASK] entities as inputs. Furthermore, if entity annotation is available in the task, the model can compute entity representations based on the rich entity-centric information encoded in the corresponding entity embeddings.</p><p>Another key contribution of this paper is that it extends the transformer using our entity-aware self-attention mechanism. Unlike existing CWRs, our model needs to deal with two types of tokens, i.e., words and entities. Therefore, we assume that it is beneficial to enable the mechanism to easily determine the types of tokens. To this end, we enhance the self-attention mechanism by adopting different query mechanisms based on the attending token and the token attended to.</p><p>We validate the effectiveness of our proposed model by conducting extensive experiments on five standard entity-related tasks: entity typing, relation classification, NER, cloze-style QA, and extractive QA. Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on five tasks: entity typing on the Open Entity dataset <ref type="bibr" target="#b14">(Choi et al., 2018)</ref>, relation classification on the TACRED dataset <ref type="bibr" target="#b49">(Zhang et al., 2017)</ref>, NER on the CoNLL-2003 dataset <ref type="bibr" target="#b36">(Tjong Kim Sang and De Meulder, 2003)</ref>, clozestyle QA on the ReCoRD dataset <ref type="bibr" target="#b47">(Zhang et al., 2018a)</ref>, and extractive QA on the SQuAD 1.1 dataset <ref type="bibr" target="#b32">(Rajpurkar et al., 2016)</ref>. We publicize our source code and pretrained representations at https://github.com/studio-ousia/luke.</p><p>The main contributions of this paper are summarized as follows:</p><p>‚Ä¢ We propose LUKE, a new contextualized representations specifically designed to address entityrelated tasks. LUKE is trained to predict randomly masked words and entities using a large amount of entity-annotated corpus obtained from Wikipedia.</p><p>‚Ä¢ We introduce an entity-aware self-attention mechanism, an effective extension of the original mechanism of transformer. The proposed mechanism considers the type of the tokens (words or entities) when computing attention scores.</p><p>‚Ä¢ LUKE achieves strong empirical performance and obtains state-of-the-art results on five popular datasets: Open Entity, TACRED, CoNLL-2003, ReCoRD, and SQuAD 1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static Entity Representations</head><p>Conventional entity representations assign a fixed embedding to each entity in the KB. They include knowledge embeddings trained on knowledge graphs <ref type="bibr" target="#b12">(Bordes et al., 2013;</ref><ref type="bibr" target="#b45">Yang et al., 2015;</ref><ref type="bibr" target="#b37">Trouillon et al., 2016)</ref>, and embeddings trained using textual contexts or descriptions of entities retrieved from a KB <ref type="bibr" target="#b43">(Yamada et al., 2016</ref><ref type="bibr" target="#b44">(Yamada et al., , 2017</ref><ref type="bibr" target="#b13">Cao et al., 2017;</ref><ref type="bibr" target="#b18">Ganea and Hofmann, 2017)</ref>. Similar to our pretraining task, NTEE <ref type="bibr" target="#b44">(Yamada et al., 2017)</ref> and RELIC <ref type="bibr">(Ling et al., 2020)</ref> use an approach that trains entity embeddings by predicting entities given their textual contexts obtained from a KB. The main drawbacks of this line of work, when representing entities in text, are that (1) they need to resolve entities in the text to corresponding KB entries to represent the entities, and (2) they cannot represent entities that do not exist in the KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextualized Word Representations</head><p>Many recent studies have addressed entity-related tasks based on the contextualized representations of entities in text computed using the word representations of CWRs <ref type="bibr" target="#b30">Peters et al., 2019;</ref><ref type="bibr" target="#b41">Wang et al., 2019b</ref>. Representative examples of CWRs are ELMo <ref type="bibr" target="#b29">(Peters et al., 2018)</ref> and BERT <ref type="bibr" target="#b17">(Devlin et al., 2019)</ref>, which are based on deep bidirectional long short-term memory (LSTM) and the transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>, respectively. BERT is trained using an MLM, a pretraining task that masks random words in the text and trains the model to predict the masked words. Most recent CWRs, such as RoBERTa , XLNet <ref type="bibr" target="#b46">(Yang et al., 2019)</ref>, Span-BERT , ALBERT <ref type="bibr" target="#b22">(Lan et al., 2020)</ref>, BART , and T5 <ref type="bibr" target="#b31">(Raffel et al., 2020)</ref>, are based on transformer trained using a task equivalent to or similar to the MLM. Similar to our proposed pretraining task that masks entities instead of words, several recent CWRs, e.g., Span-BERT, ALBERT, BART, and T5, have extended the MLM by randomly masking word spans instead of single words. Furthermore, various recent studies have explored methods to enhance CWRs by injecting them with knowledge from external sources, such as KBs. ERNIE  and Know-BERT <ref type="bibr" target="#b30">(Peters et al., 2019)</ref> use a similar idea to enhance CWRs using static entity embeddings sep-arately learned from a KB. WKLM <ref type="bibr" target="#b42">(Xiong et al., 2020)</ref> trains the model to detect whether an entity name in text is replaced by another entity name of the same type. KEPLER <ref type="bibr" target="#b41">(Wang et al., 2019b)</ref> conducts pretraining based on the MLM and a knowledge-embedding objective <ref type="bibr" target="#b12">(Bordes et al., 2013)</ref>. K-Adapter  was proposed concurrently with our work, and extends CWRs using neural adapters that inject factual and linguistic knowledge. This line of work is related to ours because our pretraining task also enhances the model using information in the KB.</p><p>Unlike the CWRs mentioned above, LUKE uses an improved transformer architecture with an entity-aware self-attention mechanism that is designed to effectively solve entity-related tasks. LUKE also outputs entity representations by learning how to compute them during pretraining. It achieves superior empirical results to existing CWRs and knowledge-enhanced CWRs in all of our experiments. <ref type="figure">Figure 1</ref> shows the architecture of LUKE. The model adopts a multi-layer bidirectional transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>. It treats words and entities in the document as input tokens, and computes a representation for each token. Formally, given a sequence consisting of m words w 1 , w 2 , ..., w m and n entities e 1 , e 2 , ..., e n , our model computes D-dimensional word representations h w 1 , h w 2 , ..., h wm , where h w ‚àà R D , and entity representations h e 1 , h e 2 , ..., h en , where h e ‚àà R D . The entities can be Wikipedia entities (e.g., Beyonc√© in <ref type="figure">Figure 1</ref>) or special entities (e.g., [MASK]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LUKE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Representation</head><p>The input representation of a token (word or entity) is computed using the following three embeddings:</p><p>‚Ä¢ Token embedding represents the corresponding token. We denote the word token embedding by A ‚àà R Vw√óD , where V w is the number of words in our vocabulary. For computational efficiency, we represent the entity token embedding by decomposing it into two small matrices, B ‚àà R Ve√óH and U ‚àà R H√óD , where V e is the number of entities in our vocabulary. Hence, the full matrix of the entity token embedding can be computed as BU.</p><p>‚Ä¢ Position embedding represents the position of the token in a word sequence. A word and an entity appearing at the i-th position in the sequence are represented as C i ‚àà R D and D i ‚àà R D , respectively. If an entity name contains multiple words, its position embedding is computed by averaging the embeddings of the corresponding positions, as shown in <ref type="figure">Figure 1</ref>.</p><p>‚Ä¢ Entity type embedding represents that the token is an entity. The embedding is a single vector denoted by e ‚àà R D .</p><p>The input representation of a word and that of an entity are computed by summing the token and position embeddings, and the token, position, and entity type embeddings, respectively. Following past work <ref type="bibr" target="#b17">(Devlin et al., 2019;</ref>, we insert special tokens [CLS] and [SEP] into the word sequence as the first and last words, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity-aware Self-attention</head><p>The self-attention mechanism is the foundation of the transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref>, and relates tokens each other based on the attention score between each pair of tokens. Given a sequence of input vectors x 1 , x 2 , ..., x k , where x i ‚àà R D , each of the output vectors y 1 , y 2 , ..., y k , where y i ‚àà R L , is computed based on the weighted sum of the transformed input vectors. Here, each input and output vector corresponds to a token (a word or an entity) in our model; therefore, k = m + n. The i-th output vector y i is computed as:</p><formula xml:id="formula_0">y i = k j=1 Œ± ij Vx j e ij = Kx j Qx i ‚àö L Œ± ij = softmax(e ij )</formula><p>where Q ‚àà R L√óD , K ‚àà R L√óD , and V ‚àà R L√óD denote the query, key, and value matrices, respectively.</p><p>Because LUKE handles two types of tokens (i.e., words and entities), we assume that it is beneficial to use the information of target token types when computing the attention scores (e ij ). With this in mind, we enhance the mechanism by introducing an entity-aware query mechanism that uses a different query matrix for each possible pair of token types of x i and x j . Formally, the attention score e ij is computed as follows:</p><formula xml:id="formula_1">e ij = Ô£± Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£≤ Ô£¥ Ô£¥ Ô£¥ Ô£¥ Ô£≥ Kx j Qx i , if both x i and x j are words Kx j Q w2e x i , if x i is word and x j is entity Kx j Q e2w x i , if x i is entity and x j is word Kx j Q e2e x i , if both x i and x j are entities where Q w2e , Q e2w , Q e2e ‚àà R L√óD are query ma- trices.</formula><p>Note that the computational costs of the original mechanism and our proposed mechanism are identical except the additional cost of computing gradients and updating the parameters of the additional query matrices at the training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pretraining Task</head><p>To pretrain LUKE, we use the conventional MLM and a new pretraining task that is an extension of the MLM to learn entity representations. In particular, we treat hyperlinks in Wikipedia as entity annotations, and train the model using a large entityannotated corpus retrieved from Wikipedia. We randomly mask a certain percentage of the entities by replacing them with special [MASK] entities 1 and then train the model to predict the masked entities. Formally, the original entity corresponding to a masked entity is predicted by applying the softmax function over all entities in our vocabulary:</p><formula xml:id="formula_2">y = softmax(BTm + b o ) m = layer norm gelu(W h h e + b h )</formula><p>where h e is the representation corresponding to the masked entity, T ‚àà R H√óD and W h ‚àà R D√óD are weight matrices, b o ‚àà R Ve and b h ‚àà R D are bias vectors, gelu(¬∑) is the gelu activation function <ref type="bibr" target="#b19">(Hendrycks and Gimpel, 2016)</ref>, and layer norm(¬∑) is the layer normalization function (Lei <ref type="bibr">Ba et al., 2016)</ref>. Our final loss function is the sum of MLM loss and cross-entropy loss on predicting the masked entities, where the latter is computed identically to the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Modeling Details</head><p>Our model configuration follows RoBERTa LARGE , pretrained CWRs based on a bidirectional transformer and a variant of BERT <ref type="bibr" target="#b17">(Devlin et al., 2019)</ref>. In particular, our model is based on the bidirectional transformer with D = 1024 hidden dimensions, 24 hidden layers, L = 64 attention head dimensions, and 16 self-attention heads. The number of dimensions of the entity token embedding is set to H = 256. The total number of parameters is approximately 483 M, consisting of 355 M in RoBERTa and 128 M in our entity embeddings. The input text is tokenized into words using RoBERTa's tokenizer with the vocabulary consisting of V w = 50K words. For computational efficiency, our entity vocabulary does not include all entities but only the V e = 500K entities most frequently appearing in our entity annotations. The entity vocabulary also includes two special entities, i.e., <ref type="bibr">[MASK]</ref> and <ref type="bibr">[UNK]</ref>.</p><p>The model is trained via iterations over Wikipedia pages in a random order for 200K steps. To reduce training time, we initialize the parameters that LUKE have in common with RoBERTa (parameters in the transformer and the embeddings for words) using RoBERTa. Following past work <ref type="bibr" target="#b17">(Devlin et al., 2019;</ref>, we mask 15% of all words and entities at random. If an entity does not exist in the vocabulary, we replace it with the [UNK] entity. We perform pretraining using the original self-attention mechanism rather than our entity-aware self-attention mechanism because we want an ablation study of our mechanism but can not afford to run pretraining twice. Query matrices of our self-attention mechanism (Q w2e , Q e2w , and Q e2e ) are learned using downstream datasets. Further details of our pretraining are described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments using five entityrelated tasks: entity typing, relation classification, NER, cloze-style QA, and extractive QA. We use similar model architectures for all tasks based on a simple linear classifier on top of the representations of words, entities, or both. Unless otherwise specified, we create the input word sequence by inserting tokens of [CLS] and [SEP] into the original word sequence as the first and the last tokens, respectively. The input entity sequence is built using [MASK] entities, special entities introduced for the task, or Wikipedia entities. The token embedding of a task-specific special entity is initialized using that of the [MASK] entity, and the query matrices of our entity-aware self-attention mechanism (Q w2e , Q e2w , and Q e2e ) are initialized using the original query matrix Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Prec. Rec. F1 UFET  77.4 60.6 68.0 BERT  76.4 71.0 73.6 ERNIE  78.4 72.9 75.6 KEPLER <ref type="bibr" target="#b41">(Wang et al., 2019b)</ref> 77.2 74.2 75.7 KnowBERT <ref type="bibr" target="#b30">(Peters et al., 2019)</ref> 78.6 73.7 76.1 K-Adapter  79.3 75.8 77.5 RoBERTa  77.6 75.0 76.2 LUKE 79.9 76.6 78.2 Because we use RoBERTa as the base model in our pretraining, we use it as our primary baseline for all tasks. We omit a description of the baseline models in each section if they are described in Section 2. Further details of our experiments are available in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Entity Typing</head><p>We first conduct experiments on entity typing, which is the task of predicting the types of an entity in the given sentence. Following , we use the Open Entity dataset <ref type="bibr" target="#b14">(Choi et al., 2018)</ref>, and consider only nine general entity types. Following , we report loose micro-precision, recall, and F1, and employ the micro-F1 as the primary metric.</p><p>Model We represent the target entity using the [MASK] entity, and enter words and the entity in each sentence into the model. We then classify the entity using a linear classifier based on the corresponding entity representation. We treat the task as multi-label classification, and train the model using binary cross-entropy loss averaged over all entity types.</p><p>Baselines UFET <ref type="bibr" target="#b14">(Choi et al., 2018</ref>) is a conventional model that computes context representations using the bidirectional LSTM. We also use BERT, RoBERTa, ERNIE, KnowBERT, KEPLER, and K-Adapter as baselines.</p><p>Results <ref type="table" target="#tab_1">Table 1</ref> shows the experimental results. LUKE significantly outperforms our primary baseline, RoBERTa, by 2.0 F1 points, and the previous best published model, KnowBERT, by 2.1 F1 points. Furthermore, LUKE achieves a new state of the art by outperforming K-Adapter by 0.7 F1 points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relation Classification</head><p>Relation classification determines the correct relation between head and tail entities in a sentence. We conduct experiments using TACRED dataset <ref type="bibr" target="#b49">(Zhang et al., 2017)</ref>, a large-scale relation classification dataset containing 106,264 sentences with 42 relation types. Following , we report the micro-precision, recall, and F1, and use the micro-F1 as the primary metric.</p><p>Model We introduce two special entities, [HEAD] and [TAIL], to represent the head and the tail entities, respectively, and input words and these two entities in each sentence to the model. We then solve the task using a linear classifier based on a concatenated representation of the head and tail entities. The model is trained using cross-entropy loss.</p><p>Baselines C-GCN (Zhang et al., 2018b) uses graph convolutional networks over dependency tree structures to solve the task. MTB (Baldini Soares et al., 2019) learns relation representations based on BERT through the matching-the-blanks task using a large amount of entity-annotated text. We also compare LUKE with BERT, RoBERTa, SpanBERT, ERNIE, KnowBERT, KEPLER, and K-Adapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The experimental results are presented in <ref type="table" target="#tab_2">Table 2</ref>. LUKE clearly outperforms our primary baseline, RoBERTa, by 1.4 F1 points, and the previous best published models, namely MTB and KnowBERT, by 1.2 F1 points. Furthermore, it achieves a new state of the art by outperforming K-Adapter by 0.7 F1 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Named Entity Recognition</head><p>We conduct experiments on the NER task using the standard CoNLL-2003 dataset <ref type="bibr" target="#b36">(Tjong Kim Sang and De Meulder, 2003)</ref>. Following past work, we  report the span-level F1.</p><p>Model Following Sohrab and Miwa (2018), we solve the task by enumerating all possible spans (or n-grams) in each sentence as entity name candidates, and classifying them into the target entity types or non-entity type, which indicates that the span is not an entity. For each sentence in the dataset, we enter words and the [MASK] entities corresponding to all possible spans. The representation of each span is computed by concatenating the word representations of the first and last words in the span, and the entity representation corresponding to the span. We classify each span using a linear classifier with its representation, and train the model using cross-entropy loss. We exclude spans longer than 16 words for computational efficiency. During the inference, we first exclude all spans classified into the non-entity type. To avoid selecting overlapping spans, we greedily select a span from the remaining spans based on the logit of its predicted entity type in descending order if the span does not overlap with those already selected. Following <ref type="bibr" target="#b17">Devlin et al. (2019)</ref>, we include the maximal document context in the target document. We also use ELMo, BERT, and RoBERTa as baselines. To conduct a fair comparison with RoBERTa, we report its performance using the model described above with the span representation computed by concatenating the representations of the first and last words of the span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The experimental results are shown in <ref type="table" target="#tab_4">Table 3</ref>. LUKE outperforms RoBERTa by 1.9 F1 points. Furthermore, it achieves a new state of the art on this competitive dataset by outperforming the previous state of the art reported in <ref type="bibr" target="#b10">Baevski et al. (2019)</ref> by 0.8 F1 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cloze-style Question Answering</head><p>We evaluate our model on the ReCoRD dataset <ref type="bibr" target="#b47">(Zhang et al., 2018a)</ref>, a cloze-style QA dataset consisting of over 120K examples. An interesting characteristic of this dataset is that most of its questions cannot be solved without external knowledge. The following is an example question and its answer in the dataset: Question: According to claims in the suit, "Parts of 'Stairway to Heaven,' instantly recognizable to the music fans across the world, sound almost identical to significant portions of 'X."' Answer: Taurus Given a question and a passage, the task is to find the entity mentioned in the passage that fits the missing entity (denoted by X in the question above).</p><p>In this dataset, annotations of entity spans (start and end positions) in a passage are provided, and the answer is contained in the provided entity spans one or multiple times. Following past work, we evaluate the models using exact match (EM) and token-level F1 on the development and test sets.</p><p>Model We solve this task by assigning a relevance score to each entity in the passage and selecting the entity with the highest score as the answer. Following , given a question q 1 , q 2 , ..., q j , and a passage p 1 , p 2 , ..., p l , the input word sequence is constructed as: [CLS]q 1 , q 2 , ..., q j [SEP]</p><p>[SEP]p 1 , p 2 , ..., p l [SEP]. Further, we input [MASK] entities corresponding to the missing entity and all entities in the passage. We compute the relevance score of each entity in the passage using a linear classifier with the concatenated representation of the missing entity and the corresponding entity. We train the model using binary cross-entropy loss averaged over all entities in the passage, and select the entity with the highest score (logit) as the answer.</p><p>Baselines DocQA+ELMo ) is a model based on ELMo, bidirectional attention flow <ref type="bibr" target="#b34">(Seo et al., 2017)</ref>, and self-attention mechanism. XLNet+Verifier <ref type="bibr" target="#b25">(Li et al., 2019</ref>) is a model based on XLNet with rule-based answer verification, and is the winner of a recent competition Name Dev EM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev F1</head><p>Test EM Test F1 DocQA+ELMo <ref type="bibr" target="#b47">(Zhang et al., 2018a</ref><ref type="bibr">) 44.1 45.4 45.4 46.7 BERT (Wang et al., 2019a</ref> --71.3 72.0 XLNet+Verifier <ref type="bibr" target="#b25">(Li et al., 2019)</ref> 80.6 82.1 81.5 82.7 RoBERTa  89.0 89.5 --RoBERTa (ensemble)    based on this dataset <ref type="bibr" target="#b28">(Ostermann et al., 2019)</ref>. We also use BERT and RoBERTa as baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results are presented in <ref type="table" target="#tab_6">Table 4</ref>. LUKE significantly outperforms RoBERTa, the best baseline, on the development set by 1.8 EM points and 1.9 F1 points. Furthermore, it achieves superior results to RoBERTa (ensemble) on the test set without ensembling the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Extractive Question Answering</head><p>Finally, we conduct experiments using the wellknown Stanford Question Answering Dataset (SQuAD) 1.1 consisting of 100K question/answer pairs <ref type="bibr" target="#b32">(Rajpurkar et al., 2016)</ref>. Given a question and a Wikipedia passage containing the answer, the task is to predict the answer span in the passage. Following past work, we report the EM and token-level F1 on the development and test sets.</p><p>Model We construct the word sequence from the question and the passage in the same way as in the previous experiment. Unlike in the other experiments, we input Wikipedia entities into the model based on entity annotations automatically generated on the question and the passage using a mapping from entity names (e.g., "U.S.") to their referent entities (e.g., United States). The mapping is automatically created using the entity hyperlinks in Wikipedia as described in detail in Appendix C. We solve this task using the same model architecture as that of BERT and RoBERTa. In particular, we use two linear classifiers independently on top of the word representations to predict the span boundary of the answer (i.e., the start and end positions), and train the model using cross-entropy loss.</p><p>Baselines We compare our models with the results of recent CWRs, including BERT, RoBERTa, SpanBERT, XLNet, and ALBERT. Because the results for RoBERTa and ALBERT are reported only on the development set, we conduct a comparison Name Dev EM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev F1</head><p>Test EM Test F1 BERT <ref type="bibr" target="#b17">(Devlin et al., 2019)</ref> 84.2 91.1 85.1 91.8 SpanBERT <ref type="bibr">) --88.8 94.6 XLNet (Yang et al., 2019</ref> 89.0 94.5 89.9 95.1 ALBERT <ref type="bibr" target="#b22">(Lan et al., 2020)</ref> 89.3 94.8 --RoBERTa  88.9 94.6 --LUKE 89.8 95.0 90.2 95.4  with these models using this set. To conduct a fair compassion with RoBERTa, we use the same model architecture and hyper-parameters as those of RoBERTa .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The experimental results are presented in <ref type="table" target="#tab_7">Table 5</ref>. LUKE outperforms our primary baseline, RoBERTa, by 0.9 EM points and 0.4 F1 points on the development set. Furthermore, it achieves a new state of the art on this competitive dataset by outperforming XLNet by 0.3 points both in terms of EM and F1. Note that XLNet uses a more sophisticated model involving beam search than the other models considered here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we provide a detailed analysis of LUKE by reporting three additional experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effects of Entity Representations</head><p>To investigate how our entity representations influence performance on downstream tasks, we perform an ablation experiment by addressing NER on the CoNLL-2003 dataset and extractive QA on the SQuAD dataset without inputting any entities. In this setting, LUKE uses only the word sequence to compute the representation for each word. We address the tasks using the same model architectures as those for RoBERTa described in the corresponding sections. As shown in <ref type="table" target="#tab_8">Table 6</ref>, this setting clearly degrades performance, i.e., 1.4 F1 points on the CoNLL-2003 dataset and 0.6 EM points on the SQuAD dataset, demonstrating the effectiveness of our entity representations on these two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of Entity-aware Self-attention</head><p>We conduct an ablation study of our entity-aware self-attention mechanism by comparing the performance of LUKE using our mechanism with that using the original mechanism of the transformer. As shown in <ref type="table" target="#tab_10">Table 7</ref>, our entity-aware self-attention mechanism consistently outperforms the original mechanism across all tasks. Furthermore, we observe significant improvements on two kinds of tasks, relation classification (TACRED) and QA (ReCoRD and SQuAD). Because these tasks involve reasoning based on relationships between entities, we consider that our mechanism enables the model (i.e., attention heads) to easily focus on capturing the relationships between entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effects of Extra Pretraining</head><p>As mentioned in Section 3.4, LUKE is based on RoBERTa with pretraining for 200K steps using our Wikipedia corpus. Because past studies <ref type="bibr" target="#b22">Lan et al., 2020)</ref> suggest that simply increasing the number of training steps of CWRs tends to improve performance on downstream tasks, the superior experimental results of LUKE compared with those of RoBERTa may be obtained because of its greater number of pretraining steps.</p><p>To investigate this, we train another model based on RoBERTa with extra pretraining based on the MLM using the Wikipedia corpus for 200K training steps. The detailed configuration used in the pretraining is available in Appendix A.</p><p>We evaluate the performance of this model on the CoNLL-2003 and SQuAD datasets using the same model architectures as those for RoBERTa described in the corresponding sections. As shown in <ref type="table" target="#tab_11">Table 8</ref>, the model achieves similar performance to the original RoBERTa on both datasets, which indicates that the superior performance of LUKE is not owing to its longer pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose LUKE, new pretrained contextualized representations of words and entities based on the transformer. LUKE outputs the contextualized representations of words and entities using an improved transformer architecture with using a novel entity-aware self-attention mechanism. The experimental results prove its effectiveness on various entity-related tasks. Future work involves applying LUKE to domain-specific tasks, such as those in biomedical and legal domains.      <ref type="table" target="#tab_12">Table 9</ref>. <ref type="table" target="#tab_1">Table 10</ref> shows the hyper-parameters used for the extra pretraining of RoBERTa on our Wikipedia corpus described in Section 5. As shown in the Table, we use the same hyper-parameters as the ones used to train LUKE. We train the model for 200K steps and update all parameters throughout the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Experiments</head><p>We conduct the experiments using NVIDIA's Py-Torch Docker container 19.02 hosted on a server with two Intel Xeon E5-2698 v4 CPUs and eight V100 GPUs. For each dataset, excluding SQuAD, we conduct hyper-parameter tuning using grid search based on the performance on the development set. We evaluate performance using EM on the ReCoRD dataset, and F1 on the other datasets. Because our computational resources are limited, we use the following constrained search space: ‚Ä¢ learning rate: 1e-5, 2e-5, 3e-5    <ref type="table" target="#tab_1">Table 12</ref>). We optimize the model using AdamW with learning rate warmup and linear decay of the learning rate. We also use early stopping based on performance on the development set. The details of the datasets used in our experiments are provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Open Entity</head><p>The Open Entity dataset used in  consists of training, development, and test sets, where each set contains 1,998 examples with labels of nine general entity types. The dataset is downloaded from the website for . <ref type="bibr">2</ref> We compute the reported results using our code based on that of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 TACRED</head><p>The dataset is obtained from the LDC website. <ref type="bibr">3</ref> We compute the reported results using our code based on that of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 CoNLL-2003</head><p>The CoNLL-2003 dataset comprises training, development, and test sets, containing 14,987, 3,466, and 3,684 sentences, respectively. Each sentence contains annotations of four entity types, namely person, location, organization, and miscellaneous. The dataset is downloaded from the relevant website. <ref type="bibr">4</ref> The reported results are computed using the conlleval script obtained from the website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 ReCoRD</head><p>The ReCoRD dataset consists of 100,730 training, 10,000 development, and 10,000 test questions created based on 80,121 unique news articles. The dataset is obtained from the relevant website. <ref type="bibr">5</ref> We compute the performance on the development set using the official evaluation script downloaded from the website. Performance on the test set is obtained by submitting our model to the leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 SQuAD 1.1</head><p>The SQuAD 1.1 dataset contains 87,599 training, 10,570 development, and 9,533 test questions created based on 536 Wikipedia articles. The dataset is downloaded from the relevant website. <ref type="bibr">6</ref> We compute performance on the development set using the official evaluation script downloaded from the website. Performance on the test set is obtained by submitting our model to the leaderboard.</p><p>3 https://catalog.ldc.upenn.edu/ LDC2018T24 4 https://www.clips.uantwerpen.be/ conll2003/ner 5 https://sheng-z.github.io/ ReCoRD-explorer 6 https://rajpurkar.github.io/ SQuAD-explorer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Adding Entity Annotations to SQuAD dataset</head><p>For each question-passage pair in the SQuAD dataset, we first create a mapping from the entity names (e.g., "U.S.") to their referent Wikipedia entities (e.g., United States) using the entity hyperlinks on the source Wikipedia page of the passage. We then perform simple string matching to extract all entity names in the question and the passage, and treat all matched entity names as entity annotations for their referent entities. We ignore an entity name if the name refers to multiple entities on the page. Further, to reduce noise, we also exclude an entity name if its link probability, the probability that the name appears as a hyperlink in Wikipedia, is lower than 1%. We use the March 2016 version of Wikipedia to collect the entity hyperlinks and the link probabilities of the entity names.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Baselines LSTM-CRF<ref type="bibr" target="#b21">(Lample et al., 2016</ref>) is a model based on the bidirectional LSTM with conditional random fields (CRF). Akbik et al. (2018) address the task using the bidirectional LSTM with CRF enhanced with character-level contextualized representations. Similarly,<ref type="bibr" target="#b10">Baevski et al. (2019)</ref> use the bidirectional LSTM with CRF enhanced with CWRs based on a bidirectional transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>trained using a large amount of entity-annotated corpus arXiv:2010.01057v1 [cs.CL] 2 Oct 2020Transformer with Entity-aware Self-attention</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Question Answering</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Named Entity Recognition</cell><cell></cell></row><row><cell>Pretraining</cell><cell></cell><cell></cell><cell>Predict lives</cell><cell></cell><cell cols="3">Predict Angeles</cell><cell cols="2">Predict Los_Angeles</cell><cell cols="4">Relation Classification</cell><cell></cell></row><row><cell></cell><cell>h w1</cell><cell>h w2</cell><cell>h w3</cell><cell cols="2">h w4 h w5</cell><cell>h w6</cell><cell>h w7</cell><cell>h e1</cell><cell>h e2</cell><cell cols="2">Entity Typing</cell><cell></cell><cell cols="2">Predict LOCATION</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Transformer</cell><cell></cell><cell></cell><cell></cell><cell>h w1</cell><cell>h w2</cell><cell>...</cell><cell>h w7</cell><cell>h e1</cell></row><row><cell>Token Emb.</cell><cell>A [CLS]</cell><cell>A Beyonc√©</cell><cell>A [MASK]</cell><cell>A in</cell><cell>A Los</cell><cell>A [MASK]</cell><cell>A [SEP]</cell><cell cols="2">BU Beyonc√© BU [MASK]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Position Emb.</cell><cell>C 1</cell><cell>C 2</cell><cell>C 3</cell><cell>C 4</cell><cell>C 5</cell><cell>C 6</cell><cell>C 7</cell><cell>D 2</cell><cell>D 5 + D 6 2</cell><cell>A [CLS]</cell><cell>A Beyonc√©</cell><cell>...</cell><cell>A [SEP]</cell><cell>BU [MASK]</cell></row><row><cell>Entity Type Emb.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>e</cell><cell>e</cell><cell>C 1</cell><cell>C 2</cell><cell>...</cell><cell>C 7</cell><cell>D 5 + D 6 2</cell></row><row><cell></cell><cell>[CLS]</cell><cell>Beyonc√©</cell><cell>[MASK]</cell><cell>in</cell><cell>Los</cell><cell>[MASK]</cell><cell>[SEP]</cell><cell>Beyonc√©</cell><cell>[MASK]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>e</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Words</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Entities</cell><cell>[CLS]</cell><cell>Beyonc√©</cell><cell>...</cell><cell>[SEP]</cell><cell>[MASK]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Words</cell><cell></cell><cell></cell><cell>Entities</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of entity typing on the Open Entity dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of relation classification on the TA-CRED dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results of named entity recognition on the CoNLL-2003 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results of cloze-style question answering on the ReCoRD dataset. All models except RoBERTa (ensemble) are based on a single model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results of extractive question answering on the SQuAD 1.1 dataset.</figDesc><table><row><cell>Name</cell><cell>CoNLL-2003 (Test F1)</cell><cell>SQuAD (Dev EM)</cell><cell>SQuAD (Dev F1)</cell></row><row><cell>LUKE w/o entity inputs</cell><cell>92.9</cell><cell>89.2</cell><cell>94.8</cell></row><row><cell>LUKE</cell><cell>94.3</cell><cell>89.8</cell><cell>95.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of our entity representations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of our entity-aware self-attention mechanism.</figDesc><table><row><cell>Name</cell><cell>CoNLL-2003 (Test F1)</cell><cell>SQuAD (Dev EM)</cell><cell>SQuAD (Dev F1)</cell></row><row><cell>RoBERTa w/ extra training</cell><cell>92.5</cell><cell>89.1</cell><cell>94.7</cell></row><row><cell>RoBERTa</cell><cell>92.4</cell><cell>88.9</cell><cell>94.6</cell></row><row><cell>LUKE</cell><cell>94.3</cell><cell>89.8</cell><cell>95.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Results of RoBERTa additionally trained using our Wikipedia corpus.</figDesc><table><row><cell>Appendix for "LUKE: Deep</cell></row><row><cell>Contextualized Entity Representations</cell></row><row><cell>with Entity-aware Self-attention"</cell></row><row><cell>A Details of Pretraining</cell></row><row><cell>As input corpus for pretraining, we use the De-</cell></row><row><cell>cember 2018 version of Wikipedia, comprising ap-</cell></row><row><cell>proximately 3.5 billion words and 11 million entity</cell></row><row><cell>annotations. We generate input sequences by split-</cell></row><row><cell>ting the content of each page into sequences com-</cell></row><row><cell>prising ‚â§ 512 words and their entity annotations</cell></row><row><cell>(i.e., hyperlinks). We optimize the model using</cell></row><row><cell>AdamW with learning rate warmup and linear de-</cell></row><row><cell>cay of the learning rate. To stabilize training, we</cell></row><row><cell>update only those parameters that are randomly ini-</cell></row><row><cell>tialized (i.e., fix the parameters that are initialized</cell></row><row><cell>using RoBERTa) in the first 100K steps, and update</cell></row><row><cell>all parameters in the remaining 100K steps. We</cell></row><row><cell>run the pretraining on NVIDIA's PyTorch Docker</cell></row><row><cell>container 19.02 hosted on a server with two Intel</cell></row><row><cell>Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Hyper-parameters used to pretrain LUKE.</figDesc><table><row><cell>Name</cell><cell>Value</cell></row><row><cell>Maximum word length</cell><cell>512</cell></row><row><cell>Batch size</cell><cell>2048</cell></row><row><cell>Peak learning rate</cell><cell>1e-5</cell></row><row><cell>Learning rate decay</cell><cell>linear</cell></row><row><cell>Warmup steps</cell><cell>2500</cell></row><row><cell cols="2">Mask probability for words 15%</cell></row><row><cell>Dropout</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>0.01</cell></row><row><cell>Gradient clipping</cell><cell>none</cell></row><row><cell>Adam Œ≤ 1</cell><cell>0.9</cell></row><row><cell>Adam Œ≤ 2</cell><cell>0.999</cell></row><row><cell>Adam</cell><cell>1e-6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Hyper-parameters used for the extra pretraining of RoBERTa on our Wikipedia corpus.</figDesc><table><row><cell>V100 GPUs. The training takes approximately 30</cell></row><row><cell>days. The detailed hyper-parameters are shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Hyper-parameters and other details of our experiments.</figDesc><table><row><cell>Name</cell><cell>Value</cell></row><row><cell>Maximum word length</cell><cell>512</cell></row><row><cell>Learning rate decay</cell><cell>linear</cell></row><row><cell>Warmup ratio</cell><cell>0.06</cell></row><row><cell>Dropout</cell><cell>0.1</cell></row><row><cell>Weight decay</cell><cell>0.01</cell></row><row><cell>Gradient clipping</cell><cell>none</cell></row><row><cell>Adam Œ≤ 1</cell><cell>0.9</cell></row><row><cell>Adam Œ≤ 2</cell><cell>0.98</cell></row><row><cell>Adam</cell><cell>1e-6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12</head><label>12</label><figDesc>We do not tune the hyper-parameters of the SQuAD dataset, and use the ones described in. The hyper-parameters and other details, including the training time, number of GPUs used, and the best score on the development set, are shown inTable 11. For the other hyper-parameters, we simply follow (see</figDesc><table><row><cell>: Common hyper-parameters used in our ex-</cell></row><row><cell>periments.</cell></row><row><cell>‚Ä¢ batch size: 4, 8, 16, 32, 64</cell></row><row><cell>‚Ä¢ number of training epochs: 2, 3, 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>TACRED dataset contains 68,124 training examples, 22,631 development examples, and 15,509 test examples with labels of their relation types. The total number of relation types is 42. The</figDesc><table /><note>2 https://github.com/thunlp/ERNIE</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that LUKE uses two different [MASK] tokens: the [MASK] word for MLM and the [MASK] entity for our proposed pretraining task.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert (zhang</surname></persName>
		</author>
		<idno>67.2 64.8 66.0</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-Gcn (</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ernie (zhang</surname></persName>
		</author>
		<idno>2019) 70.0 66.1 68.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Spanbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshi</surname></persName>
		</author>
		<idno>2020) 70.8 70.9 70.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mtb (baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soares</surname></persName>
		</author>
		<idno>71.5</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Knowbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
		<idno>2019) 71.6 71.4 71.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kepler (wang</surname></persName>
		</author>
		<idno>2019b) 70.4 73.0 71.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K-Adapter (</forename><surname>Wang</surname></persName>
		</author>
		<idno>2020) 68.9 75.4 72.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Roberta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextual String Embeddings for Sequence Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>References Alan Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cloze-driven Pretraining of Self-attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5360" to="5369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matching the Blanks: Distributional Similarity for Relation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multirelational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1623" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ultra-Fine Entity Typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple and Effective Multi-Paragraph Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="845" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What Does BERT Look at? An Analysis of BERT&apos;s Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Joint Entity Disambiguation with Local Neural Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Octavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2619" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415v3</idno>
		<title level="m">Gaussian Error Linear Units (GELUs)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Span-BERT: Improving Pre-training by Representing and Predicting Spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural Architectures for Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450v1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer Normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pingan Smart Health and SJTU at COIN -Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiepeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhexi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</title>
		<meeting>the First Workshop on Commonsense Inference in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio Baldini</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F√©vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03765v1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">2020. Learning Cross-Context Entity Representations from Text. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692v1</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Commonsense Inference in Natural Language Processing (COIN) -Shared Task Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing</title>
		<meeting>the First Workshop on Commonsense Inference in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge Enhanced Contextual Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing and Measuring the Geometry of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8594" to="8603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bidirectional Attention Flow for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hananneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Exhaustive Model for Nested Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golam</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Sohrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2843" to="2849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th√©o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3266" to="3280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuihong</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01808v3</idno>
		<title level="m">Daxin Jiang, Ming Zhou, and others. 2020. K-Adapter: Infusing Knowledge into Pre-trained Models with Adapters</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">KEPLER: A Unified Model for Knowledge Embedding and Pretrained Language Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06136v1</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations of Texts and Entities from Knowledge Base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="397" to="411" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237v1</idno>
		<title level="m">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12885v1</idno>
		<title level="m">ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Positionaware Attention and Supervised Data Improve Slot Filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced Language Representation with Informative Entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
