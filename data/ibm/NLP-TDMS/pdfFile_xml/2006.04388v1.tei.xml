<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCALab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<email>wangwenhai362@163.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCALab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
							<email>xlhu@mail.tsinghua.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCALab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCALab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCALab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-stage detector basically formulates object detection as dense classification and localization (i.e., bounding box regression). The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates the classification to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classification and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference (i.e., separately trained but compositely used in test) and (2) the inflexible Dirac delta distribution for localization when there is ambiguity and uncertainty which is often the case in complex scenes. To address the problems, we design new representations for these elements. Specifically, we merge the quality estimation into the class prediction vector to form a joint representation of localization quality and classification, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0% AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5%) and ATSS (43.6%) with higher or comparable inference speed, under the same backbone and training settings. Notably, our best model can achieve a single-model single-scale AP of 48.2%, at 10 FPS on a single 2080Ti GPU. Code and pretrained models are available at https://github.com/implus/GFocal.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, dense detectors have gradually led the trend of object detection, whilst the attention on the representation of bounding boxes and their localization quality estimation leads to the encouraging advancement. Specifically, bounding box representation is modeled as a simple Dirac delta distribution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>, which is widely used over past years. As popularized in FCOS <ref type="bibr" target="#b25">[26]</ref>,  <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31]</ref> for the separate usage of the quality branch (i.e., IoU or centerness score) during training and test. (b): Our joint representation of classification and localization quality enables high consistency between training and inference.</p><p>predicting an additional localization quality (e.g., IoU score <ref type="bibr" target="#b28">[29]</ref> or centerness score <ref type="bibr" target="#b25">[26]</ref>) brings consistent improvements of detection accuracy, when the quality estimation is combined (usually multiplied) with classification confidence as final scores <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> for the rank process of Non-Maximum Suppression (NMS) during inference. Despite their success, we observe the following problems in existing practices:</p><p>Inconsistent usage of localization quality estimation and classification score between training and inference: <ref type="bibr" target="#b0">(1)</ref> In recent dense detectors, the localization quality estimation and classification score are usually trained independently but compositely utilized (e.g., multiplication) during inference <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>  <ref type="figure" target="#fig_0">(Fig. 1(a)</ref>); <ref type="bibr" target="#b1">(2)</ref> The supervision of the localization quality estimation is currently assigned for positive samples only <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>, which is unreliable as negatives may get chances to have uncontrollably higher quality predictions ( <ref type="figure" target="#fig_1">Fig. 2(a)</ref>). These two factors result in a gap between training and test, and would potentially degrade the detection performance, e.g., negative instances with randomly high-quality scores could rank in front of positive examples with lower quality prediction during NMS. We demonstrate some background patches (A and B) with extremely high predicted quality scores (e.g., IoU score &gt; 0.9), based on the optimized IoU-branch model in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. The scatter diagram in (b) denotes the randomly sampled instances with their predicted scores, where the blue points clearly illustrate the weak correlation between predicted classification scores and predicted IoU scores for separate representations. The part in red circle contains many possible negatives with large localization quality predictions, which may potentially rank in front of true positives and impair the performance. Instead, our joint representation (green points) forces them to be equal and thus avoids such risks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inflexible representation of bounding boxes:</head><p>The widely used bounding box representation can be viewed as Dirac delta distribution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31]</ref> of the target box coordinates. However, it fails to consider the ambiguity and uncertainty in datasets (see the unclear boundaries of the figures in <ref type="figure">Fig. 3</ref>). Although some recent works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref> model boxes as Gaussian distributions, it is too simple to capture the real distribution of the locations of bounding boxes. In fact, the real distribution can be more arbitrary and flexible <ref type="bibr" target="#b9">[10]</ref>, without the necessity of being symmetric like the Gaussian function.</p><p>To address the above problems, we design new representations for the bounding boxes and their localization quality. For localization quality representation, we propose to merge it with the classification score into a single and unified representation: a classification vector where its value at the ground-truth category index refers to its corresponding localization quality (typically the IoU score between the predicted box and the corresponding ground-truth box in this paper). In this way, we unify classification score and IoU score into a joint and single variable (denoted as "classification-IoU joint representation"), which can be trained in an end-to-end fashion, whilst directly utilized during inference ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). As a result, it eliminates the training-test inconsistency ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>) and enables the strongest correlation ( <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>) between localization quality and classification. Further, the negatives will be supervised with 0 quality scores, thereby the overall quality predictions become more confidential and reliable. It is especially beneficial for dense object detectors as they rank all candidates regularly sampled across an entire image. For bounding box representation, we propose to represent the arbitrary distribution (denoted as "General distribution" in this paper) of box locations by directly learning the discretized probability distribution over its continuous space, without introducing any other stronger priors (e.g., Gaussian <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref>). Consequently, we can obtain more reliable and accurate bounding box estimations, whilst being aware of a variety of their underlying distributions (see the predicted distributions in <ref type="figure">Fig. 3</ref> and Supplementary Materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ambiguous &amp; Flatten</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Certain &amp; Sharp Ambiguous &amp; Flatten</head><p>Certain &amp; Sharp <ref type="figure">Figure 3</ref>: Due to occlusion, shadow, blur, etc., the boundaries of many objects are not clear enough, so that the ground-truth labels (white boxes) are sometimes not credible and Dirac delta distribution is limited to indicate such issues. Instead, the proposed learned representation of General distribution for bounding boxes can reflect the underlying information by its shape, where a flatten distribution denotes the unclear and ambiguous boundaries (see red circles) and a sharp one stands for the clear cases. The predicted boxes by our model are marked green.</p><p>The improved representations then pose challenges for optimization. Traditionally for dense detectors, the classification branch is optimized with Focal Loss <ref type="bibr" target="#b17">[18]</ref> (FL). FL can successfully handles the class imbalance problem via reshaping the standard cross entropy loss. However, for the case of the proposed classification-IoU joint representation, in addition to the imbalance risk that still exists, we face a new problem with continuous IoU label (0∼1) as supervisions, as the original FL only supports discrete {1, 0} category label currently. We successfully solve the problem by extending FL from {1, 0} discrete version to its continuous variant, termed Generalized Focal Loss (GFL). Different from FL, GFL considers a much general case in which the globally optimized solution is able to target at any desired continuous value, rather than the discrete ones. More specifically in this paper, GFL can be specialized into Quality Focal Loss (QFL) and Distribution Focal Loss (DFL), for optimizing the improved two representations respectively: QFL focuses on a sparse set of hard examples and simultaneously produces their continuous 0∼1 quality estimations on the corresponding category; DFL makes the network to rapidly focus on learning the probabilities of values around the continuous locations of target bounding boxes, under an arbitrary and flexible distribution.</p><p>We demonstrate three advantages of GFL: (1) It bridges the gap between training and test when one-stage detectors are facilitated with additional quality estimation, leading to a simpler, joint and effective representation of both classification and localization quality; (2) It well models the flexible underlying distribution for bounding boxes, which provides more informative and accurate box locations; (3) The performance of one-stage detectors can be consistently boosted without introducing additional overhead. On COCO test-dev, GFL achieves 45.0% AP with ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5%) and ATSS (43.6%). Our best model can achieve a single-model single-scale AP of 48.2% whilst running at 10 FPS on a single 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Representation of localization quality. Existing practices like Fitness NMS <ref type="bibr" target="#b26">[27]</ref>, IoU-Net <ref type="bibr" target="#b11">[12]</ref>, MS R-CNN <ref type="bibr" target="#b10">[11]</ref>, FCOS <ref type="bibr" target="#b25">[26]</ref> and IoU-aware <ref type="bibr" target="#b28">[29]</ref> utilize a separate branch to perform localization quality estimation in a form of IoU or centerness score. As mentioned in Sec. 1, this separate formulation causes the inconsistency between training and test as well as unreliable quality predictions. Instead of introducing an additional branch, PISA <ref type="bibr" target="#b1">[2]</ref> and IoU-balance <ref type="bibr" target="#b27">[28]</ref> assign different weights in the classification loss based on their localization qualities, aiming at enhancing the correlation between the classification score and localization accuracy. However, the weight strategy is of implicit and limited benefits since it does not change the optimum of the loss objectives for classification.</p><p>Representation of bounding boxes. Dirac delta distribution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31]</ref> governs the representation of bounding boxes over past years. Recently, Gaussian assumption <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref> is adopted to learn the uncertainty by introducing a predicted variance. Unfortunately, existing representations are either too rigid or too simplified, which can not reflect the complex underlying distribution in real data. In this paper, we further relax the assumption and directly learn the more arbitrary, flexible General distribution of bounding boxes, whilst being more informative and accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first review the original Focal Loss <ref type="bibr" target="#b17">[18]</ref> (FL) for learning dense classification scores of one-stage detectors. Next, we present the details for the improved representations of localization quality estimation and bounding boxes, which are successfully optimized via the proposed Quality Focal Loss (QFL) and Distribution Focal Loss (DFL), respectively. Finally, we summarize the formulations of QFL and DFL into a unified perspective termed Generalized Focal Loss (GFL), as a flexible extension of FL, to facilitate further promotion and general understanding in the future.</p><p>Focal Loss (FL). The original FL <ref type="bibr" target="#b17">[18]</ref> is proposed to address the one-stage object detection scenario where an extreme imbalance between foreground and background classes often exists during training. A typical form of FL is as follows (we ignore α t in original paper <ref type="bibr" target="#b17">[18]</ref> for simplicity):</p><formula xml:id="formula_0">FL(p) = −(1 − p t ) γ log(p t ), p t = p, when y = 1 1 − p, when y = 0 (1)</formula><p>where y ∈ {1, 0} specifies the ground-truth class and p ∈ [0, 1] denotes the estimated probability for the class with label y = 1. γ is the tunable focusing parameter. Specifically, FL consists of a standard cross entropy part − log(p t ) and a dynamically scaling factor part (1 − p t ) γ , where the scaling factor (1 − p t ) γ automatically down-weights the contribution of easy examples during training and rapidly focuses the model on hard examples.</p><p>Quality Focal Loss (QFL). To solve the aforementioned inconsistency problem between training and test phases, we present a joint representation of localization quality (i.e., IoU score) and classification score ("classification-IoU" for short), where its supervision softens the standard one-hot category label and leads to a possible float target y ∈ [0, 1] on the corresponding category (see the classification branch in <ref type="figure" target="#fig_2">Fig. 4</ref>). Specifically, y = 0 denotes the negative samples with 0 quality score, and 0 &lt; y ≤ 1 stands for the positive samples with target IoU score y. Note that the localization quality label y follows the conventional definition as in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12]</ref>: IoU score between the predicted bounding box and its corresponding ground-truth bounding box during training, with a dynamic value being 0∼1. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref>, we adopt the multiple binary classification with sigmoid operators σ(·) for multi-class implementation. For simplicity, the output of sigmoid is marked as σ.</p><p>Since the proposed classification-IoU joint representation requires dense supervisions over an entire image and the class imbalance problem still occurs, the idea of FL must be inherited. However, the current form of FL only supports {1, 0} discrete labels, but our new labels contain decimals. Therefore, we propose to extend the two parts of FL for enabling the successful training under the case of joint representation:</p><formula xml:id="formula_1">(1) The cross entropy part − log(p t ) is expanded into its complete version − (1 − y) log(1 − σ) + y log(σ) ; (2)</formula><p>The scaling factor part (1 − p t ) γ is generalized into the absolute distance between the estimation σ and its continuous label y, i.e., |y − σ| β (β ≥ 0), here | · | guarantees the non-negativity. Subsequently, we combine the above two extended parts to formulate the complete loss objective, which is termed as Quality Focal Loss (QFL):</p><formula xml:id="formula_2">QFL(σ) = − y − σ β (1 − y) log(1 − σ) + y log(σ) .</formula><p>(2) Note that σ = y is the global minimum solution of QFL. QFL is visualized for several values of β in <ref type="figure" target="#fig_3">Fig. 5</ref>(a) under quality label y = 0.5. Similar to FL, the term y − σ β of QFL behaves as a modulating factor: when the quality estimation of an example is inaccurate and deviated away from label y, the modulating factor is relatively large, thus it pays more attention to learning this hard example. As the quality estimation becomes accurate, i.e., σ → y, the factor goes to 0 and the loss for well-estimated examples is down-weighted, in which the parameter β controls the down-weighting rate smoothly (β = 2 works best for QFL in our experiments).</p><p>Distribution Focal Loss (DFL). Following <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref>, we adopt the relative offsets from the location to the four sides of a bounding box as the regression targets (see the regression branch in <ref type="figure" target="#fig_2">Fig. 4</ref>). Conventional operations of bounding box regression model the regressed label y as Dirac delta different distributions, the same integral target  </p><formula xml:id="formula_3">(a) (b) (c) (1) (2) (3)</formula><formula xml:id="formula_4">distribution δ(x − y), where it satisfies +∞ −∞ δ(x − y) dx = 1</formula><p>and is usually implemented through fully connected layers. More formally, the integral form to recover y is as follows:</p><formula xml:id="formula_5">y = +∞ −∞ δ(x − y)x dx.<label>(3)</label></formula><p>According to the analysis in Sec. 1, instead of the Dirac delta <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref> or Gaussian <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> assumptions, we propose to directly learn the underlying General distribution P (x) without introducing any other priors. Given the range of label y with minimum y 0 and maximum y n (y 0 ≤ y ≤ y n , n ∈ N + ), we can have the estimated valueŷ from the model (ŷ also meets y 0 ≤ŷ ≤ y n ):</p><formula xml:id="formula_6">y = +∞ −∞ P (x)x dx = yn y0 P (x)x dx.<label>(4)</label></formula><p>To be consistent with convolutional neural networks, we convert the integral over the continuous domain into a discrete representation, via discretizing the range [y 0 , y n ] into a set {y 0 , y 1 , ..., y i , y i+1 , ..., y n−1 , y n } with even intervals ∆ (we use ∆ = 1 for simplicity). Consequently, given the discrete distribution property n i=0 P (y i ) = 1, the estimated regression valueŷ can be presented as:ŷ</p><formula xml:id="formula_7">= n i=0 P (y i )y i .<label>(5)</label></formula><p>As a result, P (x) can be easily implemented through a softmax S(·) layer consisting of n + 1 units, with P (y i ) being denoted as S i for simplicity. Note thatŷ can be trained in an end-to-end fashion with traditional loss objectives like SmoothL1 <ref type="bibr" target="#b6">[7]</ref>, IoU Loss <ref type="bibr" target="#b26">[27]</ref> or GIoU Loss <ref type="bibr" target="#b23">[24]</ref>. However, there are infinite combinations of values for P (x) that can make the final integral result being y, as shown in <ref type="figure" target="#fig_3">Fig. 5(b)</ref>, which may reduce the learning efficiency. Intuitively compared against <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref>, distribution <ref type="formula" target="#formula_5">(3)</ref> is compact and tends to be more confident and precise on the bounding box estimation, which motivates us to optimize the shape of P (x) via explicitly encouraging the high probabilities of values that are close to the target y. Furthermore, it is often the case that the most appropriate underlying location, if exists, would not be far away from the coarse label. Therefore, we introduce the Distribution Focal Loss (DFL) which forces the network to rapidly focus on the values near label y, by explicitly enlarging the probabilities of y i and y i+1 (nearest two to y, y i ≤ y ≤ y i+1 ). As the learning of bounding boxes are only for positive samples without the risk of class imbalance problem, we simply apply the complete cross entropy part in QFL for the definition of DFL:</p><formula xml:id="formula_8">DFL(S i , S i+1 ) = − (y i+1 − y) log(S i ) + (y − y i ) log(S i+1 ) .<label>(6)</label></formula><p>Intuitively, DFL aims to focus on enlarging the probabilities of the values around target y (i.e., y i and y i+1 ). The global minimum solution of DFL, i.e, S i = yi+1−y yi+1−yi , S i+1 = y−yi yi+1−yi (see <ref type="figure">Supplementary  Materials)</ref>, can guarantee the estimated regression targetŷ infinitely close to the corresponding label y, i.e.,ŷ = n j=0 P (y j )y j = S i y i + S i+1 y i+1 = yi+1−y yi+1−yi y i + y−yi yi+1−yi y i+1 = y, which also ensures its correctness as a loss function.   Generalized Focal Loss (GFL). Note that QFL and DFL can be unified into a general form, which is called the Generalized Focal Loss (GFL) in the paper. Assume that a model estimates probabilities for two variables y l , y r (y l &lt; y r ) as p y l , p yr (p y l ≥ 0, p yr ≥ 0, p y l + p yr = 1), with a final prediction of their linear combination beingŷ = y l p y l + y r p yr (y l ≤ŷ ≤ y r ). The corresponding continuous label y for the predictionŷ also satisfies y l ≤ y ≤ y r . Taking the absolute distance |y −ŷ| β (β ≥ 0) as modulating factor, the specific formulation of GFL can be written as: GFL(p y l , p yr ) = − y − (y l p y l + y r p yr ) β (y r − y) log(p y l ) + (y − y l ) log(p yr ) . (7) Properties of GFL. GFL(p y l , p yr ) reaches its global minimum with p * y l = yr−y yr−y l , p * yr = y−y l yr−y l , which also means that the estimationŷ perfectly matches the continuous label y, i.e.,ŷ = y l p * y l + y r p * yr = y (see the proof in <ref type="figure">Supplementary Materials)</ref>. Obviously, the original FL <ref type="bibr" target="#b17">[18]</ref> and the proposed QFL and DFL are all special cases of GFL (see Supplementary Materials for details). Note that GFL can be applied to any one-stage detectors. The modified detectors differ from the original detectors in two aspects. First, during inference, we directly feed the classification score (joint representation with quality estimation) as NMS scores without the need of multiplying any individual quality prediction if there exists (e.g., centerness as in FCOS <ref type="bibr" target="#b25">[26]</ref> and ATSS <ref type="bibr" target="#b30">[31]</ref>). Second, the last layer of the regression branch for predicting each location of bounding boxes now has n + 1 outputs instead of 1 output, which brings negligible extra computing cost as later shown in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>Training Dense Detectors with GFL. We define training loss L with GFL:</p><formula xml:id="formula_9">L = 1 N pos z L Q + 1 N pos z 1 {c * z &gt;0} λ 0 L B + λ 1 L D ,<label>(8)</label></formula><p>where L Q is QFL and L D is DFL. Typically, L B denotes the GIoU Loss as in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref>. N pos stands for the number of positive samples. λ 0 (typically 2 as default, similarly in <ref type="bibr" target="#b2">[3]</ref>) and λ 1 (practically 1 4 , averaged over four directions) are the balance weights for L Q and L D , respectively. The summation is calculated over all locations z on the pyramid feature maps <ref type="bibr" target="#b16">[17]</ref>. 1 {c * z &gt;0} is the indicator function, being 1 if c * z &gt; 0 and 0 otherwise. Following the common practices in the official codes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15]</ref>, we also utilize the quality scores to weight L B and L D during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior Distribution</head><p>FCOS <ref type="bibr" target="#b25">[26]</ref> ATSS <ref type="bibr" target="#b30">[31]</ref> AP AP 50 AP 75 AP S AP M AP L AP AP 50 AP 75 AP S AP M AP L Dirac delta <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref> 38.5 56.8 41. <ref type="bibr" target="#b5">6</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>Our experiments are conducted on COCO benchmark <ref type="bibr" target="#b18">[19]</ref>, where trainval35k (115K images) is utilized for training and we use minival (5K images) as validation for our ablation study. The main results are reported on test-dev (20K images) which can be obtained from the evaluation server. For fair comparisons, all results are produced under mmdetection <ref type="bibr" target="#b2">[3]</ref>, where the default hyper-parameters are adopted. Unless otherwise stated, we adopt 1x learning schedule (12 epochs) without multi-scale training for the following studies, based on ResNet-50 <ref type="bibr" target="#b8">[9]</ref> backbone. More training/test details can be found in Supplementary Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B B</head><p>A cls: 0.095 IoU: 0.927 cls: 0.101 IoU: 0.913 (a) predicted classification score (cls) and predicted IoU score (IoU) (b) (c) <ref type="figure">Figure 8</ref>: Single-model single-scale speed (ms) vs. accuracy (AP) on COCO test-dev among state-of-the-art approaches. GFL achieves better speed-accuracy trade-off than many competitive counterparts.</p><p>We first investigate the effectiveness of the QFL (Table 1). In <ref type="table" target="#tab_1">Table 1</ref>(a), we compare the proposed joint representation with its separate or implicit counterparts. Two alternatives for representing localization quality: IoU <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12]</ref> and centerness <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref> are also adopted in the experiments. In general, we construct 4 variants that use separate or implicit representation, as illustrated in <ref type="figure" target="#fig_4">Fig. 6</ref>. According to the results, we observe that the joint representations optimized by QFL consistently achieve better performance than all the counterparts, whilst IoU always performs better than centerness as a measurement of localization quality (Supplementary Materials). Table 1(b) shows that QFL can also boost the performance of other popular one-stage detectors, and Table 1(c) shows that β = 2 is the best setting for QFL. We illustrate the effectiveness of joint representation by sampling instances with its predicted classification and IoU scores of both IoU-branch model and ours, as shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. It demonstrates that the proposed joint representation trained with QFL can benefit the detection due to its more reliable quality estimation, and yields the strongest correlation between classification and quality scores according to its definition. In fact, in our joint representation, the predicted classification score is equal to the estimated quality score exactly. <ref type="bibr">QFL</ref>   Second, we investigate the effectiveness of the DFL (Table 2). To quickly select a reasonable value of n, we first illustrate the distribution of the regression targets in <ref type="figure" target="#fig_3">Fig. 5(c)</ref>. We will show in later experiments, the recommended choice of n for ATSS is 14 or 16. In <ref type="table" target="#tab_3">Table 2</ref>(a), we compare the effectiveness of different data representations for bounding box regression. We find that the General distribution achieves superior or at least comparable results, whilst DFL can further boost its performance. Qualitative comparisons are depicted in <ref type="figure">Fig. 7</ref>. It is observed that the proposed General distribution can provide more accurate bounding box locations than Gaussian and   <ref type="bibr" target="#b32">[33]</ref>, while others are measured on the same machine with a single GeForce RTX 2080Ti GPU under the same mmdetection <ref type="bibr" target="#b2">[3]</ref> framework, using a batch size of 1 whenever possible. "n/a" means that both trained models and timing results from original papers are not available. R: ResNet. X: ResNeXt. HG: Hourglass. DCN: Deformable Convolutional Network.</p><p>Dirac delta distribution, especially under the case with considerable occlusions (More discussions in Supplementary Materials). Based on the improved ATSS trained by GFL, we report the effect of n and ∆ in DFL by fixing one and varying another in <ref type="table" target="#tab_3">Table 2</ref>(b) and (c). The results demonstrate that the selection of n is not sensitive and ∆ is suggested to be small (e.g., 1) in practice. To illustrate the effect of General distribution, we plot several representative instances with its distributed bounding box over four directions in <ref type="figure">Fig. 3</ref>, where the proposed distributed representation can effectively reflect the uncertainty of bounding boxes by its shape (see more examples in <ref type="figure">Supplementary Materials)</ref>.</p><p>Third, we perform the ablation study on ATSS with ResNet-50 backbone to show the relative contributions of QFL and DFL <ref type="table" target="#tab_5">(Table 3)</ref>. FPS (Frames-per-Second) is measured on the same machine with a single GeForce RTX 2080Ti GPU using a batch size of 1 under the same mmdetection <ref type="bibr" target="#b2">[3]</ref> framework. We observe that the improvement of DFL is orthogonal to QFL, and joint usage of both (i.e., GFL) improves the strong ATSS baseline by absolute 1% AP score. Furthermore, according to the inference speeds, GFL brings negligible additional overhead and is considered very practical.</p><p>Finally, we compare GFL (based on ATSS) with state-of-the-art approaches on COCO test-dev in <ref type="table" target="#tab_7">Table 4</ref>. Following previous works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref>, the multi-scale training strategy and 2x learning schedule (24 epochs) are adopted during training. For a fair comparison, we report the results of single-model single-scale testing for all methods, as well as their corresponding inference speeds (FPS). GFL with ResNet-101 <ref type="bibr" target="#b8">[9]</ref> achieves 45.0% AP at 14.6 FPS, which is superior than all the existing detectors with the same backbone, including SAPD <ref type="bibr" target="#b32">[33]</ref> (43.5%) and ATSS <ref type="bibr" target="#b30">[31]</ref> (43.6%). Further, Deformable</p><p>Convolutional Networks (DCN) <ref type="bibr" target="#b35">[36]</ref> consistently boost the performances over ResNe(X)t backbones, where GFL with ResNeXt-101-32x4d-DCN obtains state-of-the-art 48.2% AP at 10 FPS. <ref type="figure">Fig. 8</ref> demonstrates the visualization of the accuracy-speed trade-off, where it can be observed that our proposed GFL pushes the envelope of accuracy-speed boundary to a high level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>To effectively learn qualified and distributed bounding boxes for dense object detectors, we propose Generalized Focal Loss (GFL) that generalizes the original Focal Loss from {1, 0} discrete formulation to the continuous version. GFL can be specialized into Quality Focal loss (QFL) and Distribution Focal Loss (DFL), where QFL encourages to learn a better joint representation of classification and localization quality, and DFL provides more informative and precise bounding box estimations by modeling their locations as General distributions. Extensive experiments validate the effectiveness of GFL. We hope GFL can serve as a simple yet effective baseline for the community.</p><p>A More Discussions about the Distributions <ref type="figure" target="#fig_6">Fig. 9</ref> depicts the ideas of Dirac delta, Gaussian, and the proposed General distributions, where the assumption goes from rigid (Dirac delta) to flexible (General). We also list several key comparisons about these distributions in <ref type="table">Table 5</ref>. It can be observed that the loss objective of the Gaussian assumption is actually a dynamically weighted L2 Loss, where its training weight is related to the predicted variance σ. It is somehow similar to that of Dirac delta (standard L2 Loss) when optimized at the edge level. Moreover, it is not clear how to integrate the Gaussian assumption into the IoU-based Loss formulations, since it heavily couples the expression of the target representation with its optimization objective. Therefore, it can not enjoy the benefits of the IoU-based optimization <ref type="bibr" target="#b23">[24]</ref>, as it is proved to be very effective in practice. In contrast, our proposed General distribution decouples the representation and loss objective, making it feasible for any type of optimizations, including both edge level and box level. , which both have more limitations in modeling real data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Dirac delta <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref> Gaussian <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> General (ours) Probability Density IoU-based Loss Optimization Level edge box edge edge box <ref type="table">Table 5</ref>: Comparisons between three distributions. "edge" level denotes optimization over four respective directions, whilst "box" level means IoU-based Losses <ref type="bibr" target="#b23">[24]</ref> that consider the bounding box as a whole.   <ref type="figure" target="#fig_0">Figure 10</ref>: We demonstrate an example in 2D space by fixing the input feature vector and introduce a small disturbance (norm of 0.1) over it. The regression targets are 1.5, 2.5, 3.5 respectively. It is observed that Dirac delta distribution leads to more regression errors after the same disturbance, and the error increases with the growth of regression target. In contrast, our proposed General distribution remains stable and insensitive to the disturbance.</p><formula xml:id="formula_10">δ(x − y) N (x, σ 2 ) P (x) Inference Target x x P (x)x dx</formula><p>We also find that the bounding box regression of Dirac delta distribution (including Gaussian distribution based on the analysis from <ref type="table">Table 5</ref>) behaves more sensitive to feature perturbations, making it less robust and susceptible to noise, as shown in the simulation experiment ( <ref type="figure" target="#fig_0">Fig. 10</ref>). It proves that General distribution enjoys more benefits than the other counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Global Minimum of GFL(p y l , p yr )</head><p>Let's review the definition of GFL:</p><p>GFL(py l , py r ) = − y − (y l py l + yrpy r ) β (yr − y) log(py l ) + (y − y l ) log(py r ) , given py l + py r = 1.</p><p>For simplicity, GFL(py l , py r ) can then be expanded as: Furthermore, given = 0, for arbitrary variable (py l , py r ) = (p * y l + , p * yr − ) in the domain of definition, we can have:</p><formula xml:id="formula_11">GFL</formula><formula xml:id="formula_12">R(p * y l + , p * yr − ) &gt; R(py l , py r ) &gt; 0, L(p * y l + , p * yr − ) = (yr − y l ) β &gt; 0 = L(p * y l , p * yr )</formula><p>. Therefore, it is easy to deduce: GFL(py l , py r ) = L(py l , py r )R(py l , py r ) ≥ L(p * y l , p * yr )R(p * y l , p * yr ) = 0, where "=" holds only when py l = p * y l , py r = p * yr . The global minimum property of GFL somehow explains why the IoU or centerness guided variants in <ref type="figure" target="#fig_4">Fig. 6</ref> would not have obvious advantages. In fact, the weighted guidance does not essentially change the global minimum of the original classification loss (e.g., Focal Loss), whilst their optimal classification targets are still one-hot labels. In contrast, the proposed GFL indeed modifies the global minimum and force the predictions to approach the accurate IoU between the estimated boxes and ground-truth boxes, which is obviously beneficial for the rank process of NMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C FL, QFL and DFL are special cases of GFL</head><p>In this section, we show how GFL can be specialized into the form of FL, QFL and DFL, respectively. FL: Letting β = γ, y l = 0, yr = 1, py r = p, py l = 1 − p and y ∈ {1, 0} in GFL, we can obtain FL:</p><formula xml:id="formula_13">FL(p) = GFL(1 − p, p) = − y − p γ (1 − y) log(1 − p) + y log(p) , y ∈ {1, 0}</formula><p>= −(1 − pt) γ log(pt), pt = p, when y = 1 1 − p, when y = 0 (9) QFL: Having y l = 0, yr = 1, py r = σ and py l = 1 − σ in GFL, the form of QFL can be written as:</p><formula xml:id="formula_14">QFL(σ) = GFL(1 − σ, σ) = − y − σ β (1 − y) log(1 − σ) + y log(σ) .<label>(10)</label></formula><p>DFL: By substituting β = 0, y l = yi, yr = yi+1, py l = P (y l ) = P (yi) = Si, py r = P (yr) = P (yi+1) = Si+1 in GFL, we can have DFL:</p><formula xml:id="formula_15">DFL(Si, Si+1) = GFL(Si, Si+1) = − (yi+1 − y) log(Si) + (y − yi) log(Si+1) .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of Experimental Settings</head><p>Training Details: The ImageNet pretrained models <ref type="bibr" target="#b8">[9]</ref> with FPN <ref type="bibr" target="#b16">[17]</ref> are utilized as the backbones. During training, the input images are resized to keep their shorter side being 800 and their longer side less or equal to 1333. In ablation study, the networks are trained using the Stochastic Gradient Descent (SGD) algorithm for 90K iterations (denoted as 1x schedule) with 0.9 momentum, 0.0001 weight decay and 16 batch size. The initial learning rate is set as 0.01 and decayed by 0.1 at iteration 60K and 80K, respectively.</p><p>Inference Details: During inference, the input image is resized in the same way as in the training phase, and then passed through the whole network to output the predicted bounding boxes with a predicted class. Then we use the threshold 0.05 to filter out a variety of backgrounds, and output top 1000 candidate detections per feature pyramid. Finally, NMS is applied under the IoU threshold 0.6 per class to produce the final top 100 detections per image as results.</p><p>E Why is IoU-branch always superior than centerness-branch?</p><p>The ablation study in original paper also demonstrates that for FCOS/ATSS, IoU performs consistently better than centerness, as a measurement of localization quality. Here we give a convincing reason why this is the case. We discover the major problem of centerness is that its definition leads to unexpected small ground-truth label, which makes a possible set of ground-truth bounding boxes extremely hard to be recalled (as shown in <ref type="figure" target="#fig_0">Fig. 11</ref>). From the label distributions demonstrated in <ref type="figure" target="#fig_0">Fig. 12</ref>, we observe that most of IoU labels is larger than 0.4 yet centerness labels tend to be much smaller (even approaching 0). The small values of centerness labels prevent a set of ground-truth bounding boxes from being recalled, as their final scores for NMS would be potentially small since their predicted centerness scores are already supervised by these extremely small signals.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Examples of Distributed Bounding Boxes</head><p>We demonstrate more examples with General distributed bounding boxes predicted by GFL (ResNet-50 backbone). As demonstrated in <ref type="figure" target="#fig_0">Fig. 13</ref>, we show several cases with boundary ambiguities: does the slim and almost invisible backpack strap belong to the box of the bag (left top)? does the partially occluded umbrella handle belong to the entire umbrella (left down)? In these cases, our models even produce more reasonable coordinates of bounding boxes than the ground-truth ones. In <ref type="figure" target="#fig_0">Fig. 14,</ref> more examples with clear boundaries and sharp General distributions are shown, where GFL is very confident to generate accurate bounding boxes, e.g., the bottom parts of the orange and skiing woman. <ref type="figure" target="#fig_0">Figure 13</ref>: Examples with huge boundary ambiguities and uncertainties, where the learned General distributions tend to be flatten. In some cases, we even observe a distribution with two peaks. Interestingly, they do correspond to two different most likely boundaries in the image, e.g., the boundaries of the umbrella whether its heavily occluded handle is considered. Predictions are marked green in images, whilst ground-truth boxes are white. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparisons between existing separate representation and proposed joint representation of classification and localization quality estimation. (a): Current practices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>predicted classification score (cls) and predicted IoU score (IoU) Unreliable IoU predictions of current dense detector with IoU-branch. (a):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The comparisons between conventional methods and our proposed GFL in the head of dense detectors. GFL includes QFL and DFL. QFL effectively learns a joint representation of classification score and localization quality estimation. DFL models the locations of bounding boxes as General distributions whilst forcing the networks to rapidly focus on learning the probabilities of values close to the target coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>(a): The illustration of QFL under quality label y = 0.5. (b): Different flexible distributions can obtain the same integral target according to Eq. (4), thus we need to focus on learning probabilities of values around the target for more reasonable and confident predictions (e.g., (3)). (c): The histogram of bounding box regression targets of ATSS over all training samples on COCO trainval35k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Illustrations of modified versions for separate/implicit and joint representation. The baseline without quality branch is also provided.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( a )Figure 7 :</head><label>a7</label><figDesc>Dirac delta distribution (b) Gaussian distribution (c) General distribution (ours) Qualitative comparisons between Dirac delta (a), Gaussian (b) and our proposed General (c) distribution for bounding box regression on COCO minival, based on ATSS<ref type="bibr" target="#b30">[31]</ref>. White boxes denote the ground-truth labels, and the predicted ones are marked green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Illustrations of three distributions, from rigid (Dirac delta) to flexible (General). The proposed General distribution is more flexible as its shape can be arbitrary. In contrast, Dirac delta distribution roots at a fixed point and Gaussian distribution follows a relatively rigid, symmetric expression, e.g.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Loss 2 2</head><label>2</label><figDesc>Objective (for box part) (x−y) IoU-based Loss (x−y) 2 2σ 2 + 1 2 log(σ 2 ) ( P (x)x dx−y) 2 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(py l , py r ) = − y − (y l py l + yrpy r ) β (yr − y) log(py l ) + (y − y l ) log(py r ) = y − (y l py l + yrpy r ) β L(·,·) − (yr − y) log(py l ) + (y − y l ) log(py r ) R(·,·) = L(py l , py r )R(py l , py r ),R(py l , py r ) = − (yr − y) log(py l ) + (y − y l ) log(py r ) = − (yr − y) log(py l ) + (y − y l ) log(1 − py l ) ≥ − (yr − y) log( yr − y yr − y l ) + (y − y l ) log( y − y l yr − y l ) = R(p * y l , p * yr ) &gt; 0,where p * y l = yr − y yr − y l , p * yr = y − y l yr − y l . L(py l , py r ) = y − (y l py l + yrpy r ) β ≥ L(p * y l , p * yr ) = 0, where p * y l = yr − y yr − y l , p * yr = y − y l yr − y l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>/anchor_heads/atsssmcsc_head.py</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 : 14 Figure 12 :</head><label>111412</label><figDesc>We demonstrate possible cases of ground-truth/predicted bounding box along with the positive points. The matrix points denote the feature pyramid layer with stride = 8. Centerness label is easier to get very small values by its definition, whilst IoU label is more reliable as the supervisions from bounding boxes will always push it close to 1.0. mean = 0.64, std = 0.18IoU label, mean = 0.84, std = 0.Label distributions over all positive training samples on COCO, based on pretrained GFL detector (ResNet-50 backbone).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Examples with extremely clear boundaries. The learned General distributions are relatively sharp whilst producing very accurate box estimations. Predictions are marked green in images, whilst ground-truth boxes are white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>AP AP 50 AP 75 AP S AP M AP L AP AP 50 AP 75 AP S AP M AP L w/o quality branch 37.8 56.2 40.8 21.2 42.1 48.2 38.0 56.5 40.7 20.6 42.1 49.1 centerness-branch [26] 38.5 56.8 41.6 22.4 42.4 49.1 39.2 57.4 42.2 23.0 42.8 51.1 IoU-branch [29, 12] 38.7 56.7 42.0 21.6 43.0 50.3 39.6 57.6 43.0 23.3 43.7 51.Comparisons between separate/implicit and joint representation (ours):The joint representation optimized by QFL achieves better performance than other counterparts. We also observe that the quality predictions (especially IoU scores) are necessary for obtaining competitive AP.MethodAP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell>Type</cell><cell></cell><cell></cell><cell cols="2">FCOS [26]</cell><cell></cell><cell>ATSS [31]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell cols="3">centerness-guided [28] 37.9 56.7</cell><cell cols="3">40.7 21.2 42.1 49.4 38.2 56.2</cell><cell>41.0 21.5 41.9 49.7</cell></row><row><cell>IoU-guided [28]</cell><cell cols="2">38.2 57.0</cell><cell cols="3">41.1 22.5 42.2 48.9 38.9 57.4</cell><cell>41.8 22.8 42.4 50.6</cell></row><row><cell>joint w/ QFL (ours)</cell><cell cols="2">39.0 57.8</cell><cell cols="3">41.9 22.0 43.1 51.0 39.9 58.5</cell><cell>43.0 22.4 43.9 52.7</cell></row><row><cell cols="4">(a) FoveaBox [13] FoveaBox [13] + joint w/ QFL 37.0 55.7 36.4 55.8 RetinaNet [18] 35.6 55.5 RetinaNet [18] + joint w/ QFL 36.4 56.3 SSD512 [20] 29.4 49.1</cell><cell>38.8 19.4 40.4 47.7 39.6 20.2 41.2 48.8 38.1 20.1 39.4 46.8 39.1 20.4 40.0 48.7 30.6 11.4 34.1 44.9</cell><cell>0 1 2 2.5</cell><cell>37.6 55.4 39.0 58.1 39.9 58.5 39.7 58.1</cell><cell>40.3 41.7 43.0 42.7</cell></row><row><cell cols="2">SSD512 [20] + joint w/ QFL</cell><cell cols="2">30.2 50.3</cell><cell>31.7 13.3 34.4 45.5</cell><cell>4</cell><cell>38.2 55.4</cell><cell>41.6</cell></row></table><note>(b) Applying joint representations with QFL to other one-stage detectors: About 0.6- 0.8 % AP gains are obtained without any additional overhead for inference.β (QFL) AP AP 50 AP 75(c) Varying β for QFL based on ATSS: β = 2 performs best.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Study on QFL (ResNet-50 backbone). All experiments are reproduced in mmdetection [3] and validated on COCO minival.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>22.4 42.4 49.1 39.2 57.4 42.2 23.0 42.8 51.1 Gaussian [10, 4] 38.6 56.5 41.6 21.7 42.5 50.0 39.3 57.0 42.4 23.6 42.9 51.0 General (ours) 38.8 56.6 42.0 22.5 42.9 49.8 39.3 57.1 42.5 23.5 43.0 51.2 General w/ DFL (ours) 39.0 57.0 42.3 22.6 43.0 50.6 39.5 57.3 42.8 23.6 43.2 51.2 (a) Performances under different data representation of bounding box regression targets: the proposed General distribution supervised by DFL improves favorably over the competitive baselines.n ∆ AP AP 50 AP 75 AP S AP M AP L AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>n</cell><cell>∆</cell><cell></cell><cell></cell></row><row><cell>12</cell><cell></cell><cell>40.1 58.4</cell><cell>43.1 23.1 43.8 52.5</cell><cell></cell><cell cols="2">0.5 40.2 58.4</cell><cell>43.0 22.3 43.8 53.1</cell></row><row><cell>14 16</cell><cell>1</cell><cell>40.2 58.3 40.2 58.6</cell><cell>43.6 23.3 44.2 52.2 43.4 23.0 44.3 53.0</cell><cell>16</cell><cell>1 2</cell><cell>40.2 58.6 39.9 58.3</cell><cell>43.4 23.0 44.3 53.0 42.9 22.5 43.8 51.8</cell></row><row><cell>18</cell><cell></cell><cell>40.1 58.1</cell><cell>43.1 22.6 43.9 52.6</cell><cell></cell><cell>4</cell><cell>39.8 58.5</cell><cell>42.8 22.8 43.4 52.3</cell></row><row><cell cols="4">(b) Varying n by fixing ∆ = 1 on ATSS (w/ GFL): The perfor-</cell><cell cols="4">(c) Varying ∆ by fixing n = 16 on ATSS (w/ GFL): Small ∆</cell></row><row><cell cols="4">mance is robust to a range of n according to its target distribution in</cell><cell cols="4">usually leads to better performance whilst ∆ = 1 is good enough</cell></row><row><cell cols="2">Fig. 5(c).</cell><cell></cell><cell></cell><cell cols="2">for practice.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Study on DFL (ResNet-50 backbone). All experiments are reproduced in mmdetection [3] and validated on COCO minival.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>DFL FPS AP AP 50 AP 75</figDesc><table><row><cell>19.4 39.2 57.4</cell><cell>42.2</cell></row><row><cell>19.4 39.9 58.5</cell><cell>43.0</cell></row><row><cell>19.4 39.5 57.3</cell><cell>42.8</cell></row><row><cell>19.4 40.2 58.6</cell><cell>43.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>The effect of QFL and DFL on ATSS: The effects of QFL and DFL are orthogonal, whilst utilizing both can boost 1% AP over the strong ATSS baseline, without introducing additional overhead practically.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparisons between state-of-the-art detectors (single-model and single-scale results) on COCO test-dev. "MStrain" denotes multi- scale training. FPS values with* are from</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Prime sample attention in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04821</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gaussian yolov3: An accurate and fast object detector using localization uncertainty for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayoung</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuk-Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Centripetalnet: Pursuing high-quality keypoint pairs for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Foveabox: Beyond anchor-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03797</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning from noisy anchors for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05086</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grid r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPs</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving object localization with fitness nms and bounded iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Iou-balanced loss functions for single-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengkai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05641</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Iou-aware single-stage object detector for accurate localization. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengkai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchorbased and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<editor>NeurIPs</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Soft anchor-point object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Iou-uniform r-cnn: Breaking through the limitations of rpn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liman</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05190</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
