<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DocBERT: BERT for Document Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Adhikari</surname></persName>
							<email>adadhika@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achyudh</forename><surname>Ram</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
							<email>r33tang@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DocBERT: BERT for Document Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT large to small bidirectional LSTMs, reaching BERT base parity on multiple datasets using 30× fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Until recently, the dominant paradigm in approaching natural language processing (NLP) tasks has been to concentrate on neural architecture design, using only task-specific data and word embeddings such as GloVe <ref type="bibr" target="#b13">(Pennington et al., 2014)</ref>. The NLP community is, however, witnessing a dramatic paradigm shift toward the pre-trained deep language representation model, which achieves the state of the art in question answering, sentiment classification, and similarity modeling, to name a few. Bidirectional Encoder Representations from Transformers (BERT; <ref type="bibr" target="#b4">Devlin et al., 2019)</ref> represents one of the latest developments in this line of work. It outperforms its predecessors, ELMo <ref type="bibr" target="#b14">(Peters et al., 2018)</ref> and GPT <ref type="bibr" target="#b15">(Radford et al., 2018)</ref>, by a wide margin on multiple NLP tasks.</p><p>This approach consists of two stages: first, BERT is pre-trained on vast amounts of text, with an unsupervised objective of masked language modeling and next-sentence prediction. Next, this pre-trained network is then fine-tuned on taskspecific, labeled data.</p><p>BERT, however, has not yet been fine-tuned for document classification. Why is this worth exploring? For one, modeling syntactic structure has been arguably less important for document classification than for typical BERT tasks such as natural language inference and paraphrasing. This claim is supported by our observation that logistic regression and support vector machines are exceptionally strong document classification baselines. For another, documents often have multiple labels across many classes, which is again uncharacteristic of the tasks that BERT examines.</p><p>In this paper, we first describe fine-tuning BERT for document classification to establish state-ofthe-art results on four popular datasets. This increase in model quality, however, comes at a heavy computational expense. BERT contains hundreds of millions of parameters, while the previous baseline uses less than four million and performs inference forty times faster.</p><p>To alleviate this computational burden, we apply knowledge distillation <ref type="bibr" target="#b5">(Hinton et al., 2015)</ref> to transfer knowledge from BERT large , the large BERT variant, to the previous, much smaller stateof-the-art BiLSTM. As a result of this procedure, with a few additional tricks for effective knowledge transfer, we achieve results comparable to BERT base , the smaller BERT variant, using a model with 30× fewer parameters.</p><p>Our contributions in this paper are two fold: First, we establish state-of-the-art results for document classification by simply fine-tuning BERT; Second, we demonstrate that BERT can be distilled into a much simpler neural model that provides competitive accuracy at a far more modest computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Over the last few years, neural network-based architectures have dominated the task of document classification. <ref type="bibr" target="#b9">Liu et al. (2017a)</ref> develop XML-CNN for addressing this problem's multi-label nature, which they call extreme classification. XML-CNN is based on the popular KimCNN <ref type="bibr" target="#b7">(Kim, 2014)</ref>, except with wider convolutional filters, adaptive dynamic max-pooling <ref type="bibr" target="#b3">(Chen et al., 2015;</ref><ref type="bibr" target="#b6">Johnson and Zhang, 2015)</ref>, and an additional bottleneck layer to better capture the features of large documents. Another popular model, Hierarchical Attention Network (HAN; <ref type="bibr" target="#b20">Yang et al., 2016)</ref> explicitly models hierarchical information from documents to extract meaningful features, incorporating word-and sentence-level encoders (with attention) to classify documents. <ref type="bibr" target="#b18">Yang et al. (2018)</ref> propose a generative approach for multi-label document classification, using encoder-decoder sequence generation models (SGMs) for generating labels for each document. Contrary to the previous papers, <ref type="bibr" target="#b0">Adhikari et al. (2019)</ref> propose LSTM reg , a simple, properly-regularized singlelayer BiLSTM, which represents the current state of the art.</p><p>In the current paradigm of pre-trained models, methods like BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and XL-Net <ref type="bibr" target="#b19">(Yang et al., 2019)</ref> have been shown to achieve the state of the art in a variety of tasks including question answering, named entity recognition, and natural language inference. However, these models have a prohibitively large number of parameters and require substantial computational resources, even to carry out a single inference pass. Similar concerns related to high inference latency or heavy run-time memory requirements have led to a myriad of works, such as error-based weightpruning <ref type="bibr" target="#b8">(LeCun et al., 1990)</ref>, and more recently, model sparsification and channel pruning <ref type="bibr" target="#b11">(Louizos et al., 2018;</ref><ref type="bibr" target="#b10">Liu et al., 2017b)</ref>.</p><p>Knowledge distillation (KD; <ref type="bibr" target="#b2">Ba and Caruana, 2014;</ref><ref type="bibr" target="#b5">Hinton et al., 2015)</ref> has been shown to be an effective compression technique which "distills" information learned by a larger model (the teacher) into a smaller model (the student). KD uses the class probabilities produced by a pretrained teacher, the soft targets, to train a student model over a transfer set (the examples over which distillation takes place). Being model agnostic, the approach is suitable for our study, as it enables the transfer of knowledge between different types of architectures, unlike most of the other model compression techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>To adapt BERT base and BERT large models for document classification, we follow <ref type="bibr" target="#b4">Devlin et al. (2019)</ref> and introduce a fully-connected layer over the final hidden state corresponding to the [CLS] input token. During fine-tuning, we optimize the entire model end-to-end, with the additional softmax classifier parameters W ∈ IR K×H , where H is the dimension of the hidden state vectors and K is the number of classes. We minimize the crossentropy and binary cross-entropy loss for singlelabel and multi-label tasks, respectively.</p><p>Next, we distill knowledge from the fine-tuned BERT large into the much smaller LSTM reg , which represents the previous state of the art <ref type="bibr" target="#b0">(Adhikari et al., 2019)</ref>. We perform KD by using the training examples, along with minor augmentations to form the transfer set. In accordance with <ref type="bibr" target="#b5">Hinton et al. (2015)</ref>, we combine the two objectives of classification using the target labels (L classif ication ) and distillation (L distill ) using the soft targets, for each example of the transfer set.</p><p>We use the target labels to minimize the standard cross-entropy or binary cross-entropy loss depending on the type of dataset (multi-label or single-label). For the distillation objective, we minimize the Kullback-Leibler (KL) divergence KL(p||q) where p and q are the class probabilities produced by the student and the teacher models, respectively. We define the final objective as:</p><formula xml:id="formula_0">L = L classif ication + λ · L distill<label>(1)</label></formula><p>where λ ∈ R weighs the losses' contributions to the final objective. We use Nvidia Tesla V100 and P100 GPUs for fine-tuning BERT and run the rest of the experiments on RTX 2080 Ti and GTX 1080 GPUs. We use PyTorch 0.4.1 as the backend framework, and Scikit-learn 0.19.2 for computing the tf-idf vectors and implementing LR and SVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use the following four datasets to evaluate BERT: Reuters-21578 (Reuters; <ref type="bibr" target="#b1">Apté et al., 1994)</ref>, arXiv Academic Paper dataset (AAPD; <ref type="bibr" target="#b18">Yang et al., 2018)</ref>, IMDB reviews, and Yelp 2014 reviews. Reuters and AAPD are multi-label datasets while documents in IMDB and Yelp '14 contain only a single label.</p><p>For Reuters, we use the standard ModApté splits <ref type="bibr" target="#b1">(Apté et al., 1994)</ref>; for AAPD, we use the splits provided by <ref type="bibr" target="#b18">Yang et al. (2018)</ref>; for IMDB and Yelp, following <ref type="bibr" target="#b20">Yang et al. (2016)</ref>, we randomly sample 80% of the data for training and 10% each for validation and test.</p><p>We summarize the statistics of the datasets used in our study in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training and Hyperparameters</head><p>While fine-tuning BERT, we optimize the number of epochs, batch size, learning rate, and maximum sequence length (MSL), the number of tokens that documents are truncated to. We observe that model quality is quite sensitive to the number of epochs, and thus the setting must be tailored for each dataset. We train on Reuters, AAPD, and IMDB for 30, 20, and 4 epochs, respectively. Due to resource constraints, we train on Yelp for only one epoch. As is the case with <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>, we find that choosing a batch size of 16, learning rate of 2×10 −5 , and MSL of 512 tokens yields optimal performance on the validation sets of all datasets. More details are provided in the appendix.</p><p>For distillation, we train the LSTM reg model to capture the learned representations from BERT large using the objective shown in Equation (1). We use a batch size of 128 for the multilabel tasks and 64 for the single-label tasks. We find the learning rates and dropout rates used in <ref type="bibr" target="#b0">Adhikari et al. (2019)</ref> to be optimal even for the distillation process.</p><p>To build an effective transfer set for distillation as suggested by <ref type="bibr" target="#b5">Hinton et al. (2015)</ref>, we augment the training splits of the datasets by applying POS-guided word swapping and random masking . The transfer set sizes for Reuters, IMDB and AAPD are 3×, 4×, and 4× their training splits respectively, whereas only 1× (i.e., no data augmentation) the corresponding training split for Yelp2014 due to computational restrictions. We use a λ of 1 for the multi-label datasets and 4 for the single-label datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>We report the mean F 1 scores for multi-label datasets and accuracy for single-label datasets, along with the corresponding standard deviation, across five runs in <ref type="table" target="#tab_2">Table 2</ref>. We copy values for rows 1-9 from <ref type="bibr" target="#b0">Adhikari et al. (2019)</ref>. Due to resource limitations, we report the scores from only a single run for BERT base and BERT large .</p><p>Consistent with <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>, BERT large achieves state-of-the-art results on all four datasets, followed by BERT base (see <ref type="table" target="#tab_2">Table 2</ref>, rows 10 and 11). The considerably simpler LSTM reg model (row 9) achieves high scores, coming close to the quality of BERT base . However, it is worth noting that all of the models above row 10 take only a fraction of the time and memory required for training the BERT models.</p><p>Surprisingly, distilled LSTM reg (KD-LSTM reg , row 12) achieves parity with BERT base on average for Reuters, AAPD, and IMDB. In fact, it outperforms BERT base (on both dev and test) in at least one of the five runs. For Yelp, we see that KD-LSTM reg reduces the difference between BERT base and LSTM reg , but not to the same extent as in the other datasets.</p><p>To put things in perspective, <ref type="table" target="#tab_3">Table 3</ref> reports the inference times on the validation sets of all the datasets. We calculate the inference times with batch size 128 for all the datasets on a single RTX 2080 Ti. The relative speedup achieved by KD-LSTM reg is at least around 40× with respect to   BERT base . Additionally, <ref type="figure">Figure 1</ref> shows the comparison between the number of parameters and prediction quality on the validation sets. These plots convey the effectiveness of the KD-LSTM reg model with different numbers of hidden units: 32, 64, 128, 256, and 512. We find that KD-LSTM reg , with just 256 hidden units (i.e., ∼ 1% parame-ters of BERT base ) attains parity with BERT base on Reuters, while for AAPD, 512 hidden units (∼ 3% parameters of BERT base ) are enough to overtake BERT base .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper we improve the baselines for document classification by fine-tuning BERT. We also use the knowledge learned by BERT models to improve the effectiveness of a single-layered lightweight BiLSTM model, LSTM reg , using knowledge distillation. In fact, we show that the distilled LSTM reg model achieves BERT base parity on a majority of datasets, resulting in over 30× compression in terms of the number of parameters and at least 40× faster inference times.</p><p>For future work, it would be interesting to study the effects of distillation over a range of neuralnetwork architectures. Alternatively, formulating specific model compression techniques in the context of transformer models deserves exploration. Epoch analysis. The bottom two subplots in <ref type="figure" target="#fig_0">Figure 2</ref> illustrate the F 1 score of BERT fine-tuned using different numbers of epochs for AAPD and Reuters. Contrary to <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>, who achieve the state of the art on small datasets with only a few epochs of fine-tuning, we find that smaller datasets require many more epochs to converge. On both the datasets (see <ref type="figure" target="#fig_0">Figure 2)</ref>, we see a significant drop in model quality when the BERT models are fine-tuned for only four epochs, as suggested in the original paper. On Reuters, using four epochs results in an F 1 worse than even logistic regression <ref type="table" target="#tab_2">(Table 2</ref>, row 1).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Results on the validation set from varying the MSL and the number of epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the datasets. C denotes the number of classes in the dataset, N the number of samples, and W and S the average number of words and sentences per document, respectively.</figDesc><table><row><cell>Using Hedwig, 1 an open-source deep learning</cell></row><row><cell>toolkit with a number of implementations of doc-</cell></row><row><cell>ument classification models, we compare the fine-</cell></row><row><cell>tuned BERT models against HAN, KimCNN,</cell></row><row><cell>XMLCNN, SGM, and LSTM reg . For simple yet</cell></row><row><cell>competitive baselines, we run the default logis-</cell></row><row><cell>tic regression (LR) and support vector machine</cell></row></table><note>(SVM) implementations from Scikit-Learn (Pe- dregosa et al., 2011), trained on the tf-idf vectors of the documents.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Val. Acc. Test Acc. Val. Acc. Test Acc.</figDesc><table><row><cell># Model</cell><cell cols="2">Reuters</cell><cell cols="2">AAPD</cell><cell></cell><cell>IMDB</cell><cell></cell><cell>Yelp '14</cell></row><row><cell></cell><cell>Val. F 1</cell><cell>Test F 1</cell><cell>Val. F 1</cell><cell>Test F 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 LR</cell><cell>77.0</cell><cell>74.8</cell><cell>67.1</cell><cell>64.9</cell><cell>43.1</cell><cell>43.4</cell><cell>61.1</cell><cell>60.9</cell></row><row><cell>2 SVM</cell><cell>89.1</cell><cell>86.1</cell><cell>71.1</cell><cell>69.1</cell><cell>42.5</cell><cell>42.4</cell><cell>59.7</cell><cell>59.6</cell></row><row><cell>3 KimCNN Repl.</cell><cell cols="8">83.5 ±0.4 80.8 ±0.3 54.5 ±1.4 51.4 ±1.3 42.9 ±0.3 42.7 ±0.4 66.5 ±0.1 66.1 ±0.6</cell></row><row><cell>4 KimCNN Orig.</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>37.6 8</cell><cell>-</cell><cell>61.0 8</cell></row><row><cell cols="6">5 XML-CNN Repl. 88.8 ±0.5 86.2 ±0.3 70.2 ±0.7 68.7 ±0.4 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>6 HAN Repl.</cell><cell cols="8">87.6 ±0.5 85.2 ±0.6 70.2 ±0.2 68.0 ±0.6 51.8 ±0.3 51.2 ±0.3 68.2 ±0.1 67.9 ±0.1</cell></row><row><cell>7 HAN Orig.</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.4 3</cell><cell>-</cell><cell>70.5 3</cell></row><row><cell>8 SGM Orig.</cell><cell cols="3">82.5 ±0.4 78.8 ±0.9 -</cell><cell>71.0 2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>9 LSTM reg</cell><cell cols="8">89.1 ±0.8 87.0 ±0.5 73.1 ±0.4 70.5 ±0.5 53.4 ±0.2 52.8 ±0.3 69.0 ±0.1 68.7 ±0.1</cell></row><row><cell>10 BERT base</cell><cell>90.5</cell><cell>89.0</cell><cell>75.3</cell><cell>73.4</cell><cell>54.4</cell><cell>54.2</cell><cell>72.1</cell><cell>72.0</cell></row><row><cell>11 BERT large</cell><cell>92.3</cell><cell>90.7</cell><cell>76.6</cell><cell>75.2</cell><cell>56.0</cell><cell>55.6</cell><cell>72.6</cell><cell>72.5</cell></row><row><cell>12 KD-LSTM reg</cell><cell cols="8">91.0 ±0.2 88.9 ±0.2 75.4 ±0.2 72.9 ±0.3 54.5 ±0.1 53.7 ±0.3 69.7 ±0.1 69.4 ±0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for each model on the validation and test sets. Best values are bolded. Repl. reports the mean of five runs from our reimplementations; Orig. refers to point estimates from † Yang et al. (2018), ‡ Yang et al. (2016), and † †<ref type="bibr" target="#b16">Tang et al. (2015)</ref>. KD-LSTM reg represents the distilled LSTM reg using the fine-tuned BERT large . Effectiveness of KD-LSTM reg vs. BERT base and BERT large</figDesc><table><row><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.70</cell><cell></cell><cell></cell></row><row><cell>F1 score</cell><cell>0.80 0.85</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Reuters/KD-LSTM Reuters/BERT Base Reuters/BERT Large AAPD/KD-LSTM AAPD/BERT Base AAPD/BERT Large</cell><cell>F1 score</cell><cell>0.60 0.65</cell><cell></cell><cell></cell><cell>IMDB/KD-LSTM IMDB/BERT Base IMDB/BERT Large Yelp14/KD-LSTM Yelp14/BERT Base Yelp14/BERT Large</cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.55</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>32</cell><cell>64</cell><cell cols="2">128 Number of hidden units</cell><cell>256</cell><cell>512</cell><cell></cell><cell>0.50</cell><cell>32</cell><cell>64</cell><cell>128 Number of hidden units</cell><cell>256</cell><cell>512</cell></row><row><cell></cell><cell cols="3">Figure 1: Dataset LSTM reg</cell><cell cols="2">BERT base</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Reuters</cell><cell cols="2">0.5 (1×)</cell><cell cols="2">30.3 (60×)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>AAPD</cell><cell cols="2">0.3 (1×)</cell><cell cols="2">15.8 (50×)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>IMDB</cell><cell cols="2">6.8 (1×)</cell><cell cols="2">243.6 (40×)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Yelp'14 20.6 (1×) 1829.9 (90×)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of inference latencies (seconds) on validation sets with batch size 128.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/castorini/hedwig</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada, and enabled by computational resources provided by Compute Ontario and Compute Canada.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Hyperparameter Analysis MSL analysis. A decrease in the maximum sequence length (MSL) corresponds to only a minor loss in F 1 on Reuters (see top-left subplot in <ref type="figure">Figure</ref> 2), possibly due to Reuters having shorter documents. On IMDB (top-right subplot in <ref type="figure">Figure 2</ref>), lowering the MSL corresponds to a drastic fall in accuracy, suggesting that the entire document is necessary for this dataset.</p><p>On the one hand, these results appear obvious. Alternatively, one can argue that, since IMDB contains longer documents, truncating tokens may hurt less. The top two subplots in <ref type="figure">Figure 2</ref> show that this is not the case, since truncating to even 256 tokens causes accuracy to fall lower than that of the much smaller LSTM reg (see <ref type="table">Table 2</ref>). From these results, we conclude that any amount of truncation is detrimental in document classification, but the level of degradation may differ. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rethinking complex neural network architectures for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achyudh</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4046" to="4051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated learning of decision rules for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chidanand</forename><surname>Apté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Damerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sholom</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="251" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arxiv/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning for extreme multi-label text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning sparse neural networks through l 0 regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno>arxiv/1712.01312</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scikit-learn: machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duchesnay</forename><surname>Andédouard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GloVe: global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distilling taskspecific knowledge from BERT into simple neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno>arxiv/1903.12136</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SGM: sequence generation model for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3915" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">XLNet: generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>arxiv/1906.08237</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
