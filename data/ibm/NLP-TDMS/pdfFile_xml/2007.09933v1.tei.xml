<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MotionSqueeze: Neural Motion Feature Learning for Video Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NPRC †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjin</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NPRC †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NPRC †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NPRC †</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MotionSqueeze: Neural Motion Feature Learning for Video Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>video understanding</term>
					<term>action recognition</term>
					<term>motion feature learn- ing</term>
					<term>efficient video processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motion plays a crucial role in understanding videos and most state-of-the-art neural models for video classification incorporate motion information typically using optical flows extracted by a separate off-the-shelf method. As the frame-by-frame optical flows require heavy computation, incorporating motion information has remained a major computational bottleneck for video understanding. In this work, we replace external and heavy computation of optical flows with internal and light-weight learning of motion features. We propose a trainable neural module, dubbed MotionSqueeze, for effective motion feature extraction. Inserted in the middle of any neural network, it learns to establish correspondences across frames and convert them into motion features, which are readily fed to the next downstream layer for better prediction. We demonstrate that the proposed method provides a significant gain on four standard benchmarks for action recognition with only a small amount of additional cost, outperforming the state of the art on Something-Something-V1&amp;V2 datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The most distinctive feature of videos, from those of images, is motion. In order to grasp a full understanding of a video, we need to analyze its motion patterns as well as the appearance of objects and scenes in the video <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>. With significant progress of neural networks on the image domain, convolutional neural networks (CNNs) have been widely used to learn appearance features from video frames <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40]</ref> and recently extended to learn temporal features using spatio-temporal convolution across multiple frames <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>. The results, however, have shown that spatio-temporal convolution alone is not enough for learning motion patterns; convolution is effective in capturing translation-equivariant patterns but not in modeling relative movement of objects <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46]</ref>. As a result, most <ref type="figure">Fig. 1</ref>: Video classification performance comparison on Something-Something-V1 <ref type="bibr" target="#b9">[10]</ref> in terms of accuracy, computational cost, and model size. The proposed method (MSNet) achieves the best trade-off between accuracy and efficiency compared to state-of-the-art methods of TSM <ref type="bibr" target="#b20">[21]</ref>, TRN <ref type="bibr" target="#b46">[47]</ref>, ECO <ref type="bibr" target="#b47">[48]</ref>, I3D <ref type="bibr" target="#b1">[2]</ref>, NL-I3D <ref type="bibr" target="#b40">[41]</ref>, and GCN <ref type="bibr" target="#b41">[42]</ref>. Best viewed in color.</p><p>state-of-the-art methods still incorporate explicit motion features, i.e., dense optical flows, extracted by an external off-the-shelf methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b42">43]</ref>. This causes a major computational bottleneck in video-processing models for two reasons. First, calculating optical flows frame-by-frame is a time-consuming process; obtaining optical flows of a video is typically an order of magnitude slower than feed-forwarding the video through a deep neural network. Second, processing optical flows often requires a separate stream in the model to learn motion representations <ref type="bibr" target="#b30">[31]</ref>, which results in doubling the number of parameters and the computational cost. To address these issues, several methods have attempted to internalize motion modeling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>. They, however, all either impose a heavy computation on their architectures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> or underperform other methods using external optical flows <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>To tackle the limitation of the existing methods, we propose an end-to-end trainable block, dubbed the MotionSqueeze (MS) module, for effective motion estimation. Inserted in the middle of any neural network for video understanding, it learns to establish correspondences across adjacent frames efficiently and convert them into effective motion features. The resultant motion features are readily fed to the next downstream layer and used for final prediction. To validate the proposed MS module, we develop a video classification architecture, dubbed the MotionSqueeze network (MSNet), that is equipped with the MS module. In comparison with recent methods, shown in <ref type="figure">Figure 1</ref>, the proposed method provides the best trade-off in terms of accuracy, computational cost, and model size in video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Video classification architectures. One of the main problems in video understanding is to categorize videos given a set of pre-defined target classes. Early methods based on deep neural networks have focused on learning spatio-temporal or motion features. Tran et al. <ref type="bibr" target="#b34">[35]</ref> propose a 3D CNN (C3D) to learn spatiotemporal features while Simonyan and Zisserman <ref type="bibr" target="#b30">[31]</ref> employ an independent temporal stream to learn motion features from precomputed optical flows. Carreira and Zisserman <ref type="bibr" target="#b1">[2]</ref> design two-stream 3D CNNs (two-stream I3D) by integrating two former methods, and achieve the state-of-the-art performance at that time. As the two-stream 3D CNNs are powerful but computationally demanding, subsequent work has attempted to improve the efficiency. Tran et al. <ref type="bibr" target="#b36">[37]</ref> and Xie et al. <ref type="bibr" target="#b42">[43]</ref> propose to decompose 3D convolutional filters into 2D spatial and 1D temporal filters. Chen et al. <ref type="bibr" target="#b2">[3]</ref> adopt group convolution techniques while Zolfaghari et al. <ref type="bibr" target="#b47">[48]</ref> propose to study mixed 2D and 3D networks with the frame sampling method of temporal segment networks (TSN) <ref type="bibr" target="#b39">[40]</ref>. Tran et al. <ref type="bibr" target="#b35">[36]</ref> analyze the effect of 3D group convolutional networks and propose the channel-separated convolutional network (CSN). Lin et al. <ref type="bibr" target="#b20">[21]</ref> propose the temporal shift module (TSM) that simulates 3D convolution using 2D convolution with a part of input feature channels shifted along the temporal axis. It enables 2D convolution networks to achieve a comparable classification accuracy to 3D CNNs. Unlike these approaches, we focus on efficient learning of motion features.</p><p>Learning motions in a video. While two-stream-based architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> have demonstrated the effectiveness of pre-computed optical flows, the use of optical flows typically degrades the efficiency of video processing. To address the issue, Ng et al. <ref type="bibr" target="#b25">[26]</ref> use a multi-task learning of both optical flow estimation and action classification and Stroud et al. <ref type="bibr" target="#b31">[32]</ref> propose to distill motion features from pre-trained two-stream 3D CNNs. These methods do not use pre-computed optical flows during inference, but still need them at the training phase. Other methods design network architectures that learn motions internally without external optical flows <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>. Sun et al. <ref type="bibr" target="#b33">[34]</ref> compute spatial and temporal gradients between appearance features to learn motion features. Lee et al. <ref type="bibr" target="#b19">[20]</ref> and Jiang et al. <ref type="bibr" target="#b15">[16]</ref> propose a convolutional module to extract motion features by spatial shift and subtraction operation between appearance features. Despite their computational efficiency, they do not reach the classification accuracy of two-stream networks <ref type="bibr" target="#b30">[31]</ref>. Fan et al. <ref type="bibr" target="#b6">[7]</ref> implement the optimization process of TV-L1 <ref type="bibr" target="#b43">[44]</ref> as iterative neural layers, and design an end-to-end trainable architecture (TVNet). Piergiovanni and Ryoo <ref type="bibr" target="#b26">[27]</ref> extend the idea of TVNet by calculating channel-wise flows of feature maps at the intermediate layers of the CNN. These variational methods achieve a good performance, but require a high computational cost due to iterative neural layers. In contrast, our method learns to extract effective motion features with a marginal increase of computation.</p><p>Learning visual correspondences. Our work is inspired by recent methods that learn visual correspondences between images using neural networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>. Fischer et al. <ref type="bibr" target="#b5">[6]</ref> estimate optical flows using a convolutional neural network, which construct a correlation tensor from feature maps and regresses displacements from it. Sun et al. <ref type="bibr" target="#b32">[33]</ref> use a stack of correlation layers for coarseto-fine optical flow estimation. While these methods require dense ground-truth optical flows in training, the structure of correlation computation and subsequent displacement estimation is widely adopted in other correspondence problems with different levels of supervision. For example, recent methods for semantic correspondence, i.e., matching images with intra-class variation, typically follow a similar pipeline to learn geometric transformation between images in a more weakly-supervised regime <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. In this work, motivated by this line of research, we develop a motion feature module that does not require any correspondence supervision for learning.</p><p>Similarly to our work, a few recent methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45]</ref> have attempted to incorporate correspondence information for video understanding. Zhao et al. <ref type="bibr" target="#b44">[45]</ref> use correlation information between feature maps of consecutive frames to replace optical flows. The size of their full model, however, is comparable to the two-stream networks <ref type="bibr" target="#b30">[31]</ref>. Liu et al. <ref type="bibr" target="#b21">[22]</ref> propose the correspondences proposal (CP) module to learn correspondences in a video. Unlike ours, they focus on analyzing spatio-temporal relationship within the whole video, rather than motion, and the model is not fully differentiable and thus less effective in learning. In contrast, we introduce a fully-differentiable motion feature module that can be inserted in the middle of any neural network for video understanding.</p><p>The main contribution of this work is three-fold.</p><p>• We propose an end-to-end trainable, model-agnostic, and lightweight module for motion feature extraction. • We develop an efficient video recognition architecture that is equipped with the proposed motion module. • We demonstrate the effectiveness of our method on four different benchmark datasets and achieve the state-of-the-art on Something-Something-V1&amp;V2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed approach</head><p>The overall architecture for video understanding is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. Let us assume a neural network that takes a video of T frames as input and predicts the category of the video as output, where convolutional layers are used to transform input frames into frame-wise appearance features. The proposed motion feature module, dubbed MotionSqueeze (MS) module, is inserted to produce frame-wise motion features using pairs of adjacent appearance features. The resultant motion features are added to the appearance features for final prediction. In this section, we first explain the MS module, and describe the details of our network architecture for video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MotionSqueeze (MS) module</head><p>The MS module is a learnable motion feature extractor, which can replace the use of explicit optical flows for video understanding. As described in <ref type="figure">Figure 3</ref>, given two feature maps from adjacent frames, it learns to extract effective motion features in three steps: correlation computation, displacement estimation, and feature transformation.</p><p>Correlation computation. Let us denote the two adjacent input feature maps by F (t) and F (t+1) , each of which is 3D tensors of size H × W × C. The spatial resolution is H × W and the C dimensional features on spatial position x by F x . A correlation score of position x with respect to displacement p is defined as</p><formula xml:id="formula_0">s(x, p, t) = F (t) x · F (t+1) x+p ,<label>(1)</label></formula><p>where · denotes dot product. For efficiency, we compute the correlation scores of position x only in its neighborhood of size P = 2k + 1 by restricting a maximum displacement: p ∈ [−k, k] 2 . For t th frame, a resultant correlation tensor S (t) is of size H × W × P 2 . The cost of computing the correlation tensor is equivalent to that of 1 × 1 convolutions with P 2 kernels; the correlation computation can be implemented as 2D convolutions on t th feature map using t + 1 th feature map as P 2 kernels. The total FLOPs in a single video amounts to T HW CP 2 . We apply a convolution layer before computing correlations, which learns to weight informative feature channels for learning visual correspondences. In practice, we set the neighborhood P = 15 given the spatial resolution 28 × 28 and apply an 1 × 1 × 1 layer with C/2 channels. For correlation computation, we adopt C++/Cuda implemented version of correlation layer in FlowNet <ref type="bibr" target="#b5">[6]</ref>.</p><p>Displacement estimation. From the correlation tensor S (t) , we estimate a displacement field for motion information. A straightforward but non-differentiable method would be to take the best matching displacement for position x by argmax p s(x, p, t). To make the operation differentiable, we can use a weighted average of displacements using softmax, called soft-argmax <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>, which is de- <ref type="figure">Fig. 3</ref>: Overall process of MotionSqueeze (MS) module. The MS module estimates motion across two frame-wise feature maps (F (t) , F (t+1) ) of adjacent frames. A correlation tensor S (t) is obtained by computing correlations, and then a displacement tensor D (t) is estimated using the tensor. Through the transformation process of convolution layers, the final motion feature M (t) is obtained. See text for details.</p><p>fined as</p><formula xml:id="formula_1">d(x, t) = p exp(s(x, p, t)) p exp(s(x, p , t)) p.<label>(2)</label></formula><p>This method, however, is sensitive to noisy outliers in the correlation tensor since it is influenced by all correlation values. We thus use the kernel-soft-argmax <ref type="bibr" target="#b18">[19]</ref> that suppresses such outliers by masking a 2D Gaussian kernel on the correlation values; the kernel is centered on each target position so that the estimation is more influenced by closer neighbors. Our kernel-soft-argmax for displacement estimation is defined as</p><formula xml:id="formula_2">d(x, t) = p exp(g(x, p, t)s(x, p, t)/τ ) p exp(g(x, p , t)s(x, p , t)/τ ) p,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">g(x, p, t) = 1 √ 2πσ exp( p − argmax p s(x, p, t) σ 2 ).<label>(4)</label></formula><p>Note that g(x, p, t) is the Gaussian kernel and we empirically set the standard deviation σ to 5. τ is a temperature factor adjusting the softmax distribution; as τ decreases, softmax approaches argmax. We set τ = 0.01 in our experiments. In addition to the estimated displacement map, we use a confidence map of correlation as auxiliary motion information, which is obtained by pooling the highest correlation on each position x:</p><formula xml:id="formula_4">s * (x, t) = max p s(x, p, t).<label>(5)</label></formula><p>The confidence map may be useful for identifying displacement outliers and learning informative motion features.</p><p>We concatenate the (2-channel) displacement map and the (1-channel) confidence map into a displacement tensor D (t) of size H × W × 3 for the next step of motion feature transformation. An example of them is visualized in <ref type="figure" target="#fig_1">Figure 4</ref>.</p><p>Feature transformation. We convert the displacement tensor D (t) to an effective motion feature M (t) that is readily incorporated into downstream layers. The tensor D (t) is fed to four depth-wise separable convolution <ref type="bibr" target="#b13">[14]</ref> layers, one 1 × 7 × 7 layer followed by three 1 × 3 × 3 layers, and transformed into a motion feature M (t) with the same number of channels C as that of the original input F (t) . The depth-wise separable convolution approximates 2D convolution with a significantly less computational cost <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>. Note that all depth-wise and point-wise convolution layers are followed by batch normalization <ref type="bibr" target="#b14">[15]</ref> and ReLU <ref type="bibr" target="#b24">[25]</ref>. As in the temporal stream layers of <ref type="bibr" target="#b30">[31]</ref>, this feature transformation process is designed to learn task-specific motion features with convolution layers by interpreting the semantics of displacement and confidence. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, the MS module generates motion feature M (t) using two adjacent appearance features F (t) and F (t+1) and then add it to the input of the next layer. Given T frames, we simply pad the final motion feature</p><formula xml:id="formula_5">M (T ) with M (T −1) by setting M (T ) = M (T −1) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MotionSqueeze network (MSNet).</head><p>The MS module can be inserted into any video understanding architecture to improve the performance by motion feature modeling. In this work, we introduce standard convolutional neural networks (CNNs) with the MS module, dubbed MSNet, for video classification. We adopt the ImageNet-pretrained ResNet <ref type="bibr" target="#b11">[12]</ref> as the CNN backbone and insert TSM <ref type="bibr" target="#b20">[21]</ref> for each residual block of the ResNet. TSM enables 2D convolution to obtain the effect of 3D convolution by shifting a part of input feature channels along the temporal axis before the convolution operation. Following the default setting in <ref type="bibr" target="#b20">[21]</ref>, we shift 1/8 of the input features channels forward and another 1/8 of the channels backward in each TSM.</p><p>The overall architecture of the proposed model is shown in <ref type="figure" target="#fig_0">Figure 2</ref>; a single MS module is inserted after the third stage of the ResNet. We fuse the motion feature into the appearance feature by element-wise addition:</p><formula xml:id="formula_6">F (t) = F (t) + M (t) .<label>(6)</label></formula><p>In section 4.5, we extensively evaluate different fusion methods, e.g., concatenation and multiplication, and show that additive fusion is better than the others. After fusing both features, the combined feature is passed through the next downstream layers. The network outputs over T frames are temporally averaged to produce a final output and the cross-entropy with softmax is used as a loss function for training. By default setting, MSNet learns both appearance and motion features jointly in a single network at the cost of only 2.5% and 1.2% increase in FLOPs and the number of parameters, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Something-Something V1&amp;V2 <ref type="bibr" target="#b9">[10]</ref> are trimmed video datasets for human action classification. Both datasets consist of 174 classes with 108,499 and 220,847 videos in total, respectively. Each video contains one action and the duration spans from 2 to 6 seconds. Something-Something V1&amp;V2 are motion-oriented datasets where temporal relationships are more salient than in others.</p><p>Kinetics <ref type="bibr" target="#b16">[17]</ref> is a popular large-scale video dataset, consisting of 400 classes with over 250,000 videos. Each video lasts around 10 seconds with a single action. HMDB51 <ref type="bibr" target="#b17">[18]</ref> contains 51 classes with 6,766 videos. Kinetics and HMDB-51 focus more on appearance information rather than motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Clip sampling. In both training and testing, instead of an entire video, we use a clip of frames that are sampled from the video. We use the segment-based sampling method <ref type="bibr" target="#b39">[40]</ref> for the Something-Something V1&amp;V2 while adopting the dense frame sampling method <ref type="bibr" target="#b1">[2]</ref> for Kinetics and HMDB-51.</p><p>Training. For each video, we sample a clip of 8 or 16 frames, resize them into 240 × 320 images, and crop 224 × 224 images from the resized images <ref type="bibr" target="#b47">[48]</ref>. The minibatch SGD with Nestrov momentum is used for optimization, and the batch size is set to 48. We use scale jittering for data augmentation. For the Something-Something V1&amp;V2, we set the training epochs to 40 and the initial learning rate to 0.01; the learning rate is decayed by 1/10 after 20 th and 30 th epochs. For Kinetics, we set the training epochs to 80 and the initial learning rate to 0.01; the learning rate is decayed by 1/10 after 40 and 60 epochs. In training our model on HMDB-51, we fine-tune the Kinetics-pretrained model as in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref>. We set the training epochs to 35 and the initial learning rate to 0.001; the learning rate is decayed by 1/10 after 15 th and 30 th epochs.</p><p>Inference. Given a video, we sample a clip and test its center crop. For Something-Something V1&amp;V2, we evaluate both the single clip prediction and the average prediction of 10 randomly-sampled clips. For Kinetics and HMDB-51, we evaluate the average prediction of uniformly-sampled 10 clips from each video.  <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref>, 3D CNN methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref>, ResNet with TSM (TSM ResNet) <ref type="bibr" target="#b20">[21]</ref>, and the proposed method, respectively. Most of the results are copied from the corresponding papers, except for TSM ResNet; we evaluate the official pre-trained model of TSM ResNet using a single center-cropped clip per video in terms of top-1 and top-5 accuracies. Our method, which uses TSM ResNet as a backbone, achieves 50.9% and 63.0% on Something-Something V1 and V2 at top-1 accuracy, respectively, which outperforms most of 2D CNN and 3D CNN methods, while using a single clip with 8 input frames only. Compared to the TSM ResNet baseline, our method obtains a significant gain of about 5.3% points and 4.2% points at top-1 accuracy at the cost of only 2.5% and 1.2% increase in FLOPs and parameters, respectively. When using 16 frames, our method further improves achieving 52.1% and 64.7% at top-1 accuracy, respectively. Following the evaluation procedure of two-stream networks, we evaluate the ensemble model (MSNet-R50 En ) by averaging prediction scores of the 8-frame and 16-frame models. With the same number of clips for evaluation, it achieves top-1 accuracy 1.8% points and 1.6% points higher than TSM two-stream networks with 22% less computation, even no optical flow needed. Our 10-clip model achieves 55.1% and 67.1% at top-1 accuracy on Something-Something V1 and V2, respectively, which is the state-of-the-art on both of the datasets. As shown in <ref type="figure">Figure 1</ref>, our model provides the best trade-off in terms of accuracy, FLOPs, and the number of parameters.  CNNs, motion representation methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>, two-stream CNNs with optical flows <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref>, and the proposed method, respectively. OFF, MFNet, and STM <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34</ref>] use a sub-network or lightweight modules to calculate temporal gradients of frame-wise feature maps. TVNet <ref type="bibr" target="#b6">[7]</ref> and Rep-flow <ref type="bibr" target="#b26">[27]</ref> internalize iterative TV-L1 flow operations in their networks. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the proposed model using 16 frames outperforms all the other conventional CNNs and the motion representation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>, while being competitive with the R(2+1)D two-stream <ref type="bibr" target="#b36">[37]</ref> that uses pre-computed optical flows. Furthermore, our model is highly efficient than all the other methods in terms of FLOPs, clips, and the number of frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-art methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with other motion representation methods</head><p>Run-time. We also evaluate in <ref type="table" target="#tab_2">Table 2</ref> the inference speeds of several models to demonstrate the efficiency of our method. All the run-times reported are measured on a single GTX Titan Xp GPU, ignoring the time of data loading. For this experiment, we set the spatial size of the input to 224 × 224 and the batch size to 1. The official codes are used for ResNet, TSM ResNet, and Repflow <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref> except for R(2+1)D <ref type="bibr" target="#b36">[37]</ref> we implemented. In evaluating Repflow <ref type="bibr" target="#b26">[27]</ref>, we use 20 iterations for optimization as in the original paper. The speed of the two-stream networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37]</ref> includes computation time for TV-L1 method on the GPU. The run-time results clearly show the cost of iterative optimizations used in two-stream networks and Rep-flow. In contrast, our model using 16 frames is about 160× faster than the two-stream networks. Compared to Rep-flow ResNet-50, our method performs about 4× faster due to the absence of the iterative optimization process in Rep-flow.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation studies</head><p>We conduct ablation studies of the proposed method on Something-Something V1 <ref type="bibr" target="#b9">[10]</ref> dataset. We use ImageNet pre-trained TSM ResNet-18 as a default backbone and use 8 input frames for all experiments in this section. Displacement estimation in MS module. In <ref type="table" target="#tab_4">Table 3</ref>, we experiment with different variants of the displacement tensor D (t) in the MS module. We first compare soft-argmax ('S') and kernel-soft-argmax ('KS') for displacement estimation. As shown in the upper part of <ref type="table" target="#tab_4">Table 3</ref>. the kernel-soft-argmax outperforms the soft-argmax, showing the noise reduction effect of Gaussian kernel.</p><p>In the lower part of <ref type="table" target="#tab_4">Table 3</ref>, we evaluate the effect of additional features: confidence maps ('CM') and backward displacement tensor ('BD'). The backward displacement tensor is estimated from F (t+1) to F (t) . We concatenate the forward and backward displacement tensors, and then pass them to the feature transformation layers. We obtain 0.9% points gain by appending the confidence map to the displacement tensor. Furthermore, by adding backward displacement we obtain another 0.5% points gain at top-1 accuracy, indicating that forward and backward displacement maps complement each other to enrich motion information. We use the kernel-soft-argmax with the confidence map ('KS + CM') as a default method for all other experiments. Size of matching region. In <ref type="figure" target="#fig_1">Figure 4</ref>, we evaluate performance varying the spatial size of matching regions of the MS module. Even with a small matching region P = 3, it provides a noticeable performance gain of over 2.7% points  to the baseline. The performance tends to increase as the matching region becomes larger due to the larger displacement it can handle between frames. The performance is saturated after P = 15.</p><p>Position of MS module. In <ref type="table" target="#tab_5">Table 4</ref>, we evaluate different positions of the MS module. We denote that res N by the N -th stage of the ResNet. For each stage, it is inserted right after its final residual block. The result shows that while the MS module is beneficial in most cases, both accuracy and efficiency gains depend on the position of the module. It performs the best at res 3 ; appearance features from res 2 are not strong enough for accurate feature matching while spatial resolutions of appearance features from res 4 and res 5 are not high enough. The position of the module also affects FLOPs; the computational cost quadratically increases with spatial resolution due to convolution layers of the feature transformation. When inserting multiple MS modules (res 2,3,4 ) at the backbone, it marginally improves top-1 accuracy as 0.2% points. Multiple modules appear to generate similar motion information even in different levels of features.</p><p>Fusing strategy of MS module. In <ref type="table" target="#tab_6">Table 5</ref>, we evaluate different fusion strategies for the MS module; 'MS only', 'multiply', 'concat', and 'add'. In the case of 'MS only', we only pass M (t) into downstream layers without F (t) . We apply element-wise multiplication and element-wise addition, respectively, for 'multiply' and 'add'. In the case of 'concat', we concatenate F (t) and M (t) , whose channel size is transformed to C via an 1 × 1 × 1 convolution layer. 'MS only' is less accurate than the baseline because visual semantic information is discarded. While both 'multiply' and 'concat' clearly improve the accuracy, 'add' achieves the best performance with 45.5% at top-1 accuracy. We find that additive fusion is the most effective and stable in amplifying appearance features of moving objects.</p><p>Effect of MS module on different backbones. In <ref type="figure" target="#fig_2">Figure 5</ref>, we also evaluate the effect of the MS module on ResNet-18, MobileNet-V2, and I3D. We insert one MS module where the spatial resolution of the feature map remains the same. For ResNet-18 and MobileNet-V2, we finetune models pre-trained on ImageNet.</p><p>We train I3D from scratch. Our MS module benefits both 2D CNNs and 3D CNNs to obtain higher accuracy. The module significantly improves ResNet-18 and MobileNet-V2 by 21.3% and 19.2% points, respectively, in top-1 accuracy. Since 2D CNNs do not use any spatio-temporal features, it obtains significantly higher gain from the MS module. The MS module also improves I3D and TSM ResNet-18 by 2.4% and 4.0% points, respectively, in top-1 accuracy. The gain on 3D CNNs, although relatively small, verifies that the motion features by the MS module are complementary even to the spatio-temporal features; the MS module learns explicit motion information across adjacent frames whereas TSM covers long-term temporal length using (pseudo-)temporal convolutions.</p><p>Comparison with two-stream networks. In <ref type="table" target="#tab_7">Table 6</ref>, we compare the proposed method with variants of TSM two-stream networks <ref type="bibr" target="#b30">[31]</ref> that use TV-L1 optical flows <ref type="bibr" target="#b43">[44]</ref>. We denote the two-stream networks by Two-stream Nr+(N f ×Ns) where N r , N f and N s indicate the number of frames, optical flows, and their stacking size, respectively. For each frame, the two-stream networks use N s stacked optical flows, which are extracted using the subsequent frames in the original video. Note that those frames for optical flow extraction are not used in our method (MSNet). The second row of <ref type="table" target="#tab_7">Table 6</ref>, Two-stream 8+(8×5) , shows the performance of standard TSM two-stream networks that use 5 stacked optical flows for the temporal stream. Using the multiple optical flows for each frame outperforms our model in terms of accuracy but requires substantially larger FLOPs as well as an additional computation for calculating optical flows. For a fair comparison, we report the performance of the two-stream networks, Twostream 8+(8×1) , that do not stack multiple optical flows. Our model outperforms the two-stream networks by 0.8% points at top-1 accuracy, with about two times fewer FLOPs. Note that both Two-stream 8+(8×5) and Two-stream 8+(8×1) use optical flows obtained from the original video with a higher frame rate than the input video clip (sampled frames); our method (MSNet) observes the input video clip only. We thus evaluate other two-stream networks, Two-stream 8+(8×1)(low) , that uses low-fps optical flows as input; we sample a sequence of frames in 3 fps from the original video and extract TV-L1 optical flows using the sequence. As shown in <ref type="table" target="#tab_7">Table 6</ref>, the top-1 accuracy gap between ours and the two-stream network increases to 1.4% points. The result implies that given low-fps videos, our method may further improve over the two-stream networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualization</head><p>In <ref type="figure" target="#fig_3">Figure 6</ref>, we present visualization results on Something-Something V1 and Kinetics datasets. They show that our MS module effectively learns to estimate motion without any direct supervision used in training. The first row of each subfigure shows 6 uniformly sampled frames from a video. The second and third rows show color-coded displacement maps <ref type="bibr" target="#b0">[1]</ref> and confidence maps, respectively; we apply min-max normalization on the confidence map. The resolution of all the displacement and confidence maps is set to 56×56 for better visualization. As shown in the figures, the MS module captures reliable displacements in most cases: horizontal and vertical movements <ref type="figure" target="#fig_3">(Figure 6a, 6c, 6d</ref>), rotational movements ( <ref type="figure" target="#fig_3">Figure 6b</ref>), and non-severe deformation <ref type="figure" target="#fig_3">(Figure 6a, 6d</ref>). See the supplementary material for additional details and results. We will make our code and data available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented an efficient yet effective motion feature block, the MS module, that learns to generate motion features on the fly for video understanding. The MS module can be readily inserted into any existing video architectures and trained by backpropagation. The ablation studies on the module demonstrate the effectiveness of the proposed method in terms of accuracy, computational cost, and model size. Our method outperforms existing state-of-the-art methods on Something-Something-V1&amp;V2 for video classification with only a small amount of additional cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of "MotionSqueeze: Neural Motion Feature Learning for Video Understanding"</head><p>We present additional results and details that are omitted in our main paper due to the lack of space. All our code and data are released online at our project page: http://cvlab.postech.ac.kr/research/MotionSqueeze/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Effects of depth-wise separable (DWS) convolutions</head><p>We use DWS convolutions rather than standard convolutions to build the feature transformation (FT) layers deeper and wider while saving computational cost. <ref type="table">Table 7</ref> shows the results of different forms of FT layers on Something-Something V1 <ref type="bibr" target="#b9">[10]</ref> . The accuracy increases as the FT layers become deeper and have wider receptive fields, and the DWS convolutions show the best accuracy-FLOPs tradeoff.</p><p>2 Comparison with the CP module <ref type="bibr" target="#b21">[22]</ref> As we mentioned in the main paper, the CP module is the one of the most relevant work to our method in the sense that it leverage correspondences between input video frames. Here we provide more detailed comparisons to it. Difference in motivation and design. Unlike our MS module, which focuses on extracting effective motion features across consecutive frames, the CP module <ref type="bibr" target="#b21">[22]</ref> is designed to capture long-term spatio-temporal relationship within an input video <ref type="bibr" target="#b40">[41]</ref> by computing a non-local correlation tensor across all frames. The CP module selects k most likely corresponding features in the correlation tensor with an 'arg top-k' operation, and the operation thus makes the correlation tensor non-differentiable. Performance comparison. We have already shown in <ref type="table" target="#tab_0">Table 1</ref> of the main paper that the result of our method is better than that of the CP module (from the original paper <ref type="bibr" target="#b21">[22]</ref>) on Something-Something V2 <ref type="bibr" target="#b9">[10]</ref>. The comparison, however, may not be totally fair in the sense that the backbone and the other experimental settings are not the same. For an apples-to-apples comparison between the MS module and the CP module, we conduct an additional experiment using the same backbone and setup. We re-implement the CP module in Pytorch based on the official Tensorflow code * . As a baseline network, we use ImageNet pre-trained TSM ResNet-18 using 8 input frames. Either MS or CP module is inserted after the third stage of the network. <ref type="table" target="#tab_9">Table 8</ref> summarizes the comparative results of the MS module and the CP module on Something-Something V1 <ref type="bibr" target="#b9">[10]</ref>. The CP module is effective for improving accuracy while consuming almost 6G FLOPs more than the baseline; the computational cost of the nonlocal correlation tensor is quadratic to the number of input frames. In contrast, the MS module performs 0.9% points and 0.8% points higher at top-1 and top-5 <ref type="table">Table 7</ref>: Performance comparison with different forms of feature transformation (FT) layers. n × (k, k) denotes n standard convolution layers with a kernel size of k. * denotes our FT layers in <ref type="figure">Fig. 3</ref> of the paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Backbone architectures in experiments</head><p>In our main paper, we evaluate the effect of the MS module on different backbone architectures: ResNet <ref type="bibr" target="#b11">[12]</ref>, TSM ResNet <ref type="bibr" target="#b20">[21]</ref>, MobileNet-V2 <ref type="bibr" target="#b28">[29]</ref> and I3D <ref type="bibr" target="#b1">[2]</ref>. We provide details of the backbone architectures here.</p><p>ResNet &amp; TSM ResNet. <ref type="table" target="#tab_10">Table 9</ref> shows the architecture of ResNet <ref type="bibr" target="#b11">[12]</ref> and TSM ResNet <ref type="bibr" target="#b20">[21]</ref>. As a default, one MS module is inserted right after res 3 . I3D. <ref type="figure">Figure 7a, 7b</ref> show the architecture of I3D <ref type="bibr" target="#b1">[2]</ref> used in our experiment; we reduce the first convolution kernel from 7×7×7 to 1×7×7 as we only use a sampled clip of 8 frames. The MS module is inserted after Inc(b) of <ref type="figure">Figure 7b</ref>. MobileNet-V2. <ref type="figure">Figure 8</ref> and <ref type="table" target="#tab_0">Table 10</ref> show the architecture of MobileNet-V2 <ref type="bibr" target="#b28">[29]</ref>. The MS module is inserted right after stage 3 of <ref type="table" target="#tab_0">Table 10</ref>. As the feature channel size of the backbone is small enough, we omit the channel reduction layer in the MS module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Additional examples of visualization</head><p>We present more results of visualization on Something-Something V1 <ref type="bibr" target="#b9">[10]</ref> in <ref type="figure" target="#fig_6">Figure 9</ref> and Kinetics-400 <ref type="bibr" target="#b16">[17]</ref> in <ref type="figure">Figure 10</ref>. From the top of each figure, RGB frames, color-coded displacement maps <ref type="bibr" target="#b0">[1]</ref>, and confidence maps are illustrated. We visualize examples of horizontal, vertical movements <ref type="figure" target="#fig_6">(Figure 9a</ref>, 9b, 9c, 10a, 10b, 10c), rotations <ref type="figure" target="#fig_6">(Figure 9d, 10d</ref>), scale changes <ref type="figure" target="#fig_6">(Figure 9e, 10e</ref>), and deformations <ref type="figure" target="#fig_6">(Figure 9f, 10f</ref>). We also report some failure cases in the last row of figures <ref type="figure" target="#fig_6">(Figure 9g, 9h, 10g, 10h)</ref>; estimated displacement maps around regions of occlusion or severe deformation are often inaccurate.  . The module transforms C channels to C channels with an expansion factor p. DW-conv denotes a depth-wise convolution <ref type="bibr" target="#b13">[14]</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Overall architecture of the proposed approach. The model first takes T video frames as input and converts them into frame-wise appearance features using convolutional layers. The proposed MotionSqueeze (MS) module generates motion features using the frame-wise appearance features, and combines the motion features into the next downstream layer. ⊕ denotes element-wise addition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Top-1 accuracy and FLOPs with different patch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Top-1 accuracy and FLOPs with MS module on different backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(Fig. 6 :</head><label>6</label><figDesc>a) Label: "Pulling two ends of something so that it gets stretched." (b) Label: "Wiping off something of something." (c) Label: "Pull ups." (d) Label: "Skateboarding." Visualization on Something-Something-V1 (top) and Kinetics (bottom) datasets. RGB images, displacement maps, and the confidence maps are shown from the top row in each subfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>I3D (BN-Inception<ref type="bibr" target="#b14">[15]</ref>) backbone. A bottleneck(p, C ) module of MobileNet-V2<ref type="bibr" target="#b28">[29]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) "Moving sth. closer to sth.".(b) "Removing sth., revealing sth. behind".(c) "Moving sth. and sth. away from each other". (d) "Pouring sth. into sth.". (e) "Pretending to put sth. on a surface". (f) "Pulling two ends of sth. so that it gets stretched". (g) "Pretending to squeeze sth.". (h) "Tearing sth. into two pieces".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Visualization on Something-Something V1<ref type="bibr" target="#b9">[10]</ref> dataset. Video frames, displacement maps, and confidence maps are shown from the top row in each subfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 summarizes</head><label>1</label><figDesc></figDesc><table /><note>the results on Something-Something V1&amp;V2. Each section of the table contains results of 2D CNN methods</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on Something-Something V1&amp;V2. The symbol † denotes the reproduced by ours.</figDesc><table><row><cell>model</cell><cell cols="7">flow #frame FLOPs #param SomethingV1 SomethingV2</cell></row><row><cell></cell><cell></cell><cell>×clips</cell><cell></cell><cell cols="4">top-1 top-5 top-1 top-5</cell></row><row><cell>TSN [40]</cell><cell>8</cell><cell>16G×1</cell><cell cols="2">10.7M 19.5</cell><cell>-</cell><cell>33.4</cell><cell>-</cell></row><row><cell>TRN [47]</cell><cell>8</cell><cell cols="3">16G×N/A 18.3M 34.4</cell><cell>-</cell><cell>48.8</cell><cell>-</cell></row><row><cell>TRN Two-stream [47]</cell><cell cols="4">8+8 16G×N/A 18.3M 42.0</cell><cell>-</cell><cell>55.5</cell><cell>-</cell></row><row><cell>MFNet [20]</cell><cell>10</cell><cell>N/A×10</cell><cell>-</cell><cell cols="2">43.9 73.1</cell><cell>-</cell><cell>-</cell></row><row><cell>CPNet [22]</cell><cell>24</cell><cell>N/A×96</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">57.7 84.0</cell></row><row><cell>ECO En Lite [48]</cell><cell>92</cell><cell>267×1</cell><cell cols="2">150M 46.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">ECO Two-stream [48] 92+92 N/A×1</cell><cell cols="2">300M 49.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>I3D from [42]</cell><cell>32</cell><cell cols="4">153G×2 28.0M 41.6 72.2</cell><cell>-</cell><cell>-</cell></row><row><cell>NL-I3D from [42]</cell><cell>32</cell><cell cols="4">168G×2 35.3M 44.4 76.0</cell><cell>-</cell><cell>-</cell></row><row><cell>NL-I3D+GCN [42]</cell><cell>32</cell><cell cols="4">303G×2 62.2M 46.1 76.8</cell><cell>-</cell><cell>-</cell></row><row><cell>S3D-G [43]</cell><cell>64</cell><cell>71G×1</cell><cell cols="3">11.6M 48.2 78.7</cell><cell>-</cell><cell>-</cell></row><row><cell>DFB-Net [23]</cell><cell>16</cell><cell>N/A×1</cell><cell>-</cell><cell cols="2">50.1 79.5</cell><cell>-</cell><cell>-</cell></row><row><cell>STM [16]</cell><cell>16</cell><cell cols="6">67G×30 24.0M 50.7 80.4 64.2 89.8</cell></row><row><cell>TSM [21]</cell><cell>8</cell><cell>33G×1</cell><cell cols="5">24.3M 45.6 74.2 58.8 85.4</cell></row><row><cell>TSM [21]</cell><cell>16</cell><cell>65G×1</cell><cell cols="5">24.3M 47.3 77.1 61.2 86.9</cell></row><row><cell>TSM En [21]</cell><cell>16+8</cell><cell>98G×1</cell><cell cols="5">48.6M 49.7 78.5 62.9 88.1</cell></row><row><cell cols="8">TSM Two-stream [21] 16+16 129G×1 48.6M 52.6 81.9 65.0  † 89.4  †</cell></row><row><cell cols="4">TSM Two-stream [21] 16+16 129G×6 48.6M</cell><cell>-</cell><cell>-</cell><cell cols="2">66.0 90.5</cell></row><row><cell>MSNet-R50 (ours)</cell><cell>8</cell><cell>34G×1</cell><cell cols="5">24.6M 50.9 80.3 63.0 88.4</cell></row><row><cell>MSNet-R50 (ours)</cell><cell>16</cell><cell>67G×1</cell><cell cols="5">24.6M 52.1 82.3 64.7 89.4</cell></row><row><cell>MSNet-R50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>En (ours) 16+8 101G×1 49.2M 54.4 83.8 66.6 90.6 MSNet-R50 En (ours) 16+8 101G×10 49.2M 55.1 84.0 67.1 91.0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>summarizes comparative results with other motion representation methods<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> based on RGB frames. The comparison is done on Kinetics and HMDB51 since the previous methods commonly report their results on them. Each section of the table contains results of conventional 2D and 3D</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with motion representation methods. The symbol ‡ denotes that we only report the backbone FLOPs.</figDesc><table><row><cell>model</cell><cell cols="5">flow #frame FLOPs speed Kinetics HMDB51</cell></row><row><cell></cell><cell></cell><cell cols="3">×clips (V/s) Top-1</cell><cell>Top-1</cell></row><row><cell>ResNet-50 from [27]</cell><cell>32</cell><cell cols="2">132G×25 22.8</cell><cell>61.3</cell><cell>59.4</cell></row><row><cell>R(2+1)D [37]</cell><cell>32</cell><cell cols="2">152G×115 8.7</cell><cell>72.0</cell><cell>74.3</cell></row><row><cell>MFNet from [27]</cell><cell>10</cell><cell>80G  ‡ ×10</cell><cell>-</cell><cell>-</cell><cell>56.8</cell></row><row><cell>OFF(RGB) [34]</cell><cell>1</cell><cell>N/A×25</cell><cell>-</cell><cell>-</cell><cell>57.1</cell></row><row><cell>TVNet [7]</cell><cell>18</cell><cell>N/A×250</cell><cell>-</cell><cell>-</cell><cell>71.0</cell></row><row><cell>STM [16]</cell><cell>16</cell><cell>67G×30</cell><cell>-</cell><cell>73.7</cell><cell>72.2</cell></row><row><cell>Rep-flow (ResNet-50) [27]</cell><cell>32</cell><cell cols="2">132G  ‡ ×25 3.7</cell><cell>68.5</cell><cell>76.4</cell></row><row><cell>Rep-flow (R(2+1)D) [27]</cell><cell>32</cell><cell cols="2">152G  ‡ ×25 2.0</cell><cell>75.5</cell><cell>77.1</cell></row><row><cell>ResNet-50 Two-stream from [27]</cell><cell cols="3">32+32 264G×25 0.2</cell><cell>64.5</cell><cell>66.6</cell></row><row><cell>R(2+1)D Two-stream [37]</cell><cell cols="3">32+32 304G×115 0.2</cell><cell>73.9</cell><cell>78.7</cell></row><row><cell cols="3">OFF(RGB+Flow+RGB Diff) [34] 1+5+5 N/A×25</cell><cell>-</cell><cell>-</cell><cell>74.2</cell></row><row><cell>TSM (reproduced)</cell><cell>8</cell><cell cols="2">33G×10 64.1</cell><cell>73.5</cell><cell>71.9</cell></row><row><cell>MSNet-R50 (ours)</cell><cell>8</cell><cell cols="2">34G×10 54.2</cell><cell>75.0</cell><cell>75.8</cell></row><row><cell>MSNet-R50 (ours)</cell><cell>16</cell><cell cols="3">67G×10 31.2 76.4</cell><cell>77.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison with different displacement estimations.</figDesc><table><row><cell>model</cell><cell cols="2">FLOPs top-1</cell><cell>top-5</cell></row><row><cell>baseline</cell><cell>14.6G</cell><cell>41.5</cell><cell>71.8</cell></row><row><cell>S</cell><cell>14.8G</cell><cell>43.8</cell><cell>74.9</cell></row><row><cell>KS</cell><cell>14.9G</cell><cell>44.6</cell><cell>75.4</cell></row><row><cell>KS + CM</cell><cell>14.9G</cell><cell>45.5</cell><cell>76.5</cell></row><row><cell cols="2">KS + CM + BD 15.1G</cell><cell>46.0</cell><cell>76.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison with different positions of the MS module.</figDesc><table><row><cell>model</cell><cell cols="2">FLOPs top-1</cell><cell>top-5</cell></row><row><cell>baseline</cell><cell>14.6G</cell><cell>41.5</cell><cell>71.8</cell></row><row><cell>res 2</cell><cell>15.6G</cell><cell>45.1</cell><cell>76.1</cell></row><row><cell>res 3</cell><cell>14.9G</cell><cell>45.5</cell><cell>76.5</cell></row><row><cell>res 4</cell><cell>14.7G</cell><cell>42.6</cell><cell>73.2</cell></row><row><cell>res 5</cell><cell>14.6G</cell><cell>41.1</cell><cell>71.8</cell></row><row><cell>res 2,3,4</cell><cell>16.0G</cell><cell>45.7</cell><cell>76.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison with different fusing strategies.</figDesc><table><row><cell>model</cell><cell cols="2">FLOPs top-1</cell><cell>top-5</cell></row><row><cell>baseline</cell><cell>14.6G</cell><cell>41.5</cell><cell>71.8</cell></row><row><cell>MS only</cell><cell>14.1G</cell><cell>38.8</cell><cell>70.7</cell></row><row><cell>multiply</cell><cell>14.9G</cell><cell>44.5</cell><cell>75.9</cell></row><row><cell>concat.</cell><cell>15.7G</cell><cell>45.0</cell><cell>76.1</cell></row><row><cell>add</cell><cell>14.9G</cell><cell>45.5</cell><cell>76.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison with twostream networks.</figDesc><table><row><cell>model</cell><cell>flow FLOPs top-1 top-5</cell></row><row><cell>baseline</cell><cell>14.6G 41.5 71.8</cell></row><row><cell>Two-stream 8+(8×5)</cell><cell>31.4G 46.8 77.3</cell></row><row><cell>Two-stream 8+(8×1)</cell><cell>28.9G 44.7 75.2</cell></row><row><cell>Two-stream 8+(8×1)(low)</cell><cell>28.9G 44.1 74.9</cell></row><row><cell>MSNet</cell><cell>14.9G 45.5 76.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Performance comparison between the CP module<ref type="bibr" target="#b21">[22]</ref> and the MS module.</figDesc><table><row><cell>model</cell><cell cols="2">FLOPs Top-1</cell><cell>Top-5</cell></row><row><cell>baseline</cell><cell>14.6G</cell><cell>41.5</cell><cell>71.8</cell></row><row><cell>CP module [22]</cell><cell>20.4G</cell><cell>44.9</cell><cell>75.6</cell></row><row><cell>MS module</cell><cell>15.0G</cell><cell>45.8</cell><cell>76.4</cell></row><row><cell cols="4">accuracy, respectively, while consuming 26% less FLOPs, compared to the CP</cell></row><row><cell>module.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>ResNet &amp; TSM ResNet backbone.</figDesc><table><row><cell>Layers</cell><cell>ResNet-18</cell><cell></cell><cell>TSM ResNet-18</cell><cell></cell><cell cols="2">ResNet-50</cell><cell cols="2">TSM ResNet-50 Output size</cell></row><row><cell>conv1</cell><cell></cell><cell></cell><cell cols="3">1×7×7, 64, stride 1,2,2</cell><cell></cell><cell></cell><cell>T×112×112</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">1×3×3 max pool, stride 2</cell><cell></cell><cell></cell></row><row><cell>res2</cell><cell>1×3×3, 64 1×3×3, 64</cell><cell>×2</cell><cell>  1×3×3, 64 TSM 1×3×3, 64   ×2</cell><cell> </cell><cell cols="2"> 1×1×1, 256 1×3×3, 256 1×1×1, 256  ×3</cell><cell> 1×1×1, 256  TSM    1×1×1, 256 1×3×3, 256    ×3</cell><cell>T×56×56</cell></row><row><cell>res3</cell><cell>1×3×3, 128 1×3×3, 128</cell><cell>×2</cell><cell>  1×3×3, 128 TSM 1×3×3, 128   ×2</cell><cell> </cell><cell cols="2"> 1×1×1, 512 1×3×3, 512 1×1×1, 512  ×4</cell><cell cols="2"> 1×1×1, 512  TSM    1×1×1, 512 1×3×3, 512    ×4 T×28×28</cell></row><row><cell>res4</cell><cell>1×3×3, 256 1×3×3, 256</cell><cell>×2</cell><cell>  1×3×3, 256 TSM 1×3×3, 256   ×2</cell><cell cols="2">  1×1×1, 1024 1×3×3, 1024 1×1×1, 1024</cell><cell>  ×6</cell><cell cols="2"> 1×1×1, 1024  TSM    1×1×1, 1024 1×3×3, 1024    ×6 T×14×14</cell></row><row><cell>res5</cell><cell>1×3×3, 512 1×3×3, 512</cell><cell>×2</cell><cell>  1×3×3, 512 TSM 1×3×3, 512   ×2</cell><cell cols="2">  1×1×1, 2048 1×3×3, 2048 1×1×1, 2048</cell><cell>  ×3</cell><cell cols="2"> 1×1×1, 2048  TSM    1×1×1, 2048 1×3×3, 2048    ×3 T×7×7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">global average pool, FC</cell><cell></cell><cell></cell><cell># of classes</cell></row><row><cell cols="4">(a) A 3D Inception module of</cell><cell></cell><cell cols="4">(b) I3D [2] architecture.</cell></row><row><cell>I3D [2]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>MobileNet-V2 backbone. Bottleneck modules in Figure 8 are applied to the backbone.</figDesc><table><row><cell>Layers</cell><cell>MobileNet-V2</cell><cell>Output size</cell></row><row><cell cols="3">stage1 1×7×7, 32, stride 1,2,2 T×112×112</cell></row><row><cell>stage2</cell><cell>bottleneck(1,16) bottleneck(6,24) × 2</cell><cell>T×56×56</cell></row><row><cell cols="2">stage3 bottleneck(6,32) × 3</cell><cell>T×28×28</cell></row><row><cell>stage4</cell><cell>bottleneck(6,64) × 4 bottleneck(6,96) × 3</cell><cell>T×14×14</cell></row><row><cell></cell><cell>bottleneck(6,160) × 3</cell><cell></cell></row><row><cell>stage5</cell><cell>bottleneck(6,320)</cell><cell>T×7×7</cell></row><row><cell></cell><cell>1×1×1, 1280, stride 1,1,1</cell><cell></cell></row><row><cell cols="2">global average pool, FC</cell><cell># of classes</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* https://github.com/xingyul/cpnet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-fiber networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 7</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 7</meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving landmark localization with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV) (2019) 3, 8</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV) (2019) 3, 8</meeting>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 3, 4</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 3, 4</meeting>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning video representations from correspondence proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 4, 8, 9</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 4, 8, 9</meeting>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action recognition with spatialtemporal discriminative filter banks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Winter Conference on Applications of Computer Vision</title>
		<meeting>Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 1, 2, 3</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 1, 2, 3</meeting>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the integration of optical flow and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Güney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. German Conference on Pattern Recognition</title>
		<meeting>German Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">D3d: Distilled 3d networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video classification with channelseparated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng-Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deepconvolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV) (2016) 1, 3</title>
		<meeting>European Conference on Computer Vision (ECCV) (2016) 1, 3</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recognize actions by disentangling components of dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Trajectory convolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems</title>
		<meeting>Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Cleaning pool</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Playing trombone&quot;. (d) &quot;Sharpening pencil</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Capoeira&quot;. (f)</title>
		<imprint/>
	</monogr>
	<note>Deadlifting</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<title level="m">Salsa dancing&quot;. (h) &quot;Washing feet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">10: Visualization on Kinetics-400 [17] dataset. Video frames, displacement maps, and confidence maps are shown from the top row in each subfigure</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
