<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised NMF Models for Topic Modeling in Learning Tasks 1</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Haddock</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lara</forename><surname>Kassab</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixian</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alona</forename><surname>Kryshchenko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Grotheer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Sizikova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuntian</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Merkh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W M A</forename><surname>Madushani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miju</forename><surname>Ahn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deanna</forename><surname>Needell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Leonard</surname></persName>
						</author>
						<title level="a" type="main">Semi-supervised NMF Models for Topic Modeling in Learning Tasks 1</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>semi-supervised nonnegative matrix factor- ization</term>
					<term>maximum likelihood estimation</term>
					<term>multiplicative updates</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose several new models for semi-supervised nonnegative matrix factorization (SSNMF) and provide motivation for SSNMF models as maximum likelihood estimators given specific distributions of uncertainty. We present multiplicative updates training methods for each new model, and demonstrate the application of these models to classification, although they are flexible to other supervised learning tasks. We illustrate the promise of these models and training methods on both synthetic and real data, and achieve high classification accuracy on the 20 Newsgroups dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Frequently, one is faced with the problem of performing a (semi-)supervised learning task on extremely highdimensional data which contains redundant information. A common approach is to first apply a dimensionalityreduction or feature extraction technique (e.g., PCA <ref type="bibr" target="#b33">[34]</ref>), and then train the model for the learning task on the new, learned representation of the data. One problematic aspect of this two-step approach is that the learned representation of the data may provide "good" fit, but could suppress data features which are integral to the learning task <ref type="bibr" target="#b17">[18]</ref>. For this reason, supervisionaware dimensionality-reduction models have become increasingly important in data analysis; such models aim to use supervision in the process of learning the lowerdimensional representation, or even learn this representation alongside the supervised learning model <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>In this work, we propose new semi-supervised nonnegative matrix factorization (SSNMF) formulations which provide a dimensionality-reducing topic model and a model for a supervised learning task. Our contributions are:</p><p>• we motivate these proposed SSNMF models and that of <ref type="bibr" target="#b27">[28]</ref> as maximum likelihood estimators (MLE) given specific models of uncertainty in the observations;</p><p>• we derive multiplicative updates for the proposed models that allow for missing data and partial supervision;</p><p>• we perform experiments on synthetic and real data which illustrate the promise of these models in both topic modeling and supervised learning tasks; and</p><p>• we demonstrate the promise of SSNMF models for classification relative to the performance of other classifiers on a common benchmark data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Organization</head><p>The paper is organized as follows. We begin by briefly defining notation in Section 1.2 and provide a preliminary review of the fundamental models, nonnegative matrix factorization (NMF) <ref type="bibr" target="#b25">[26]</ref> and SSNMF <ref type="bibr" target="#b27">[28]</ref> in Section 1.3, other related work in Section 1.4, and the proposed models in Section 1.5. We motivate the proposed models and that of <ref type="bibr" target="#b27">[28]</ref> via MLE in Section 2.1, present the multiplicative update methods for training in Section 2.2, and present details of a framework for classification with these models in Section 2.3. We present experimental evidence illustrating the promise of the SSNMF models in Section 3, including experiments on synthetic data in Section 3.1 which are motivated by the MLE of Section 2.1, and experiments on the 20 Newsgroup dataset in Section 3.2. Finally, we end with some conclusions and discussion of future work in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Notation</head><p>Our models make use of two matrix similarity measures. The first is the standard Frobenius norm, A − B F . The second is the information divergence or I-divergence, a measure defined between nonnegative matrices A and B,</p><formula xml:id="formula_0">D(A B) = i,j Aij log Aij Bij − Aij + Bij ,<label>(1)</label></formula><p>where D(A B) ≥ 0 with equality if and only if A = B <ref type="bibr" target="#b26">[27]</ref>. Because the information divergence reduces to the Kullback-Leibler divergence when A and B represent probability distributions, i.e., Aij = Bij = 1, it is often referred to as the generalized Kullback-Leibler divergence <ref type="bibr" target="#b11">[12]</ref>.</p><p>In the following, A/B indicates element-wise division, A B indicates element-wise multiplication, and AB denotes standard matrix multiplication. We denote the set of non-zero indices of a matrix by supp(A) := {(i, j) : Aij = 0}. When an n1 × n2 matrix is to be restricted to have only nonnegative entries, we write A ≥ 0 and A ∈ R n 1 ×n 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>≥0</head><p>. We let 1 k denote the length-k vector consisting of ones, 1 k = 1, · · · 1 ∈ R k , and similarly 0 k denotes the vector of all zeros, 0 k = 0, · · · 0 ∈ R k . We let N z µ, σ 2 denote the Gaussian density function for a random variable z with mean µ and variance σ 2 , and PO (z|ν) denotes the Poisson density function for a random variable z with nonnegative intensity parameter ν.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Preliminaries</head><p>In this section, we give a brief overview of the NMF and SSNMF methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nonnegative Matrix Factorization</head><p>Given a nonnegative matrix X ∈ R n 1 ×n 2 ≥0 and a target dimension r ∈ N, NMF decomposes X into a product of two low-dimensional nonnegative matrices. The model seeks A and S so that X ≈ AS, where A ∈ R n 1 ×r ≥0 is called the dictionary matrix and S ∈ R r×n 2 ≥0 is called the representation matrix. Typically, r is chosen such that r &lt; min{n1, n2} to reduce the dimension of the original data matrix or reveal latent themes in the data. Data points are typically stored as columns of X, thus n1 represents the number of features, and n2 represents the number of samples. The columns of A are generally referred to as topics, which are characterized by features of the data set. Each column of S provides the approximate representation of the respective column in X in the lowerdimensional space spanned by the columns of A. Thus, the data points are well approximated by an additive linear combination of the latent topics.</p><p>Several formulations for this nonnegative approximation, X ≈ AS, have been studied <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">46]</ref>; e.g., argmin A≥0,S≥0 X − AS 2 F and argmin A≥0,S≥0</p><formula xml:id="formula_1">D(X AS),<label>(2)</label></formula><p>where D(· ·) is the information divergence defined in <ref type="bibr" target="#b0">(1)</ref>.</p><p>In what follows, we refer to the left formulation of (2) as · F -NMF and the right formulation of (2) as D(· ·)-NMF. We refer the reader to <ref type="bibr" target="#b8">[9]</ref> for discussions of similarity measures and generalized divergences (where information divergence is a particular case), and <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref> for generalized nonnegative matrix approximations with Bregman divergences.</p><p>Multiplicative update algorithms for both formulations of (2) have been proposed <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. These algorithms are widely adopted because they are easy to implement, do not require user-specified hyperparameters, preserve the nonnegativity constraints, and have desirable monotonicity properties <ref type="bibr" target="#b26">[27]</ref>. Other popular algorithms include projected gradient descent and alternating least-squares <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>NMF has gained popularity recently due to large scale data demands of applications such as document clustering <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b32">33]</ref>, image processing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>, financial data mining <ref type="bibr" target="#b9">[10]</ref>, audio processing <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>, and genetics <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised NMF</head><p>SSNMF is a modification of NMF to jointly incorporate a data matrix and a (partial) class label matrix. Given a data matrix X ∈ R n 1 ×n 2 ≥0 and a class label matrix Y ∈</p><formula xml:id="formula_2">R k×n 2 ≥0 , SSNMF is defined by argmin A,S,B≥0 W (X − AS) 2 F Reconstruction Error +λ L (Y − BS) 2 F Classification Error ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">A ∈ R n 1 ×r ≥0 , B ∈ R k×r ≥0 , S ∈ R r×n 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>≥0</head><p>, and the regularization parameter λ &gt; 0 governs the relative importance of the supervision term <ref type="bibr" target="#b27">[28]</ref>. See <ref type="figure" target="#fig_2">Figure 1</ref> for an illustration of the SSNMF model. We denote this objective function as F1(A, B, S; X, Y). The binary weight matrix W accommodates missing data by indicating observed and unobserved data entries (that is, Wij = 1 if Xij is observed, and Wij = 0 otherwise). Similarly, L ∈ R k×n 2 is a weight matrix that indicates the presence or absence of a label (that is, L:,j = 1 k if the label of X:,j is known, and L:,j = 0 k otherwise). Multiplicative updates have been previously developed for SSNMF for the Frobenius norm, and the resulting performance of clustering and classification is improved by incorporating data labels into NMF <ref type="bibr" target="#b27">[28]</ref>. To differentiate this model from the proposed SSNMF models, we refer to the model defined by (3) as ( · F , · F )-SSNMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Related Work</head><p>In this section, we describe related work most relevant to our own. This is not meant to be a comprehensive study of these areas. We focus on work in three main areas: statistical motivation for NMF models, models for simultaneous dimension reduction and supervised learning, and semi-supervised and joint NMF models. <ref type="figure" target="#fig_2">Figure 1</ref>: Given the number of classes k, and a desired dimension r, SSNMF is formulated as a joint factorization of a data matrix X ∈ R n1×n2 ≥0 and a label matrix Y ∈ R k×n2 ≥0 , sharing representation factor S ∈ R r×n2 ≥0 .</p><formula xml:id="formula_4">X A S Y B S ≈ ≈ n 1 × n 2 n 1 × r r × n 2 k × n 2 k × r r × n 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Motivation for NMF</head><p>The most common discrepancy measures for NMF are the Frobenius norm and the information divergence. One reason for this popularity is that · F -NMF and D(· ·)-NMF correspond to the MLE given an assumed latent generative model and a Gaussian and Poisson model of uncertainty, respectively <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41]</ref>. In <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">41]</ref>, the authors go further towards a Bayesian approach, introduce application-appropriate prior distributions on the latent factors, and apply maximum a posteriori (MAP) estimation. Additionally, under certain conditions, D(· ·)-NMF is equivalent to probabilistic latent semantic indexing <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimension Reduction and Learning</head><p>There has been much work developing dimensionalityreduction models that are supervision-aware. Semisupervised clustering makes use of known label information or other supervision and the data features while forming clusters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref>. These techniques generally make use of label information in the cluster initialization or during cluster updating via must-link and cannotlink constraints; empirically, these approaches are seen to increase mutual information between computed clusters and user-assigned labels <ref type="bibr" target="#b0">[1]</ref>. Semi-supervised feature extraction makes use of supervision information in the feature extraction process <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39]</ref>. These approaches are generally filter -or wrapper -based approaches, and distinguished by their underlying supervision type <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised and Joint NMF</head><p>Since the seminal work of Lee et al. <ref type="bibr" target="#b27">[28]</ref>, semi-supervised NMF models have been studied in a variety of settings. The works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref> propose models which exploit cannot-link or must-link supervision. In <ref type="bibr" target="#b6">[7]</ref>, the authors introduce a model with information divergence penalties on the reconstruction and on supervision terms which influence the learned factorization to approximately reconstruct coefficients learned before factorization by a support-vector machine (SVM). Several <ref type="table">Table 1</ref>: Overview of NMF and SSNMF models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Objective · F -NMF <ref type="bibr" target="#b25">[26]</ref> argmin</p><formula xml:id="formula_5">A,S≥0 X − AS 2 F D(· ·)-NMF [27] argmin A,S≥0 D(X AS) ( · F , · F )-SSNMF [28] argmin A,B,S≥0 W (X − AS) 2 F + λ L (Y − BS) 2 F ( · F , D(· ·))-SSNMF argmin A,B,S≥0 W (X − AS) 2 F + λD(L Y L BS) (D(· ·), · F )-SSNMF argmin A,B,S≥0 D(W X W AS) + λ L (Y − BS) 2 F (D(· ·), D(· ·))-SSNMF argmin A,B,S≥0 D(W X W AS) + λD(L Y L BS)</formula><p>works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> propose a supervised NMF model that incorporates Fisher discriminant constraints into NMF for classification. Furthermore, joint factorization of two data matrices, like that of SSNMF, is described more generally and denoted Simultaneous NMF in <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Overview of Proposed Models</head><p>Our proposed models generalize NMF to supervised learning tasks and provide a topic model which simultaneously provides a lower dimensional representation of the data and a predictive model for targets. We denote the data matrix as X ∈ R n 1 ×n 2 ≥0 and the supervision matrix as Y ∈ R k×n 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>≥0</head><p>. Following along with <ref type="bibr" target="#b27">[28]</ref>, the data observations are the columns of X and the associated targets (e.g., labels) are the columns of Y. Our models seek</p><formula xml:id="formula_6">A ∈ R n 1 ×r ≥0 , S ∈ R r×n 2 ≥0</formula><p>, and B ∈ R k×r ≥0 which jointly factorize X and Y; that is X ≈ AS and Y ≈ BS. We point out the simple fact that these joint factorizations can be stacked into a single NMF (visualized in <ref type="figure" target="#fig_2">Figure 1</ref>)</p><formula xml:id="formula_7">X Y ≈ A B S.<label>(4)</label></formula><p>In each model, the matrix A ∈ R n 1 ×r ≥0 provides a basis for the lower-dimensional space, S ∈ R r×n 2 ≥0 provides the coefficients representing the projected data in this space, and B ∈ R k×r ≥0 provides the supervision model which predicts the targets given the representation of points in the lower-dimensional space. We allow for missing data and labels or confidence-weighted errors via the dataweighting matrix W ∈ R n 1 ×n 2 ≥0 and the label-weighting matrix L ∈ R k×n 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>≥0</head><p>. Each resulting joint-factorization model is defined by the error functions applied to the reconstruction and supervision factorization terms. We denote the model   <ref type="bibr" target="#b4">5)</ref> as (R(·, ·), S(·, ·))-SSNMF for specific choices of R and S. Here, R(·, ·) and S(·, ·) are the error functions applied to the reconstruction term and supervision term, respectively. For clarity, we include <ref type="table">Table 1</ref> below which summarizes existing and proposed models, where each proposed model is of the form (5) for specific choices of error functions R and S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SSNMF Models: Motivation and Methods</head><p>In this section, we present a statistical MLE motivation of several variants of the SSNMF model, introduce the general semi-supervised models, and provide a multiplicative updates method for each variant. While historically the focus of SSNMF studies have been on classification <ref type="bibr" target="#b27">[28]</ref>, we highlight that this joint factorization model can be applied quite naturally to regression tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Maximum Likelihood Estimation</head><p>In this section, we demonstrate that specific forms of our proposed variants of SSNMF are maximum likelihood estimators for given models of uncertainty or noise in the data matrices X and Y. These different uncertainty models have their likelihood function maximized by different error functions chosen for reconstruction and supervision errors, R and S. We summarize these results next; each MLE derived is a specific instance of a general model discussed in Section 2.2 or in <ref type="bibr" target="#b27">[28]</ref>.</p><p>Maximum Likelihood Estimators Suppose that the observed data X and supervision information Y have entries given as the sum of random variables,</p><formula xml:id="formula_8">Xγ,τ = r i=1 xγ,i,τ and Yη,τ = r i=1 yη,i,τ ,</formula><p>and that the set of Xγ,τ and Yη,τ are statistically independent conditional on A, B, and S. respectively, the maximum likelihood estimator is</p><formula xml:id="formula_9">argmin A,B,S≥0 X − AS 2 F + σ1 σ2 Y − BS 2 F . 2.</formula><p>When xγ,i,τ and yη,i,τ have distributions N (xγ,i,τ |Aγ,iSi,τ , σ1) and PO (yη,i,τ |Bη,iSi,τ ) respectively, the maximum likelihood estimator is</p><formula xml:id="formula_10">argmin A,B,S≥0 X − AS 2 F + 2rσ1D(Y BS).</formula><p>3. When xγ,i,τ and yη,i,τ have distributions</p><formula xml:id="formula_11">PO (xγ,i,τ |Aγ,iSi,τ ) and N (yη,i,τ |Bη,iSi,τ , σ2)</formula><p>respectively, the maximum likelihood estimator is </p><formula xml:id="formula_12">argmin A,B,S≥0 D(X AS) + 1 2rσ2 Y − BS 2 F .</formula><formula xml:id="formula_13">D(X AS) + D(Y BS).</formula><p>We note that 4 follows from <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41]</ref>, but the others are distinct from previous MLE derivations due to the difference in the distributions assumed on data X and supervision Y. Here, we provide only the MLE derivation for 2 as the other derivations are similar; these are included in the appendix for completeness. We demonstrate that the MLE, in the case that the uncertainty on X is Gaussian distributed and on Y is Poisson distributed, is a specific instance of the ( · F , D(· ·))-SSNMF model.</p><p>Our models for the distribution of observed entries of X and Y assume that the mean is given by E[X] = AS and E[Y] = BS, and the uncertainty in the set of observations in X is governed by a Gaussian distribution while the set in Y is governed by a Poisson distribution. That is, we consider hierarchical models for X and Y where</p><formula xml:id="formula_14">Xγ,τ = r i=1 xγ,i,τ and xγ,i,τ ∼ N (xγ,i,τ |Aγ,iSi,τ , σ1) , Yη,τ = r i=1 yη,i,τ and yη,i,τ ∼ PO (yη,i,τ |Bη,iSi,τ ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note then that</head><formula xml:id="formula_15">Xγ,τ ∼ N Xγ,τ r i=1 A γ,i S i,τ , rσ 1 , and Yη,τ ∼ PO Yη,τ r i=1 B η,i S i,τ</formula><p>due to the summable property of Gaussian and Poisson random variables. We note that this assumes different distributions on the two collections of rows of the NMF (4), with Gaussian and Poisson models of uncertainty.</p><p>Assuming that the set of Xγ,τ and Yη,τ are statistically independent conditional on A, B, and S, we have that the likelihood p(X, Y|A, B, S) is</p><formula xml:id="formula_16">γ,τ N Xγ,τ r i=1 A γ,i S i,τ , rσ 1 η,τ PO Yη,τ r i=1 B η,i S i,τ .<label>(6)</label></formula><p>We apply the monotonic natural logarithmic function to the likelihood and ignore terms that are invariant with the factor matrices. This transforms the likelihood into a ( · F , D(· ·))-SSNMF objective while preserving the maximizer. That is, the log likelihood (excluding additive terms which are constant with respect to A, B, and S) is</p><formula xml:id="formula_17">ln p (X, Y|A, B, S) = + − 1 2rσ 1 γ,τ Xγ,τ − r i=1 A γ,i S i,τ 2 − η,τ (BS)η,τ − Yη,τ log(BS)η,τ + log Γ (Yη,τ + 1) = + − 1 2rσ 1 X − AS 2 F + 2rσ 1 D(Y BS) .</formula><p>Here, = + denotes suppression of additive terms that do not depend upon A, B, and S. Thus, the maximum likelihood estimators for A, B, and S are given by argmin A,B,S≥0</p><formula xml:id="formula_18">X − AS 2 F + 2rσ1D(Y BS).</formula><p>We see that the MLE in the case of Gaussian uncertainty on the observations in X and Poisson uncertainty on the observations in Y, is a specific instance of the ( · F , D(· ·))-SSNMF objective where the regularization parameter λ is a multiple of the variance of the Gaussian distribution. The other MLEs are derived similarly; see Appendix A.</p><p>An instance of each of the models in <ref type="table">Table 1</ref> are MLE for a given model of uncertainty in the observed data X and supervision Y. While this motivates our exploration of these models, we present them in more general context next and provide training methods for the general form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">General Models and Mult. Updates</head><p>Recall that ( · F , · F )-SSNMF is defined by <ref type="formula" target="#formula_2">(3)</ref> and multiplicative updates are derived in <ref type="bibr" target="#b27">[28]</ref>. Now, we propose the general form of ( · F , D(· ·))-SSNMF, (D(· ·), · F )-SSNMF, and (D(· ·), D(· ·))-SSNMF and present multiplicative updates methods for each model. These three models are novel forms of SSNMF, and besides their statistical motivation via MLE, we demonstrate their promise experimentally in Section 3. As in <ref type="bibr" target="#b27">[28]</ref>, our multiplicative updates methods allow for missing (or certainty-weighted) data and missing (or certainty-weighted) supervision information via matrices W and L, which represent our knowledge or certainty of the corresponding entries of X and Y, respectively. When W is a matrix of all ones (or more generally has all equal entries) and L is a matrix of all zeros, the SSNMF models reduce to either the · F -NMF or D(· ·)-NMF. The SSNMF model is fully supervised when supp(Y) ⊂ supp(L) and Y contains supervision information for each element in X.</p><p>The first proposed semi-supervised NMF model is ( · F , D(· ·))-SSNMF, which is defined by the solution to argmin A,B,S≥0</p><formula xml:id="formula_19">W (X − AS) 2 F + λD(L Y L BS). (7)</formula><p>We denote this objective function as F2(A, B, S; X, Y).</p><p>Similar to the previous SSNMF model, this model seeks a joint factorization of the data matrix X and target matrix Y; however, the error functions applied to the reconstruction and classification terms in the objective differ. The multiplicative updates for this model are provided in Algorithm 1. We provide intuition for the derivation of only this method as the others are similar. The multiplicative updates for A, B, and S which minimize <ref type="bibr" target="#b6">(7)</ref> are derived as follows. The gradient of the objective function of (7) with respect to A, B and S are, respectively,</p><formula xml:id="formula_20">∇ A F2 = −2[W (X − AS)]S , ∇ B F2 = LS − L Y L BS L S , and ∇ S F2 = λB L − λB L Y L BS L − 2A [W (X−AS)].</formula><p>Algorithm 1 ( · F , D(· ·))-SSNMF mult. updates</p><formula xml:id="formula_21">Input: X, W ∈ R n1×n2 ≥0 , Y, L ∈ R k×n2 ≥0 , r, λ, N 1: Initialize A ∈ R n1×r ≥0 , S ∈ R r×n2 ≥0 , B ∈ R k×r ≥0 2: for i = 1, ..., N do 3: A ← A (W X)S (W AS)S 4: B ← B LS (L Y) (L BS) L S 5: S ← S 2A (W X)+λB [ (L Y) (L BS) L] 2A (W AS)+λB L</formula><p>The multiplicative updates method, Algorithm 1, can be viewed as an entrywise gradient descent method, where the stepsizes are chosen individually for each entry of the updating matrix to ensure nonnegativity. That is, the updates in Algorithm 1 are given by</p><formula xml:id="formula_22">A → A − Γ ∇ A F2 when Γ = A 2(W AS)S , B → B − Γ ∇ B F2 when Γ = B LS , and S → S − Γ ∇ S F2 when Γ = S 2A (W AS) + λB L . The next proposed semi-supervised NMF model is (D(· ·), · F )-SSNMF, defined by the solution to argmin A,B,S≥0 D(W X W AS) + λ L (Y − BS) 2 F . (8)</formula><p>We denote this objective function as F3(A, B, S; X, Y). Again, this model seeks a joint factorization of the data matrix X and target matrix Y; here the reconstruction error is penalized by the information divergence, while the supervision error is penalized by the Frobenius norm.</p><p>Multiplicative updates for this model are provided in Algorithm 2.</p><p>The third, and final, proposed semi-supervised NMF model is (D(· ·), D(· ·))-SSNMF, defined by the solution to argmin A,B,S≥0</p><formula xml:id="formula_23">D(W X W AS) + λD(L Y L BS). (9)</formula><p>We denote this objective function as F4(A, B, S; X, Y). Again, this model seeks a joint factorization of the data matrix X and target matrix Y; here both the reconstruction error and supervision error are penalized by the information divergence error function. The multiplicative updates for this model are provided in Algorithm 3.</p><p>As previously stated in Section 2.1, an instance of each family of models, ( · F , · F )-SSNMF, ( · F , D(· ·))-SSNMF, (D(· ·), · F )-SSNMF, and (D(· ·), D(· ·))-SSNMF, correspond to the MLE in the case that the data X and supervision Y are sampled from specific distributions with mean given by a latent lower-dimensional factorization model. One might expect that each model is most appropriately applied when the associated model of uncertainty is a reasonable assumption (i.e., one has a Algorithm 2 (D(· ·), · F )-SSNMF mult. updates</p><formula xml:id="formula_24">Input: X, W ∈ R n1×n2 ≥0 , Y, L ∈ R k×n2 ≥0 , r, λ, N 1: Initialize A ∈ R n1×r ≥0 , S ∈ R r×n2 ≥0 , B ∈ R k×r ≥0 2: for i = 1, ..., N do 3: A ← A WS (W X) (W AS) W S 4: B ← B (L Y)S (L BS)S 5: S ← S A [ (W X) (W AS) W]+2λB (L Y) A W+2λB (L BS) Algorithm 3 (D(· ·), D(· ·))-SSNMF mult. updates Input: X, W ∈ R n1×n2 ≥0 , Y, L ∈ R k×n2 ≥0 , r, λ, N 1: Initialize A ∈ R n1×r ≥0 , S ∈ R r×n2 ≥0 , B ∈ R k×r ≥0 2: for i = 1, ..., N do 3: A ← A WS (W X) (W AS) W S 4: B ← B LS (L Y) (L BS) L S 5: S ← S A [ (W X) (W AS) W]+λB [ (L Y) (L BS) L] A W+λB L</formula><p>priori information indicating so). For example, we expect that the Gaussian uncertainty assumption associated to ( · F , · F )-SSNMF is likely most appropriate when the supervised learning task is a regression task, and likely not as appropriate for a classification task where the targets have discrete form.</p><p>We note that the iteration complexity of each of these methods scales with complexity of multiplication of matrices of size n1 × max{k, r} and max{k, r} × n2. In our implementation of each of these methods, we ensure that there is no division by zero by adding a small positive value to all entries of divisors. Implementation of these methods and code for experiments is available in the Python package SSNMF <ref type="bibr" target="#b18">[19]</ref>. Finally, we note that the behavior of these models and methods are highly dependent on the hyperparameters r, λ, and N . One can select the parameters according to a priori information or use a heuristic selection technique; we use both and indicate selected parameters and method of selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification Framework</head><p>Here we describe a framework for using any of the SS-NMF models for classification tasks. Given training data Xtrain (with any missing data indicated by matrix Wtrain) and labels Ytrain, and testing data Xtest (with unknown data indicated by matrix Wtest), we first train our (R(· ·), S(· ·))-SSNMF model to obtain learned dictionaries Atrain and Btrain. We then use these learned matrices to obtain the representation of test data in the subspace spanned by Atrain, Stest, and the predicted labels for the test data Ytest. This process is: 3. Compute predicted labels asŶtest = label(BtrainStest), where label(·) assigns the largest entry of each column to 1 and all other entries to 0.</p><p>In step 1, we compute Atrain, Btrain, and Strain using implementations of the multiplicative updates methods described above. In step 2, we use either a nonnegative least-squares method (if R = · F ) or one-sided multiplicative updates only updating Stest (if R = D(· ·)). We note that this framework is significantly different than the classification framework proposed in <ref type="bibr" target="#b27">[28]</ref>; in particular, we use the classifier B learned by SSNMF, rather than independent SVM trained on the SSNMF-learned lower-dimensional representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Numerical Experiments</head><p>In this section, we present numerical experiments of the proposed models applied to both synthetic and real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synthetic Data Experiments</head><p>We expect that, for each pair of distributions of uncertainty, the MLE model derived in Section 2.1 will produce larger likelihood (i.e., smaller relative error) than any other SSNMF model. To confirm this hypothesis, we generate synthetic data according to the four distributions in Section 2.1, train the SSNMF models, and measure the relative error (error is negative likelihood).</p><p>Through all experiments, we use the same factor matrices A ∈ R 500×5 , S ∈ R 5×500 , and B ∈ R 500×5 . Here A is generated with all entries sampled from the uniform distribution on [0, 1], and S and B are generated as sparse random matrices with density 0.5 (support sampled uniformly amongst matrix entries) with nonzero entries sampled from the uniform distribution on [0, 1]. In each experiment, we train each of the SSNMF models with rank r = 5 and λ = 1 by running N = 100000 iterations of the multiplicative updates of <ref type="bibr" target="#b27">[28]</ref> and Algorithms 1, 2, and 3 using all data and supervision; each algorithm is initialized with the same matrices A (0) , B (0) , and S (0) and we denote the resulting approximate factor matrices as A (N ) , B (N ) , and S (N ) . We present relative errors averaged over five trials (independent initializations of A (0) , B (0) , and S (0) ). Here, we let the relative error for objective function F be .</p><p>In our first experiment, we generate data Xij ∼ N (Xij|(AS)ij, 1) and supervision Yij ∼ All results are given in <ref type="table" target="#tab_2">Table 2</ref>; lowest average relative errors are listed in bold for all experiments. Note that in each experiment, as expected, the MLE model produces smaller relative error than other SSNMF models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">20 Newsgroups Data Experiments</head><p>The 20 Newsgroups data set <ref type="bibr" target="#b36">[37]</ref> is a collection of approximately 20,000 newsgroup documents. <ref type="bibr" target="#b0">1</ref> The data set consists of six groups partitioned roughly according to subjects, with a total of 20 subgroups, and is commonly used as an experimental benchmark for document classification and clustering; see e.g., <ref type="bibr" target="#b27">[28]</ref>. Here, we compare classification accuracy, unlike in Section 3.1 where we compare reconstruction errors.</p><p>We consider a subset of the data set, summarized in <ref type="table" target="#tab_3">Table 3</ref>. We treat the groups as classes and assign them labels, and we treat the subgroups as (un-labeled) latent topics in the data. We remove headers, footers, and quotes from all documents, subsample the data set to obtain a balanced data set across classes (1796 document per class), and split the data set into train (60%), validation (20%), and test (20%) sets. We compute the term frequency-inverse document frequency (TF-IDF) representation for documents using TFIDFVectorizer <ref type="bibr" target="#b34">[35]</ref>. The NLTK English stopword list <ref type="bibr" target="#b2">[3]</ref>, and words appearing in less than 5 documents, or more than 70% of the documents were removed. We use the tokenizer [a-zA-Z]+ and limit the vocabulary size to 5000. We compare to the linear Support Vector Machine (SVM) classifier and Multinomial Naive Bayes (NB) (see e.g., <ref type="bibr" target="#b31">[32]</ref>) using the Scikit-learn implementation with default parameters <ref type="bibr" target="#b34">[35]</ref>, where the groups in <ref type="table" target="#tab_3">Table 3</ref> are treated as classes. We 1 Our results present this data in its raw form; in particular, we do not capitalize words to reflect common usage. Results are in no way meant to be a political statement. consider all SSNMF models with the training process described in Section 2.3 with the maximum number of iterations (number of multiplicative updates) N = 50; our stopping criterion is the earlier of N iterations or relative error (10) below tolerance tol.</p><p>We also apply SVM as a classifier to the lowdimensional representation obtained from NMF as follows. We consider the default implementation <ref type="bibr" target="#b34">[35]</ref> of · F -NMF with multiplicative updates, random initialization, and maximum number of iterations N = 400. We apply NMF on the train data to obtain a vocabulary dictionary matrix Atrain and a document representation Strain. Next, we train an SVM classifier using Strain and the labels of the train set. We test our model by (i) computing the document representation of the test data Stest from the learned dictionary Atrain (i.e., step 2 of Section 2.3), then (ii) applying the trained SVM classifier on Stest to obtain the test predicted labels.</p><p>For both NMF and all four SSNMF models, we consider rank (the number of topics 2 ) equal to 13. We select the hyperparameters tol and λ for the models by searching over different values and selecting those with the highest average classification accuracy on the validation set; see Appendix C. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Class. accuracy % (sd) ( · F , · F ) 79.37 (0.47) ( · F , D(· ·)) 79.51 (0.38) (D(· ·), · F ) 81.88 (0.44) (D(· ·), D(· ·)) 81.50 (0.47) · F -NMF + SVM 70.99 (2.71) SVM 80.70 (0.27) Multinomial NB 82. <ref type="bibr" target="#b27">28</ref> We report in <ref type="table" target="#tab_4">Table 4</ref> the mean and standard deviation of the test classification accuracy for each of the models over 11 trials. We define the test classification accuracy as n i=1 δ(Yi,Ŷi)/n, where δ(u, v) = 1 for u = v, and 0 otherwise, and where Yi andŶi are true and predicted labels, respectively. We observe that (D(· ·), · F )-SSNMF produces the highest average classification accuracy, and is comparable to Multinomial NB. <ref type="figure">Figure 2</ref>: The normalized B train matrix for the (D(· ·), · F ) SSNMF decomposition corresponding to the median test classification accuracy equal to 81.78.</p><p>In <ref type="table" target="#tab_4">Table 4</ref>, we separate models that simultaneously perform dimensionality-reduction and classification from those which only perform classification.</p><p>Note that the SSNMF models, which provide both dimensionalityreduction and classification in that lower-dimensional space, do not suffer great accuracy loss over models which perform classification in the high-dimensional space. We observe that the (D(· ·), · F )-SSNMF performs significantly better than the · F -NMF + SVM in terms of accuracy. In Appendix C, we present NMF and SSNMF model results, and compare keywords, classifier matrices, and clustering performance.</p><p>Here, we consider the "typical" decomposition for the (D(· ·), · F )-SSNMF by selecting the decomposition corresponding to the median test classification accuracy. We display in <ref type="figure">Figure 2</ref> the column-sum normalized Btrain matrix of the decomposition, where each column illustrates the distribution of topic association to classes. We display in <ref type="table">Table 5</ref> the top 10 keywords (i.e. those that have the highest weight in topic column of Atrain) for each topic of the (D(· ·), · F )-SSNMF of <ref type="figure">Figure 2</ref>.</p><p>We (qualitatively) observe from <ref type="table">Table 5</ref> that topic 5 ("armenian", "fbi"), topic 8 ("arab"), and topic 11 ("weapons") captures the subjects of Middle East and guns ("israel", "government", "gun"). All three topics are associated with class Politics; see <ref type="figure">Figure 2</ref>. We also observe that topic 1 ("space", "government") and topic 9 ("chip", "key", "algorithm") relate to electronics/cryptography. Both are associated to class Sciences. Topic 2 is related to autos ("car", "engine"), topic 6 captures the specific subject of baseball, and topic 7 of hockey. Indeed, all three topics are associated to class Recreation, and topic 12 ("available","key","phone") is shared between Sciences and Recreation. Topics 3 and 13 capture topics related to religion and beliefs ("god", "believe", "religious") and are associated to class Religion. Topic 10 ("earth", "space") is shared between Religion and Sciences. Topic 4 captures computer subjects ("x" for Windows 10, "graphics", and "mac"). Indeed, topic 4 is the only topic associated to class Computers in <ref type="figure">Figure 2</ref>.</p><p>While the learned topics in <ref type="table">Table 5</ref> are not one-to-one with the subgroups in <ref type="table" target="#tab_3">Table 3</ref>, these topics appear relatively coherent. We see in <ref type="table" target="#tab_4">Table 4</ref> that these learned topics serve the classification task well; that is, the data rep-resentation in this significantly lower-dimensional space is able to achieve nearly the same accuracy as the higherdimensional multinomial NB model. We expect that this is due to the relevance of the topic modeling and classification tasks on this data set. We note that while (D(· ·), · F )-SSNMF outperforms the other SSNMF models in terms of classification accuracy, this does not imply that a different model would not produce a lower overall relative error for the objective function. Additionally, the strong performance of (D(· ·), · F )-SSNMF could be due to a number of factors including the choice of hyperparameters. Ongoing and future work will further investigate this phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we have have proposed several SSNMF models, and have demonstrated that these models and that of <ref type="bibr" target="#b27">[28]</ref> are MLE in the case of specific distributions of uncertainty assumed on the data and labels. We provided multiplicative update training methods for each model, and demonstrated the ability of these models to perform classification.</p><p>In future work, we plan to take a Bayesian approach to SSNMF by assuming data-appropriate priors and performing maximum a posteriori estimation. Furthermore, we will form a general framework of MLE models for exponential family distributions of uncertainty, and study the class of models where multiplicative update methods are feasible. <ref type="table" target="#tab_2">Topic 1   Topic 2 Topic 3  Topic 4  Topic 5  Topic 6 Topic 7  Topic 8  Topic 9  Topic 10  Topic 11  Topic 12 Topic 13  would  game  god  x  would  game  players  people  would  one  israel  like  god  space  team  would  thanks  armenian  one  team  israel  chip  us  guns  anyone  people  government  car  one  anyone  one  like  car  gun  key  get  people  available  church  use  games  jesus  graphics  people  car  last  right  algorithm  could  gun  key  one  key  engine  think  know  fbi  baseball  year  government  use  like  well  probably christians  chip  year  bible  use  armenians  think  game  us  using  earth  weapons  right  jesus  get  like  believe  mac  israeli  get  hockey  say  bit  space  know  phone  would  clipper  know  christian  please  killed  season  would  jews  like  know  like  another  religious  one  espn  christ  would  fire  last  go  arab  system  see  government  also  christian  could  get  say  get  jews  would  time  one  data  used  would  big  different   Table 5</ref>: Top keywords representing each topic of the (D(· ·), · F )-SSNMF model referred to in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Appendix</head><p>In this appendix, we provide the remaining MLE derivations <ref type="bibr">(1, 3, and 4)</ref> in Appendix A, the intuition for the derivation of multiplicative updates for (D(· ·), · F )-SSNMF and (D(· ·), D(· ·))-SSNMF (Algorithms 2 and 3) in Appendix B, and additional experimental results on the 20 Newsgroups data set in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MLE Derivations</head><p>We begin by demonstrating that the MLE, in the case that the uncertainty on the X and Y observations is Gaussian distributed, is a specific instance of ( · F , · F )-SSNMF of <ref type="bibr" target="#b27">[28]</ref>. Our models for the distribution of the observed entries of X and Y will assume that the mean is given by an exact factorization, E[X] = AS and E[Y] = BS, and the uncertainty in each set of observations is governed by a Gaussian distribution. That is, we consider the hierarchical models for X and Y in which</p><formula xml:id="formula_26">Xγ,τ = r i=1</formula><p>xγ,i,τ and xγ,i,τ ∼ N (xγ,i,τ |Aγ,iSi,τ , σ1) ,</p><formula xml:id="formula_27">Yη,τ = r i=1</formula><p>yη,i,τ and yη,i,τ ∼ N (yη,i,τ |Bη,iSi,τ , σ2) .</p><p>Here and throughout, γ and η are row indices of X and Y respectively, τ is a column index of X and Y, and i indexes the random variable summands which form Xγ,τ and Yη,τ . Note then that</p><formula xml:id="formula_28">Xγ,τ ∼ N Xγ,τ r i=1</formula><p>Aγ,iSi,τ , rσ1 , and</p><formula xml:id="formula_29">Yη,τ ∼ N Yη,τ r i=1</formula><p>Bη,iSi,τ , rσ2</p><p>due to the summable property of Gaussian random variables. We note that this assumes different Gaussian models of uncertainty on the two collections of rows of the NMF (4).</p><p>Assuming that the set of Xγ,τ and Yη,τ are statistically independent conditional on A, B, and S, we have that the likelihood p(X, Y|A, B, S) is</p><formula xml:id="formula_30">γ,τ N Xγ,τ r i=1 A γ,i S i,τ , rσ 1 η,τ N Yη,τ r i=1 B η,i S i,τ , rσ 2 .<label>(11)</label></formula><p>We apply the monotonic natural logarithmic function to the likelihood, and ignore terms that do not vary with the factor matrices. This transforms the likelihood function into a ( · F , · F )-SSNMF objective, while preserving the maximizer. That is, the log likelihood (excluding additive terms which are constant with respect to A, B, and S) is</p><formula xml:id="formula_31">ln p (X, Y|A, B, S) = + − 1 2rσ 1 γ,τ Xγ,τ − r i=1 A γ,i S i,τ 2 − λ 2rσ 2 η,τ Yη,τ − r i=1 B η,i S i,τ 2 = + − 1 2rσ 1 X − AS 2 F + σ 1 σ 2 Y − BS 2 F .</formula><p>Thus, the maximum likelihood estimators for A, B, and S are given by argmin A,B,S≥0</p><formula xml:id="formula_32">X − AS 2 F + σ 1 σ 2 Y − BS 2 F .</formula><p>We see that the MLE in the case of Gaussian uncertainty on both sets of observations, X and Y, is a specific instance of ( · F , · F )-SSNMF objective where the regularization parameter λ, which defines the relative weighting of the supervision term, is given as a ratio of the variances of the distributions. Next, we demonstrate that the MLE, in the case that the uncertainty on X is Poisson distributed and on Y is Gaussian distributed, is a specific instane of the (D(· ·), · F )-SSNMF model. This MLE derivation follows from that of 2 by swapping the roles of X and Y, and rescaling the resulting log likelihood; however, we include a sketch of the derivation to be thorough.</p><p>Again, our models for observed X and Y assume that the mean is given by an exact factorization, E[X] = AS and E[Y] = BS, with the uncertainty in X governed by a Poisson distribution and the uncertainty in Y governed by a Gaussian distribution. That is, we consider the hierarchical models for X and Y in which</p><formula xml:id="formula_33">X γ,τ = r i=1 x γ,i,τ and x γ,i,τ ∼ PO (x γ,i,τ |A γ,i S i,τ ) , Y η,τ = r i=1</formula><p>y η,i,τ and y η,i,τ ∼ N (y η,i,τ |B η,i S i,τ , σ 2 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note then that</head><formula xml:id="formula_34">X γ,τ ∼ PO X γ,τ r i=1 A γ,i S i,τ , and Y η,τ ∼ N Y η,τ r i=1 B η,i S i,τ , rσ 2</formula><p>due to the summable property of Gaussian and Poisson random variables. We note this assumes a Poisson and Gaussian model of uncertainty on the two collections of rows of the NMF (4).</p><p>Then proceeding as in <ref type="formula" target="#formula_0">(11)</ref> and <ref type="bibr" target="#b5">(6)</ref> and assuming that the set of X γ,τ and Y η,τ are statistically independent conditional on A, B, and S, we have that the log likelihood (excluding additive terms which are constant with respect to A, B, and S) is</p><formula xml:id="formula_35">ln p (X, Y|A, B, S) = + − D(X AS) + 1 2rσ 2 Y − BS 2 F .</formula><p>Thus, the maximum likelihood estimators for A, B, and S are given by argmin A,B,S≥0</p><formula xml:id="formula_36">D(X AS) + 1 2rσ 2 Y − BS 2 F .</formula><p>We see that the MLE in the case of Poisson uncertainty on the observations in X and Gaussian uncertainty on the observations in Y is a specific instance of the (D(· ·), · F )-SSNMF objective where the regularization parameter λ is the inverse of a multiple of the variance of the Gaussian distribution. Finally, we demonstrate that the MLE, in the case that the uncertainty on X and Y are Poisson distributed, is a specific instance of the (D(· ·), D(· ·))-SSNMF model. This result follows from <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41]</ref>; we sketch the derivation to be thorough.</p><p>Again, we assume that the distributions of the observed X and Y have means given by an exact factorization, E[X] = AS and E[Y] = BS, with the uncertainty in both governed by a Poisson distribution. That is, we consider the hierarchical models for X and Y in which</p><formula xml:id="formula_37">X γ,τ = r i=1</formula><p>x γ,i,τ and x γ,i,τ ∼ PO (x γ,i,τ |A γ,i S i,τ ) ,</p><formula xml:id="formula_38">Y η,τ = r i=1</formula><p>y η,i,τ and y η,i,τ ∼ PO (y η,i,τ |B η,i S i,τ ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note then that</head><formula xml:id="formula_39">X γ,τ ∼ PO X γ,τ r i=1 A γ,i S i,τ , and Y η,τ ∼ PO Y η,τ r i=1 B η,i S i,τ</formula><p>due to the summable property of Poisson random variables. We note that assumes different Poisson models of uncertainty on the two collections of rows of the NMF (4).</p><p>Then proceeding as in <ref type="bibr" target="#b10">(11)</ref> and <ref type="bibr" target="#b5">(6)</ref> and assuming that the set of X γ,τ and Y η,τ are statistically independent conditional on A, B, and S, we have that the log likelihood (excluding additive terms which are constant with respect to A, B, and S) is We see that the MLE in the case of Poisson uncertainty on the observations in X and Y is a specific instance of the (D(· ·), D(· ·))-SSNMF objective where the regularization parameter is λ = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Multiplicative Updates Derivations</head><p>Here we provide intuition for the derivation of multiplicative updates for (D(· ·), · F )-SSNMF and (D(· ·), D(· ·))-SSNMF (Algorithms 2 and 3).</p><p>First, note that the multiplicative updates for (D(· ·), · F )-SSNMF (Algorithm 2) follow from those for ( · F , D(· ·))-SSNMF (Algorithm 1) by swapping the roles of X and Y, and A and B.</p><p>Next, the multiplicative updates for (D(· ·), D(·, ·)-SSNMF (Algorithm 3) are derived as follows. The gradients of F 4 (A, B, S; X, Y) with respect to A, B, and S are, respectively</p><formula xml:id="formula_40">∇ A F 4 = WS − W X W AS W S , ∇ B F 4 = LS − L Y L BS L S , and ∇ S F 4 = −A (W X) (W AS) W + A W − λB (L Y) (L BS) L + λB L.</formula><p>The multiplicative updates of Algorithm 3 are given by</p><formula xml:id="formula_41">A ← A − Γ ∇ A F 4 when Γ = A WS , B ← B − Γ ∇ B F 4 when Γ = B LS , and S ← S − Γ ∇ S F 4 when Γ = S A W + λB L .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional 20 Newsgroups Results</head><p>In this section, we include additional analysis and results for the 20 Newsgroups data set. First, we summarize in <ref type="table">Table 6</ref> the hyperparameters used for the methods described in Section 3.2. We select the hyperparameters that result in the highest average classification accuracy of the validation set. For the SSNMF models, we search over tol ∈ {10 −4 , 10 −3 , 10 −2 }, and λ ∈ {10, 10 2 , 10 3 }, and for the NMF model, we search over tol ∈ {10 −5 , 10 −4 , 10 −3 , 10 −2 }.</p><p>Model hyperparameters ( · F , · F ) tol = 10 −4 , λ = 10 2 ( · F , D(· ·)) tol = 10 −4 , λ = 10 (D(· ·), · F ) tol = 10 −3 , λ = 10 2 (D(· ·), D(· ·)) tol = 10 −3 , λ = 10 3 · F -NMF tol = 10 −4 <ref type="table">Table 6</ref>: Hyperparameter selection for NMF and SS-NMF models by selecting the hyperparameters that result with the highest average classification accuracy of the validation set (over 10 trials).</p><p>As in Section 3.2, we consider the "typical" (achieving median accuracy within trials) decomposition for NMF, and remaining SSNMF models. We display in <ref type="figure" target="#fig_5">Figure 3</ref> the B train matrices for each of the median accuracy SSNMF decompositions, and in <ref type="figure">Figure 4</ref> the coefficients matrix of the SVM classifier for the median accuracy NMF decomposition. Further, we report in <ref type="bibr">Tables 7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11</ref>, and 12 the top 10 keywords representing each topic for each of the models.</p><p>For the ( · F , · F )-SSNMF model, we (qualitatively) observe from <ref type="table">Table 7</ref> that topics 4, 5, 7 and 10 are overlapping topics associated to the class Computers; see <ref type="figure" target="#fig_5">Figure 3a</ref>. Similarly, topics 1 and 9 that capture the subjects of crypt(ography), electronics, and space, have various overlapping keywords ("space", "key","chip"), and are associated with the class Sciences. On the other hand, topics associated to class Politics and Religion are less overlapping. Lastly, topics 3 and 8 are recreation topics ("game", "team", "car") relating to autos where in addition topic 3 ("hockey", "player", "nhl") is specific to hockey and topic 8 ("baseball") is specific to baseball.</p><p>For the ( · F , D(· ·))-SSNMF model, we (qualitatively) observe from <ref type="table">Table 9</ref> that topic 2 ("armenian"), topic 6 ("jews","gun", "fire") and topic 12 ("guns") are related political topics ("people", "government", "israel", "fbi"). The topics are associated to the class Politics; see <ref type="figure" target="#fig_5">Figure 3b</ref>. Further, recreation topics include topic 7 ("player", "hockey", "baseball") relating to hockey and baseball, topic 9 ("car","team") relating to autos, and a broad topic 11 ("game", "good", "great", "play", "better"). Lastly, topics 1, 3 and 8 are associated to class Computers. Topic 1 ("window","graphics","software","windows"), and topic 3 ("mac", "sun", "graphics"), are specific in comparison to topic 8 ("ordinary","yeah","monitor") which is broad and not as informative.</p><p>For the (D(· ·), D(· ·))-SSNMF model, we observe from Figure 3c that topic 5 ("space", "moon", "time"), topic 7 ("key","chip","clipper"), and topic 11 ("data","government","buy") are associated with class Sciences.</p><p>Further, we (qualitatively) observe from <ref type="table">Table 11</ref> that topic 4 ("church", "believe","bible"), and topic 6 ("atheists", "paul") are both related to religion ("god", "jesus") and are associated to class Religion; see <ref type="figure" target="#fig_5">Figure 3c</ref>. Lastly, topic 12 ("car", "hockey", "players", "baseball") is a recreation topic that captures autos, hockey, and baseball subjects, whereas topics 2 and 9 are broad and not as informative.</p><p>(a) ( · F , · F )-SSNMF decomposition corresponding to the median test classification accuracy equal to 79.44.</p><p>(b) ( · F , D(· ·))-SSNMF decomposition corresponding to the median test classification accuracy equal to 79.56.</p><p>(c) (D(· ·), D(· ·))-SSNMF decomposition corresponding to the median test classification accuracy equal to 81.39. For the NMF-· F model, we (qualitatively) observe from <ref type="table" target="#tab_2">Table 12</ref>, that topic 12 ("car", "engine", "oil") is related to autos, and topic 4 ("game", "team", "hockey","baseball") captures other recreation games like hockey and baseball. We observe in <ref type="figure">Figure 4</ref> that topics 4 and 12 are associated to the class Recreation. Further, topic 8 ("book","true","evidence") relates to atheism, and topic 11 ("god","jesus","christ","faith") relates to religion and specifically Christianity. Lastly, we observe in <ref type="figure">Figure 4</ref> that topic 13 is a shared across 3 classes (Computers, Sciences, and Religion), and is not as informative as the other topics. <ref type="figure">Figure 4</ref>: The normalized coefficients matrix of the SVM classifier (with NMF-· F ) corresponding to the median test classification accuracy equal to 71.67. Here, all negative coefficients are thresholded to 0, and then each column is normalized to showcases the distribution of the topic over classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering Analysis</head><p>In this section, we measure the performance of the NMF and SSNMF topic models in a clusteringmotivated score. In these experiments, we measure the similarity of ground-truth clusters, encoded by a given label matrix M, to NMF/SSNMF computed clusters, encoded by the SSNMF/NMF representation matrix S. We denote by M the (columnwise) one-hot encoded label matrix which maps documents to the subgroups to which they belong 3 , M ∈ {0, 1} 13×8980 (subgroups by documents). We denote by S be the representation matrix computed by NMF/SSNMF, in which the ith row provides the association of each document with the ith topic.</p><p>We employ two approaches to clustering or mixture assignment. The first is hard clustering in which the documents are assigned to a single cluster corresponding to computed topics. In this approach, we apply a mask to the representation matrix,Ŝ = label(S), where label(·) assigns the largest entry of each column to 1 and all other entries to 0. The second approach is soft clustering in which the documents are assigned to a distribution of clusters corresponding to the topics. In this approach, we normalize each of the columns of the representation matrix to produceŜ. Now, in either approach, we apply a metric P which measures the association between the th topicdocuments associationŜ and the best ground truth subgroup-documents association, M I (here M I is the <ref type="bibr" target="#b2">3</ref> In the 20 Newsgroups data set, each document belongs to only one subgroup.</p><p>Ith row of M); that is, for topic , we define I as</p><formula xml:id="formula_42">I = argmax i Ŝ M i 1 M i 1 ,</formula><p>and define score P for the th topic as</p><formula xml:id="formula_43">P (Ŝ ) = Ŝ M I 1 M I 1 ,</formula><p>where · 1 denotes the 1 -norm. We note that this metric is similar to that of <ref type="bibr" target="#b43">[44]</ref>; we use score P instead as it allows us to measure clustering performance topic-wise. We also note that the learned topics of NMF and SSNMF methods need not be in oneto-one correspondence with the subgroups in <ref type="table" target="#tab_3">Table 3</ref> as topics are also learnt for the classification task at hand. Now, we present in <ref type="table">Table 8</ref> the average (averaged over topics) score P for the representation matrices computed by each of the NMF/SSNMF models in both the hard-clustering and soft-clustering settings. The scores P for each topic (for both hard-clustering and soft-clustering) and the maximizing subgroup (indicated by I) are listed in the bottom four rows of the <ref type="table">keyword table associated to each model; see  the last four rows of Tables 7, 9</ref>, 10, 11, and 12. <ref type="table" target="#tab_2">Topic 1  Topic 2  Topic 3 Topic 4  Topic 5  Topic 6  Topic 7  Topic 8 Topic 9 Topic 10 Topic 11  Topic 12  Topic 13  would  people  game  use  x  would  x  game  would  please  god  israel  god  like  israel  team  thanks  software  jews  thanks  car  key  x  church  people  one  space  gun  car  x  c  fbi  get  team  could  would  jesus  would  would  one  one  year  using  know  time  need  like  one  use  one  one  people  Keywords  chip  jews  hockey  window  r  israel  image  games  space  anyone  would  killed  jesus  key  would  espn  know  thanks  like  window baseball  use  like  people  armenians  believe  use  armenian  would  graphics  widget  law  problem  one  chip  graphics  think  police  christ  good  government players  pc  system  arabs  windows  think  know  work  like  jewish  religion  phone  said  nhl  program  please  government  mac  would  get  help  say  well  think  edu  turkish  games  anyone  motif  right  version  get  like  apple  faith  israeli</ref>   <ref type="table">Table 7</ref>: Top keywords representing each topic of the ( · F , · F )-SSNMF model referred to in <ref type="figure" target="#fig_5">Figure 3a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topics</head><p>Model Hard Clustering Soft Clustering ( · F , · F ) 0.3895 0.3647 ( · F , D(· ·)) 0.3857 0.3703 (D(· ·), · F ) 0.3874 0.3553 (D(· ·), D(· ·)) 0.3874 0.3732 · F -NMF 0.4348 0.3080 <ref type="table">Table 8</ref>: Listed scores are average over 11 trials; in each trial, we average score P across all topics. <ref type="table" target="#tab_2">Topic 1  Topic 2  Topic 3  Topic 4  Topic 5  Topic 6 Topic 7  Topic 8  Topic 9  Topic 10  Topic 11  Topic 12  Topic 13  x  israel  thanks  would  would  people  game  ordinary  car  god  game  people  god  know  would  x  chip  one  would  player  yeah  team  jesus  good  gun  one  would  people  anyone  use  key  jews  hockey  monitors  game  deleted  one  one  would  use  government  get  clipper  could  israel  espn  ok  games  science  car  guns  people  Keywords window  israeli  mac  space  space  gun  would  big  like  would  get  israel  jesus  graphics  one  sun  government  like  fbi  baseball  know  year  moses  anyone  said  church  please  fbi  graphics  key  know  one  new  shareware  get  post  great  armenian  think  software  armenians  file  much  get  law  wings  way  would  passages  play  government  bible  windows  armenian  one  get  use  fire  think  anyone  one  come  year  well  believe  mac  also  please  people  chip  think  players  good  think  commandments  better  would</ref>   <ref type="table">Table 9</ref>: Top keywords representing each topic of the ( · F , D(· ·))-SSNMF model referred to in <ref type="figure" target="#fig_5">Figure 3b</ref>.  <ref type="table">Table 10</ref>: Clustering results for topics of the (D(· ·), · F )-SSNMF model referred to in <ref type="figure">Figure 2</ref>.  <ref type="table">Table 11</ref>: Top keywords representing each topic of the (D(· ·), D(· ·))-SSNMF model referred to in <ref type="figure" target="#fig_5">Figure 3c</ref>. <ref type="table" target="#tab_2">Topics  Topic 1  Topic 2  Topic 3  Topic 4  Topic 5  Topic 6  Topic 7  Topic 8  Topic 9  Topic 10 Topic 11 Topic 12  Topic 13  people  key</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topics</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .</head><label>1</label><figDesc>When xγ,i,τ and yη,i,τ have distributions N (xγ,i,τ |Aγ,iSi,τ , σ1) and N (yη,i,τ |Bη,iSi,τ , σ2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F</head><label></label><figDesc>(A (N ) , B (N ) , S (N ) ; AS, BS) F (A (0) , B (0) , S (0) ; AS, BS)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ln p (X, Y|A, B, S) = + − [D(X AS) + D(Y BS)] . Thus, the maximum likelihood estimators for A, B, and S are given by argmin A,B,S≥0 D(X AS) + D(Y BS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>The normalized B train matrix for each of the SSNMF decomposition corresponding to the median test classification accuracy. Each column is normalized to represent the distribution of the topic over classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1. Compute Atrain, Btrain, Strain as argmin A,B,S≥0R(Wtrain Xtrain, Wtrain AS)+λS(Ytrain, BS).2. Solve Stest = argmin</figDesc><table><row><cell>R(Wtest</cell><cell>Xtest, Wtest</cell></row><row><cell>S≥0</cell><cell></cell></row><row><cell>AtrainS).</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results with synthetic data. Xij ∼ PO (Xij|(AS)ij) and supervision Yij ∼ N (Yij|(BS)ij, 1/2r), and compare each model's average relative error for objective function F3. In our last experiment, we generate data Xij ∼ PO (Xij|(AS)ij) and supervision Yij ∼ PO (Yij|(BS)ij), and compare each model's average relative error for objective function F4.</figDesc><table><row><cell>Experiment</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>SSNMF</cell><cell>F 1 err.</cell><cell>F 2 err.</cell><cell>F 3 err.</cell><cell>F 4 err.</cell></row><row><cell>( · F , · F )</cell><cell>0.2917</cell><cell>0.0081</cell><cell>0.0078</cell><cell>0.0204</cell></row><row><cell>( · F , D(· ·))</cell><cell>0.2927</cell><cell>0.0065</cell><cell>0.0109</cell><cell>0.0199</cell></row><row><cell>(D(· ·), · F )</cell><cell>0.2930</cell><cell>0.0093</cell><cell>0.0071</cell><cell>0.0210</cell></row><row><cell>(D(· ·), D(· ·))</cell><cell>0.2922</cell><cell>0.0076</cell><cell>0.0075</cell><cell>0.0172</cell></row><row><cell cols="5">N (Yij|(BS)ij, 1), and compare each model's average</cell></row><row><cell cols="5">relative error for objective function F1. In our second ex-</cell></row><row><cell cols="5">periment, we generate data Xij ∼ N (Xij|(AS)ij, 1/2r)</cell></row><row><cell cols="5">and supervision Yij ∼ PO (Yij|(BS)ij), and com-</cell></row><row><cell cols="5">pare each model's average relative error for objective</cell></row><row><cell>function F2.</cell><cell cols="4">In our third experiment, we gener-</cell></row><row><cell>ate data</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>20 Newsgroups and subgroups.</figDesc><table><row><cell>Groups</cell><cell>Subgroups</cell></row><row><cell>Computers</cell><cell>graphics, mac.hardware, windows.x</cell></row><row><cell>Sciences</cell><cell>crypt(ography), electronics, space</cell></row><row><cell>Politics</cell><cell>guns, mideast</cell></row><row><cell>Religion</cell><cell>atheism, christian(ity)</cell></row><row><cell>Recreation</cell><cell>autos, baseball, hockey</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Mean (and std. dev.) of test classification accuracy for each of the models on 20 Newsgroups data.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A larger choice of rank could be made to learn hidden topics within subgroups.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors are appreciative of useful conversations with William Swartworth, Joshua Vendrow, and Liza Rebrova.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">game  israel  god  would  god  key  would  game  people  one  car  x  x  games  one  would  space  atheists  would  people  win  gun  like  team  anyone  mac  would  arab  one  could  one  chip  government  etc  get  use  game  thanks  know  think  people  jesus  u  people  clipper  gun  turbo  right  get  year  graphics  Keywords  would  get  government  church  use  would  could  said  know  jews  would  like  get  problem  back  would  people  moon  think  space  israel  games  armenian  think  hockey  use  please  hit  fire  believe  time  paul  know  one  get  well  data  players  window  use  well  well  bible  old  jesus  one  jews  would  us  anyone  last  would  one  one  like  think  one  know  using  armenians  cup  time  government  one  please  se  like  israeli  christian  may  also  like  batf  find  armenians  buy  baseball</ref>  <ref type="table">Table 12</ref><p>: Top keywords representing each topic of the NMF-· F + SVM model referred to in <ref type="figure">Figure 4</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semisupervised clustering by seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn. Citeseer</title>
		<meeting>Int. Conf. Mach. Learn. Citeseer</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Email surveillance using non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Math. Organ. Th</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="264" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian inference for nonnegative matrix factorisation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Cemgil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Intel. Neurosc</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization for semi-supervised data clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="379" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Nonnegative matrix factorization for semi-supervised dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1112.3714</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">New algorithms for non-negative matrix factorization in applications to blind source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust. Spe. Sig. Process</title>
		<meeting>Int. Conf. Acoust. Spe. Sig. ess</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of financial data using non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Fréin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Drakakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rickard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Mathematical Forum</title>
		<meeting>Int. Mathematical Forum<address><addrLine>Hikari</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1853" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data An</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3913" to="3927" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">3-d shape estimation and image restoration: Exploiting defocus and motionblur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised clustering via matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Changshui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIAM Int. Conf. on Data Mining</title>
		<meeting>SIAM Int. Conf. on Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="73" to="99" />
			<date type="published" when="2004-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relation between PLSA and NMF and implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR Conf. on Research and Development in Inform. Retrieval</title>
		<meeting>ACM SIGIR Conf. on Research and Development in Inform. Retrieval</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="601" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An exemplar-based NMF approach to audio event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karsmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vanrumste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guillamet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vitria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Catalonian Conf. on Artif. Intel</title>
		<meeting>Catalonian Conf. on Artif. Intel</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haddock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kassab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssnmf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Non-negative sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Neural Networks for Sig. Process</title>
		<meeting>IEEE Workshop on Neural Networks for Sig. ess</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="557" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semisupervised non-negative matrix factorization with dissimilarity and similarity regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neur. Net. Lear</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fisher nonnegative matrix factorization for learning local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comp. Vis</title>
		<meeting>Asian Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="27" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast projectionbased methods for the least squares nonnegative matrix approximation problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Anal. Data Min</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="51" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. A</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="713" to="730" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Kamvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page">788</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neur. In</title>
		<meeting>Adv. Neur. In</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Let</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="7" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast Bregman divergence NMF using Taylor expansion and coordinate descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Projected gradient methods for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2756" to="2779" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularized non-negative matrix factorization for identifying differentially expressed genes and clustering samples: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM T. Comput. Bio. Bioin</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="974" to="987" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<title level="m">troduction to information retrieval. Cambridge university press</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Text mining using non-negative matrix factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIAM Int. Conf. on Data Mining</title>
		<meeting>SIAM Int. Conf. on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="452" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inference of population structure using multilocus genotype data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pritchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Donnelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetics</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="945" to="959" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Document clustering using nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Process. Manag</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="373" to="386" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised feature selection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sheikhpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gharaghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chahooki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized nonnegative matrix approximations with Bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neur. In</title>
		<meeting>Adv. Neur. In</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayesian extensions to non-negative matrix factorisation for audio signal modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Godsill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoust., Speech and Sig. Process</title>
		<meeting>IEEE Int. Conf. on Acoust., Speech and Sig. ess</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1825" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Constrained k-means clustering with background knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schrödl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int</title>
		<meeting>Int</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The role of dimensionality reduction in classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artif. Intel</title>
		<meeting>AAAI Conf. on Artif. Intel</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2128" to="2134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGIR Conf. on Research and Development in Inform. Retrieval</title>
		<meeting>ACM SIGIR Conf. on Research and Development in Inform. Retrieval</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A modified non-negative matrix factorization algorithm for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Pattern Recognition</title>
		<meeting>Int. Conf. on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="495" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kullback-Leibler divergence for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Artif. Neural Networks</title>
		<meeting>Int. Conf. on Artif. Neural Networks</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploiting discriminant information in nonnegative matrix factorization with application to frontal face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Buciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Neural Networ</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="683" to="695" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
