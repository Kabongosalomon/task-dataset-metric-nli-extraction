<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FEELVOS: Fast End-to-End Embedding Learning for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
							<email>voigtlaender@vision.rwth-aachen.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FEELVOS: Fast End-to-End Embedding Learning for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many of the recent successful methods for video object segmentation (VOS) are overly complicated, heavily rely on fine-tuning on the first frame, and/or are slow, and are hence of limited practical use. In this work, we propose FEELVOS as a simple and fast method which does not rely on fine-tuning. In order to segment a video, for each frame FEELVOS uses a semantic pixel-wise embedding together with a global and a local matching mechanism to transfer information from the first frame and from the previous frame of the video to the current frame. In contrast to previous work, our embedding is only used as an internal guidance of a convolutional network. Our novel dynamic segmentation head allows us to train the network, including the embedding, end-to-end for the multiple object segmentation task with a cross entropy loss. We achieve a new state of the art in video object segmentation without fine-tuning with a J &amp;F measure of 71.5% on the DAVIS 2017 validation set. We make our code and models available at https://github.com/tensorflow/ models/tree/master/research/feelvos. * Work done during an internship at Google Inc.</p><p>† Now at Waymo LLC. Simple Fast End-to-end Strong PML [6] OSMN [40] FAVOS [7] VideoMatch [17] RGMP [37] FEELVOS (ours) PReMVOS [26] OnAVOS [35]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation (VOS) is a fundamental task in computer vision, with important applications including video editing, robotics, and self-driving cars. In this work, we focus on the semi-supervised VOS setup in which the ground truth segmentation masks of one or multiple objects are given for the first frame in a video. The task is then to automatically estimate the segmentation masks of the given objects for the rest of the video. With the recent advances in deep learning and the introduction of the DAVIS datasets <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref>, there has been tremendous progress in tackling the semi-supervised VOS task. However, many of the most suc- <ref type="table">Table 1</ref>. Design goals overview. The table shows which of our design goals (described in more detail in the text) are achieved by recent methods. Our method is the only one which fulfills all design goals. cessful methods rely on fine-tuning of the model using the first-frame annotations and have very high runtimes which are not suitable for most practical applications. Additionally, several successful methods rely on extensive engineering, resulting in a high system complexity with many components. For example, the 2018 DAVIS challenge was won by PReMVOS <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> which employs four different neural networks together with fine-tuning and a merging algorithm resulting in a total runtime of around 38 seconds per video frame. While it delivered impressive results, the practical usability of such algorithms is limited. Design Goals. In order to ensure maximum practical usability, in this work, we develop a method for video object segmentation with the following design goals: A VOS method should be • Simple: Only a single neural network and no simulated data is used. • Fast: The whole system is fast for deployment. In particular, the model does not rely on first-frame finetuning. • End-to-end: The multi-object segmentation problem, where each video contains a different number of objects, is tackled in an end-to-end way. Groundtruth mask for the rst frame is given softmax <ref type="figure">Figure 1</ref>. Overview of the proposed FEELVOS method. In order to segment the image of the current frame, backbone features and pixelwise embedding vectors are extracted for it. Afterwards the embedding vectors are globally matched to the first frame and locally matched to the previous frame to produce a global and a local distance map. These distance maps are combined with the backbone features and the predictions of the previous frame and then fed to a dynamic segmentation head which produces the final segmentation. For details about handling of multiple objects see <ref type="figure" target="#fig_0">Fig. 3</ref>.</p><p>• Strong: The system should deliver strong results, with more than 65% J &amp;F score on the DAVIS 2017 validation set. <ref type="table">Table 1</ref> shows an overview of which of our design goals are achieved by current methods that do not use fine-tuning, and by PReMVOS <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> and OnAVOS <ref type="bibr" target="#b34">[35]</ref> as examples for methods with fine-tuning. For a more detailed discussion of the individual methods, see Section 2. Towards building a method which fulfills all design goals, we take inspiration from the Pixel-Wise Metric Learning (PML) <ref type="bibr" target="#b5">[6]</ref> method. PML learns a pixel-wise embedding using a triplet loss and at test time assigns a label to each pixel by nearest neighbor matching in pixel space to the first frame. PML fulfills the design goals of being simple and fast, but does not learn the segmentation in an end-toend way and often produces noisy segmentations due to the hard assignments via nearest neighbor matching.</p><p>We propose Fast End-to-End Embedding Learning for Video Object Segmentation (FEELVOS) to meet all of our design goals (see <ref type="figure">Fig. 1</ref> for an overview). Like PML <ref type="bibr" target="#b5">[6]</ref>, FEELVOS uses a learned embedding and nearest neighbor matching, but we use this mechanism as an internal guidance of the convolutional network instead of using it for the final segmentation decision. This allows us to learn the embedding in an end-to-end way using a standard cross entropy loss on the segmentation output. By using the nearest neighbor matching only as a soft cue, the network can recover from partially incorrect nearest neighbor assignments and still produce accurate segmentations. We achieve a new state of the art for multi-object segmentation without fine-tuning on the DAVIS 2017 validation dataset with a J &amp;F mean score of 71.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Object Segmentation with First-Frame Finetuning. Many approaches for semi-supervised video object segmentation rely on fine-tuning using the first-frame ground truth. OSVOS <ref type="bibr" target="#b0">[1]</ref> uses a convolutional network, pre-trained for foreground-background segmentation, and fine-tunes it on the first-frame ground truth of the target video at test time. OnAVOS <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref> and OSVOS-S <ref type="bibr" target="#b26">[27]</ref> extend OSVOS by an online adaptation mechanism, and by semantic information from an instance segmentation network, respectively. Another approach is to learn to propagate the segmentation mask from one frame to the next using optical flow as done by MaskTrack <ref type="bibr" target="#b27">[28]</ref>. This approach is extended by LucidTracker <ref type="bibr" target="#b19">[20]</ref> which introduces an elaborate data augmentation mechanism. Hu et al. <ref type="bibr" target="#b14">[15]</ref> propose a motion-guided cascaded refinement network which works on a coarse segmentation from an active contour model. MaskRNN <ref type="bibr" target="#b15">[16]</ref> uses a recurrent neural network to fuse the output of two deep networks. Location-sensitive embeddings used to refine an initial foreground prediction are explored in LSE <ref type="bibr" target="#b8">[9]</ref>. MoNet <ref type="bibr" target="#b37">[38]</ref> exploits optical flow motion cues by feature alignment and a distance transform layer. Using reinforcement learning to estimate a region of interest to be segmented is explored by Han et al. <ref type="bibr" target="#b12">[13]</ref>. DyeNet <ref type="bibr" target="#b21">[22]</ref> uses a deep recurrent network which combines a temporal propagation and a re-identification module. PReMVOS <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> combines four different neural networks to-gether with extensive fine-tuning and a merging algorithm and won the 2018 DAVIS Challenge <ref type="bibr" target="#b1">[2]</ref> and also the 2018 YouTube-VOS challenge <ref type="bibr" target="#b38">[39]</ref>.</p><p>Despite achieving impressive results, all previously mentioned methods do not meet the design goal of being fast, since they rely on fine-tuning on the first frame. Video Object Segmentation without First-Frame Finetuning. While there is a strong focus in semi-supervised VOS on exploiting the first-frame information by finetuning, there are some recent works which aim to achieve a better runtime and usability by avoiding fine-tuning. OSMN <ref type="bibr" target="#b39">[40]</ref> combines a segmentation network with a modulator, which manipulates intermediate layers of the segmentation network without requiring fine-tuning. FAVOS <ref type="bibr" target="#b6">[7]</ref> uses a part-based tracking method to obtain bounding boxes for object parts and then produces segmentation masks using a region-of-interest based segmentation network. The main inspiration of the proposed FEELVOS approach is PML <ref type="bibr" target="#b5">[6]</ref>, which uses a pixel-wise embedding learned with a triplet loss together with a nearest neighbor classifier. VideoMatch <ref type="bibr" target="#b16">[17]</ref> uses a soft matching layer which is very similar to PML and considers for each pixel in the current frame the closest k nearest neighbors to each pixel in the first frame in a learned embedding space. Unlike PML, it directly optimizes the resulting segmentation instead of using a triplet loss. However, the final segmentation result is still directly derived from the matches in the embedding space which makes it hard to recover from incorrect matches. In our work, we use the embedding space matches only as a soft cue which is refined by further convolutions.</p><p>OSMN <ref type="bibr" target="#b39">[40]</ref>, FAVOS <ref type="bibr" target="#b6">[7]</ref>, PML <ref type="bibr" target="#b5">[6]</ref>, and VideoMatch [17] all achieve very high speed and effectively bypass finetuning, but we show that the proposed FEELVOS produces significantly better results.</p><p>RGMP <ref type="bibr" target="#b36">[37]</ref> uses a Siamese encoder with two shared streams. The first stream encodes the video frame to be segmented together with the estimated segmentation mask of the previous frame. The second stream encodes the first frame of the video together with its given ground truth segmentation mask. The features of both streams are then concatenated and combined by a global convolution block and multiple refinement modules to produce the final segmentation mask. The architecture of RGMP has similarities with ours, in particular both RGMP and FEELVOS use both the first and the previous video frame images and segmentation masks as information which is exploited inside the network. However, RGMP combines these sources of information just by stacking together features, while we employ a feature-based matching mechanism inspired by PML <ref type="bibr" target="#b5">[6]</ref> which allows us to systematically handle multiple objects in an end-to-end way. Like FEELVOS, RGMP does not require any fine-tuning, is fast, and achieves impressive results. However, RGMP does not meet the design goal of tackling the multi-object segmentation task in an end-toend way. The whole network needs to be run once for each object and the multi-object segmentation is performed by heuristic merging. Additionally RGMP does not fulfill the design goal of being simple, since it relies on an elaborate training procedure which involves multiple datasets, synthetic data generation and backpropagation through time. We show that despite using a much simpler training procedure and no simulated data, FEELVOS produces better results than RGMP. Instance Embedding Learning. Li et al. <ref type="bibr" target="#b20">[21]</ref> propose an embedding based method for performing video object segmentation in an unsupervised way, i.e., without using any ground truth information from the first frame. For image instance segmentation, Fathi et al. <ref type="bibr" target="#b11">[12]</ref> propose learning an instance embedding for each pixel. We adopt the same embedding distance formulation, but do not use their proposed seed points and we train the embedding in a different way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Overview. We propose FEELVOS for fast semi-supervised video object segmentation. FEELVOS uses a single convolutional network and requires only a single forward pass for each video frame. See <ref type="figure">Fig. 1</ref> for an overview of FEELVOS. The proposed architecture uses DeepLabv3+ <ref type="bibr" target="#b4">[5]</ref> (with its output layer removed) as its backbone to extract features with a stride of 4. On top of that, we add an embedding layer which extracts embedding feature vectors at the same stride. Afterwards, for each object we compute a distance map by globally matching the embedding vectors of the current frame to the embedding vectors belonging to this object in the first frame. Additionally, we use the predictions of the previous time frame in order to compute for each object another distance map by locally matching the current frame embeddings to the embedding vectors of the previous frame. Both global and local matching will be described in more detail below. Like in MaskTrack <ref type="bibr" target="#b27">[28]</ref> or RGMP <ref type="bibr" target="#b36">[37]</ref> we also use the predictions of the previous frame directly as an additional cue. Finally, we combine all available cues, i.e., the global matching distance maps, the local matching distance maps, the predictions from the previous frame, and the backbone features. We then feed them to a dynamic segmentation head which produces for each pixel (with a stride of 4) a posterior distribution over all objects which are present in the first frame. The whole system is trained end-to-end for multi-object segmentation without requiring a direct loss on the embedding. In the following, we will describe each of the components in more detail. Semantic Embedding. For each pixel p, we extract a semantic embedding vector e p in the learned embedding space. The idea of the embedding space is that pixels belonging to the same object instance (in the same frame or in different frames) will be close in the embedding space <ref type="bibr">Figure 2</ref>. Global and local matching. For a given object (in this case the duck), global matching matches the embedding vectors of the current frame to the embedding vectors of the first frame which belong to the object and produces a distance map. Dark color denotes low distances. Note that the global distance map is noisy and contains false-positives in the water. Local matching is used to match the current frame embeddings to the embeddings of the previous frame which belong to the object. For local matching, matches for a pixel are only allowed in a local window around it. and pixels which belong to distinct objects will be far away. Note that this is not explicitly enforced, since instead of using distances in the embedding space directly to produce a segmentation like in PML <ref type="bibr" target="#b5">[6]</ref> or VideoMatch <ref type="bibr" target="#b16">[17]</ref>, we use them as a soft cue which can be refined by the dynamic segmentation head. However, in practice the embedding indeed behaves in this way since this delivers a strong cue to the dynamic segmentation head for the final segmentation.</p><p>Similar to Fathi et al. <ref type="bibr" target="#b11">[12]</ref>, we define the distance between pixels p and q in terms of their corresponding embedding vectors e p and e q by</p><formula xml:id="formula_0">d(p, q) = 1 − 2 1 + exp( e p − e q 2 )</formula><p>.</p><p>The distance values are always between 0 and 1. For identical pixels, the embedding distance is d(p, p) = 1 − 2 1+exp(0) = 0, and for pixels which are very far away in the embedding space, we have d(p, q) = 1 − 2 1+exp(∞) = 1. Global Matching. Similar to PML <ref type="bibr" target="#b5">[6]</ref> and VideoMatch <ref type="bibr" target="#b16">[17]</ref>, we transfer semantic information from the first video frame for which we have the ground truth to the current frame to be segmented by considering nearest neighbors in the learned embedding space.</p><p>Let P t denote the set of all pixels (with a stride of 4) at time t and P t,o ⊆ P t the set of pixels at time t which belong to object o. We then compute the global matching distance map G t,o (p) for each ground truth object o and each pixel p ∈ P t of the current video frame t as the distance to its nearest neighbor in the set of pixels P 1,o of the first frame which belong to object o, i.e.,</p><formula xml:id="formula_2">G t,o (p) = min q∈P1,o d(p, q).</formula><p>(2)</p><p>Note that P 1,o is never empty, since we consider exactly the objects o which are present in the first frame, and note that background is handled like any other object. G t,o (p) provides for each pixel and each object of the current frame a soft cue of how likely it belongs to this object. See <ref type="figure">Fig. 2</ref> for an example visualization of the global matching distance map. It can be seen that the duck is relatively well captured, but the distance map is noisy and contains many false-positive small distances in the water. This is a strong motivation for not using these distances directly to produce segmentations but rather as an input to a segmentation head which can recover from noisy distances.</p><p>In practice we compute the global matching distance maps by a large matrix product, from which we derive all pairwise distances between the current and the first frame and then apply the object-wise minimization. Local Previous Frame Matching. In addition to transferring semantic information from the first frame using the learned embedding, we also use it to transfer information between adjacent frames to effectively enable tracking and dealing with appearance changes. Similar to the global matching distance map, we define the distance mapĜ t,o (p) with respect to the previous frame bŷ</p><formula xml:id="formula_3">G t,o (p) = min q∈Pt−1,o d(p, q) if P t−1,o = ∅ 1 otherwise<label>(3)</label></formula><p>where the time index of P changed from 1 to t − 1. Additionally, P t−1,o is now given by our own predictions instead of by the first-frame ground truth, which means that it can be empty, in which case we define the distance as 1.</p><p>When matching the current frame to the first frame, each pixel of the current frame needs to be compared to each pixel of the first frame, since the objects might have moved a lot over time. However, when matching with respect to the previous frame, we can exploit the fact that the motion between two frames is usually small to avoid false positive matches and save computation time. Hence, in practice we do not useĜ t,o (p) but instead we use a local matching distance map. Inspired by FlowNet <ref type="bibr" target="#b10">[11]</ref>, for pixel p of frame t we only consider pixels q of frame t − 1 in a local neighborhood of p when searching for a nearest neighbor. For a given window size k, we define the neighborhood N (p) as the set of pixels (regardless of which frame they are coming from) which are at most k pixels away from p in both x and y direction. This means that N (p) usually comprises (2 · k + 1) 2 elements in the same frame where p is coming from, and fewer elements close to the image boundaries. The local matching distance map L t,o (p) for time t, object o, and pixel p is defined by</p><formula xml:id="formula_4">L t,o (p) = min q∈P p t−1,o d(p, q) if P p t−1,o = ∅ 1 otherwise,<label>(4)</label></formula><p>where P p t−1,o := P t−1,o ∩ N (p) is the set of pixels of frame t − 1 which belong to object o and are in the neighborhood of p. Note that L t,o (p) can be efficiently computed using cross-correlation. We found that in practice using local previous frame matching, i.e., L t,o (p), produces better results and is more efficient than using the global previous frame distance mapĜ t,o (p). <ref type="figure">Fig. 2</ref> also shows an example visualization of a local matching distance map. Note that all pixels which are too far away from the previous frame mask are assigned a distance of 1. Since the motion between the previous and current frame was small, local matching produces a very sharp and accurate distance map. Previous Frame Predictions. In addition to using our own predictions of the previous time frame for local previous frame matching, we found it helpful to use these predictions, i.e., the posterior probability map over objects, also directly as features as an additional cue. Dynamic Segmentation Head. In order to systematically and efficiently deal with a variable number of objects, we propose a dynamic segmentation head which is dynamically instantiated once for each object with shared weights (see <ref type="figure" target="#fig_0">Fig. 3</ref>). The inputs to the dynamic segmentation head for object o at time t are i) the global matching distance map G t,o (·), ii) the local matching distance map L t,o (·), iii) the probability distribution for object o predicted at time t − 1, and iv) the shared backbone features. The dynamic segmentation head consists of a few convolutional layers (see the implementation details below) which extract a onedimensional feature map of logits for one object. Note that only three of the 259 dimensions of the input of the dynamic segmentation head differ between distinct objects, but we found that these three dimensions in practice deliver a strong enough cue to produce accurate logits when combined with the backbone features. After for each object a one-dimensional feature map of logits is extracted, we stack them together, apply softmax over the object dimension and apply a cross entropy loss.</p><p>The segmentation head needs to be run once for each object, but the majority of the computation occurs in extracting the shared backbone features which allows FEELVOS to scale well to multiple objects. Additionally, we are able to train end-to-end for multi-object segmentation even for a variable number of objects. Both properties are in strong contrast to many recent methods like RGMP <ref type="bibr" target="#b36">[37]</ref> which evaluate a full network, designed for single-object segmentation, once for each object and heuristically combine the individual results.</p><p>Training procedure. Our training procedure is deliberately simple. For each training step, we first randomly select a mini-batch of videos. For each video we randomly select three frames: one frame which serves as the reference frame, i.e., it plays the role of the first frame of a video, and two adjacent frames from which the first serves as the previous frame and the second one serves as the current frame to be segmented. We apply the loss only to the current frame. During training, we use the ground truth of the previous frame for local matching and also use it to define the previous frame predictions by setting them to 1 for the correct object and to 0 for all other objects for each pixel. Note that our training procedure is much simpler than the one used in RGMP <ref type="bibr" target="#b36">[37]</ref> which requires synthetic generation of data, and backpropagation through time.</p><p>Inference. Inference for FEELVOS is straightforward and requires only a single forward pass per frame. Given a test video with the ground truth for the first frame, we first extract the embedding vectors for the first frame. Afterwards, we go through the video frame-by-frame, compute the em-bedding vectors for the current frame, apply global matching to the first frame and local matching to the previous frame, run the dynamic segmentation head for each object, and apply a pixelwise argmax to produce the final segmentation. For the previous-frame prediction features we use the soft probability map predicted at the previous frame. Implementation Details. As the backbone of our network, we use the recent DeepLabv3+ architecture <ref type="bibr" target="#b4">[5]</ref> which is based on the Xception-65 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33]</ref> architecture and applies depthwise separable convolutions <ref type="bibr" target="#b13">[14]</ref>, batch normalization <ref type="bibr" target="#b17">[18]</ref>, Atrous Spatial Pyramid Pooling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, and a decoder module which produces features with a stride of 4.</p><p>On top of this we add an embedding layer consisting of one depthwise separable convolution, i.e., a 3 × 3 convolution performed separately for each channel followed by a 1 × 1 convolution, allowing interactions between channels. We extract embedding vectors of dimension 100.</p><p>For the dynamic segmentation head, we found that a large receptive field is important. In practice, we use 4 depthwise separable convolutional layers with a dimensionality of 256, a kernel size of 7 × 7 for the depthwise convolutions, and a ReLU activation function. On top of this we add a 1 × 1 convolution to extract logits of dimension 1.</p><p>Computing distances for all pairs of pixels for global matching during training is expensive. We found that in practice it is unnecessary to consider all pairs. Hence, during training we randomly subsample the set of reference pixels from the first frame to contain at most 1024 pixels per object, and found that it does not affect the results much.</p><p>For local matching we use a window size of k = 15 applied on the embedding vectors which are extracted with a stride of 4. We start training using weights for DeepLabv3+ which were pre-trained on ImageNet <ref type="bibr" target="#b9">[10]</ref> and COCO <ref type="bibr" target="#b22">[23]</ref>. As training data we use the DAVIS 2017 <ref type="bibr" target="#b30">[31]</ref> training set (60 videos) and the YouTube-VOS <ref type="bibr" target="#b38">[39]</ref> training set (3471 videos). We apply a bootstrapped cross entropy loss <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref> which only takes into account the 15% hardest pixels for calculating the loss. We optimize using gradient descent with a momentum of 0.9, and a learning rate of 0.0007 for 200,000 steps with a batch size of 3 videos (i.e. 9 images) per GPU using 16 Tesla P100 GPUs. We apply flipping and scaling as data augmentations, and crop the input images randomly to a size of 465 × 465 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>After training, our network is evaluated on the DAVIS 2016 <ref type="bibr" target="#b28">[29]</ref> validation set, the DAVIS 2017 <ref type="bibr" target="#b30">[31]</ref> validation and test-dev sets, and the YouTube-Objects <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19]</ref> dataset. The DAVIS 2016 validation set consists of 20 videos for each of which a single instance is annotated. The DAVIS 2017 dataset comprises a training set of 60 sequences with multiple annotated instances and a validation set that extends the DAVIS 2016 validation set to a total of 30 videos  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEELVOS (ours) FEELVOS (-YTB-VOS)</head><p>PReMVOS <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24]</ref> VideoMatch <ref type="bibr" target="#b16">[17]</ref> OnAVOS <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref> OSVOS-S <ref type="bibr" target="#b26">[27]</ref> DyeNet <ref type="bibr" target="#b21">[22]</ref> RGMP <ref type="bibr" target="#b36">[37]</ref> OSMN <ref type="bibr" target="#b39">[40]</ref> FAVOS <ref type="bibr" target="#b6">[7]</ref>  with multiple instances annotated. The DAVIS 2017 testdev set also contains 30 sequences. The YouTube-Objects dataset <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19]</ref> consists of 126 videos with sparse annotations of a single instance per video.</p><p>We adopt the evaluation measures defined by DAVIS <ref type="bibr" target="#b28">[29]</ref>. The first evaluation criterion is the mean intersectionover-union (mIoU) between the predicted and the ground truth segmentation masks, denoted by J . The second evaluation criterion is the contour accuracy F, described in more detail in <ref type="bibr" target="#b28">[29]</ref>. Finally, J &amp;F is the average of J and F. Main Results. <ref type="table" target="#tab_2">Tables 2 and 3</ref> compare our results on the DAVIS 2017 validation and test-dev sets to recent other methods which do not employ fine-tuning and to PRe-MVOS <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> and OnAVOS <ref type="bibr" target="#b34">[35]</ref>. Note that PML   [6] does not provide results for DAVIS 2017. On the validation set, FEELVOS improves over the previously best non-finetuning method RGMP <ref type="bibr" target="#b36">[37]</ref> both in terms of mIoU J and contour accuracy F. For non-fine-tuning methods FEELVOS achieves a new state of the art with a J &amp;F score of 71.5% which is 4.8% higher than RGMP and 2.4% higher when not using YouTube-VOS data for training (denoted by -YTB-VOS). FEELVOS's result is stronger than the result of OnAVOS <ref type="bibr" target="#b34">[35]</ref> which heavily relies on finetuning. However, FEELVOS cannot match the results of the heavily engineered and slow PReMVOS <ref type="bibr" target="#b25">[26]</ref>. On the DAVIS 2017 test-dev set, FEELVOS achieves a J &amp;F score of 57.8%, which is 4.9% higher than the result of RGMP <ref type="bibr" target="#b36">[37]</ref>. Here, the runtime of RGMP and FEELVOS is almost identical since FEELVOS's runtime is almost independent of the number of objects and the test-dev set contains more objects per sequence. <ref type="figure" target="#fig_1">Fig. 4</ref> shows the J &amp;F score and runtime of current methods with and without fine-tuning. It can be seen that FEELVOS achieves a very good speed/accuracy trade-off with a runtime of 0.51 seconds per frame. <ref type="table">Table 4</ref> shows the results on the simpler DAVIS 2016 validation set which only has a single object instance annotated per sequence. Here, FEELVOS's result with a J &amp;F score of 81.7% is comparable to RGMP, which achieves  <ref type="table">Table 6</ref>. Ablation study on DAVIS 2017. FF and PF denote first frame and previous frame, respectively, and GM and LM denote global matching and local matching. PFP denotes using the previous frame predictions as input to the dynamic segmentation head.</p><p>81.8%. However, RGMP heavily relies on simulated training data, which our method does not require. Without the simulated data, RGMP achieves only 68.8% J &amp;F. Again, FEELVOS is not able to reach the results of fine-tuning based methods, but it achieves a very good speed/accuracy trade-off and only uses a single neural network. <ref type="table" target="#tab_4">Table 5</ref> shows the results on YouTube-Objects <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b18">19]</ref>. Note that the evaluation protocol for this dataset is not always consistent and the results marked with * might not be directly comparable. FEELVOS achieves a J score of 82.1% which is even better than the results of the finetuning based methods OSVOS <ref type="bibr" target="#b0">[1]</ref> and OnAVOS <ref type="bibr" target="#b34">[35]</ref>. Ablation Study. In <ref type="table">Table 6</ref> we analyze the effect of the individual components of FEELVOS on the DAVIS 2017 validation set. For simplicity, we only use the smaller DAVIS 2017 training set as training data for these experiments. Line 1 is the proposed FEELVOS which uses first-frame global matching (FF-GM), previous frame local matching (PV-LM), and previous frame predictions (PFP) as inputs for the dynamic segmentation head. This setup achieves a J &amp;F score of 69.1%.</p><p>In line 2, we replace the previous frame local matching (PF-LM) by previous frame global matching (PF-LM), which significantly degrades the results by almost 5% to 64.2% and shows the effectiveness of restricting the previous frame matching to a local window.</p><p>In line 3, we disable previous frame matching completely. Here the results drop even more to 54.9% which shows that matching to the previous frame using the learned embedding is extremely important to achieve good results.</p><p>In line 4, we additionally disable the use of previous frame predictions (PFP) as features to the dynamic segmentation head. In this setup, each frame can be segmented individually and only information from matching globally to the first frame is used. In this case, the results deteriorate even further to 52.6%.</p><p>In line 5, we use previous frame local matching (PF-LM) again, but disable the use of previous frame predictions (PFP). In this case, the result is 63.3% which is much better than the result of line 3 which used PFP but no PF-LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YouTube Objects</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0%</head><p>25% 50% 75% 100% DAVIS 2017 <ref type="figure">Figure 5</ref>. Qualitative results on the DAVIS 2017 validation set and on YouTube-Objects. In the third row the many similar looking fishes cause FEELVOS to lose track of some of them. In the last row, FEELVOS fails to segment some of the back of the cat.</p><p>This shows that previous frame local matching is a more effective way to transfer information from the previous frame than just using the previous frame predictions as features. It also shows that both ways to transfer information over time are complementary and their combination is most effective.</p><p>In line 6, we use PF-LM and PFP but disable the firstframe global matching. This means that the first-frame information is only used to initialize the mask used for PF-LM and PFP but no longer as explicit guidance for each frame. Here the result deteriorates compared to line 1 by 13% which shows that matching to the first frame is extremely important to achieve good results.</p><p>In summary, we showed that each component of FEELVOS is useful, that matching in embedding space to the previous frame is extremely effective, and that the proposed local previous frame matching performs significantly better than globally matching to the previous frame.</p><p>Qualitative Results. <ref type="figure">Fig. 5</ref> shows qualitative results of FEELVOS on the DAVIS 2017 validation set and the YouTube-Objects dataset. It can be seen that in many cases FEELVOS is able to produce accurate segmentations even in difficult cases like large motion in the judo sequence or a truncated first frame for the car. In the challenging fish sequence (third row), FEELVOS looses track of some of the fishes, probably because their appearance is very similar. In the last row, FEELVOS fails to segment some parts of the back of the cat. This is most likely because the back texture was not seen in the first frame. However, afterwards, FEELVOS is able to recover from that error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We started with the observation that there are many strong methods for VOS, but many of them lack practical usability. Based on this insight, we defined several design goals which a practical useful method for VOS should fulfill. Most importantly we aim for a fast and simple method which yet achieves strong results. To this end, we propose FEELVOS which learns a semantic embedding for segmenting multiple objects in an end-to-end way. The key components of FEELVOS are global matching to the first frame of the video and local matching to the previous frame. We showed experimentally that each component of FEELVOS is highly effective and we achieve new state of the art results on DAVIS 2017 for VOS without fine-tuning. Overall, FEELVOS is a fast and practical useful method for VOS and we hope that our work will inspire more methods which fulfill the design criteria defined by us and advance the state of the art for practical useful methods in VOS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>GFigure 3 .</head><label>3</label><figDesc>lo b a l m a t c h in g d is t a n c e m a p ( 1 p e r o b je c t ) L o c a l m a t c h in g d is t a n c e m a p ( 1 p e r o b je c t ) P r e v io u s f r a m e p r e d ic t io n s ( 1 p e r o b je c t ) Dynamic segmentation head for systematic handling of multiple objects. The lightweight segmentation head is dynamically instantiated once for each object in the video and produces a one-dimensional feature map of logits for each object. The logits for each object are then stacked together and softmax is applied. The dynamic segmentation head can be trained with a standard cross entropy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Quality versus timing on DAVIS 2017. The proposed FEELVOS shows a very good speed/accuracy trade-off. FEELVOS (-YTB-VOS) denotes training without YouTube-VOS [39] training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FT</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results on the DAVIS 2017 test-dev set. FT denotes fine-tuning, and t/s denotes time per frame in seconds. †: timing extrapolated from DAVIS 2016 assuming linear scaling in the number of objects.</figDesc><table><row><cell>Region and contour quality (J &amp;F )</cell><cell>.6 .7 .8</cell></row><row><cell></cell><cell>1</cell><cell>10</cell></row><row><cell></cell><cell>Time per frame (seconds)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Quantitative results on the YouTube-Objects dataset. FT denotes fine-tuning. *: The used evaluation protocol for YouTube-Objects is inconsistent, e.g. sometimes the first frame is ignored; the results marked with * might not be directly comparable.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We would like to thank Alireza Fathi, Andre Araujo, Bryan Seybold, Jonathon Luiten, and Jordi Pont-Tuset for helpful discussions and Yukun Zhu for help with open-sourcing our models.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00557</idno>
		<title level="m">The 2018 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video object segmentation by learning location-sensitive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reinforcement cutting-agent learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motionguided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Instance embedding transfer to unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobyov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">PReMVOS: Proposal-generation, refinement and merging for the davis challenge on video object segmentation 2018. The 2018 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">PReMVOS: Proposal-generation, refinement and merging for the YouTube-VOS challenge on video object segmentation 2018. The 1st Large-scale Video Object Segmentation Challenge -ECCV Workshops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PReMVOS: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fullresolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks -coco detection and segmentation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for the 2017 DAVIS challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation -CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Bridging categorylevel and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">YouTube-VOS: Sequence-tosequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
