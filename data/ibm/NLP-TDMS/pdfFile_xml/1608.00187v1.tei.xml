<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Relationship Detection with Language Priors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>cwlu@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
							<email>ranjaykrishna@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<email>feifeili@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Relationship Detection with Language Priors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual relationships capture a wide variety of interactions between pairs of objects in images (e.g. "man riding bicycle" and "man pushing bicycle"). Consequently, the set of possible relationships is extremely large and it is difficult to obtain sufficient training examples for all possible relationships. Because of this limitation, previous work on visual relationship detection has concentrated on predicting only a handful of relationships. Though most relationships are infrequent, their objects (e.g. "man" and "bicycle") and predicates (e.g. "riding" and "pushing") independently occur more frequently. We propose a model that uses this insight to train visual models for objects and predicates individually and later combines them together to predict multiple relationships per image. We improve on prior work by leveraging language priors from semantic word embeddings to finetune the likelihood of a predicted relationship. Our model can scale to predict thousands of types of relationships from a few examples. Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. We further demonstrate that understanding relationships can improve content based image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* = equal contribution</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While objects are the core building blocks of an image, it is often the relationships between objects that determine the holistic interpretation. For example, an image with a person and a bicycle might involve the man riding, pushing, or even falling off of the bicycle <ref type="figure">(Figure 1</ref>). Understanding this diversity of relationships is central to accurate image retrieval and to a richer semantic understanding of our visual world.</p><p>Visual relationships are a pair of localized objects connected via a predicate ( <ref type="figure">Figure 2</ref>). We represent relationships as object 1predicateobject 2 1 . Visual relationship detection involves detecting and localizing pairs of objects in an image and also classifying the predicate or interaction between each pair ( <ref type="figure">Figure 2</ref>). While it poses similar challenges as object detection <ref type="bibr" target="#b0">[1]</ref>, one critical difference is that the size of the semantic space of possible relationships is much larger than that of objects. Since relationships are composed of two objects, there is a greater skew of rare relationships as object co-occurrence is infrequent in falling off riding pushing next to carrying <ref type="figure">Fig. 1</ref>: Even though all the images contain the same objects (a person and a bicycle), it is the relationship between the objects that determine the holistic interpretation of the image.</p><p>images. So, a fundamental challenge in visual relationship detection is learning from very few examples. Visual Phrases <ref type="bibr" target="#b5">[6]</ref> studied visual relationship detection using a small set of 13 common relationships. Their model requires enough training examples for every possible object 1predicateobject 2 combination, which is difficult to collect owing to the infrequency of relationships. If we have N objects and K predicates, Visual Phrases <ref type="bibr" target="#b5">[6]</ref> would need to train O(N 2 K) unique detectors separately. We use the insight that while relationships (e.g. "person jumping over a fire hydrant") might occur rarely in images, its objects (e.g. person and fire hydrant) and predicate (e.g. jumping over) independently appear more frequently. We propose a visual appearance module that learns the appearance of objects and predicates and fuses them together to jointly predict relationships. We show that our model only needs O(N + K) detectors to detect O(N 2 K) relationships.</p><p>Another key observation is that relationships are semantically related to each other. For example, a "person riding a horse" and a "person riding an elephant" are semantically similar because both elephant and horse are animals. Even if we haven't seen many examples of "person riding an elephant", we might be able to infer it from a "person riding a horse". Word vector embeddings <ref type="bibr" target="#b6">[7]</ref> naturally lend themselves in linking such relationships because they capture semantic similarity in language (e.g. elephant and horse are cast close together in a word vector space). Therefore, we also propose a language module that uses pre-trained word vectors <ref type="bibr" target="#b6">[7]</ref> to cast relationships into a vector space where similar relationships are optimized to be close to each other. Using this embedding space, we can finetune the prediction scores of our relationships and even enable zero shot relationship detection.</p><p>In this paper, we propose a model that can learn to detect visual relationships by (1) (1) learning visual appearance models for its objects and predicates and (2) using the relationship embedding space learnt from language. We train our model by optimizing a bi-convex function. To benchmark the task of visual relationship detection, we introduce a new dataset that contains 5000 images with 37, 993 relationships. Existing datasets that contain relationships were designed <ref type="bibr" target="#b0">1</ref> In natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, relationships are defined as subjectpredicateobject . In this paper, we define them as object1predicate -object2 for simplicity.</p><p>person -on -motorcycle person -wear -helmet motorcycle -has -wheel Input Output <ref type="figure">Fig. 2</ref>: Visual Relationship Detection: Given an image as input, we detect multiple relationships in the form of object 1relationshipobject 2 . Both the objects are localized in the image as bounding boxes. In this example, we detect the following relationships: persononmotorcycle , personwearhelmet and motorcyclehaswheel .</p><p>for improving object detection <ref type="bibr" target="#b5">[6]</ref> or image retrieval <ref type="bibr" target="#b7">[8]</ref> and hence, don't contain sufficient variety of relationships or predicate diversity per object category. Our model outperforms all previous models in visual relationship detection. We further study how our model can be used to perform zero shot visual relationship detection. Finally, we demonstrate that understanding relationships can improve image-based retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual relationship prediction involves detecting the objects that occur in an image as well as understanding the interactions between them. There has been a series of work related to improving object detection by leveraging object cooccurrence statistics <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Structured learning approaches have improved scene classification along with object detection using hierarchial contextual data from co-occurring objects <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Unlike these methods, we study the context or relationships in which these objects co-occur. Some previous work has attempted to learn spatial relationships between objects <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref> to improve segmentation <ref type="bibr" target="#b18">[19]</ref>. They attempted to learn four spatial relationships: "above", "below", "inside", and "around" <ref type="bibr" target="#b12">[13]</ref>. While we believe that that learning spatial relationships is important, we also study nonspatial relationships such as pull (actions), taller than (comparative), etc.</p><p>There have been numerous efforts in human-object interaction <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22</ref>] and action recognition <ref type="bibr" target="#b22">[23]</ref> to learn discriminative models that distinguish between relationships where object 1 is a human ( e.g. "playing violin" <ref type="bibr" target="#b23">[24]</ref>). Visual relationship prediction is more general as object 1 is not constrained to be a human and the predicate doesn't have to be a verb.</p><p>Visual relationships are not a new concept. Some papers explicitly collected relationships in images <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and videos <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> and helped models map these relationships from images to language. Relationships have also improved object localization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref>. A meaning space of relationships have aided the cognitive task of mapping images to captions <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. Finally, they have been used to generate indoor images from sentences <ref type="bibr" target="#b38">[39]</ref> and   to improve image search <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref>. In this paper, we formalize visual relationship prediction as a task onto itself and demonstrate further improvements in image retrieval. The most recent attempt at relationship prediction has been in the form of visual phrases. Learning appearance models for visual phrases has shown to improve individual object detection, i.e. detecting "a person riding a horse" improves the detection and localization of "person" and "horse" <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41]</ref>. Unlike our model, all previous work has attempted to detect only a handful of visual relationships and do not scale because most relationships are infrequent. We propose a model that manages to scale and detect millions of types of relationships. Additionally, our model is able to detect unseen relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual Relationship Dataset</head><p>Visual relationships put objects in context; they capture the different interactions between pairs of objects. These interactions (shown in <ref type="figure" target="#fig_1">Figure 3</ref>) might be verbs (e.g. wear), spatial (e.g. on top of), prepositions (e.g. with), comparative (e.g. taller than), actions (e.g. kick) or a preposition phrase (e.g. drive on). A dataset for visual relationship prediction is fundamentally different from a dataset for object detection. A relationship dataset should contain more than just objects localized in images; it should capture the rich variety of interactions between pairs of objects (predicates per object category). For example, a person can be associated with predicates such as ride, wear, kick etc. Additionally, the dataset should contain a large number of possible relationships types.</p><p>Existing datasets that contain relationships were designed to improve object detection <ref type="bibr" target="#b5">[6]</ref> or image retrieval <ref type="bibr" target="#b7">[8]</ref>. The Visual Phrases <ref type="bibr" target="#b5">[6]</ref> dataset focuses on 17 common relationship types. But, our goal is to understand the rich variety of infrequent relationships. On the other hand, even though the Scene Graph dataset <ref type="bibr" target="#b7">[8]</ref> has 23,190 relationship types 2 , it only has 2.3 predicates per object category. Detecting relationships on the Scene Graph dataset <ref type="bibr" target="#b7">[8]</ref> essentially boils down to object detection. Therefore, we designed a dataset specifically for benchmarking visual relationship prediction.</p><p>Our dataset <ref type="table" target="#tab_0">(Table 1)</ref> contains 5000 images with 100 object categories and 70 predicates. In total, the dataset contains 37,993 relationships with 6,672 relationship types and 24.25 predicates per object category. Some example relationships are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The distribution of relationships in our dataset highlights the long tail of infrequent relationships <ref type="figure" target="#fig_1">(Figure 3</ref>(left)). We use 4000 images in our training set and test on the remaining 1000 images. 1,877 relationships occur in the test set but never occur in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Visual Relationship Prediction Model</head><p>The goal of our model is to detect visual relationships from an image. During training (Section 4.1), the input to our model is a fully supervised set of images with relationship annotations where the objects are localized as bounding boxes and labelled as object 1predicateobject 2 . At test time (Section 4.2), our input is an image with no annotations. We predict multiple relationships and localize the objects in the image. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates a high level overview of our detection pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Approach</head><p>In this section, we describe how we train our visual appearance and language modules. Both the modules are combined together in our objective function.</p><p>Visual Appearance Module While Visual Phrases <ref type="bibr" target="#b5">[6]</ref> learned a separate detector for every single relationship, we model the appearance of visual relationships V () by learning the individual appearances of its comprising objects and predicate. While relationships are infrequent in real world images, the objects and predicates can be learnt as they independently occur more frequently. Furthermore, we demonstrate that our model outperforms Visual Phrases' detectors, showing that learning individual detectors outperforms learning detectors for relationships together <ref type="table" target="#tab_2">(Table 2)</ref>.</p><p>First, we train a convolutional neural network (CNN) (VGG net <ref type="bibr" target="#b43">[44]</ref>) to classify each of our N = 100 objects. Similarly, we train a second CNN (VGG net <ref type="bibr" target="#b43">[44]</ref>) to classify each of our K = 70 predicates using the union of the bounding boxes of the two participating objects in that relationship. Now, for each ground truth relationship R i,k,j where i and j are the object classes (with bounding boxes O 1 and O 2 ) and k is the predicate class, we model V ( <ref type="figure" target="#fig_2">Figure 4</ref>) as:</p><formula xml:id="formula_0">V (R i,k,j , Θ| O1, O2 ) = Pi(O1)(z T k CNN(O1, O2) + s k )Pj(O2) (1)</formula><p>where Θ is the parameter set of {z k , s k }. z k and s k are the parameters learnt to convert our CNN features to relationship likelihoods. k = 1, . . . , K represent the K predicates in our dataset. P i Projection Function First, we use pre-trained word vectors (word2vec) <ref type="bibr" target="#b6">[7]</ref> to cast the two objects in a relationship into an word embedding space <ref type="bibr" target="#b6">[7]</ref>. Next, we concatenate these two vectors together and transform it into the relationship vector space using a projection parameterized by W, which we learn. This projection presents how two objects interact with each other. We denote word2vec() as the function that converts a word to its 300 dim. vector. The relationship projection function (shown in <ref type="figure" target="#fig_2">Figure 4</ref>) is defined as:</p><formula xml:id="formula_1">f (R i,k,j , W) = w T k [word2vec(ti), word2vec(tj)] + b k (2)</formula><p>where t j is the word (in text) of the j th object category. w k is a 600 dim. vector and b k is a bias term. W is the set of</p><formula xml:id="formula_2">{{w 1 , b 1 }, . . . , {w k , b k }},</formula><p>where each row presents one of our K predicates.</p><p>Training Projection Function We want to optimize the projection function f () such that it projects similar relationships closer to one another. For example, we want the distance between manridinghorse to be close to manridingcow but farther from carhaswheel . We formulate this by using a heuristic where the distance between two relationships is proportional to the word2vec distance between its component objects and predicate:</p><formula xml:id="formula_3">[f (R, W) − f (R , W)] 2 d(R, R ) = constant, ∀R, R<label>(3)</label></formula><p>where d(R, R ) is the sum of the cosine distances (in word2vec space <ref type="bibr" target="#b6">[7]</ref>) between of the two objects and the predicates of the two relationships R and R . Now, to satisfy Eq 3, we randomly sample pairs of relationships ( R, R ) and minimize their variance:</p><formula xml:id="formula_4">K(W) = var({ [f (R, W) − f (R , W)] 2 d(R, R ) ∀R, R })<label>(4)</label></formula><p>where var() is a variance function. The sample number we use is 500K.</p><p>Likelihood of a Relationship The output of our projection function should ideally indicate the likelihood of a visual relationship. For example, our model should not assign a high likelihood score to a relationship like dogdrivecar , which is unlikely to occur. We model this by enforcing that if R occurs more frequently than R in our training data, then it should have a higher likelihood of occurring again. We formulate this as a rank loss function:</p><formula xml:id="formula_5">L(W) = {R,R } max{f (R , W) − f (R, W) + 1, 0}<label>(5)</label></formula><p>While we only enforce this likelihood prior for the relationships that occur in our training data, the projection function f () generalizes it for all object 1 predicateobject 2 combinations, even if they are not present in our training data. The max operator here is to encourage correct ranking (with margin) f (R, W) − f (R , W) ≥ 1. Minimizing this objective enforces that a relationship with a lower likelihood of occurring has a lower f () score.</p><p>Objective function So far we have presented our visual appearance module (V ()) and the language module (f ()). We combine them to maximize the rank of the ground truth relationship R with bounding boxes O 1 and O 2 using the following rank loss function:</p><formula xml:id="formula_6">C(Θ, W) = O 1 O 2 ,R max{1 − V (R, Θ| O1, O2 )f (R, W) + max O 1 ,O 2 = O 1 ,O 2 ,R =R V (R , Θ| O 1 , O 2 )f (R , W), 0}<label>(6)</label></formula><p>We use a ranking loss function to make it more likely for our model to choose the correct relationship. Given the large number of possible relationships, we find that a classification loss performs worse. Therefore, our final objective function combines Eq 6 with Eqs 4 and 5 as:</p><formula xml:id="formula_7">min Θ,W {C(Θ, W) + λ1L(W) + λ2K(W)}<label>(7)</label></formula><p>where λ 1 = 0.05 and λ 2 = 0.002 are hyper-parameters that were obtained though grid search to maximize performance on the validation set. Note that both Eqs 6 and 5 are convex functions. Eq 4 is a biqudratic function with respect to W. So our objective function Eq 7 has a quadratic closed form. We perform stochastic gradient descent iteratively on Eqs 6 and 5. It converges in 20 ∼ 25 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Testing</head><p>At test time, we use RCNN <ref type="bibr" target="#b42">[43]</ref> to produce a set of candidate object proposals for every test image. Next, we use the parameters learnt from the visual appearance model (Θ) and the language module (W) to predict visual relationships (R * i,k,j ) for every pair of RCNN object proposals O 1 , O 2 using:</p><formula xml:id="formula_8">R * = arg max R V (R, Θ| O 1 , O 2 )f (R, W)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our model by detecting visual relationships from images. We show that our proposed method outperforms previous state-of-the-art methods on our dataset (Section 5.1) as well as on previous datasets (Section 5.3). We also measure how our model performs in zero-shot learning of visual relationships (Section 5.2). Finally, we demonstrate that understanding visual relationship can improve common computer vision tasks like content based image retrieval (Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visual Relationship Detection</head><p>Setup. Given an input image, our task is to extract a set of visual relationships object 1predicateobject 2 and localize the objects as bounding boxes We evaluate visual relationship detection using three conditions: predicate detection (where we only predict the predicate given the object classes and boxes), phrase detection (where we label a region of an image with a relationship) and relationship detection (where we detect the objects and label the predicate between them). in the image. We train our model using the 4000 training images and perform visual relationship prediction on the 1000 test images.</p><p>The evaluation metrics we report is recall @ 100 and recall @ 50 <ref type="bibr" target="#b44">[45]</ref>. Recall @ x computes the fraction of times the correct relationship is predicted in the top x confident relationship predictions. Since we have 70 predicates and an average of 18 objects per image, the total possible number of relationship predictions is 100×70×100, which implies that the random guess will result in a recall @ 100 of 0.00014. We notice that mean average precision (mAP) is another widely used metric. However, mAP is a pessimistic evaluation metric because we can not exhaustively annotate all possible relationships in an image. Consider the case where our model predicts persontaller thanperson . Even if the prediction is correct, mAP would penalize the prediction if we do not have that particular ground truth annotation.</p><p>Detecting a visual relationship involves classifying both the objects, predicting the predicate and localization both the objects. To study how our model performs on each of these tasks, we measure visual relationship prediction under the following conditions:</p><p>1. In predicate detection ( <ref type="figure">Figure 5(left)</ref>), our input is an image and set of localized objects. The task is to predict a set of possible predicates between pairs of objects. This condition allows us to study how difficult it is to predict relationships without the limitations of object detection <ref type="bibr" target="#b42">[43]</ref>. 2. In phrase detection ( <ref type="figure">Figure 5</ref>(middle)), our input is an image and our task is to output a label object 1predicateobject 2 and localize the entire relationship as one bounding box having at least 0.5 overlap with ground truth box. This is the evaluation used in Visual Phrases <ref type="bibr" target="#b5">[6]</ref>. 3. In relationship detection ( <ref type="figure">Figure 5(right)</ref>), our input is an image and our task is to output a set of object 1predicateobject 2 and localize both object 1 and object 2 in the image having at least 0.5 overlap with their ground truth boxes simultaneously.</p><p>Comparison Models. We compare our method with some state-of-that-art approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b43">44]</ref>. We further perform ablation studies on our model, considering just the visual appearance and the language module, including the likelihood term (Eq 4) and embedding term (Eq 5) to study their contributions.</p><p>-Visual phrases. Similar to Visual Phrases <ref type="bibr" target="#b5">[6]</ref>, we train deformable parts models for each of the 6, 672 relationships (e.g. "chair under table") in our training set. -Joint CNN. We train a CNN model <ref type="bibr" target="#b43">[44]</ref> to predict the three components of a relationship together. Specifically, we train a 270 (100 + 100 + 70) way classification model that learns to score the two objects (100 categories each) and predicate (70 categories). This model represents the Visual phrases -Visual appearance (Ours -V only). We only use the visual appearance module of our model described in Eq 6 by optimizing V (). -Likelihood of a relationship (Ours -L only). We only use the likelihood of a relationship described in Eq 5 by optimizing L(). -Visual appearance + naive frequency (Ours -V + naive FC ).</p><p>One of the contributions of our model is the ability to use a language prior via our semantic projection function f () (Eq 2). Here, we replace f () with a function that maps a relationship to its frequency in our training data. Using this naive function, we hope to test the effectiveness of f (). -Visual appearance + Likelihood (Ours -V + L only). We use both the visual appearance module (Eq 6) and the likelihood term (Eq 5) by optimizing both V () and L(). The only part of our model missing is K() Eq 4, which projects similar relationships closer. -Visual appearance + likelihood + regularizer (Ours -V + L + Reg.). We use the visual appearance module and the likelihood term and add an L 2 regularizer on W . -Full Model (Ours -V + L + K ). This is our full model. It contains the visual appearance module (Eq 6), the likelihood term (Eq 5) and the embedding term (Eq 4) from similar relationships.</p><p>Results. Visual Phrases <ref type="bibr" target="#b5">[6]</ref> and Joint CNN <ref type="bibr" target="#b43">[44]</ref> train an individual detector for every relationship. Since the space of all possible relationships is large (we have 6,672 relationship types in the training set), there is a shortage of training examples for infrequent relationships, causing both models to perform poorly on predicate, phrase and relationship detection ( <ref type="table" target="#tab_2">Table 2</ref>). (Ours -V only) can't discriminative between similar relationships by itself resulting in 1.85 R@100 for relationship detection. Similarly, (Ours -L only) always predicts the most frequent relationship personwearshirt and results in 0.08 R@100, which is the percentage of the most frequent relationship in our testing data. These problems are remedied when both V and L are combined in (Ours -V + L only) with an increase of 3% R@100 in on both phrase and relationship detection and more than 10% increase in predicate detection. (V + Naive FC) is missing our relationship projection function f (), which learns the likelihood of a predicted relationship and performs worse than (Ours -V + L only) and (Ours - V + L + K). Also, we observe that (Ours -V + L + K) has an 11% improvement in comparison to (Ours -V + L only) in predicate detection, demonstrating that the language module from similar relationships significantly helps improve visual relationship detection. Finally, (Ours -V + L + K) outperforms (Ours -V + L + Reg.) showcasing the K() is acting not only as a regularizer but is learning to preserve the distances between similar relationships. By comparing the performance of all the models between relationship and predicate detection, we notice a 30% drop in R@100. This drop in recall is largely because we have to localize two objects simultaneously, amplifying the object detection errors. Note that even when we have ground truth object proposals (in predicate detection), R@100 is still 47.87.</p><p>Qualitative Results. In <ref type="figure" target="#fig_5">Figure 6</ref>(a)(b)(c), Visual Phrase and Joint CNN incorrectly predict a common relationship: persondrivecar and carnext totree . These models tend to predict the most common relationship as they see a lot of them during training. In comparison, our model correctly predicts and localizes the objects in the image. But it still does not predict the correct predicate for (e) and (f) because person rideelephant and handholdphone rarely occur in our training set. However, our full model (Ours -V + L + K) leverages similar relationships it has seen before and is able to correctly detect the relationships in (e) and (f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Zero-shot Learning</head><p>Owing to the long tail of relationships in real world images, it is difficult to build a dataset with every possible relationship. Therefore, a model that detects visual  relationships should also be able to perform zero-shot prediction of relationships it has never seen before. Our model is able to leverage similar relationships it has already seen to detect unseen ones. Setup. Our test set contains 1, 877 relationships that never occur in our training set (e.g. elephantstand onstreet ). These unseen relationships can be inferred by our model using similar relationships (e.g. dogstand on street ) from our training set. We report our results for detecting unseen relationships in <ref type="table" target="#tab_3">Table 3</ref> for predicate, phrase, and relationship detection.</p><p>Results. (Ours -V) achieves a low 3.52 R@100 in predicate detection because visual appearances are not discriminative enough to predict unseen relationships. (Ours -L only) performs poorly in predicate detection (5.09 R@100) because it automatically returns the most common predicate. By comparing (Ours -V + L+ K) and (Ours -V + L only), we find the use of K gains an improvement of 30% since it utilizes similar relationships to enable zero shot predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visual Relationship Detection on Existing Dataset</head><p>Our goal in this paper is to understand the rich variety of infrequent relationships. Our comparisons in Section 3 show that existing datasets either do not have enough diveristy of predicates per object category or enough relationship types. Therefore, we introduced a new dataset (in Section 3) and tested our visual relationship detection model in Section 5.1 and Section 5.2. In this section, we run additional experiments on the existing visual phrases dataset <ref type="bibr" target="#b5">[6]</ref> to provide further benchmarks. Setup. The visual phrase dataset contains 17 phrases (e.g. "dog jumping"). We evaluate the models (introduced in Section 5.1) for visual relationship detection on 12 of these phrases that can be represented as a object 1predicate object 2 relationship. To study zero-shot learning, we remove two phrases ("person lying on sofa" and "person lying on beach") from the training set, and attempt to recognize them in the testing set. We report mAP, R@50 and R@100.</p><p>Results. In <ref type="table" target="#tab_4">Table 4</ref> we see that our method is able to perform better than the existing Visual Phrases' model even though the dataset is small and contains only 12 relationships. We get a mAP of 0.59 using our entire model as compared to a mAP of 0.38 using Visual Phrases' model. We also outperform the Joint CNN baseline, which achieves a mAP of 0.54. Considering that (Ours -V only) model performs similarly to the baselines, we believe that our full model's improvements on this dataset are heavily influenced by the language priors. By learning to embed similar relationships close to each other, the language model's aid can be thought of as being synonymous to the improvements achieved through training set augmentation. Finally, we see a similar improvements in zero shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Image based Retrieval</head><p>An important task in computer vision is image retrieval. An improved retrieval model should be able to infer the relationships between objects in images. We will demonstrate that the use of visual relationships can improve retrieval quality.</p><p>Setup. Recall that our test set contains 1000 images. Every query uses 1 of these 1000 images and ranks the remaining 999. We use 54 query images in our experiments. Two annotators were asked to rank image results for each of the 54 queries. To avoid bias, we consider the results for a particular query as ground  truth only if it was selected by both annotators. We evaluate performance using R@1, R@5 and R@10 and median rank <ref type="bibr" target="#b7">[8]</ref>. For comparison, we use three image descriptors that are commonly used in image retrieval: CNN <ref type="bibr" target="#b43">[44]</ref>, GIST <ref type="bibr" target="#b45">[46]</ref> and SIFT <ref type="bibr" target="#b46">[47]</ref>. We rank results for a query using the L 2 distance from the query image. Given a query image, our model predicts a set of visual relationships {R 1 , . . . , R n } with a probability of {P q 1 , . . . , P q n } respectively. Next, for every image I i in our test set, it predicts R 1 , . . . , R n with a confidence of {P i 1 , . . . , P i n }. We calculate a matching score between an image with the query as n j=1 P q j * P i j . We also compare our model with Visual Phrases' detectors <ref type="bibr" target="#b5">[6]</ref>.</p><p>Results. SIFT <ref type="bibr" target="#b46">[47]</ref> and GIST <ref type="bibr" target="#b45">[46]</ref> descriptors perform poorly with a median rank of 54 and 68 <ref type="table" target="#tab_5">(Table 5</ref>) because they simply measure structural similarity between images. CNN <ref type="bibr" target="#b43">[44]</ref> descriptors capture object-level information and performs better with a median rank of 20. Our method captures the visual relationships present in the query image, which is important for high quality image retrieval, improving with a median rank of 4. When queried using an image of a "person riding a horse" <ref type="figure" target="#fig_6">(Figure 7)</ref>, SIFT returns images that are visually similar but are not semantically relevant. CNN retrieves one image that contains a horse and one that contains both a man and a horse but neither of them capture the relationship: "person riding a horse". Visual Phrases and our model are able to detect the relationship personridehorse and perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a model to detect multiple visual relationships in a single image. Our model learned to detect thousands of relationships even when there were Algorithm 1 Training Algorithm 1: input: training set of images with annotated subjectpredicateobject relationships annotated 2: Train object detectors on images using RCNN <ref type="bibr" target="#b42">[43]</ref> 3: Train predicate classifier on images using VGG <ref type="bibr" target="#b43">[44]</ref> 4: Initialize f (W) (Eq. 2) with word vectors for objects using word2vec() <ref type="bibr" target="#b6">[7]</ref> 5: repeat 6:</p><p>Compute the visual appearance model V (Θ) (Eq. 1) 7:</p><p>Compute relationship semantic distance to build K(W) (Eq. 4) 8:</p><p>Compute the likelihood score L(W) (Eq. 5) 9:</p><p>Backpropagate and optimize {Θ, W} (Eq. 7) using stochastic gradient descent 10: until {Θ, W} have converged 11: output: {Θ, W} very few training examples. We learned the visual appearance of objects and predicates and combined them to predict relationships. To finetune our predictions, we utilized a language prior that mapped similar relationships togetheroutperforming previous state of the art <ref type="bibr" target="#b5">[6]</ref> on the visual phrases dataset <ref type="bibr" target="#b5">[6]</ref> as well as our dataset. We also demonstrated that our model can be used for zero shot learning of visual relationships. We introduced a new dataset with 37, 993 relationships that can be used for further benchmarking. Finally, by understanding visual relationships, our model improved content based image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Training Algorithm</head><p>While we describe the theory and training procedure in the main text of this paper (Section 4.1), we include an algorithm box (Algorithm 1) to explain our training procedure in an alternate format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Mean Average Precision on Visual Relationship Detection</head><p>As discussed in our paper, mean average precision (mAP) is a pessimistic evaluation metric for visual relationship detection because our dataset does not exhaustively annotate every possible relationship between two pairs of objects. For example, consider the case when our model predicts that a personnext to bicycle when the ground truth annotation is personpushbicycle . In such a case, the prediction is not incorrect but would be penalized by mAP. However, to facilitate future comparisons against our model using this dataset, we report the mAP scores in <ref type="table" target="#tab_6">Table 6</ref>.</p><p>We see a similar trend in the mAP scores as we did with the recall @ 50 and recall @ 100 values. The Visual Phrases <ref type="bibr" target="#b5">[6]</ref> and Joint CNN baselines along with (Ours -L only) perform poorly on all three tasks: phrase, relationship and predicate detection. The visual only model (Ours -V only) improved upon  these results by leveraging the visual appearances of objects to aid it's predicate detection. Our complete model (Ours -V + L + K) achieves a mAP of 1.52 on relationship predication since it is penalized for missing annotations. However, it still performs better than all the other ablated models. It also attains a 29.47 mAP on predicate detection, demonstrating that our model learns to recognize predicates from one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Mean Average Precision on Zero-shot Learning</head><p>Similar to the previous section, we also include the mAP scores for zero shot learning in <ref type="table" target="#tab_7">Table 7</ref>. Again, we see that the the inclusion of K() allows our model to levearage similar relationships to improve zero shot learning in all three experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Human Evaluation on our Dataset</head><p>We ran an experiment to evaluate the human performance on our dataset. We randomly selecting 1000 pairs of objects from the dataset and then asked humans on Amazon Mechanical Turk to decide which of the 70 predicates were correct for each pair. We found that humans managed a 98.1% recall @ 50 and 96.4% mAP. This demonstrates that while this task is easy for humans, Visual Relationship Detection is still a hard unsolved task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>(left) A log scale distribution of the number of instances to the number of relationships in our dataset. Only a few relationships occur frequently and there is a long tail of infrequent relationships. (right) Relationships in our dataset can be divided into many categories, 5 of which are shown here: verb, spatial, preposition, comparative and action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>A overview of our visual relationship detection pipeline. Given an image as input, RCNN<ref type="bibr" target="#b42">[43]</ref> generates a set of object proposals. Each pair of object proposals is then scored using a (1) visual appearance module and a (2) language module. These scores are then thresholded to output a set of relationship labels (e.g. personridinghorse ). Both objects in a relationship (e.g. person and horse) are localized as bounding boxes. The parameters of those two modules (W and Θ) are iteratively learnt in Section 4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 5: We evaluate visual relationship detection using three conditions: predicate detection (where we only predict the predicate given the object classes and boxes), phrase detection (where we label a region of an image with a relationship) and relationship detection (where we detect the objects and label the predicate between them). in the image. We train our model using the 4000 training images and perform visual relationship prediction on the 1000 test images. The evaluation metrics we report is recall @ 100 and recall @ 50 [45]. Recall @ x computes the fraction of times the correct relationship is predicted in the top x confident relationship predictions. Since we have 70 predicates and an average of 18 objects per image, the total possible number of relationship predictions is 100×70×100, which implies that the random guess will result in a recall @ 100 of 0.00014. We notice that mean average precision (mAP) is another widely used metric. However, mAP is a pessimistic evaluation metric because we can not exhaustively annotate all possible relationships in an image. Consider the case where our model predicts persontaller thanperson . Even if the prediction is correct, mAP would penalize the prediction if we do not have that particular ground truth annotation. Detecting a visual relationship involves classifying both the objects, predicting the predicate and localization both the objects. To study how our model performs on each of these tasks, we measure visual relationship prediction under the following conditions:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 6(d)(e)(f) compares the various components of our model. Without the relationship likelihood score, (Ours -V only) incorrectly classifies a wheel as a clock in (d) and mislabels the predicate in (e) and (f). Without any visual priors, (Ours -L only) always reports the most frequent relationship personwearshirt . (Ours -V + L) fixes (d) by correcting the visual model's misclassification of the wheel as a clock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>(a), (b) and (c) show results from our model, Visual Phrases [6] and Joint CNN [44] on the same image. All ablation studies results for (d), (e) and (f) are reported below the corresponding image. Ticks and crosses mark the correct and incorrect results respectively. Phrase, object 1 and object 2 boxes are in blue, red and green respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Examples retrieval results using an image as the query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between our visual relationship benchmarking dataset with existing datasets that contain relationships. Relationships and Objects are abbreviated to Rel. and Obj. because of space constraints.</figDesc><table><row><cell></cell><cell cols="4">Images Rel. Types Rel. Instances # Predicates per Obj. Category</cell></row><row><cell cols="2">Visual Phrases [6] 2,769</cell><cell>13</cell><cell>2,040</cell><cell>120</cell></row><row><cell>Scene Graph [8]</cell><cell>5,000</cell><cell>23,190</cell><cell>109,535</cell><cell>2.3</cell></row><row><cell>Ours</cell><cell>5,000</cell><cell>6,672</cell><cell>37,993</cell><cell>24.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>O 1 ) and P j (O 2 ) are the CNN likelihoods of categorizing box O 1 as object category i and box O 2 as category j. CNN(O 1 , O 2 ) is the predicate CNN features extracted from the union of the O 1 and O 2 boxes.Language Module One of our key observations is that relationships are semantically related to one another. For example, personridehorse is semantically similar to personrideelephant . Even if we have not seen any examples of personrideelephant , we should be able to infer it from similar relationships that occur more frequently (e.g. personride -</figDesc><table /><note>horse ). Our language module projects relationships into an embedding space where similar relationships are optimized to be close together. We first describe the function that projects a relationship to the vector space (Equation 2) and then explain how we train this function by enforcing similar relationships to be close together in a vector space (Equation 4) and by learning a likelihood prior on relationships (Equation 5).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for visual relationship detection (Section 5.1). R@100 and R@50 are abbreviations of Recall @ 100 and Recall @ 50. Note that in predicate det., we are predicting multiple predicates per image (one between every pair of objects) and hence R@100 is less than 1.</figDesc><table><row><cell></cell><cell cols="5">Phrase Det. Relationship Det. Predicate Det.</cell></row><row><cell></cell><cell cols="5">R@100 R@50 R@100 R@50 R@100 R@50</cell></row><row><cell>Visual Phrases [6]</cell><cell>0.07 0.04</cell><cell>-</cell><cell>-</cell><cell>1.91</cell><cell>0.97</cell></row><row><cell>Joint CNN [44]</cell><cell cols="2">0.09 0.07 0.09</cell><cell>0.07</cell><cell>2.03</cell><cell>1.47</cell></row><row><cell>Ours -V only</cell><cell cols="2">2.61 2.24 1.85</cell><cell>1.58</cell><cell>7.11</cell><cell>7.11</cell></row><row><cell>Ours -L only</cell><cell cols="2">0.08 0.08 0.08</cell><cell>0.08</cell><cell cols="2">18.22 18.22</cell></row><row><cell cols="3">Ours -V + naive FC 6.39 6.65 5.47</cell><cell>5.27</cell><cell cols="2">28.87 28.87</cell></row><row><cell>Ours -V + L only</cell><cell cols="2">8.59 9.13 9.18</cell><cell>9.04</cell><cell cols="2">35.20 35.20</cell></row><row><cell cols="3">Ours -V + L + Reg. 8.91 9.60 9.63</cell><cell>9.71</cell><cell cols="2">36.31 36.31</cell></row><row><cell>Ours -V + L + K</cell><cell cols="5">17.03 16.17 14.70 13.86 47.87 47.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results for zero-shot visual relationship detection (Section 5.2). Visual Phrases, Joint CNN and Ours -V + naive FC are omitted from this experiment as they are unable to do zero-shot learning.</figDesc><table><row><cell></cell><cell cols="4">Phrase Det. Relationship Det. Predicate Det.</cell></row><row><cell></cell><cell cols="4">R@100 R@50 R@100 R@50 R@100 R@50</cell></row><row><cell>Ours -V only</cell><cell>1.12 0.95 0.78</cell><cell>0.67</cell><cell>3.52</cell><cell>3.52</cell></row><row><cell>Ours -L only</cell><cell>0.01 0.00 0.01</cell><cell>0.00</cell><cell>5.09</cell><cell>5.09</cell></row><row><cell cols="2">Ours -V + L only 2.56 2.43 2.66</cell><cell>2.27</cell><cell>6.11</cell><cell>6.11</cell></row><row><cell cols="2">Ours -V + L + K 3.75 3.36 3.52</cell><cell>3.13</cell><cell cols="2">8.45 8.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Visual phrase detection results on Visual Phrases dataset<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table><row><cell></cell><cell cols="4">Phrase Detection Zero-Shot Phrase Detection</cell></row><row><cell></cell><cell cols="3">R@100 R@50 mAP R@100 R@50</cell><cell>mAP</cell></row><row><cell>Visual Phrase [6]</cell><cell>52.7 49.3 38.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint CNN</cell><cell>75.3 71.5 54.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours V only</cell><cell cols="3">72.0 68.6 53.4 13.5 11.3</cell><cell>5.3</cell></row><row><cell cols="2">Ours V + naive FC 77.8 73.4 55.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours V + L only</cell><cell cols="3">79.3 76.7 57.3 17.8 15.1</cell><cell>8.8</cell></row><row><cell>Ours V + L + K</cell><cell cols="3">82.7 78.1 59.2 11.4 23.9</cell><cell>18.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Example image retrieval using a image of a personridehorse (Section 5.4). Note that a higher recall and lower median rank indicates better performance.</figDesc><table><row><cell></cell><cell cols="4">Recall @ 1 Recall @ 5 Recall @ 10 Median Rank</cell></row><row><cell>GIST [46]</cell><cell>0.00</cell><cell>5.60</cell><cell>8.70</cell><cell>68</cell></row><row><cell>SIFT [47]</cell><cell>0.70</cell><cell>6.10</cell><cell>10.3</cell><cell>54</cell></row><row><cell>CNN [44]</cell><cell>3.15</cell><cell>7.70</cell><cell>11.5</cell><cell>20</cell></row><row><cell>Visual Phrases [6]</cell><cell>8.72</cell><cell>18.12</cell><cell>28.04</cell><cell>12</cell></row><row><cell>Our Model</cell><cell>10.82</cell><cell>30.02</cell><cell>47.00</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>mAP results for visual relationship detection (Section 5.1).</figDesc><table><row><cell></cell><cell cols="3">Phrase Detection Relationship Detection Predicate Detection</cell></row><row><cell>Visual Phrases [6]</cell><cell>0.03</cell><cell>-</cell><cell>0.71</cell></row><row><cell>Joint CNN [44]</cell><cell>0.05</cell><cell>0.04</cell><cell>1.02</cell></row><row><cell>Ours -V only</cell><cell>0.93</cell><cell>0.84</cell><cell>6.42</cell></row><row><cell>Ours -L only</cell><cell>0.08</cell><cell>0.08</cell><cell>8.94</cell></row><row><cell>Ours -V + naive FC</cell><cell>1.21</cell><cell>1.19</cell><cell>11.05</cell></row><row><cell>Ours -V + L only</cell><cell>1.74</cell><cell>1.32</cell><cell>16.31</cell></row><row><cell>Ours -V + L + Reg.</cell><cell>1.78</cell><cell>1.40</cell><cell>17.95</cell></row><row><cell>Ours -V + L + K</cell><cell>2.07</cell><cell>1.52</cell><cell>29.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>mAP results for zero-shot visual relationship detection (Section 5.2).</figDesc><table><row><cell></cell><cell cols="3">Phrase Detection Relationship Detection Predicate Detection</cell></row><row><cell>Ours -V only</cell><cell>0.92</cell><cell>1.03</cell><cell>2.13</cell></row><row><cell>Ours -L only</cell><cell>0.00</cell><cell>0.00</cell><cell>3.31</cell></row><row><cell>Ours -V + L only</cell><cell>1.97</cell><cell>2.30</cell><cell>4.45</cell></row><row><cell>Ours -V + L + K</cell><cell>2.89</cell><cell>3.01</cell><cell>5.52</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the Scene Graph dataset<ref type="bibr" target="#b7">[8]</ref> was collected using unconstrained language, resulting in multiple annotations for the same relationship (e.g. mankickball and personis kickingsoccer ball ). Therefore, 23,190 is an inaccurate estimate of the number of unique relationship types in their dataset. We do not compare with the Visual Genome dataset<ref type="bibr" target="#b41">[42]</ref> because their relationships had not been released at the time this paper was written.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements . Our work is partially funded by an ONR MURI grant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Relationship Detection with Language Priors</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tree kernel-based relation extraction with context-sensitive structured parse tree information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou12</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP-CoNLL</title>
		<imprint>
			<biblScope unit="page">728</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dependency tree kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">423</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1745" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Costa: Co-occurrence statistics for zero-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2441" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to share visual appearance for multiclass object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1481" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph cut based inference with cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="239" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th international conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computer vision</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object categorization using cooccurrence, location and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context based object categorization: A critical survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="712" to="722" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting hierarchical context on a large database of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incorporating scene context and object layout into appearance modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="232" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards scalable representations of object categories: Learning a hierarchy of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discovering objects and their location in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-class segmentation with relative location prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="316" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3177" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1775" to="1789" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning semantic relationships for better action retrieval in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rossenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2712" to="2719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grounding action descriptions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th CVPR</title>
		<meeting>the 24th CVPR</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning the visual interpretation of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1681" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficiently selecting regions for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3217" to="3224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Using multiple segmentations to discover objects and their extent in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1605" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding and predicting importance in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3562" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4952</idno>
		<title level="m">From captions to visual concepts and back</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic parsing for text to 3d scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Vision and Language (VL15)</title>
		<meeting>the Fourth Workshop on Vision and Language (VL15)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding indoor scenes using 3d geometric phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
