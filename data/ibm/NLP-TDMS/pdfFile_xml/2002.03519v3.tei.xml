<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Attentive Associative Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-11">11 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
						</author>
						<title level="a" type="main">Self-Attentive Associative Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-11">11 Jun 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory. In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). The idea is implemented through a novel Self-attentive Associative Memory (SAM) operator. Found upon outer product, SAM forms a set of associative memories that represent the hypothetical high-order relationships between arbitrary pairs of memory elements, through which a relational memory is constructed from an item memory. The two memories are wired into a single sequential model capable of both memorization and relational reasoning. We achieve competitive results with our proposed two-memory model in a diversity of machine learning tasks, from challenging synthetic problems to practical testbeds such as geometry, graph, reinforcement learning, and question answering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans excel in remembering items and the relationship between them over time <ref type="bibr">(Olson et al., 2006;</ref><ref type="bibr" target="#b17">Konkel &amp; Cohen, 2009</ref>). Numerous neurocognitive studies have revealed this striking ability is largely attributed to the perirhinal cortex and hippocampus, two brain regions that support item memory (e.g., objects, events) and relational memory (e.g., locations of objects, orders of events), respectively <ref type="bibr" target="#b6">(Cohen et al., 1997;</ref><ref type="bibr" target="#b5">Buckley, 2005)</ref>. Relational memory theory posits that there exists a representation of critical relationships amongst arbitrary items, which Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). allows inferential reasoning capacity <ref type="bibr" target="#b8">(Eichenbaum, 1993;</ref><ref type="bibr">Zeithamova et al., 2012)</ref>. It remains unclear how the hippocampus can select the stored items in clever ways to unearth their hidden relationships and form the relational representation.</p><p>Research on artificial intelligence has focused on designing item-based memory models with recurrent neural networks (RNNs) <ref type="bibr" target="#b14">(Hopfield, 1982;</ref><ref type="bibr" target="#b9">Elman, 1990;</ref><ref type="bibr" target="#b13">Hochreiter &amp; Schmidhuber, 1997)</ref> and memoryaugmented neural networks (MANNs) <ref type="bibr" target="#b10">(Graves et al., 2014;</ref><ref type="bibr" target="#b19">Le et al., 2018a;</ref><ref type="bibr">2019)</ref>. These memories support long-term retrieval of previously seen items yet lack explicit mechanisms to represent arbitrary relationships amongst the constituent pieces of the memories. Recently, further attempts have been made to foster relational modeling by enabling memory-memory interactions, which is essential for relational reasoning tasks <ref type="bibr">(Santoro et al., 2017;</ref><ref type="bibr">Vaswani et al., 2017)</ref>. However, no effort has been made to model jointly item memory and relational memory explicitly.</p><p>We argue that dual memories in a single system are crucial for solving problems that require both memorization and relational reasoning. Consider graphs wherein each node is associated with versatile features-as example a road network structure where each node is associated with diverse features: graph 1 where the nodes are building landmarks and graph 2 where the nodes are flora details. The goal here is to reason over the structure and output the associated features of the nodes instead of the pointer or index to the nodes. Learning to output associated node features enables generalization to entirely novel features, i.e., a model can be trained to generate a navigation path with building landmarks (graph 1) and tested in the novel context of generating a navigation path with flora landmarks (graph 2). This may be achieved if the model stores the features and structures into its item and relational memory, separately, and reason over the two memories using rules acquired during training.</p><p>Another example requiring both item and relational memory can be understood by amalgamating the N th -farthest <ref type="bibr">(Santoro et al., 2018)</ref> and associative recall <ref type="bibr" target="#b10">(Graves et al., 2014)</ref> tasks. N th -farthest requires relational memory to return a fixed one-hot encoding representing the index to the N th -farthest item, while associative recall returns the item itself, requiring item memory. If these tasks are amalgamated to compose Relational Associative Recall (RAR)return the N th -farthest item from a query (see § 3.2), it is clear that both item and relational memories are required.</p><p>Three limitations of the current approaches are: (i) the relational representation is often computed without storing, which prevents reusing the precomputed relationships in sequential tasks <ref type="bibr">(Vaswani et al., 2017;</ref><ref type="bibr">Santoro et al., 2017)</ref>, (ii) few works that manage both items and the relationships in a single memory, make it hard to understand how relational reasoning occurs <ref type="bibr">(Santoro et al., 2018;</ref><ref type="bibr">Schlag &amp; Schmidhuber, 2018)</ref>, (iii) the memory-memory relationship is coarse since it is represented as either dot product attention <ref type="bibr">(Vaswani et al., 2017)</ref> or weighted summation via neural networks <ref type="bibr">(Santoro et al., 2017)</ref>. Concretely, the former uses a scalar to measure cosine distance between two vectors and the later packs all information into one vector via only additive interactions.</p><p>To overcome the current limitations, we hypothesize a twomemory model, in which the relational memory exists separately from the item memory. To maintain a rich representation of the relationship between items, the relational memory should be higher-order than the item memory. That is, the relational memory stores multiple relationships, each of which should be represented by a matrix rather than a scalar or vector. Otherwise, the capacity of the relational memory is downgraded to that of the item memory. Finally, as there are two separate memories, they must communicate to enrich the representation of one another.</p><p>To implement our hypotheses, we introduce a novel operator that facilitates the communication from the item memory to the relational memory. The operator, named Selfattentive Associative Memory (SAM) leverages the dot product attention with our outer product attention. Outer product is critical for constructing higher-order relational representations since it retains bit-level interactions between two input vectors, thus has potential for rich representational learning <ref type="bibr">(Smolensky, 1990)</ref>. SAM transforms a second-order (matrix) item memory into a third-order relational representation through two steps. First, SAM decodes a set of patterns from the item memory. Second, SAM associates each pair of patterns using outer product and sums them up to form a hetero-associative memory. The memory thus stores relationships between stored items accumulated across timesteps to form a relational memory.</p><p>The role of item memory is to memorize the input data over time. To selectively encode the input data, the item memory is implemented as a gated auto-associative memory. Together with previous read-out values from the relational memory, the item memory is used as the input for SAM to construct the relational memory. In return, the re-lational memory transfers its knowledge to the item memory through a distillation process. The backward transfer triggers recurrent dynamics between the two memories, which may be essential for simulating hippocampal processes <ref type="bibr" target="#b18">(Kumaran &amp; McClelland, 2012)</ref>. Another distillation process is used to transform the relational memory to the output value.</p><p>Taken together, we contribute a new neural memory model dubbed SAM-based Two-memory Model (STM) that takes inspiration from the existence of both item and relational memory in human brain <ref type="bibr" target="#b17">(Konkel &amp; Cohen, 2009)</ref>. In this design, the relational memory is higher-order than the item memory and thus necessitates a core operator that manages the information exchange from the item memory to the relational memory. The operator, namely Self-attentive Associative Memory (SAM), utilizes outer product to construct a set of hetero-associative memories representing relationships between arbitrary stored items. We apply our model to a wide range of tasks that may require both item and relational memory: various algorithmic learning, geometric and graph reasoning, reinforcement learning and questionanswering tasks. Several analytical studies on the characteristics of our proposed model are also given in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Outer product attention (OPA)</head><p>Outer product attention (OPA) is a natural extension of the query-key-value dot product attention <ref type="bibr">(Vaswani et al., 2017)</ref>. Dot product attention (DPA) for single query q and n kv pairs of key-value can be formulated as follows,</p><formula xml:id="formula_0">A°(q, K, V ) = n kv i=1 S (q · k i ) v i (1) where A°∈ R dv , q, k i ∈ R d qk , v i ∈ R dv ,</formula><p>· is dot product, and S forms softmax function. We propose a new outer product attention with similar formulation yet different meaning,</p><formula xml:id="formula_1">A ⊗ (q, K, V ) = n kv i=1 F (q ⊙ k i ) ⊗ v i (2) where A ⊗ ∈ R d qk ×dv , q, k i ∈ R d qk , v ∈ R dv , ⊙ is element-wise multiplication,</formula><p>⊗ is outer product and F is chosen as element-wise tanh function.</p><p>A crucial difference between DPA and OPA is that while the former retrieves an attended item A°, the latter forms a relational representation A ⊗ . As a relational representation, A ⊗ captures all bit-level associations between the key-scaled query and the value. This offers two benefits:</p><p>(i) a higher-order representational capacity that DPA cannot provide and (ii) a form of associative memory that can be later used to retrieve stored item by using a contraction operation P (A ⊗ ) (see Appendix § C-Prop. 6).</p><p>OPA is closely related to DPA. The relationship between the two for simple S and F is presented as follows, Proposition 1. Assume that S is a linear transformation:</p><formula xml:id="formula_2">S (x) = ax+b (a, b, x ∈ R), we can extract A°from A ⊗ by using an element-wise linear transformation F (x) = a f ⊙ x+b f (a f , b f , x ∈ R d qk ) and a contraction P: R d qk ×dv → R dv such that A°(q, K, V ) = P A ⊗ (q, K, V )<label>(3)</label></formula><p>Proof. see Appendix § A.</p><p>Moreover, when n kv = 1, applying a high dimensional transformation G (A ⊗ ) is equivalent to the well-known bilinear model (see Appendix § B-Prop. 4). By introducing OPA, we obtain a new building block that naturally supports both powerful relational bindings and item memorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-attentive Associative Memory (SAM)</head><p>We introduce a novel and generic operator based upon OPA that constructs relational representations from an item memory. The relational information is extracted via preserving the outer products between any pairs of items from the item memory. Hence, we name this operator Selfattentive Associative Memory (SAM). Given an item memory M ∈ R n×d and parametric weights θ= {W q ∈ R nq×n , W k ∈ R n kv ×n , W v ∈ R n kv ×n }, SAM retrieves n q queries, n kv keys and values from M as M q , M k and M v , respectively,</p><formula xml:id="formula_3">M q = LN (W q M )<label>(4)</label></formula><formula xml:id="formula_4">M k = LN (W k M )<label>(5)</label></formula><formula xml:id="formula_5">M v = LN (W v M )<label>(6)</label></formula><p>where LN is layer normalization operation <ref type="bibr" target="#b1">(Ba et al., 2016b)</ref>. Then SAM returns a relational representation SAM θ (M )∈ R nq×d×d , in which the s-th element of the first dimension is defined as</p><formula xml:id="formula_6">SAM θ (M ) [s] = A ⊗ (M q [s] , M k , M v ) (7) = n kv j=1 F (M q [s] ⊙ M k [j]) ⊗ M v [j] (8) where s = 1, ..., n q . M q [s], M k [j] and M v [j]</formula><p>denote the s-th row vector of matrix M q , the j-th row vector of matrix M k and M v , respectively. A diagram illustrating SAM operations is given in <ref type="figure">Fig. 1 (right)</ref>.</p><p>It should be noted that M can be any item memory including the slot-based memories <ref type="bibr">(Le et al., 2019)</ref>, direct inputs <ref type="bibr">(Vaswani et al., 2017)</ref> or associative memories <ref type="bibr" target="#b16">(Kohonen, 1972;</ref><ref type="bibr" target="#b14">Hopfield, 1982)</ref>. We choose M ∈ R d×d as a form of classical associative memory, which is biologically plausible <ref type="bibr">(Marr &amp; Thach, 1991)</ref>. Here, we follow the traditional practice that sets n = d for the associative item memory. From M we read query, key and value items to form SAM θ (M )-a new set of hetero-associative memories using Eq. 8. Each hetero-associative memory represents the relationship between a query and all values. The role of the keys is to maintain possible perfect retrieval for the item memory (Appendix § C-Prop. 6).</p><p>The high-order structure of SAM allows it to preserve bitlevel relationships between a query and a value in a matrix. SAM compresses several relationships with regard to a query by summing all the matrices to form a heteroassociative memory containing d 2 scalars, where d is the dimension of M . As there are n kv relationships given 1 query, the summation results in on average d 2 /n kv scalars of representation per relationship, which is greater than 1 if d &gt; √ n kv . By contrast, current self-attention mechanisms use dot product to measure the relationship between any pair of memory slots, which means 1 scalar per relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">SAM-based Two-Memory Model (STM)</head><p>To effectively utilize the SAM operator, we design a system which consists of two memory units M i t ∈ R d×d and M r t ∈ R nq×d×d : one for items and the other for relationships, respectively. From a high-level view, at each timestep, we use the current input data x t and the previous state of memories M i t−1 , M r t−1 to produce output o t and new state of memories M i t , M r t . The memory executions are described as follows.</p><p>M i -Write The item memory distributes the data from the input across its rows in the form of associative memory. For an input x t , we update the item memory as</p><formula xml:id="formula_7">X t = f 1 (x t ) ⊗ f 2 (x t ) M i t = M i t−1 + X t<label>(9)</label></formula><p>where f 1 and f 2 are feed-forward neural networks that output d-dimensional vectors. This update does not discriminate the input data and inherits the low-capacity of classical associative memory <ref type="bibr">(Rojas, 2013)</ref>. We leverage the gating mechanisms of LSTM <ref type="bibr" target="#b13">(Hochreiter &amp; Schmidhuber, 1997)</ref> to improve Eq. 9 as <ref type="figure">Figure 1</ref>. <ref type="bibr">STM (left)</ref> and SAM (right). SAM uses neural networks θ to extract query, key and value elements from a matrix memory M . In this illustration, nq = 3 and n kv = 4. Then, it applies outer product attention to output a 3D tensor relational representation. In STM, at every timestep, the item memory M i t is updated with new input xt using gating mechanisms (Eq. 10). The item memory plus the read-out from the relational memory is forwarded to SAM, resulting in a new relational representation to update the relational memory M r t (Eq. 11-12). The relational memory transfers its knowledge to the item memory (Eq. 13) and output value (Eq. 14).</p><formula xml:id="formula_8">M i t = F t M i t−1 , x t ⊙M i t−1 +I t M i t−1 , x t ⊙X t (10)</formula><p>where F t and I t are forget and input gates, respectively. Detailed implementation of these gates is in Appendix § D.</p><p>M r -Read As relationships stored in M r are represented as associative memories, the relational memory can be read to reconstruct previously seen items. As shown in Appendix § C-Prop. 7, the read is basically a two-step contraction,</p><formula xml:id="formula_9">v r t = softmax f 3 (x t ) ⊤ M r t−1 f 2 (x t )<label>(11)</label></formula><p>where f 3 is a feed-forward neural network that outputs a n qdimensional vector. The read value provides an additional input coming from the previous state of M r to relational construction process, as shown later in Eq. 12.</p><formula xml:id="formula_10">M i -Read M r -Write</formula><p>We use SAM to read from M i and construct a candidate relational memory, which is simply added to the previous relational memory to perform the relational update,</p><formula xml:id="formula_11">M r t = M r t−1 + α 1 SAM θ M i t + α 2 v r t ⊗ f 2 (x t ) (12)</formula><p>where α 1 and α 2 are blending hyper-parameters. The input for SAM is a combination of the current item memory M i t and the association between the extracted item from the previous relational memory v r t and the current input data x t . Here, v r t enhances the relational memory with information from the distant past. The resulting relational memory stores associations between several pairs of items in a 3D tensors of size n q × d × d. In our SAM implementation, n kv = n q . M r -Transer In this phase, the relational knowledge from M r t is transferred to the item memory by using high dimensional transformation,</p><formula xml:id="formula_12">M i t = M i t + α 3 G 1 • V f • M r t<label>(13)</label></formula><p>where V f is a function that flattens the first two dimensions of its input tensor, G 1 is a feed-forward neural network that maps R (nqd)×d → R d×d and α 3 is a blending hyper-parameter. As shown in Appendix § B-Prop. 5, with trivial G 1 , the transfer behaves as if the item memory is enhanced with long-term stored values from the relational memory. Hence, M r -Transfer is also helpful in supporting long-term recall (empirical evidences in § 3.1). In addition, at each timestep, we distill the relational memory into an output vector o t ∈ R no . We alternatively flatten and apply high-dimensional transformations as follow,</p><formula xml:id="formula_13">o t = G 3 • V l • G 2 • V l • M r t<label>(14)</label></formula><p>where V l is a function that flattens the last two dimensions of its input tensor. G 2 and G 3 are two feed-forward neural networks that map R nq×(dd) → R nq×nr and R nqnr → R no , respectively. n r is a hyper-parameter.</p><p>Unlike the contraction (Eq. 11), the distillation process does not simply reconstruct the stored items. Rather, thanks to high-dimensional transformations, it captures bi-linear representations stored in the relational memory (proof in Appendix § B). Hence, despite its vector form, the output of our model holds a rich representation that is useful for both sequential and relational learning. We discuss further on how to quantify the degree of relational distillation in Appendix § G. The summary of components of STM is presented in <ref type="figure">Fig. 1 (left)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ablation study</head><p>We test different model configurations on two classical tasks for sequential and relational learning: associative retrieval <ref type="bibr" target="#b0">(Ba et al., 2016a)</ref> and N th -farthest (Santoro et al., 2018) (see Appendix § E for task details and learning curves). Our source code is available at https://github.com/thaihungle/SAM.</p><p>Associative retrieval This task measures the ability to recall a seen item given its associated key and thus involves item memory. We use the setting with input sequence length 30 and 50 <ref type="bibr">(Zhang &amp; Zhou, 2017)</ref>. Three main factors affecting the item memory of STM are the dimension d of the auto-associative item memory, the gating mechanisms (Eq. 10) and the relational transfer (Eq. 13). Hence, we ablate our STM (d = 96, full features) by creating three other versions: small STM with transfer (d = 48), small STM without transfer (d = 48, w/o transfer) and STM without gates (d = 96, w/o gates). n q is fixed to 1 as the task does not require much relational learning. <ref type="table">Table 1</ref> reports the number of epochs required to converge and the final testing accuracy. Without the proposed gating mechanism, STM struggles to converge, which highlights the importance of extending the capacity of the autoassociative item memory. The convergence speed of STM is significantly improved with a bigger item memory size. Relational transfer seems more useful for longer input sequences since if requested, it can support long-term retrieval. Compared to other fast-weight baselines, the fullfeature STM performs far better as it needs only 10 and 20 epochs to solve the tasks of length 30 and 50, respectively.</p><p>N th -farthest This task evaluates the ability to learn the relationship between stored vectors. The goal is to find the N th -farthest vector from a query vector, which requires a relational memory for distances between vectors and a sorting mechanism over the distances. For relational reasoning tasks, the pivot is the number of extracted items n q for establishing the relational memory. Hence, we run our STM with different n q = 1, 4, 8 using the same problem setting (8 16dimensional input vectors), optimizer (Adam), batch size (1600) as in <ref type="bibr">Santoro et al. (2018)</ref>. We also run the task with TPR (Schlag &amp; Schmidhuber, 2018)-a highorder fast-weight model that is designed for reasoning. As reported in <ref type="table">Table 2</ref>, increasing n q gradually improves the accuracy of STM. As there are 8 input vectors in this task, literally, at each timestep the model needs to extract 8 items to compute all pairs of distances. However, as the extracted item is an entangled representation of all stored vectors and the temporarily computed distances are stored in separate high-order storage, even with n q = 1, 4, STM achieves moderate results. With n q = 8, STM nearly solves the task perfectly, outperforming RMC by a large margin. We have tried to tune TPR for this task without success (see Appendix § E). This illustrates the challenge of training high-order neural networks in diverse contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Algorithmic synthetic tasks</head><p>Algorithmic synthetic tasks <ref type="bibr" target="#b10">(Graves et al., 2014)</ref> examine sequential models on memorization capacity (eg., Copy, Associative recall) and simple relational reasoning (eg., Priority sort). Even without explicit relational memory, MANNs have demonstrated good performance <ref type="bibr" target="#b10">(Graves et al., 2014;</ref><ref type="bibr">Le et al., 2020)</ref>, but they are verified for only low-dimensional input vectors (&lt;8 bits). As higher-dimensional inputs necessitate higher-fidelity memory storage, we evaluate the high-fidelity reconstruction capacity of sequential models for these algorithmic tasks with 32-bit input vectors.</p><p>Two chosen algorithmic tasks are Copy and Priority sort. Item memory is enough for Copy where the models just output the input vectors seen in the same order in which they are presented. For Priority sort, a relational operation that compares the priority of input vectors is required to produce the seen input vectors in the sorted order according to the priority score attached to each input vector. The relationship is between input vectors and thus simply firstorder (see Appendix § G for more on the order of relationship).</p><p>Inspired by Associative recall and N th -farthest tasks, we create a new task named Relational Associative Recall (RAR). In RAR, the input sequence is a list of items followed by a query item. Each item is a list of several 32-bit vectors and thus can be interpreted as a concatenated long vector. The requirement is to reconstruct the seen item that is farthest or closest (yet unequal) to the query. The type of the relationship is conditioned on the last bit of the query vector, i.e., if the last bit is 1, the target is the farthest and 0 the closest. The evaluated models must compute the distances from the query item to any other seen items and then compare the distances to find the farthest/closest one. Hence, this task is similar to the N th -farthest task, which is second-order relational and thus needs relational memory. However, this task is more challenging since the models must reconstruct the seen items (32-bit vectors). Compared to N = 8 possible one-hot outputs in N th -farthest, the output space in RAR is 2 32 per step, thereby requiring high-fidelity item memory.</p><p>We evaluate our model STM (n q = 8, d = 96) with the 4 following baselines: LSTM <ref type="bibr" target="#b13">(Hochreiter &amp; Schmidhuber, 1997)</ref>, attentional LSTM <ref type="bibr" target="#b3">(Bahdanau et al., 2015)</ref>, NTM <ref type="bibr" target="#b10">(Graves et al., 2014)</ref> and <ref type="bibr">RMC (Santoro et al., 2018)</ref>. Details of the implementation are listed in Appendix § F. The learning curves (mean and error bar over 5 runs) are presented in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>LSTM is often the worst performer as it is based on vector memory. ALSTM is especially good for Copy as it has a privilege to access input vectors at every step of decoding. However, when dealing with relational reasoning, memory-less attention in ALSTM does not help much.</p><p>NTM performs well on Copy and moderately on Priority sort, yet badly on RAR possibly due to its bias towards item memory. Although equipped with self-attention relational memory, RMC demonstrates trivial performance on all tasks. This suggests a limitation of using dot-product attention to represent relationships when the tasks stress memorization or the relational complexity goes beyond dotproduct capacity. Amongst all models, only the proposed STM demonstrates consistently good performance where it almost achieves zero errors on these 3 tasks. Notably, for RAR, only STM can surpass the bottleneck error of 30 bits and reach ≈ 1 bit error, corresponding to 0% and 87% of items perfectly reconstructed, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Geometric and graph reasoning</head><p>Problems on geometry and graphs are a good testbed for relational reasoning, where geometry stipulates spatial relationships between points, and graphs the relational structure of nodes and edges. Classical problems include Convex hull, Traveling salesman problem (TSP) for geometry, and Shortest path, Minimum spanning tree for graph. Convex hull and TSP data are from Vinyals et al. <ref type="formula" target="#formula_4">(2015)</ref> where input sequence is a list of points' coordinates (number of points N ∼ <ref type="bibr">[5,</ref><ref type="bibr">20]</ref>  <ref type="table">Table 3</ref>. Prediction accuracy (%) for geometric and graph reasoning with random one-hot features. Italic numbers are tour lengthadditional metric for TSP. Average optimal tour lengths found by brute-force search for N = 5 and 10 are 2.05 and 2.88, respectively. <ref type="figure">Figure 3</ref>. Average reward vs number of games for reinforcement learning task in n-frame skip settings.</p><p>for a timestep is made when the predicted feature matches perfectly with the ground truth feature in the timestep. To measure the performance, we use the average accuracy of prediction across steps. We use the same baselines as in § 3.2 except that we replace NTM with DNC as DNC performs better on graph reasoning <ref type="bibr" target="#b11">(Graves et al., 2016)</ref>.</p><p>We report the best performance of the models on the testing datasets in <ref type="table">Table 3</ref>. Although our STM has fewest parameters, it consistently outperforms other baselines by a significant margin. As usual, LSTM demonstrates an average performance across tasks. RMC and ALSTM are only good at Convex hull. DNC performs better on graph-like problems such as Shortest path and Minimum spanning tree. For the NP-hard TSP (N = 5), despite moderate point accuracy, all models achieve nearly minimal solutions with an average tour length of 2.05. When increasing the difficulty with more points (N = 10), none of these models reach an average optimal tour length of 2.88. However, only STM approaches closer to the optimal solution without the need for pointer and beam search mechanisms. Armed with both item and relational memory, STM's superior performance suggests a qualitative difference in the way STM and other methods solve these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Reinforcement learning</head><p>Memory is helpful for partially observable Markov decision process <ref type="bibr" target="#b4">(Bakker, 2002)</ref>. We apply our memory to LSTM agents in Atari game environment using A3C training <ref type="bibr">(Mnih et al., 2016)</ref>. More details are given in Appendix § I. In Atari games, each state is represented as the vi-sual features of a video frame and thus is partially observable. To perform well, RL agents should remember and relate several frames to model the game state comprehensively. These abilities are challenged when over-sampling and under-sampling the observation, respectively. We analyze the performance of LSTM agents and their STMaugmented counterparts under these settings using a game: Pong.</p><p>To be specific, we test the two agents on different frame skips <ref type="bibr">(0,</ref><ref type="bibr">4,</ref><ref type="bibr">16,</ref><ref type="bibr">32)</ref>. We create n-frame skip setting by allowing the agent to see the environment only after every n frames, where 4-frame skip is standard in most Atari environments. When no frameskip is applied (over-sampling), the number of observations is dense and the game is long (up to 9000 steps per game), which requires high-capacity item memory. On the contrary, when a lot of frames are skipped (under-sampling), the observations become scarce and the agents must model the connection between frames meticulously, demanding better relational memory.</p><p>We run each configuration 5 times and report the mean and error bar of moving average reward (window size = 100) through training time in <ref type="figure">Fig. 3</ref>. In a standard condition (4-frame skip), both baselines can achieve perfect performance and STM outperforms LSTM slightly in terms of convergence speed. The performance gain becomes clearer under extreme conditions with over-sampling and undersampling. STM agents require fewer practices to accomplish higher rewards, especially in the 32-frame skip environment, which illustrates that having strong item and relational memory in a single model is beneficial to RL agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Error Mean</head><p>Best DNC <ref type="bibr" target="#b11">(Graves et al., 2016)</ref> 12.8 ± 4.7 3.8 NUTM <ref type="bibr">(Le et al., 2020)</ref> 5.6 ± 1.9 3.3 TPR (Schlag &amp; Schmidhuber, 2018) 1.34 ± 0.52 0.81 UT <ref type="bibr" target="#b7">(Dehghani et al., 2018)</ref> 1.12 ± 1.62 0.21 <ref type="bibr">MNM-p (Munkhdalai et al., 2019)</ref> 0.55 ± 0.74 0.18 STM 0.39 ± 0.18 0.15 <ref type="table">Table 4</ref>. bAbI task: mean ± std. and best error over 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Question answering</head><p>bAbI is a question answering dataset that evaluates the ability to remember and reason on textual information <ref type="bibr">(Weston et al., 2015)</ref>. Although synthetically generated, the dataset contains 20 challenging tasks such as pathfinding and basic induction, which possibly require both item and relational memory. Following Schlag &amp; Schmidhuber (2018), each story is preprocessed into a sentence-level sequence, which is fed into our STM as the input sequence. We jointly train STM for all tasks using normal supervised training (more in Appendix § J). We compare our model with recent memory networks and report the results in <ref type="table">Table 4</ref>.</p><p>MANNs such as DNC and NUTM have strong item memory, yet do not explicitly support relational learning, leading to significantly higher errors compared to other models. On the contrary, TPR is explicitly equipped with relational bindings but lack of item memory and thus clearly underperforms our STM. Universal Transformer (UT) supports a manually set item memory with dot product attention, showing higher mean error than STM with learned item memory and outer product attention. Moreover, our STM using normal supervised loss outperforms MNM-p trained with meta-level loss, establishing new state-of-the-arts on bAbI dataset. Notably, STM achieves this result with low variance, solving 20 tasks for 9/10 run (see Appendix § J).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Background on associative memory Associative memory is a classical concept to model memory in the brain <ref type="bibr">(Marr &amp; Thach, 1991)</ref>. While outer product is one common way to form the associative memory, different models employ different memory retrieval mechanisms. For example, Correlation Matrix Memory (CMM) and Hopfield network use dot product and recurrent networks, respectively <ref type="bibr" target="#b16">(Kohonen, 1972;</ref><ref type="bibr" target="#b14">Hopfield, 1982)</ref>. The distinction between our model and other associative memories lies in the fact that our model's association comes from several pieces of the memory itself rather than the input data. Also, unlike other two-memory systems <ref type="bibr">(Le et al., 2018b;</ref><ref type="bibr">2020)</ref> that simulate data/program memory in computer architec-ture, our STM resembles item and relational memory in human cognition.</p><p>Background on attention Attention is a mechanism that allows interactions between a query and a set of stored keys/values <ref type="bibr" target="#b10">(Graves et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2014)</ref>. Self-attention mechanism allows stored items to interact with each other either in forms of feed-forward <ref type="bibr">(Vaswani et al., 2017)</ref> or recurrent <ref type="bibr">(Santoro et al., 2018;</ref><ref type="bibr">Le et al., 2019)</ref> networks. Modeling memory interactions can also be achieved via attention over a set of parallel RNNs <ref type="bibr" target="#b12">(Henaff et al., 2016)</ref>. Although some form of relational memory can be kept in these approaches, they all use dot product attention to measure interactions per attention head as a scalar, and thus loose much relational information. We use outer product to represent the interactions as a matrix and thus our outer product self-attention is supposed to be richer than the current self-attention mechanisms (Prop. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SAM as fast-weight</head><p>Outer product represents Hebbian learning-a fast learning rule that can be used to build fast-weights (von der Malsburg, 1981). As the name implies, fast-weights update whenever an input is introduced to the network and stores the input pattern temporarily for sequential processing <ref type="bibr" target="#b0">(Ba et al., 2016a</ref> The tensor product representation (TPR), which is a form of high-order fast-weight, can be designed for structural reasoning <ref type="bibr">(Smolensky, 1990)</ref>. In a recent work (Schlag &amp; Schmidhuber, 2018), a third-order TPR resembles our relational memory M r t where both are 3D tensors. However, TPR does not enable interactions amongst stored patterns through self-attention mechanism. The meaning of each dimension of the TPR is not related to that of M r t . More importantly, TPR is restricted to question answering task.</p><p>SAM as bi-linear model Bi-linear pooling produces output from two input vectors by considering all pairwise bit interactions and thus can be implemented by means of outer product <ref type="bibr">(Tenenbaum &amp; Freeman, 2000)</ref>. To reduce computation cost, either low-rank factorization <ref type="bibr">(Yu et al., 2017)</ref> or outer product approximation <ref type="bibr">(Pham &amp; Pagh, 2013)</ref> is used. These approaches aim to enrich feed-forward layers with bi-linear poolings yet have not focused on maintaining a rich memory of relationships.</p><p>Low-rank bi-linear pooling is extended to perform visual attentions <ref type="bibr" target="#b15">(Kim et al., 2018)</ref>. It results in different formulation from our outer product attention, which is equivalent to full rank bi-linear pooling ( § 2.1). These methods are designed for static visual question answering while our approach is used to maintain a relational memory over time, which can be applied to any sequential problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have introduced the SAM-based Two-memory Model (STM) that implements both item and relational memory. To wire up the two memory system, we employ a novel operator named Self-attentive Associative Memory (SAM) that constructs the relational memory from outer-product relationships between arbitrary pieces of the item memory. We apply read, write and transfer operators to access, update and distill the knowledge from the two memories. The ability to remember items and their relationships of the proposed STM is validated through a suite of diverse tasks including associative retrieval, N th -farthest, vector algorithms, geometric and graph reasoning, reinforcement learning and question answering. In all scenarios, our model demonstrates strong performance, confirming the usefulness of having both item and relational memory in one model. </p><formula xml:id="formula_14">i , n j ∈ N + , ni i=1 nj j=1 q j k ij v i = nj j=1 ni i=1 q j k ij v i (15) where q j , k ij , v i ∈ R.</formula><p>Proof. We will prove by induction for all n j ∈ N + .</p><p>Base case: when n j = 1, the LHS = RHS = ni i q 1 k i1 v i . Let t ∈ N + be given and suppose Eq. 15 is true for n j = t. Then</p><formula xml:id="formula_15">ni i=1 t+1 j=1 q j k ij v i = ni i=1   q t+1 k it+1 v i + t j=1 q j k ij v i   = ni i=1 q t+1 k it+1 v i + ni i=1 t j=1 q j k ij v i = ni i=1 q t+1 k it+1 v i + t j=1 ni i=1 q j k ij v i = t+1 j=1 ni i=1 q j k ij v i</formula><p>Thus, Eq. 15 holds for n j = t + 1 and ∀n j ∈ N + by the principle of induction.</p><p>Proposition 3. Assume that S is a linear transformation: S (x) = ax+b (a, b, x ∈ R), we can extract A°from A ⊗ by using an element-wise linear transformation F (x) = a f ⊙ x+b f (a f , b f , x ∈ R d qk ) and a contraction P:</p><formula xml:id="formula_16">R d qk ×dv → R dv such that A°(q, K, V ) = P A ⊗ (q, K, V )<label>(16)</label></formula><p>where</p><formula xml:id="formula_17">A°(q, K, V ) = n kv i=1 S (q · k i ) v i (17) A ⊗ (q, K, V ) = n kv i=1 F (q ⊙ k i ) ⊗ v i (18)</formula><p>Proof. We derive the LHS. Let u i denote the scalar S (q · k i ), then</p><formula xml:id="formula_18">u i = S (q · k i ) = S   d qk j=1 q j k ij   = d qk j=1 aq j k ij + b</formula><p>where q j and k ij are the j-th elements of vector q and k i , respectively. Let l ∈ R dv denote the vector A°(q, K, V ) =</p><formula xml:id="formula_19">n kv i=1 u i v i , then the t-th element of l is l t = n kv i=1 u i v it = n kv i=1   d qk j=1 aq j k ij + b   v it = n kv i=1 d qk j=1 aq j k ij v it + b n kv i=1 v it = a n kv i=1 d qk j=1 q j k ij v it + b n kv i=1 v it<label>(19)</label></formula><p>We derive the RHS. Let d i denote the vector F (q ⊙ k i ), then the j-th element of d i is</p><formula xml:id="formula_20">d ij = F (q j k ij ) = a f j q j k ij + b f j (20) Let e ∈ R d qk ×dv denote the matrix A ⊗ (q, K, V ) = n kv i=1 d i ⊗ v i , then the j-th row, t-column element of e is e jt = n kv i=1 d ij v it = n kv i=1 a f j q j k ij + b f j v it = n kv i=1 a f j q j k ij v it + b f j n kv i=1 v it<label>(21)</label></formula><p>Let r ∈ R dv denote the vector d qk j=1 e j , then the t-th element of r is Model Addition complexity Multiplication complexity Physical storage for relationships  <ref type="table">Table 6</ref>. Wall-clock time to process a batch of data on Priority Sort task. The batch size is 128. All models are implemented using Pytorch, have around 1 million parameters and run on the same machine with Tesla V100-SXM2 GPU.</p><formula xml:id="formula_21">DPA O ((d qk n q + d v ) n kv ) O ((d qk + d v ) n q n kv ) O (n q n kv ) OPA O (n q n kv d qk d v ) O (n q d qk d v ) O (n q d qk d v )</formula><formula xml:id="formula_22">r t = d qk j=1 e jt = d qk j=1 n kv i=1 a f j q j k ij v it + b f j n kv i=1 v it = d qk j=1 n kv i=1 a f j q j k ij v it + d qk j=1 b f j n kv i=1 v it<label>(22)</label></formula><p>We can always choose a f j = a and</p><formula xml:id="formula_23">d qk j=1 b f j = b. Eq. 22 becomes, r t = a d qk j=1 n kv i=1 q j k ij v it + b n kv i v it</formula><p>According to Lemma 2, l t = r t ∀d qk , n kv ∈ N + ⇒ l = r. Also, ∃P as a contraction: P (X) = a p X with a p = [1, ..., 1] ∈ R 1×d qk .</p><p>We compare the complexity of DPA and OPA in <ref type="table" target="#tab_4">Table 5</ref>. In general, compared to that of DPA, OPA's complexity is increased by an order of magnitude, which is equivalent to the size of the patterns. In practice, we keep that value small (96) to make the training efficient. That said, due to its high-order nature, our memory model still maintains enormous memory space. In terms of speed, STM's running time is almost the same as RMC's and much faster than that of DNC or NTM. <ref type="table">Table 6</ref> compares the real running time of several memory-based models on Priority Sort task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relationship between OPA and bi-linear model</head><p>Proposition 4. Given the number of key-value pairs n kv = 1, and G is a high dimensional linear transformation</p><formula xml:id="formula_24">G : R d qk ×dv → R n , G (X) = W g V (X) where W g ∈ R n×d qk dv ,</formula><p>V is a function that flattens its input tensor, then G (A ⊗ (q, K, V )) can be interpreted as a bi-linear model between f and v 1 , that is</p><formula xml:id="formula_25">G A ⊗ (q, K, V ) [s] = d qk j=1 dv t=1 W g [s, j, t] f [j] v 1 [t] (23) where W g [s, j, t] = W g [s] [(j − 1) d v + t],s = 1, ..., n, j = 1, ..., d qk , t = 1, ..., d v , and f = F (q ⊙ k 1 ).</formula><p>Proof. By definition,</p><formula xml:id="formula_26">V (F (q ⊙ k 1 ) ⊗ v 1 ) [(j − 1) d v + t] = (F (q ⊙ k 1 ) ⊗ v 1 ) [j] [t] = F (q ⊙ k 1 ) [j] v 1 [t]</formula><p>We derive the LHS,</p><formula xml:id="formula_27">G A ⊗ (q, K, V ) [s] = (W g V (F (q ⊙ k 1 ) ⊗ v 1 )) [s] = d qk dv u=1 W g [s] [u] V (F (q ⊙ k 1 ) ⊗ v 1 ) [u] = d qk dv (j−1)dv+t (W g [s] [(j − 1) d v + t] × V (F (q ⊙ k 1 ) ⊗ v 1 ) [(j − 1) d v + t]) = d qk j=1 dv t=1 W g [s, j, t] F (q ⊙ k 1 ) [j] v 1 [t]</formula><p>which equals the RHS.</p><p>Prop. 4 is useful since it demonstrates the representational capacity of OPA is at least equivalent to bi-linear pooling, which is richer than low-rank bi-linear pooling using Hadamard product, or bi-linear pooling using identity matrix of the bi-linear form (dot product), or the vanilla linear models using traditional neural networks.</p><p>Proposition 5. Given the number of queries n q = d qk , the number of key-value pairs n kv = 1, M r t = SAM θ (M ) where M is an instance of the item memory in the past, and G is a high dimensional linear transformation G :</p><formula xml:id="formula_28">R nq×d qk ×dv → R d qk ×dv , G (X) = W g V f (X) where W g ∈ R d qk ×nqd qk , V f</formula><p>is a function that flattens the first two dimensions of its input tensor, then Eq. 13 can be interpreted as a Hebbian update to the item memory.</p><formula xml:id="formula_29">Proof. Let k 1 = M k and v 1 = M v when n kv = 1, by definition V f (SAM θ (M )) [(s − 1) d qk + j, t] = F (M q [s] ⊙ k 1 ) [j] v 1 [t]. We derive, G (SAM θ (M )) [i, t] = (W g V f (SAM θ (M ))) [i, t] = nqd qk u=1 W g [i, u] V f (SAM θ (M )) [u, t] = nqd qk (s−1)d qk +j=1 (W g [i, (s − 1) d qk + j] × F (M q [s] ⊙ k 1 ) [j] v 1 [t]) = nq s=1 d qk j=1 W g [i, s, j] f [s, j] v 1 [t] (24) where f [s, j] = F (M q [s] ⊙ k 1 ) [j] = F (M q [s, j] k 1 [j]).</formula><p>It should be noted that with trivial rank-one W g : W g [i] = d i V f (I), d i ∈ R, I is the identity matrix, Eq. 24 becomes</p><formula xml:id="formula_30">G (SAM θ (M )) [i, t] = d [i] v 1 [t] ⇒ G (SAM θ (M )) = d ⊗ v 1 where d ∈ R d qk , d [i] = d i nq s=1 F (M q [s, s] k 1 [s]). Eq. 13 reads M i t = M i t + α 3 d ⊗ v 1<label>which</label></formula><p>is a Hebbian update with the updated value v 1 . As v 1 is a stored pattern extracted from M encoded in the relational memory, the item memory is enhanced with a longterm stored value from the relational memory.</p><p>C. OPA and SAM as associative memory 1 Proposition 6. If P is a contraction: R d qk ×dv → R dv , P (X) = a p X, a p ∈ R 1×d qk , then A ⊗ (q, K, V ) is an associative memory that stores patterns {v i } n kv i=1 and 1 In this section, we use these following properties without explanation:</p><formula xml:id="formula_31">a ⊤ (b ⊗ c) = a ⊤ b c ⊤ and (b ⊗ c) a = c ⊤ a b.</formula><p>P (A ⊗ (q, K, V )) is a retrieval process. Perfect retrieval is possible under the following three conditions,</p><formula xml:id="formula_32">(1) {k i } n kv i=1 form a set of linearly independent vectors (2) q i = 0, i = 1, ..., d qk (3) F is chosen as F (x) = a f ⊙ x (a f , x ∈ R d qk , a f i = 0, i = 1, ..., d qk )</formula><p>Proof. By definition, A ⊗ (q, K, V ) forms a heteroassociative memory between x i = F (q ⊙ k i ) and v i . If</p><formula xml:id="formula_33">{x i } n kv i=1 are orthogonal, given some P with a p = x ⊤ j x ⊤ j , then P A ⊗ (q, K, V ) = x ⊤ j x ⊤ j n kv i=1 x i ⊗ v i = n kv i=1,i =j x ⊤ j x i x ⊤ j v ⊤ i + x ⊤ j x j x ⊤ j v ⊤ j = v ⊤ j</formula><p>Hence, we can perfectly retrieve some stored pattern v j using its associated P. In practice, linearly independent {x i } n kv i=1 is enough for perfect retrieval since we can apply Gram-Schmidt process to construct orthogonal</p><formula xml:id="formula_34">{x i } n kv i=1 .</formula><p>Another solution is to follow Widrow-Hoff incremental update</p><formula xml:id="formula_35">A ⊗ (q, K, V ) (0) = 0 A ⊗ (q, K, V ) (i) = A ⊗ (q, K, V ) (i − 1) + v i − A ⊗ (q, K, V ) (i − 1) x i ⊗ x i</formula><p>which also results in possible perfect retrieval given</p><formula xml:id="formula_36">{x i } n kv i=1 are linearly independent. Now, we show that if (1) (2) (3) are satisfied, {x i } n kv i=1</formula><p>are linearly independent using proof by contradiction. Assume</p><formula xml:id="formula_37">that {x i } n kv i=1 are linearly dependent, ∃ {α i ∈ R} n kv i=1 , not all zeros such that − → 0 = n kv i=1 α i x i = n kv i=1 α i F (q ⊙ k i ) = n kv i=1 α i a f ⊙ (q ⊙ k i ) = a f ⊙ q ⊙ n kv i=1 α i k i<label>(25)</label></formula><p>As <ref type="formula">(2)</ref> (3) hold true, Eq. 25 is equivalent to</p><formula xml:id="formula_38">− → 0 = n kv i=1 α i k i which contradicts (1).</formula><p>Prop. 6 is useful as it points out the potential of our OPA formulation for accurate associative retrieval over several key-value pairs. That is, despite that many items are extracted to form the relational representation, we have the chance to reconstruct any items perfectly if the task requires item memory. As later we use neural networks to generate k and q, the model can learn to satisfy conditions (1) and <ref type="formula">(2)</ref>. Although in practice, we use element-wise tanh to offer non-linear transformation, which is different from <ref type="formula" target="#formula_2">(3)</ref>, empirical results show that our model still excels at accurate associative retrieval.</p><p>Proposition 7. Assume that the gates in Eq. 10 are kept constant F t = I t = 1, the item memory construction is simplified to</p><formula xml:id="formula_39">M = N +1 i=1 x i ⊗ x i , where {x i } N +1</formula><p>i=1 are positive input patterns after feedforward neural networks and the relational memory construction is simplified to</p><formula xml:id="formula_40">M r = SAM θ (M ) ,</formula><p>and layer normalizations are excluded, then the memory retrieval is a two-step contraction</p><formula xml:id="formula_41">v r = softmax z ⊤ M r f (x)</formula><p>Proof. Without loss of generality, after seeing N + 1 pat-</p><formula xml:id="formula_42">terns {x i } N +1 i=1</formula><p>, SAM is given a (noisy or incomplete) query pattern x that corresponds to some stored pattern x p = x N +1 , that is</p><formula xml:id="formula_43">x ⊤ p x ≈ 1 x ⊤ i x ≈ 0 i = 1, N Unrolling Eq. 8 yields SAM θ (M ) [s] = n kv j=1 F (M q [s] ⊙ M k [j]) ⊗ M v [j] = n kv j=1 F W q [s] N +1 i=1 x i ⊗ x i ⊙W k [j] N +1 i=1 x i ⊗ x i ⊗ W v [j] N +1 i=1 x i ⊗ x i = n kv j=1 F N i=1 W q [s] x i ⊗ x i + W q [s] x p ⊗ x p ) ⊙ N i=1 W k [j] x i ⊗ x i + W k [j] x p ⊗ x p ⊗ N i=1 W v [j] x i ⊗ x i + W v [j] x p ⊗ x p<label>(26)</label></formula><p>When d &gt; N , it is generally possible to find W q , W k and W v that satisfy the following system of equations:</p><formula xml:id="formula_44">                   W q [s] x i = 0, i = 1, N , W q [s] x p = 1 W k [j] x i = 0, i = 1, N W k [j] x p = 1 W v [j] x i = 1, i = 1, N W v [j] x p = 1</formula><p>We also assume that F is chosen as square root function, then Eq. 26 simplifies to</p><formula xml:id="formula_45">SAM θ (M ) [s] = n kv j=1 F (x p ⊙ x p ) ⊗ N +1 i=1 x i = n kv x p ⊗ N +1 i=1 x i = n kv N +1 i=1 x p ⊗ x i</formula><p>The first contraction softmax z ⊤ M r can be interpreted as an attention to {SAM θ (M ) [s]} nq s=1 , which equals</p><formula xml:id="formula_46">n kv N +1 i=1 x p ⊗ x i</formula><p>The second contraction is similar to a normal associative memory retrieval. When we choose f (x) = x n kv , the retrieval reads</p><formula xml:id="formula_47">v r = n kv N +1 i=1 x p ⊗ x i x n kv = N +1 i=1 x ⊤ i x x p ≈ x p D. Implementation of gate functions F t M i t−1 , x t = W F x t + U F tanh M i t−1 + b F I t M i t−1 , x t = W I x t + U I tanh M i t−1 + b I Here, W F , U F , W I , W I ∈ R d×d are parametric weights, b F , b I ∈ R are biases and + is broadcasted if needed.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Learning curves on ablation study</head><p>We plot the learning curves of evaluated modes for Associative retrieval with length 30, 50 and N th -farthest in <ref type="figure" target="#fig_2">Fig.  4</ref>. For N th -farthest, the last input in the sequence is treated as the query for TPR. We keep the standard number of entities/roles and tune TPR 2 with different hidden dimensions <ref type="bibr">(40,</ref><ref type="bibr">128,</ref><ref type="bibr">256)</ref> and optimizers (Nadam and Adam). All configurations fail to converge for the normal N th -farthest as shown in <ref type="figure" target="#fig_2">Fig. 4 (right)</ref>. When we reduce the problem size to 4 8-dimensional input vectors, TPR can reach perfect performance, which indicates the problem here is more about scaling to bigger relational reasoning contexts.</p><p>F. Implementation of baselines for algorithmic and geometric/graph tasks • LSTM and ALSTM: Both use 512-dimensional hidden vectors for all tasks.</p><p>• NTM 3 , DNC 4 : Both use a 256-dimensional LSTM controller for all tasks. For algorithmic tasks, NTM uses a 128-slot external memory, each slot is a 32dimensional vector. Following the standard setting, NTM uses 1 control head for Copy, RAR and 5 control heads for Priority sort. For geometric/graph tasks, DNC is equipped with 64-dimensional 20-slot external memory and 4-head controller. In geometric/graph problems, 20 slots are about the number of points/nodes. We also tested with layer-normalized DNC without temporal link matrix and got similar results.</p><p>• RMC 5 : We use the default setting with total 1024 dimensions for memory of 8 heads and 8 slots. We also tried with different numbers of slots {1, 4, 16} and Adam optimizer but the performance did not change.</p><p>• STM: We use the same setting across tasks n q = 8, d = 96, n r = 96. α 1 ,α 2 , and α 3 are learnable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Order of relationship</head><p>In this paper, we do not formally define the concept of order of relationship. Rather, we describe it using concrete examples. When a problem requires to compute the relationship between items, we regard it as a first-order relational problem. For example, sorting is first-order relational. Copy is even zero-order relational since it can be solved without considering item relationships. When a problem requires to compute the relationship between relationships of items, we regard it as a second-order relational problem and so on.</p><p>From this observation, we hypothesize that the computational complexity of a problem roughly corresponds to the order of relationship in the problem. For example, if a problem requires a solution whose computational complexity between O (N ) and O N 2 where N is the input size, it means the solution basically computes the relationship between any pair of input items and thus corresponds to firstorder relationship. <ref type="table">Table 7</ref> summarizes our hypothesis on the order of relationship in some of our problems.</p><p>By design, our proposed STM stores a mixture of relationships between items in a relational memory, which approximately corresponds to a maximum of second-order relational capacity. The distillation process in STM transforms the relational memory to the output and thus determines the order of relationship that STM can offer. We can measure the degree that STM involves in relational mining by analyzing the learned weight G 2 of the distillation process. Intuitively, a high-rank transformation G 2 can capture more relational information from the relational memory. Trivial low-rank G corresponds to item-based retrieval without much relational mining (Prop. 5). The numerical rank of a matrix A is defined as r (A) = A 2 F / A 2 2 , which relaxes the exact notion of rank <ref type="bibr">(Rudelson &amp; Vershynin, 2007)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>General complexity Order We report the numerical rank of learned G 2 ∈ R 6144×96 for different tasks in <ref type="table">Table 8</ref>. For each task, we run the training 5 times and take the mean and std. of r (G 2 ). The rank is generally higher for tasks that have higher orders of relationship. That said, the model tends to overuse its relational capacity. Even for the zero-order Copy task, the rank for the distillation transformation is still very high.</p><formula xml:id="formula_48">Copy/Associative retrieval O (N ) 0 Sort O (N log N ) 1 Convex hull O (N log N ) 1 Shortest path 6 O (E log V ) 1 Minimum spanning tree O (E log V ) 1 RAR/N th -Farthest O N 2 log N 2 Traveling salesman problem NP-hard many</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Geometry and graph task description</head><p>In this testbed, we use RMSprop optimizer with a learning rate of 10 −4 and a batch size of 128 for all baselines. STM uses the same setting across tasks n q = 8, d = 96, n r = 96. The random one-hot features can be extended to binary features, which is much harder and will be investigated in our future works.</p><p>Convex hull Given a set of N points with 2D coordinates, the model is trained to output a list of points that forms a convex hull sorted by coordinates. Training is done with N ∼ <ref type="bibr">[5,</ref><ref type="bibr">20]</ref>. Testing is done with N = 5 and N = 10 (no prebuilt dataset available for N = 20). The output is a sequence of 20-dimensional one-hot vectors representing the features of the solution points in the convex-hull.</p><p>Traveling salesman problem Given a set of N points with 2D coordinates, the model is trained to output a list of points that forms a closed tour sorted by coordinates. Training is done with N ∼ [5, 10]. Testing is done with N = 5 and N = 10. The output is a sequence of 20-dimensional one-hot vectors representing the features of the solution points in the optimal tour.</p><p>Shortest path The graph is generated according to the following rules: (1) choose the number of nodes N ∼ [5, 20], (2) after constructing a path that goes through every node in the graph (to make the graph connected), determine randomly the edge between nodes (number of edges E ∼ [6, 30]), (3) for each edge set the weight w ∼ [1, 10]. We generate 100,000 and 10,000 graphs for training and testing, respectively. The representation for an input graph is a sequence of triplets followed by 2 feature vectors representing the source and destination node. The output is a sequence of 40-dimensional one-hot feature vectors representing the solution nodes in the shortest path.</p><p>Minimum spanning tree We use the same generated input graphs from the Shortest path task. The representation for an input graph is only a sequence of triplets. The output is a sequence of 40-dimensional one-hot feature vectors representing the features of the nodes in the solution edges of the minimum spanning tree.</p><p>Some generated samples of the four tasks are visualized in <ref type="figure" target="#fig_3">Fig. 5</ref>. Learning curves are given in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Reinforcement learning task description</head><p>We trained Openai Gym's PongNoFrameskip-v4 using Asynchronous Advantage Actor-Critic (A3C) with hyperparameters: 32 workers, shared Adam optimizer with a learning rate of 10 −4 , γ = 0.99. To extract scene features for LSTM and STM, we use 4 convolutional layers (32 kernels with 5×5 kernel sizes and a stride of 1), each of which is followed by a 2 × 2 max-pooling layer, resulting in 1024dimensional feature vectors. The LSTM 's hidden size is 512. STM uses n q = 8, d = 96, n r = 96.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. bAbI task description</head><p>We use the train/validation/test split introduced in bAbI's en-valid-10k v1.2 dataset. To make STM suitable for question answering task, each story is preprocessed into a sentence-level sequence, which is fed into our STM as the input sequence. The question, which is only 1 sentence, is preprocessed to a query vector. Then, we utilize the Inference module, which takes the query as input to extract the output answer from our relational memory M r . The preprocessing and the Inference module are the same as in <ref type="bibr">Schlag &amp; Schmidhuber (2018)</ref>. STM's hyper-parameters are fixed to n q = 20, d = 90, n r = 96. We train our model jointly for 20 tasks with a batch size of 128, using Adam optimizer with a learning rate of 0.006, β 1 = 0.9 and β 2 = 0.99. Details of all runs are listed in <ref type="table">Table 9</ref>.</p><p>K. Characteristics of memory-based neural networks <ref type="table">Table 10</ref> compares the characteristics of common neural networks with memory. Biological plausibility is determined based on the design of the model. It is unlikely that human memory employs RAM-like behaviors as in NTM, DNC, and RMC. Fixed-size memory is inevitable for online and life-long learning, which also reflects biological plausibility. Relational extraction and recurrent dynamics are often required in powerful models. As shown in the table, our proposed model exhibits all the nice features that a memory model should have.  <ref type="figure">Figure 6</ref>. Learning curves on geometry and graph tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attentive Associative Memory</head><p>Task run-1 run-2 run-3 run-4 run-5 run-6 run-7 run-8 run-9 run-10 Mean <ref type="formula">1</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Bit error per sequence vs training iteration for algorithmic synthetic tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Following</head><label></label><figDesc><ref type="bibr" target="#b10">Graves et al. (2014)</ref>, we use RMSprop optimizer with a learning rate of 10 −4 and a batch size of 128 for all baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Testing accuracy (%) on associative retrieval L=30 (left), L=50 (middle) and N th -farthest (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Samples of geometry and graph tasks. From top to bottom: Convex hull, TSP, Shortest path and Minimum spanning tree. Blue denotes the ground-truth solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2</head><label>12</label><figDesc>Comparison of models on associative retrieval task with number of epochs E. required to converge (lower is better) and convergence test accuracy A. (%, higher is better). * is reported fromZhang &amp; Zhou (2017).</figDesc><table><row><cell>Model</cell><cell>Length 30 E. A.</cell><cell cols="2">Length 50 E. A.</cell></row><row><cell>Fast weight  *</cell><cell cols="3">50 100 5000 20.8</cell></row><row><cell>WeiNet  *</cell><cell>35 100</cell><cell>50</cell><cell>100</cell></row><row><cell cols="3">STM (d = 48, w/o transfer) 10 100 100</cell><cell>100</cell></row><row><cell>STM (d = 48)</cell><cell>20 100</cell><cell>80</cell><cell>100</cell></row><row><cell>STM (d = 96, w/o gates)</cell><cell>100 24</cell><cell>100</cell><cell>20</cell></row><row><cell>STM (d = 96)</cell><cell>10 100</cell><cell>20</cell><cell>100</cell></row><row><cell>Model</cell><cell>Accuracy (%)</cell><cell></cell><cell></cell></row><row><cell>DNC  *</cell><cell>25</cell><cell></cell><cell></cell></row><row><cell>RMC  *</cell><cell>91</cell><cell></cell><cell></cell></row><row><cell>TPR</cell><cell>13</cell><cell></cell><cell></cell></row><row><cell>STM (n q = 1)</cell><cell>84</cell><cell></cell><cell></cell></row><row><cell>STM (n q = 4)</cell><cell>95</cell><cell></cell><cell></cell></row><row><cell>STM (n q = 8)</cell><cell>98</cell><cell></cell><cell></cell></row></table><note>. Comparison of models on N th -farthest task (test accu- racy). * is reported from Santoro et al. (2018).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). Graphs in Shortest path and Minimum spanning tree are generated with solutions found by Dijkstra and Kruskal algorithms, respectively. A graph input is represented as a sequence of triplets (node 1 , node 2 , edge 12 ). The desired output is a sequence of associated features of the solution points/nodes (more in Appendix § H).We generate a random one-hot associated feature for each point/node, which is stacked into the input vector. This allows us to output the node's associated features. This is unlikeVinyals et al. (2015), who just outputs the pointers to the nodes. Our modification creates a challenge for both training and testing. The training is more complex as the feature of the nodes varies even for the same graph. The testing is challenging as the associated features are likely to be different from that in the training. A correct prediction</figDesc><table><row><cell>Model</cell><cell>#Parameters</cell><cell cols="2">Convex hull N = 5 N = 10</cell><cell>N = 5</cell><cell>TSP</cell><cell>N = 10</cell><cell>Shortest path</cell><cell>Minimum spanning tree</cell></row><row><cell>LSTM</cell><cell>4.5 M</cell><cell>89.15</cell><cell>82.24</cell><cell cols="3">73.15 (2.06) 62.13 (3.19)</cell><cell>72.38</cell><cell>80.11</cell></row><row><cell>ALSTM</cell><cell>3.7 M</cell><cell>89.92</cell><cell>85.22</cell><cell cols="3">71.79 (2.05) 55.51 (3.21)</cell><cell>76.70</cell><cell>73.40</cell></row><row><cell>DNC</cell><cell>1.9 M</cell><cell>89.42</cell><cell>79.47</cell><cell cols="3">73.24 (2.05) 61.53 (3.17)</cell><cell>83.59</cell><cell>82.24</cell></row><row><cell>RMC</cell><cell>2.8 M</cell><cell>93.72</cell><cell>81.23</cell><cell cols="3">72.83 (2.05) 37.93 (3.79)</cell><cell>66.71</cell><cell>74.98</cell></row><row><cell>STM</cell><cell>1.9 M</cell><cell>96.85</cell><cell>91.88</cell><cell cols="3">73.96 (2.05) 69.43 (3.03)</cell><cell>93.43</cell><cell>94.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Le, H., Tran, T., and Venkatesh, S. Dual memory neural computer for asynchronous two-view sequential learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery; Data Mining, KDD '18, pp. 1637-1645, New York, NY, USA, 2018b. ACM. ISBN 978-1-4503-5552-0. doi: 10.1145/3219819.3219981. URL http://doi.acm.org/10.1145/3219819.3219981. Pascanu, R., and Lillicrap, T. Relational recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 7299-7310, 2018. Schlag, I. and Schmidhuber, J. Gated fast weights for onthe-fly neural program generation. In NIPS Metalearning Workshop, 2017.</figDesc><table><row><cell>A. Relationship between OPA and DPA</cell><cell></cell><cell></cell></row><row><cell>Lemma 2. For ∀n</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Schlag, I. and Schmidhuber, J. Learning to reason with</cell></row><row><cell>Le, H., Tran, T., and Venkatesh, S. Learning to remem-ber more with less memorization. In International</cell><cell cols="2">third order tensor products. In Advances in Neural Infor-mation Processing Systems, pp. 9981-9993, 2018.</cell></row><row><cell>Conference on Learning Representations, 2019. URL</cell><cell cols="2">Smolensky, P. Tensor product variable binding and the rep-</cell></row><row><cell cols="3">https://openreview.net/forum?id=r1xlvi0qYm. resentation of symbolic structures in connectionist sys-</cell></row><row><cell>Le, H., Tran, T., and Venkatesh, S. Neural stored-</cell><cell cols="2">tems. Artificial intelligence, 46(1-2):159-216, 1990.</cell></row><row><cell>program memory. ence on Learning Representations, 2020. In International Confer-URL</cell><cell cols="2">Tenenbaum, J. B. and Freeman, W. T. Separating style and content with bilinear models. Neural computation, 12</cell></row><row><cell cols="2">https://openreview.net/forum?id=rkxxA24FDr. (6):1247-1283, 2000.</cell><cell></cell></row><row><cell>Marr, D. and Thach, W. T. A theory of cerebellar cortex. In From the Retina to the Neocortex, pp. 11-50. Springer, 1991.</cell><cell cols="2">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-tion is all you need. In Advances in neural information</cell></row><row><cell>Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,</cell><cell cols="2">processing systems, pp. 5998-6008, 2017.</cell></row><row><cell>T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-chronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928-1937, 2016.</cell><cell cols="2">Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692-2700, 2015.</cell></row><row><cell>Munkhdalai, T., Sordoni, A., Wang, T., and Trischler, A. Metalearned neural memory. In Advances in Neural In-formation Processing Systems, pp. 13310-13321, 2019.</cell><cell cols="2">von der Malsburg, theory of brain function, C. http://cogprints.org/1380/. The correlation 1981. URL</cell></row><row><cell>Olson, I. R., Page, K., Moore, K. S., Chatterjee, A., and Verfaellie, M. Working memory for conjunctions relies on the medial temporal lobe. Journal of Neuroscience, 26(17):4596-4601, 2006.</cell><cell cols="2">Weston, J., Bordes, A., Chopra, S., Rush, A. M., van Merriënboer, B., Joulin, A., and Mikolov, T. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.</cell></row><row><cell>Pham, N. and Pagh, R. Fast and scalable polynomial ker-nels via explicit feature maps. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 239-247. ACM, 2013.</cell><cell cols="2">Yu, Z., Yu, J., Fan, J., and Tao, D. Multi-modal factor-ized bilinear pooling with co-attention learning for vi-sual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 1821-1830, 2017.</cell></row><row><cell>Rojas, R. Neural networks: a systematic introduction. Springer Science &amp; Business Media, 2013.</cell><cell cols="2">Zeithamova, D., Schlichting, M. L., and Preston, A. R. The hippocampus and inferential reasoning: building mem-</cell></row><row><cell>Rudelson, M. and Vershynin, R. Sampling from large matri-ces: An approach through geometric functional analysis.</cell><cell cols="2">ories to navigate future decisions. Frontiers in human neuroscience, 6:70, 2012.</cell></row><row><cell>Journal of the ACM (JACM), 54(4):21, 2007.</cell><cell>Zhang, W. and Zhou, B.</cell><cell>Learning to update auto-</cell></row><row><cell>Santoro, A., Raposo, D., Barrett, D. G., Malinowski, M., Pascanu, R., Battaglia, P., and Lillicrap, T. A sim-ple neural network module for relational reasoning. In</cell><cell cols="2">associative memory in recurrent neural networks for im-proving sequence memorization. ArXiv, abs/1709.06493, 2017.</cell></row><row><cell>Advances in neural information processing systems, pp.</cell><cell></cell><cell></cell></row><row><cell>4967-4976, 2017.</cell><cell></cell><cell></cell></row></table><note>Santoro, A., Faulkner, R., Raposo, D., Rae, J., Chrzanowski, M., Weber, T., Wierstra, D., Vinyals, O.,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Computational complexity of DPA and OPA with nq queries and n kv key-value pairs. d qk denotes query or key size, while dv value size.</figDesc><table><row><cell cols="2">Model Wall-clock time (second)</cell></row><row><cell>LSTM</cell><cell>0.1</cell></row><row><cell>NTM</cell><cell>1.8</cell></row><row><cell>RMC</cell><cell>0.3</cell></row><row><cell>STM</cell><cell>0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Order of relationship in some problems. Mean and std. of numerical rank of the leanred weight G2 for several tasks. The upper bound for the rank is 96.</figDesc><table><row><cell>Task</cell><cell>r (G 2 )</cell></row><row><cell>Associative retrieval</cell><cell>9.42±0.5</cell></row><row><cell>N th -Farthest</cell><cell>83.20±0.2</cell></row><row><cell>Copy</cell><cell>79.00±0.3</cell></row><row><cell>Sort</cell><cell>79.58±0.1</cell></row><row><cell>RAR</cell><cell>83.30±0.2</cell></row><row><cell>Convex hull</cell><cell>80.78±0.6</cell></row><row><cell cols="2">Traveling salesman problem 83.58±0.3</cell></row><row><cell>Shortest path</cell><cell>79.81±0.2</cell></row><row><cell>Minimum spanning tree</cell><cell>79.57±0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>Results from 10 runs of STM on bAbI 10k. Bold denotes best run. Characteristics of some neural memory models</figDesc><table><row><cell>Model</cell><cell>Fixed-size Relational Recurrent Biologically memory extraction dynamics plausible</cell></row><row><cell>RNN, LSTM</cell><cell></cell></row><row><cell>NTM, DNC</cell><cell></cell></row><row><cell>RMC</cell><cell></cell></row><row><cell>Transformer</cell><cell></cell></row><row><cell>UT</cell><cell></cell></row><row><cell>Attentional LSTM</cell><cell></cell></row><row><cell>STM</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ischlag/TPR-RNN 3 https://github.com/vlgiitr/ntm-pytorch 4 https://github.com/deepmind/dnc</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/L0SG/relational-rnn-pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The input is sequence of triplets, which is equivalent to sequence of edges. Hence, the complexity is based on the number of edges in the graph.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was partially funded by the Australian Government through the Australian Research Council (ARC). Prof Venkatesh is the recipient of an ARC Australian Laureate Fellowship (FL170100006).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4331" to="4339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reinforcement learning with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1475" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The role of the perirhinal cortex and hippocampus in learning, memory, and perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology Section B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="246" to="268" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memory for items and memory for relations in the procedural/declarative memory framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eichenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="131" to="178" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Ł. Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Memory, amnesia, and the hippocampal system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eichenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03969</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1564" to="1574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Correlation matrix memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="353" to="359" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relational memory and the hippocampus: representations and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalization through the recurrent interaction of episodic memories: a model of the hippocampal system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">573</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational memory encoder-decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1508" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
