<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Inception Institute of Artificial Intelligence (IIAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<addrLine>4 ACRV 5 Data61</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh</forename><forename type="middle">Sadat</forename><surname>Saleh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose the first framework (UC-Net) to employ uncertainty for RGB-D saliency detection by learning from the data labeling process. Existing RGB-D saliency detection methods treat the saliency detection task as a point estimation problem, and produce a single saliency map following a deterministic learning pipeline. Inspired by the saliency data labeling process, we propose probabilistic RGB-D saliency detection network via conditional variational autoencoders to model human annotation uncertainty and generate multiple saliency maps for each input image by sampling in the latent space. With the proposed saliency consensus process, we are able to generate an accurate saliency map based on these multiple predictions. Quantitative and qualitative evaluations on six challenging benchmark datasets against 18 competing algorithms demonstrate the effectiveness of our approach in learning the distribution of saliency maps, leading to a new state-of-the-art in RGB-D saliency detection 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object-level visual saliency detection involves separating the most conspicuous objects that attract humans from the background <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b61">62]</ref>. Recently, visual saliency detection from RGB-D images have attracted lots of interest due to the importance of depth information in human vision system and the popularity of depth sensing technologies <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b63">64]</ref>. Given a pair of RGB-D images, the task of RGB-D saliency detection aims to predict a saliency map by exploring the complementary information between color image and depth data.</p><p>The de-facto standard for RGB-D saliency detection is to train a deep neural network using ground truth (GT)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Depth GT Ours (1) Ours (2) <ref type="figure">Figure 1</ref>. Provided GT compared with UC-Net (ours) predicted saliency maps. For images with a single salient object (1 st row), we can produce consistent prediction. When multiple salient objects exist (2 nd row), we can produce diverse predictions. saliency maps provided by the corresponding benchmark datasets, where the GT saliency maps are obtained through human consensus or by the dataset creators <ref type="bibr" target="#b17">[18]</ref>. Building upon large scale RGB-D datasets, deep convolutional neural network based models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref> have made profound progress in learning the mapping from an RGB-D image pair to the corresponding GT saliency map. Considering the progress for RGB-D saliency detection under this pipeline, in this paper, we would like to argue that this pipeline fails to capture the uncertainty in labeling the GT saliency maps. According to research in human visual perception <ref type="bibr" target="#b32">[33]</ref>, visual saliency detection is subjective to some extent. Each person could have specific preferences in labeling the saliency map (which has been previous discussed in userspecific saliency detection <ref type="bibr" target="#b25">[26]</ref>). Existing approaches to RGB-D saliency detection treat saliency detection as a point estimation problem, and produce a single saliency map for each input image pair following a deterministic learning pipeline, which fails to capture the stochastic characteristic of saliency, and may lead to a partisan saliency model as shown in second row of <ref type="figure">Fig. 1</ref>. Instead of obtaining only a single saliency prediction (point estimation), we are interested in how the network produces multiple predictions (distribution estimation), which are then processed further to generate a single prediction in a similar way to how the GT saliency maps are created.</p><p>In this paper, inspired by human perceptual uncertainty, we propose a conditional variational autoencoders <ref type="bibr" target="#b49">[50]</ref> (CVAE) based RGB-D saliency detection model UC-Net to produce multiple saliency predictions by modeling the distribution of output space as a generative model conditioned on the input RGB-D images to account for the human uncertainty in annotation. However, there still exists one obstacle before we could apply the probabilistic framework, that is existing RGB-D benchmark datasets generally only provide a single GT saliency map for each RGB-D image pair. To produce diverse and accurate predictions 2 , we resort to the "hide and seek" <ref type="bibr" target="#b48">[49]</ref> principle following the orientation shifting theory <ref type="bibr" target="#b25">[26]</ref> by iteratively hiding the salient foreground from the RGB image for testing, which forces the deep network to learn the saliency map with diversity. Through this iterative hiding strategy, we obtain multiple saliency maps for each input RGB-D image pair, which reflects the diversity/uncertainty from human labeling.</p><p>Moreover, depth data in the RGB-D saliency dataset can be noisy, and a direct fusion of RGB and depth information may overwhelm the network to fit noise. To deal with the noisy depth problem, a depth correction network is proposed as an auxiliary component to produce depth images with rich semantic and geometric information. We also introduce a saliency consensus module to mimic the majority voting mechanism for saliency GT generation.</p><p>Our main contributions are summarized as: 1) We propose a conditional probabilistic RGB-D saliency prediction model that can produce diverse saliency predictions instead of a single saliency map; 2) We provide a mechanism via saliency consensus to better model how saliency detection works; 3) We present a depth correction network to decrease noise that is inherent in depth data; 4) Extensive experimental results on six RGB-D saliency detection benchmark datasets demonstrate the effectiveness of our UC-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">RGB-D Saliency Detection</head><p>Depend on how the complementary information between RGB images and depth images is fused, existing RGB-D saliency detection models can be roughly classified into three categories: early-fusion models <ref type="bibr" target="#b42">[43]</ref>, late-fusion models <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b23">24]</ref> and cross-level fusion models <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b63">64]</ref>. Qu et al. <ref type="bibr" target="#b42">[43]</ref> proposed an early-fusion model to generate feature for each superpixel of the RGB-D pair, which was then fed to a CNN to produce saliency of each superpixel. Recently, Wang et al. <ref type="bibr" target="#b53">[54]</ref> introduced a late-fusion network (i.e. AFNet) to fuse predictions from the RGB and depth branch adaptively. In a similar pipeline, Han et al. <ref type="bibr" target="#b23">[24]</ref> fused the RGB and depth information through fully connected layers. Chen et al. <ref type="bibr" target="#b6">[7]</ref> used a multi-scale multi-path network for different modality information fusion. Chen et al. <ref type="bibr" target="#b4">[5]</ref> proposed a complementary-aware RGB-D saliency detection model by fusing features from the same stage of each modality with a complementary-aware fusion block. Chen et al. <ref type="bibr" target="#b5">[6]</ref> presented attention-aware cross-level combination blocks for multi-modality fusion. Zhao et al. <ref type="bibr" target="#b63">[64]</ref> integrated a contrast prior to enhance depth cues, and employed a fluid pyramid integration framework to achieve multi-scale cross-modal feature fusion. To effectively incorporate geometric and semantic information within a recurrent learning framework, Li et al. <ref type="bibr" target="#b60">[61]</ref> introduced a depthinduced multi-scale RGB-D saliency detection network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">VAE or CVAE based Deep Probabilistic Models</head><p>Ever since the seminal work by Kingma et al. <ref type="bibr" target="#b30">[31]</ref> and Rezende et al. <ref type="bibr" target="#b44">[45]</ref>, variational autoencoder (VAE) and its conditional counterpart CVAE <ref type="bibr" target="#b49">[50]</ref> have been widely applied in various computer vision problems. To train a VAE, a reconstruction loss and a regularizer are needed to penalize the disagreement of the prior and posterior distribution of the latent representation. Instead of defining the prior distribution of the latent representation as a standard Gaussian distribution, CVAE utilizes the input observation to modulate the prior on Gaussian latent variables to generate the output. In low-level vision, VAE and CVAE have been applied to the tasks such as image background modeling <ref type="bibr" target="#b33">[34]</ref>, latent representations with sharp samples <ref type="bibr" target="#b24">[25]</ref>, difference of motion modes <ref type="bibr" target="#b56">[57]</ref>, medical image segmentation model <ref type="bibr" target="#b2">[3]</ref>, and modeling inherent ambiguities of an image <ref type="bibr" target="#b31">[32]</ref>. Meanwhile, VAE and CVAE have been explored in more complex vision tasks such as uncertain future forecast <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b52">53]</ref>, human motion prediction <ref type="bibr" target="#b46">[47]</ref>, and shape-guided image generation <ref type="bibr" target="#b11">[12]</ref>. Recently, VAE algorithms have been extened to 3D domain targeting applications such as 3D meshes deformation <ref type="bibr" target="#b51">[52]</ref>, and point cloud instance segmentation <ref type="bibr" target="#b58">[59]</ref>.</p><p>To the best of our knowledge, CVAE has not been exploited in saliency detection. Although Li et al. <ref type="bibr" target="#b33">[34]</ref> adopted VAE in their saliency prediction framework, they used VAE to model the image background, and separated salient objects from the background through the reconstruction residuals. In contrast, we use CVAE to model labeling variants, indicating human uncertainty of labeling. We are the first to employ CVAE in saliency prediction network by considering the human uncertainty in annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Model</head><p>In this section, we present our probabilistic RGB-D saliency detection model based on a conditional variational autoencoder, which learns the distribution of saliency maps rather than a single prediction. Our network is composed of five main modules: 1) La-tentNet (PriorNet and PosteriorNet) that maps the RGB-D input X i (for PriorNet) or X i and Y i (for PosteriorNet) to the low dimensional latent variables z i ∈ R K (K is dimension of the latent space); 2) DepthCorrectionNet that takes I i and D i as input to generate a refined depth image D i ; 3) SaliencyNet that maps the RGB image I i and the refined depth image D i to saliency feature maps S d i ; 4) Prediction-Net that employs stochastic features S s i from LatentNet and deterministic features S d i from SaliencyNet to produce our saliency map prediction P i ; 5) A saliency consensus module in the testing stage that mimics the mechanism of saliency GT generation to evaluate the performance with the provided single GT saliency map Y i . We will introduce each module as follows.</p><formula xml:id="formula_0">Let ξ = {X i , Y i } N i=1 be the training dataset, where X i = {I i , D i } denotes the RGB-D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Probabilistic RGB-D Saliency Model via CVAE</head><p>The Conditional Variational Autoencoder (CVAE) modulates the prior as a Gaussian distribution with parameters conditioned on the input data X. There are three types of variables in the conditional generative model: conditioning variable X (RGB-D image pair in our setting), latent variable z, and output variable Y . For the latent variable z drawn from the Gaussian distribution P θ (z|X), the output variable Y is generated from P ω (Y |X, z), then the posterior of z is formulated as Q φ (z|X, Y ). The loss of CVAE is defined as:</p><formula xml:id="formula_1">L CVAE = E z∼Q φ (z|X,Y ) [− log P ω (Y |X, z)] +D KL (Q φ (z|X, Y )||P θ (z|X)),<label>(1)</label></formula><p>where P ω (Y |X, z) is the likelihood of P (Y ) given latent variable z and conditioning variable X, the Kullback-Leibler Divergence D KL (Q φ (z|X, Y )||P θ (z|X)) works as a regularization loss to reduce the gap between the prior P θ (z|X) and the auxiliary posterior Q φ (z|X, Y ). In this way, CVAE aims to model the log likelihood P (Y ) under encoding error D KL (Q φ (z|X, Y )||P θ (z|X)). Following the standard practice in conventional CVAE <ref type="bibr" target="#b49">[50]</ref>, we design a CVAE-based RGB-D saliency detection network, and describe each component of our model in the following. LatentNet: We define P θ (z|X) as PriorNet that maps the input RGB-D image pair X to a low-dimensional latent feature space, where θ is the parameter set of PriorNet. With the same network structure and provided GT saliency map Y , we define Q φ (z|X, Y ) as PosteriorNet, with φ being the posterior net parameter set. In the LatentNet (Prior-Net and PosteriorNet), we use five convolutional layers to map the input RGB-D image X (or concatenation of X and Y for the PosteriorNet) to the latent Gaussian variable z ∼ N (µ, diag(σ 2 )), where µ, σ ∈ R K , representing the  <ref type="formula" target="#formula_1">1)</ref> is used to measure the distribution mismatch between the prior net P θ (z|X) and posterior net Q φ (z|X, Y ), or how much information is lost when using Q φ (z|X, Y ) to represent P θ (z|X). Typical using of CVAE involves multiple versions of ground truth Y [32] to produce informative z ∈ R K , with each position in z represents possible labeling variants or factors that may cause diverse saliency annotations. As we have only one version of GT, directly training with the provided single GT may fail to produce diverse predictions as the network will simply fit the provided annotation Y . Generate Multiple Predictions: To produce diverse and accurate predictions, we propose an iterative hiding technique inspired by <ref type="bibr" target="#b48">[49]</ref> following the orientation shifting theory <ref type="bibr" target="#b25">[26]</ref> to generate more annotations as shown in <ref type="figure">Fig. 5</ref>. We iteratively hide the salient region in the RGB image with mean of the training dataset. The RGB image and its corresponding GT are set as the starting point of the "new label generation" technique. We first hide the ground truth salient object in the RGB image, and feed the modified image to an existing RGB saliency detection model <ref type="bibr" target="#b41">[42]</ref> to produce a saliency map and treat it as one candidate annotation. We repeat salient object hiding technique three times for each training image 3 to obtain four different sets of annotations in total (including the provided GT), and we term this dataset as "AugedGT", which is our training dataset.</p><p>During training, different annotations (as shown in <ref type="figure">Fig.  5</ref>) in Q φ (z|X, Y ) can force the PriorNet P θ (z|X) to encode labeling variants of a given input X. As we have already obtained diverse annotations with the proposed hiding technique, we are expecting the network to produce diverse predictions for images with complicated context. During testing, we can obtain one stochastic feature S s (input of the "PredictionNet") of channel size K each time we sample as shown in <ref type="figure">Fig. 3</ref>. SaliencyNet: We design SaliencyNet to produce a deterministic saliency feature map S d from the input RGB-D data, where the refined depth data comes from the Depth-CorrectionNet. We use VGG16 <ref type="bibr" target="#b47">[48]</ref> as our encoder, and remove layers after the fifth pooling layer. To enlarge the receptive field, we follow DenseASPP <ref type="bibr" target="#b57">[58]</ref> to obtain feature <ref type="figure">Figure 5</ref>. New label generation. The 1 st row: we iteratively hide the predicted salient region, where no region is hidden in the first image. The 2 nd row: the corresponding GT of the hidden image. <ref type="figure">Figure 6</ref>. SaliencyNet, where "S1" represents the first stage of the VGG16 network, "daspp" is the DenseASPP module <ref type="bibr" target="#b57">[58]</ref>.</p><formula xml:id="formula_2">S1 S2 S3 S4 S5 daspp daspp daspp daspp daspp C c1_M</formula><p>map with the receptive field of the whole image on each stage of the VGG16 network. We then concatenate those feature maps and feed it to another convolutional layer to obtain S d . The detail of the SaliencyNet is illustrated in <ref type="figure">Fig. 6</ref>, where "c1 M" represents convolutional layer of kernel size 1 × 1, and M is channel size of S d . Feature Expanding: Statistics (z ∼ N (µ, diag(σ 2 )) in particular) from the LatentNet (PriorNet during testing as shown in <ref type="figure">Fig. 3</ref> "Sampling", or PosteriorNet during training in <ref type="figure">Fig. 2)</ref> form the input to the Feature Expanding module. Given a pair of (µ k , σ k ) in each position of the K dimensional vector, we obtain latent vector z k = σ k + µ k , where ∈ N (0, I). To fuse with deterministic feature S d , we expand z k to feature map of the same spatial size as S d by defining as two-dimensional Gaussian noise map. With k = 1, ..., K, we can obtain a K (size of the latent space) channel stochastic feature S s representing labeling variants. PredictionNet: The LatentNet produces stochastic features S s representing labeling variants, while the SaliencyNet outputs deterministic saliency features S d of input X. We propose the PredictionNet, as shown in <ref type="figure">Fig. 2</ref> to fuse features from mentioned branches. A naive concatenation of S s and S d may lead the network to learn only from the deterministic features, thus fail to model labeling variants. Inspired by <ref type="bibr" target="#b46">[47]</ref>, we mix S s and S d channel-wise; thus, the network cannot distinguish between features of the deterministic branch and the probabilistic branch. We concatenate S d and S s to form a K + M channel feature map S sd . We define K + M dimensional variable r (a learnable parameter) representing possible ranking of 1, 2, ..., K + M , and then S sd is mixed channel-wisely according to r to obtain the mixed feature S msd . Three 1 × 1 convolutional layers with output channel sizes of K, K/2, 1, are included in the PredictionNet to map S msd to a single channel saliency map P . During testing, with multiple stochastic features S s , we can obtain multiple predictions by sampling S s from the LatentNet N (µ prior , diag(σ 2 prior )) multiple times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DepthCorrectionNet</head><p>Two main approaches are employed to acquire depth data for RGB-D saliency detection: through depth sensors such as Microsoft Kinect, e.g., DES <ref type="bibr" target="#b7">[8]</ref>, and NLPR <ref type="bibr" target="#b40">[41]</ref> datasets; or computing depth from stereo cameras, examples of such datasets are SSB <ref type="bibr" target="#b39">[40]</ref> and NJU2K <ref type="bibr" target="#b27">[28]</ref>. Regardless of the capturing technique, noise is inherent in the depth data. We propose a semantic guided depth correction network to produce refined depth information as shown in <ref type="figure">Fig. 2, termed</ref> as "DepthCorrectionNet". The encoder part of the DepthCorrectionNet is the same as the "Salien-cyNet", while the decoder part is composed of four sequential convolutional layers and bilinear upsampling operation.</p><p>We assume that edges of the depth map should be aligned with edges of the RGB image. We adopt the boundary IOU loss <ref type="bibr" target="#b38">[39]</ref> as a regularizer for DepthCorrectionNet to achieve a refined depth, which is guided by intensity of the RGB image. The full loss for DepthCorrectionNet is defined as:</p><formula xml:id="formula_3">L Depth = L sl + L Ioub ,<label>(2)</label></formula><p>where L sl is the smooth 1 loss between the refined depth D and the raw depth D, L ioub is the boundary IOU loss between the refined depth D and intensity Ig of the RGB image I. Given the predicted depth map D and intensity of RGB image Ig, we follow <ref type="bibr" target="#b38">[39]</ref> to compute the first-order derivatives of D and Ig. Subsequently, we calculate the magnitude gD and gI of the gradients of D and Ig, and define the boundary IOU loss as:</p><formula xml:id="formula_4">L Ioub = 1 − 2 |gD ∩ gI| |gD | + |gI| .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Saliency Consensus Module</head><p>Saliency detection is subjective to some extent, and it is common to have multiple annotators to label one image, and the final ground truth saliency map is obtained through majority voting strategy <ref type="bibr" target="#b17">[18]</ref>. Although it is well known in the saliency detection community about how the ground truth is acquired; yet, there exists no research on embedding this mechanism into deep saliency frameworks. Current models define saliency detection as a point estimation problem instead of a distribution estimation problem. We, instead, use CVAE to obtain the saliency distribution. Next, we embed saliency consensus into our probabilistic framework to compute the majority voting of different predictions in the testing stage as shown in <ref type="figure">Fig. 3</ref>.</p><p>During testing, we sample PriorNet with fixed µ prior and σ prior to obtain a stochastic feature S s . With each S s and deterministic feature S d from SaliencyNet, we obtain one version of saliency prediction P . To obtain C different predictions P 1 , ..., P C , we sample PriorNet C times. We simultaneously feed these multiple predictions to the saliency consensus module to obtain the consensus of predictions.</p><p>Given multiple predictions {P c } C c=1 , where P c ∈ [0, 1], we first compute the binary 4 version P c b of the predictions by performing adaptive threshold <ref type="bibr" target="#b3">[4]</ref> on P c . For each pixel (u, v), we obtain a C dimensional feature vector P u,v ∈ {0, 1}. We define P mjv b ∈ {0, 1} as a one-channel saliency map representing majority voting of P u,v . We define an in-</p><formula xml:id="formula_5">dicator 1 c (u, v) = 1(P c b (u, v) = P mjv b</formula><p>(u, v)) representing whether the binary prediction is consistent with the majority voting of the predictions.</p><formula xml:id="formula_6">If P c b (u, v) = P mjv b (u, v), then 1 c (u, v) = 1. Otherwise, 1 c (u, v) = 0.</formula><p>We obtain one gray saliency map after saliency consensus as:</p><formula xml:id="formula_7">P mjv g (u, v) = C c=1 1 c (u, v) C C c=1 (P c b (u, v)} × 1 c (u, v)).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Function</head><p>At this stage, our loss function is composed of two parts i.e. L CVAE and L Depth . Furthermore, we propose to use the smoothness loss <ref type="bibr" target="#b8">[9]</ref> as a regularizer to achieve edgeaware saliency detection, based on the assumption of interclass distinction and intra-class similarity. Following <ref type="bibr" target="#b55">[56]</ref>, we define first-order derivatives of the saliency map in the smoothness term as</p><formula xml:id="formula_8">L Smooth = u,v d∈ − → x , − → y Ψ(|∂ d P u,v |e −α|∂ d Ig(u,v)| ),<label>(5)</label></formula><p>where Ψ is defined as Ψ(s) = √ s 2 + 1e −6 , P u,v is the predicted saliency map at position (u, v), and Ig(u, v) is the image intensity, d indexes over partial derivative on − → x and − → y directions. We set α = 10 following <ref type="bibr" target="#b55">[56]</ref>.</p><p>Both the smoothness loss (Eq. (5)) and the boundary IOU loss (Eq. (3)) need intensity Ig. We convert the RGB image I to a gray-scale intensity image Ig as <ref type="bibr" target="#b59">[60]</ref>:</p><formula xml:id="formula_9">Ig = 0.2126 × I lr + 0.7152 × I lg + 0.0722 × I lb ,<label>(6)</label></formula><p>where I lr , I lg and I lb represent the color components in the linear color space after Gamma function been removed from the original color space. I lr is achieved via: , I r &gt; 0.04045. <ref type="bibr" target="#b6">(7)</ref> where I r is the original red channel of image I, and we compute I g and I b in the same way as Eq. <ref type="formula">(7)</ref>.</p><formula xml:id="formula_10">I lr =        I</formula><p>With smoothness loss L Smooth , depth loss L Depth and CVAE loss L CVAE , our final loss function is defined as:</p><formula xml:id="formula_11">L sal = L CVAE + λ 1 L Depth + λ 2 L Smooth .<label>(8)</label></formula><p>In our experiments, we set λ 1 = λ 2 = 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets:</p><p>We perform experiments on six datasets including five widely used RGB-D saliency detection datasets (namely NJU2K <ref type="bibr" target="#b27">[28]</ref>, NLPR <ref type="bibr" target="#b40">[41]</ref>, SSB <ref type="bibr" target="#b39">[40]</ref>, LFSD <ref type="bibr" target="#b34">[35]</ref>, DES <ref type="bibr" target="#b7">[8]</ref>) and one newly released dataset (SIP <ref type="bibr" target="#b17">[18]</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison</head><p>Quantitative Comparison: We report performance of our method and competing methods in <ref type="table">Table 1</ref>. It shows that our method consistently achieves the best performance on all datasets, especially on SSB <ref type="bibr" target="#b39">[40]</ref> and SIP <ref type="bibr" target="#b17">[18]</ref>, our method achieves significant S-measure, E-measure, and F-measure performance boost and a decrease in MAE by a large margin. We show E-measure and F-measure curves of competing methods and ours in <ref type="figure">Fig. 7</ref>. We observe that our method produces not only stable E-measure and F-measure but also best performance. Qualitative Comparisons: In <ref type="figure">Fig. 8</ref>, we show five images comparing results of our method with one newly released RGB-D saliency detection method (DMRA <ref type="bibr" target="#b60">[61]</ref>), and two widely used methods to produce structured outputs, namely M-head <ref type="bibr" target="#b45">[46]</ref> and MC-dropout <ref type="bibr" target="#b29">[30]</ref> (we will discuss these two methods in detail in the ablation study section). We design both M-head and MC-dropout based structured saliency detection models by replacing CVAE with M-head and MC-dropout respectively. Results in <ref type="figure">Fig. 8</ref> show that our method can not only produce high accuracy predictions (compared with DMRA <ref type="bibr" target="#b60">[61]</ref>), but also diverse predictions (compared with M-head based and MC-dropout based models) for images with complex background (image in the first and last rows).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We carried out eight experiments (shown in <ref type="table" target="#tab_3">Table 2</ref>) to thoroughly analyse our framework, including network structure ("M1", "M2", "M3"), probabilistic model selection ("M4", "M5", "M6"), data source selection ("M7") and effectiveness of the new label generation technique ("M8"). We make the number bold when it's better than ours. Scale of Latent Space: We investigate the influence of the scale of the Gaussian latent space K in our network. In this paper, after parameter tuning, we find K = 8 works best. We show performance with K = 32 as "M1". Performance of "M1" is worse than our reported results, which indicates that scale of the latent space is an important parameter in our framework. We further carried out more experiments with K ∈ <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>, and found relative stable predictions with K ∈ <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. Effect of DepthCorrectionNet: To illustrate the effectiveness of the proposed DepthCorrectionNet, we remove this branch and feed the concatenation of the RGB image and depth data to the SaliencyNet, shown as "M2", which is worse than our method. On DES <ref type="bibr" target="#b7">[8]</ref> dataset, we observe the proposed solution achieves around 4% improvement on S-measure, E-measure and F-measure, which demonstrates the effectiveness of the depth correction net. Saliency Consencus Module: To mimic the saliency labeling process, we embed a saliency consensus module during test in our framework (as shown in <ref type="figure">Fig. 3</ref>) to obtain the majority voting of the multiple predictions. We remove it from our framework and test the network performance by random sample from the latent PriorNet P θ (z|X), and performance is shown in "M3", which is the best compared with competing methods. While, with the saliency consensus module embedded, we achieve even better performance, which illustrates effectiveness of the saliency consencus module. VAE vs. CVAE: We use CVAE to model labeling variants, and a PosteriorNet is used to estimate parameters for the PriorNet. To test how our model performs with prior of z as a standard normal distribution, and the posterior of z as P θ (z|X). VAE performance is shown as "M4", which is comparable with SOTA RGB-D models. With the CVAE <ref type="bibr" target="#b49">[50]</ref> based model proposed, we further boost performance of "M4", which proves effectiveness of the our solution.</p><p>Multi-head vs. CVAE: Multi-head models <ref type="bibr" target="#b45">[46]</ref> generate multiple predictions with different decoders and a shared encoder, and the loss function is always defined as the closest of the multiple predictions. We remove the LatentNet, <ref type="table">Table 1</ref>. Benchmarking results of ten leading handcrafted feature-based models and eight deep models on six RGBD saliency datasets. ↑ &amp; ↓ denote larger and smaller is better, respectively. Here, we adopt mean F β and mean E ξ <ref type="bibr" target="#b14">[15]</ref>.  and copy the decoder of the SaliencyNet multiple times to achieve multiple predictions ("M5" in this paper). We report performance in "M5" as mean of the multiple predictions. "M5" is better than SOTA models (e.g., DMRA) while there still exists gap between M-head based method ("M5") and our CVAE based model (UC-Net).</p><p>Monte-Carlo Dropout vs. CVAE: Monte-Carlo Dropout <ref type="bibr" target="#b29">[30]</ref> uses dropout during the testing stage to introduce stochastic to the network. We follow <ref type="bibr" target="#b29">[30]</ref> to remove the La-tentNet, and use dropout in the encoder and decoder of the SaliencyNet in the testing stage. We repeats five times of random dropout (dropout ratio = 0.1), and report the mean performance as "M6". Similar to "M5", "M6" also achieves the best performance comparing with SOTA models (e.g., CPFP and DMRA), while the proposed CVAE based model achieves even better performance.</p><p>HHA vs. Depth: HHA <ref type="bibr" target="#b22">[23]</ref> is a widely used technique that encodes the depth data to three channels: horizontal disparity, height above ground, and the angle the pixels local surface normal makes with the inferred gravity direction.</p><formula xml:id="formula_12">) 2 ( s r u O ) 1 ( s r u O T G h t p e D e g a m I UC-Net DMRA MH1 MH2 DP1 DP2 Figure 8</formula><p>. Comparisons of saliency maps. "MH1" and "MH2" are two predictions from M-head. "DP1" and "DP2" are predictions of two random MC-dropout during test. "Ours(1)" and "Ours(2)" are two predictions sampled from our CVAE based model. Different from M-head and MC-dropout, which produce consistent predictions for ambiguous images (5 th row), UC-Net can produce diverse predictions. HHA is widely used in RGB-D related dense prediction models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref> to obtain better feature representation. To test if HHA also works in our scenario, we replace depth with HHA, and performance is shown in "M7". We observe similar performance achieved with HHA instead of the raw depth data. New Label Generation: To produce diverse predictions, we follow <ref type="bibr" target="#b48">[49]</ref> and generate diverse annotations for the training dataset. To illustrate the effectiveness of this strategy, we train with only the SaliencyNet to produce single channel saliency map with RGB-D image as input for simplicity. "M8" and "M9" represent using the provided train-ing dataset and augmented training data respectively. We observe performance improvement of "M9" compared with "M8", which indicates effectiveness of the new label generation technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Inspired by human uncertainty in ground truth (GT) annotation, we proposed the first uncertainty network named UC-Net for RGB-D saliency detection based on a conditional variational autoencoder. Different from existing methods, which generally treat saliency detection as a point estimation problem, we propose to learn the distribution of saliency maps. Under our formulation, our model is able to generate multiple labels which have been discarded in the GT annotation generation process through saliency consensus. Quantitative and qualitative evaluations on six standard and challenging benchmark datasets demonstrated the superiority of our approach in learning the distribution of saliency maps. In the future, we would like to extend our approach to other saliency detection problems (e.g., VSOD <ref type="bibr" target="#b18">[19]</ref>, RGB SOD <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b64">65]</ref>, Co-SOD <ref type="bibr" target="#b16">[17]</ref>). Furthermore, we plan to capture new datasets with multiple human annotations to further model the statistics of human uncertainty in interactive image segmentation <ref type="bibr" target="#b36">[37]</ref>, camouflaged object detection <ref type="bibr" target="#b15">[16]</ref>, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Network training pipeline. Four main modules are included, namely a LatentNet (PriorNet (µprior, σprior) and PosteriorNet (µpost, σpost)), a SaliencyNet, a DepthCorrectionNet and a PredictionNet. The LatentNet maps the RGB-D image pair X (or together with GT Y for the PosteriorNet) to low dimensional Gaussian latent variable z. The DepthCorrectionNet refines the raw depth with a semantic guided loss. The SaliencyNet takes the RGB image and the refined depth as input to generate a saliency feature map. The PredictionNet takes both stochastic features and deterministic features to produce a final saliency map. We perform saliency consensus in the testing stage, as shown inFig. 3to generate the final saliency map according to the mechanism of GT saliency map generation. Overview of the proposed framework during testing. We sample the PriorNet multiple times to generate diverse and accurate predictions. The saliency consensus module is then used to obtain the majority voting of the final predictions.input (consisting of the RGB image I i and the depth image D i ), Y i denotes the ground truth saliency map. The whole pipeline of our model during training and testing are illustrated inFig. 2 and Fig. 3, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Detailed structure of LatentNet, where K is dimension of the latent space, "c1 4K" represents a 1 × 1 convolutional layer of output channel size 4K, "GAP" is global average pooling. mean and standard deviation of the latent Gaussian variable, as shown inFig. 4.Let us define parameter set of PriorNet and PosteriorNet as (µ prior , σ prior ) and (µ post , σ post ) respectively. The KL-Divergence in Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Competing Methods: We compare our method with 18 algorithms, including ten handcrafted conventional methods and eight deep RGB-D saliency detection models. Evaluation Metrics: Four evaluation metrics are used, including two widely used: 1) Mean Absolute Error (MAE M); 2) mean F-measure (F β ) and two recently proposed: 3) Enhanced alignment measure (mean E-measure, E ξ )<ref type="bibr" target="#b14">[15]</ref> and 4) Structure measure (S-measure, S α )<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 NLPRFigure 7 .</head><label>17</label><figDesc>E-measure (1 st row) and F-measure (2 nd row) curves on four testing datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>3.Training details: We set channel size of S d as M = 32, and scale of latent space as K = 8.</figDesc><table><row><cell>We trained our</cell></row><row><cell>model using Pytorch, and initialized the encoder of Salien-</cell></row><row><cell>cyNet and DepthCorrectionNet with VGG16 parameters</cell></row><row><cell>pre-trained on ImageNet. Weights of new layers were ini-</cell></row><row><cell>tialized with N (0, 0.01), and bias was set as constant. We</cell></row><row><cell>used the Adam method with momentum 0.9 and decreased</cell></row><row><cell>the learning rate 10% after each epoch. The base learning</cell></row><row><cell>rate was initialized as 1e-4. The whole training took 13</cell></row><row><cell>hours with training batch size 6 and maximum epoch 30 on</cell></row><row><cell>a PC with an NVIDIA GeForce RTX GPU. For input image</cell></row><row><cell>size 352 × 352, the inference time is 0.06s on average.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Handcrafted Feature based Models Deep Models Metric LHM CDB DESM GP CDCP ACSD LBE DCMC MDSF SE DF AFNet CTMF MMCI PCF TANet CPFP DMRA UC-Net Sα ↑ .514 .632 .665 .527 .669 .699 .695 .686 .748 .664 .763 .822 .849 .858 .877 .879 .878 .886 .897 F β ↑ .328 .498 .550 .357 .595 .512 .606 .556 .628 .583 .653 .827</figDesc><table><row><cell></cell><cell>[41] [36]</cell><cell>[8]</cell><cell>[44] [66]</cell><cell>[28] [20] [10]</cell><cell>[51] [22] [43] [54]</cell><cell>[24]</cell><cell>[7]</cell><cell>[5]</cell><cell>[6]</cell><cell>[64]</cell><cell>[61]</cell><cell>Ours</cell></row><row><cell>NJU2K [28]</cell><cell cols="3">E ξ ↑ .447 .572 .590 .466 .706</cell><cell>.594 .655 .619</cell><cell>.677 .624 .700 .867</cell><cell>.779 .846</cell><cell cols="4">.793 .840 .841 .850 .851 .895 .895 .910</cell><cell>.873 .920</cell><cell>.886 .930</cell></row><row><cell></cell><cell cols="3">M ↓ .205 .199 .283 .211 .180</cell><cell>.202 .153 .172</cell><cell>.157 .169 .140 .077</cell><cell>.085</cell><cell cols="4">.079 .059 .061 .053</cell><cell>.051</cell><cell>.043</cell></row><row><cell></cell><cell cols="3">Sα ↑ .562 .615 .642 .588 .713</cell><cell>.692 .660 .731</cell><cell>.728 .708 .757 .825</cell><cell>.848</cell><cell cols="4">.873 .875 .871 .879</cell><cell>.835</cell><cell>.903</cell></row><row><cell>SSB [40]</cell><cell cols="3">F β ↑ .378 .489 .519 .405 .638 E ξ ↑ .484 .561 .579 .508 .751</cell><cell>.478 .501 .590 .592 .601 .655</cell><cell>.527 .611 .617 .806 .614 .664 .692 .872</cell><cell>.758 .841</cell><cell cols="4">.813 .818 .828 .841 .873 .887 .893 .911</cell><cell>.837 .879</cell><cell>.884 .938</cell></row><row><cell></cell><cell cols="3">M ↓ .172 .166 .295 .182 .149</cell><cell>.200 .250 .148</cell><cell>.176 .143 .141 .075</cell><cell>.086</cell><cell cols="4">.068 .064 .060 .051</cell><cell>.066</cell><cell>.039</cell></row><row><cell></cell><cell cols="3">Sα ↑ .578 .645 .622 .636 .709</cell><cell>.728 .703 .707</cell><cell>.741 .741 .752 .770</cell><cell>.863</cell><cell cols="4">.848 .842 .858 .872</cell><cell>.900</cell><cell>.934</cell></row><row><cell>DES [8]</cell><cell cols="3">F β ↑ .345 .502 .483 .412 .585 E ξ ↑ .477 .572 .566 .503 .748</cell><cell>.513 .576 .542 .613 .650 .631</cell><cell>.523 .618 .604 .713 .621 .706 .684 .809</cell><cell>.756 .826</cell><cell cols="4">.735 .765 .790 .824 .825 .838 .863 .888</cell><cell>.873 .933</cell><cell>.919 .967</cell></row><row><cell></cell><cell cols="3">M ↓ .114 .100 .299 .168 .115</cell><cell>.169 .208 .111</cell><cell>.122 .090 .093 .068</cell><cell>.055</cell><cell cols="4">.065 .049 .046 .038</cell><cell>.030</cell><cell>.019</cell></row><row><cell></cell><cell cols="3">Sα ↑ .630 .632 .572 .655 .727</cell><cell>.673 .762 .724</cell><cell>.805 .756 .806 .799</cell><cell>.860</cell><cell cols="4">.856 .874 .886 .888</cell><cell>.899</cell><cell>.920</cell></row><row><cell>NLPR [41]</cell><cell cols="3">F β ↑ .427 .421 .430 .451 .609 E ξ ↑ .560 .567 .542 .571 .782</cell><cell>.429 .636 .542 .579 .719 .684</cell><cell>.649 .624 .664 .755 .745 .742 .757 .851</cell><cell>.740 .840</cell><cell cols="4">.737 .802 .819 .840 .841 .887 .902 .918</cell><cell>.865 .940</cell><cell>.891 .951</cell></row><row><cell></cell><cell cols="3">M ↓ .108 .108 .312 .146 .112</cell><cell>.179 .081 .117</cell><cell>.095 .091 .079 .058</cell><cell>.056</cell><cell cols="4">.059 .044 .041 .036</cell><cell>.031</cell><cell>.025</cell></row><row><cell></cell><cell cols="3">Sα ↑ .557 .520 .722 .640 .717</cell><cell>.734 .736 .753</cell><cell>.700 .698 .791 .738</cell><cell>.796</cell><cell cols="4">.787 .794 .801 .828</cell><cell>.847</cell><cell>.864</cell></row><row><cell>LFSD [35]</cell><cell cols="3">F β ↑ .396 .376 .612 .519 .680 E ξ ↑ .491 .465 .638 .584 .754</cell><cell>.566 .612 .655 .625 .670 .682</cell><cell>.521 .640 .679 .736 .588 .653 .725 .796</cell><cell>.756 .810</cell><cell cols="4">.722 .761 .771 .811 .775 .818 .821 .863</cell><cell>.845 .893</cell><cell>.855 .901</cell></row><row><cell></cell><cell cols="3">M ↓ .211 .218 .248 .183 .167</cell><cell>.188 .208 .155</cell><cell>.190 .167 .138 .134</cell><cell>.119</cell><cell cols="4">.132 .112 .111 .088</cell><cell>.075</cell><cell>.066</cell></row><row><cell></cell><cell cols="3">Sα ↑ .511 .557 .616 .588 .595</cell><cell>.732 .727 .683</cell><cell>.717 .628 .653 .720</cell><cell>.716</cell><cell cols="4">.833 .842 .835 .850</cell><cell>.806</cell><cell>.875</cell></row><row><cell>SIP [18]</cell><cell cols="3">F β ↑ .287 .341 .496 .411 .482 E ξ ↑ .437 .455 .564 .511 .683</cell><cell>.542 .572 .500 .614 .651 .598</cell><cell>.568 .515 .465 .702 .645 .592 .565 .793</cell><cell>.608 .704</cell><cell cols="4">.771 .814 .803 .821 .845 .878 .870 .893</cell><cell>.811 .844</cell><cell>.867 .914</cell></row><row><cell></cell><cell cols="3">M ↓ .184 .192 .298 .173 .224</cell><cell>.172 .200 .186</cell><cell>.167 .164 .185 .118</cell><cell>.139</cell><cell cols="4">.086 .071 .075 .064</cell><cell>.085</cell><cell>.051</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on RGB-D saliency datasets. 893 .905 .871 .885 .881 .893 .838 .866 F β ↑ .886 .858 .887 .884 .851 .878 .878 .884 .787 .812 E ξ ↑ .930 .905 .930 .927 .910 .923 .927 .932 .840 .866 M ↓ .043 .060 .046 .045 .059 .047 .046 .044 .084 .075 893 .900 .867 .891 .893 .898 .855 .872 F β ↑ .884 .831 .876 .868 .834 .864 .876 .882 .793 .805 E ξ ↑ .938 .894 .911 .922 .907 .921 .931 .934 .854 .870 M ↓ .039 .060 .043 .047 .057 .047 .043 .040 .073 .068 DES [8] Sα ↑ .934 .876 .896 .928 .897 .911 .896 .918 .811 .911 F β ↑ .919 .844 .868 .902 .867 .897 .868 .904 .724 .843 E ξ ↑ .967 .906 .928 .947 .930 .945 .928 .953 .794 .910 M ↓ .019 .035 .026 .024 .033 .024 .026 .023 .065 .036 NLPR [41] Sα ↑ .920 .878 .919 .918 .890 .899 .910 .915 .850 .883 F β ↑ .891 .846 .897 .878 .845 .875 .867 .889 .759 .795 E ξ ↑ .951 .911 .953 .941 .924 .937 .933 .951 .841 .883 M ↓ .025 .039 .024 .029 .037 .029 .028 .025 .057 .045 LFSD [35] Sα ↑ .864 .799 .847 .862 .820 .838 .847 .853 .729 .823 F β ↑ .855 .791 .838 .841 .802 .833 .838 .848 .661 .779 E ξ ↑ .901 .829 .879 .885 .865 .875 .879 .891 .720 .818 M ↓ .066 .101 .079 .075 .093 .079 .079 .073 .145 .108 SIP [18] Sα ↑ .875 .846 .867 .870 .851 .859 .867 .865 .810 .845 F β ↑ .867 .837 .860 .848 .821 .853 .860 .855 .751 .795 E ξ ↑ .914 .884 .908 .901 .893 .905 .908 .908 .816 .852 M ↓ .051 .068 .056 .059 .067 .057 .056 .056 .094 .079</figDesc><table><row><cell>Metric UC-Net M1 M2 M3 M4 M5 M6 M7 M8 M9</cell></row><row><cell>NJU2K [28] .866 .SSB Sα ↑ .897 [40] Sα ↑ .903 .854 .</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Diversity of prediction is related to the content of image. Image with clear content may lead to consistent prediction (1 st row inFig. 1), while complex image may produce diverse predictions (2 nd row ofFig. 1).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We found that usually after three times of hiding, there exists no salient objects in the hidden image.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">As the GT map Y ∈ {0, 1}, we produce series of binary predictions with each one representing annotation from one saliency annotator.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research was supported in part by Natural Science Foundation of China grants (61871325, 61420106007, 61671387), the Australia Research Council Centre of Excellence for Robotics Vision (CE140100016), and the National Key Research and Development Program of China under Grant 2018AAA0102803. We thank all reviewers and Area Chairs for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Contrastive Variational Autoencoder Enhances Salient Features. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PHiSeg: Capturing Uncertainty in Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Kerem Can Tezcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">M</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><forename type="middle">J</forename><surname>Hötker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoschy</forename><surname>Muehlematter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><forename type="middle">S</forename><surname>Schawkat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivio</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ender</forename><surname>Donati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="119" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Salient Object Detection: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Progressively complementarityaware fusion network for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Three-stream Attention-aware Network for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youfu</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjian</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICIMCS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised Monocular Depth Estimation with Left-Right Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Godard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
	<note>Oisin Mac Aodha</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunping</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Translate-to-Recognize Networks for RGB-D Scene Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11836" to="11845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Variational U-Net for Conditional Appearance and Shape Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Sutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjrn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8857" to="8865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhanced-alignment Measure for Binary Foreground Map Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Camouflaged Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Taking a Deeper Look at the Co-salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Local background enclosure for RGB-D salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2343" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><forename type="middle">Fu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Salient object detection for rgb-d image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CNNs-based RGB-D saliency detection via crossview transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCYB</title>
		<imprint>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PixelVAE: A Latent Variable Model for Natural Images</title>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Faruk Ahmed Adrien Ali Taga Francesco Visin David Vzquez Aaron C. Courville Ishaan Gulrajani, Kundan Kumar</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A saliency-based search mechanism for overt and covert shifts of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>VR</publisher>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1489" to="1506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic centersurround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1115" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">F3Net: Fusion, Feedback and Focus for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Danilo Jimenez Rezende, and Olaf Ronneberger. A Probabilistic U-Net for Segmentation of Ambiguous Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6965" to="6975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Methods for comparing scanpaths and saliency maps: strengths and weaknesses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baccino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="251" to="266" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SuperVAE: Superpixelwise Variational Autoencoder for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8569" to="8576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2806" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stereoscopic saliency model using contrast and depth-guided-background prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhua</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiyun</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="2227" to="2238" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Interactive Image Segmentation with First Click Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Ping</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Employing Deep Part-Object Relationships for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Non-Local Deep Features for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshaya</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhen</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">BASNet: Boundary-Aware Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">RGBD salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangqiong</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploiting Global Priors for RGB-D Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPRW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning in an Uncertain World: Representing Ambiguity Through Multiple Hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Baust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3611" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning Variations in Human Motion via Mix-and-Match Perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemeh Sadat</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirhossein</forename><surname>Habibian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00733</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Structured Output Representation using Deep Conditional Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Depth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangke</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangling</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongwei</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4204" to="4216" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Variational Autoencoders for Deforming 3D Mesh Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harikrishna</forename><surname>Mulam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adaptive Fusion for RGB-D Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="55277" to="55284" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An Iterative and Cooperative Top-Down and Bottom-Up Inference Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Occlusion Aware Unsupervised Learning of Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<editor>Rastogi Akash Villegas Ruben Sunkavalli Kalyan Shechtman Eli Hadap Sunil Yumer Ersin Lee Honglak Yan, Xinchen</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="276" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">DenseASPP for Semantic Segmentation in Street Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Saliency Preservation in Low-Resolution Grayscale Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Shivanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Yohanandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Depth-induced Multi-scale Recurrent Attention Network for Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing Li Miao Zhang Huchuan Lu Yongri</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Weakly-Supervised Salient Object Detection via Scribble Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peipei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9029" to="9038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Contrast Prior and Fluid Pyramid Integration for RGBD Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">EGNet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An innovative salient object detection using center-dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunbiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCVW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
