<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive Point Blending</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoju</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive Point Blending</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lane Detection</term>
					<term>Autonomous Driving</term>
					<term>Benchmark Dataset</term>
					<term>Neural Architecture Search</term>
					<term>Curve Lane</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the curve lane detection problem which poses more realistic challenges than conventional lane detection for better facilitating modern assisted/autonomous driving systems. Current handdesigned lane detection methods are not robust enough to capture the curve lanes especially the remote parts due to the lack of modeling both long-range contextual information and detailed curve trajectory. In this paper, we propose a novel lane-sensitive architecture search framework named CurveLane-NAS to automatically capture both long-ranged coherent and accurate short-range curve information. It consists of three search modules: a) a feature fusion search module to find a better fusion of the local and global context for multi-level hierarchy features; b) an elastic backbone search module to explore an efficient feature extractor with good semantics and latency; c) an adaptive point blending module to search a multi-level post-processing refinement strategy to combine multi-scale head prediction. Furthermore, we also steer forward to release a more challenging benchmark named CurveLanes for addressing the most difficult curve lanes. It consists of 150K images with 680K labels. 3 Experiments on the new CurveLanes show that the SOTA lane detection methods suffer substantial performance drop while our model can still reach an 80+% F1-score. Extensive experiments on traditional lane benchmarks such as CULane also demonstrate the superiority of our CurveLane-NAS, e.g. achieving a new SOTA 74.8% F1-score on CULane.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lane detection is a core task in modern assisted and autonomous driving systems to localize the accurate shape of each lane in a traffic scene. Comparing to Comparing to straight lane, the detection of curve lanes is more crucial for trajectory planning in modern assisted and autonomous driving systems. However, the proportion of curve lanes images in current large-scale datasets is very limited, 2% in CULane Dataset (around 2.6K images) and 30% in TuSimple Dataset (around 3.9K images), which hinders the real-world applicability of the autonomous driving systems. Therefore, we establish a more challenging benchmark named CurveLanes for the community. It is the largest lane detection dataset so far (150K images) and over 90% images (around 135K images) contain curve lane. straight lane, the detection of curve lanes is more crucial for further downstreaming trajectory planning tasks to keep the car properly position itself within the road lanes during steering in complex road scenarios. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in real applications, curve lane detection could be very challenging considering the long varied shape of the curve lanes and likely occlusion by other traffic objects. Furthermore, the curvature of the curve lane is greatly increased for remote parts because of interpolation which makes those remote parts hard to be traced. Moreover, real-time hardware constraints and various harsh scenarios such as poor weather/light conditions <ref type="bibr" target="#b19">[20]</ref> also limit the capacity of models.</p><p>Existing lane detection datasets such as TuSimple <ref type="bibr" target="#b27">[28]</ref> and CULane <ref type="bibr" target="#b19">[20]</ref> are not effective enough to measure the performance of curve lane detection. Because of the natural distribution of lanes in traffic scenes, most of the lanes in those datasets are straight lanes. Only about 2.1% of images in CULane (around 2.6K), and 30% in TuSimple contain curve lanes (around 3.9K). To better measure the challenging curve lane detection performance and facilitate the studies on the difficult road scenarios, we introduce a new large-scale lane detection dataset named CurveLanes consisting of 150K images with carefully annotated 680K curve lanes labels. All images are carefully picked so that almost all of the images contain at least one curve lane (more than 135K images). To our best knowledge, it is the largest lane detection dataset so far and establishes a more challenging benchmark for the community.</p><p>The most state-of-the-art lane detection methods are CNN-based methods. Dense prediction methods such as SCNN <ref type="bibr" target="#b19">[20]</ref> and SAD <ref type="bibr" target="#b10">[11]</ref> treat lane detection as a semantic segmentation task with a heavy encoder-decoder structure. However, those methods usually use a small input image which makes it hard to predict remote parts of curve lanes. Moreover, those methods are often limited to detect a pre-defined number of lanes. On the other hand, PointLaneNet <ref type="bibr" target="#b5">[6]</ref> and Line-CNN <ref type="bibr" target="#b13">[14]</ref> follow a proposal-based diagram which generates multiple point anchor or line proposals in the images thus getting rid of the inefficient decoder and pre-defined number of lanes. However, the line proposals are not flexible enough to capture variational curvature along the curve lane. Besides, PointLaneNet <ref type="bibr" target="#b5">[6]</ref> prediction is based on one fixed single feature map and fails to capture both long-range and short-range contextual information at each proposal. They suffer from a great performance drop in predicting difficult scenarios such as the curve or remote lanes.</p><p>In this paper, we present a novel lane-sensitive architecture search framework named CurveLane-NAS to solve the above limitations of current models for curve lane detection. Inspired by recent advances in network architecture search (NAS) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref>, we attempt to automatically explore and optimize current architectures to an efficient task-specific curve lane detector. A search space with a combination of multi-level prediction heads and a multi-level feature fusion is proposed to incorporate both long-ranged coherent lane information and accurate short-range curve information. Besides, since post-processing is crucial for the final result, we unify the architecture search with optimizing the post-processing step by adaptive point blending. The mutual guidance of the unified framework ensures a holistic optimization of the lane-sensitive model. Specifically, we design three search modules for the proposal-based lane detection: 1) an elastic backbone search module to allocate different computation across multi-size feature maps to explore an efficient feature extractor for a better trade-off with good semantics and latency; 2) a feature fusion search module is used to find a better fusion of the local and global context for multilevel hierarchy features; 3) an adaptive point blending module to search a novel multi-level post-processing refinement strategy to combine multi-level head prediction and allow more robust prediction over the shape variances and remote lanes. We consider a simple yet effective multi-objective search algorithm with the evolutionary algorithm to properly allocate computation with reasonable receptive fields and spatial resolution for each feature level thus reaching an optimal trade-off between efficiency and accuracy.</p><p>Experiments on the new CurveLanes show that the state-of-the-art lane detection methods suffer substantial performance drop (10%˜20% in terms of F1 score) while our model remains resilient. Extensive experiments are also conducted on multiple existing lane detection benchmarks including TuSimple and CULane. The results demonstrate the effectiveness of our method, e.g. the searched model outperforms SCNN <ref type="bibr" target="#b19">[20]</ref> and SAD <ref type="bibr" target="#b10">[11]</ref> and achieves a new SOTA result 74.8% F1-score on the CULane dataset with a reduced FLOPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Lane detection. Lane detection aims to detect the accurate location and shape of each lane on the road. It is the core problem in modern assisted and au-  tonomous driving systems. Conventional methods usually are based on handcrafted low-level features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8]</ref>. Deep learning has then been employed to extract features in an end-to-end manner. Most lane detection works follow pixellevel segmentation-based approach <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref> as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a). These approaches usually adopt the dense prediction formulation, i.e., treat lane detection as a semantic segmentation task, where each pixel in an image is assigned with a label to indicate whether it belongs to a lane or not. <ref type="bibr" target="#b35">[36]</ref> combines a semantic segmentation CNN and a recurrent neural network to enable a consistent lane detection. SCNN <ref type="bibr" target="#b19">[20]</ref> generalizes traditional deep convolutions to slice-byslice convolution, thus enabling message passing between pixels across rows and columns. However, pixel-wise prediction usually requires more computation and is limited to detect a pre-defined, fixed number of lanes. On the other hand, several works use proposal-based approaches for efficient lane detection as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b). These approaches generate multiple anchors or lines proposals in the images. PointLaneNet <ref type="bibr" target="#b5">[6]</ref> finds the lane location by predicting the offsets of each anchor point. Line-CNN <ref type="bibr" target="#b13">[14]</ref> introduces a line proposal network to propose a set of ray aiming to capture the actual lane. However, those methods predict on one single feature map and overlook the crucial semantic information for curve lanes and remote lanes in the feature hierarchy. Neural Architecture Search. NAS aims at freeing expert's labor of designing a network by automatically finding an efficient neural network architecture for a certain task and dataset. Most works search a basic CNN architectures for a classification model <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> while a few of them focus on more complicated high-level vision tasks such as semantic segmentation and object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Searching strategies in NAS area can be usually divided into three categories: 1) Reinforcement learning based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b32">33]</ref> train a RNN policy controller to generate a sequence of actions to specify CNN architecture; Zoph et al. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> apply reinforcement learning to search CNN, while the search cost is more than hundreds of GPU days. 2) Evolutionary Algorithms based methods and Network Morphism <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b11">12]</ref> try to "evolves" architectures by mutating the current best architectures; Real et al. <ref type="bibr" target="#b21">[22]</ref> introduces an age property of the tournament selection evolutionary algorithm to favor the younger CNN candidates during the search. 3) Gradient-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b2">3]</ref> try to introduce an architecture parameter for continuous relaxation of the discrete search space, thus allowing weight-sharing and differentiable optimization of the architecture. SNAS <ref type="bibr" target="#b28">[29]</ref> propose a stochastic differentiable sampling approach to improve <ref type="bibr" target="#b17">[18]</ref>. Gradient-based methods are usually fast but not so reliable since weight-sharing makes a big gap between the searching and final training. RL methods usually require massive samples to converge thus a proxy task is usually required. In this paper, by considering the task-specific problems such as real-time requirement, severe road mark degradation, vehicle occlusion, we carefully design a search space and a sample-based multi-objective search algorithm to find an efficient but accurate architecture for the curve lane detection problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CurveLane-NAS framework</head><p>In this paper, we present a novel lane-sensitive architecture search framework named CurveLane-NAS to solve the limitations of current models of curve lane detection. <ref type="figure" target="#fig_1">Figure 2</ref> (a) and (b) show existing lane detection frameworks. We extend the diagram of (b) to our multi-level refinement model with a unified architecture search framework as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (c). We propose a flexible model search space with multi-level prediction heads and multi-level feature fusion to incorporate both long-ranged coherent lane information and accurate short-range curve information. Furthermore, our search framework unifies NAS and optimizing the post-processing step in an end-to-end fashion.</p><p>The overview of our CurveLane-NAS framework can be found in <ref type="figure">Figure 3</ref>. We design three search modules: 1) an elastic backbone search module to set up an efficient allocation of computation across stages, 2) a feature fusion search module to explore a better combination of local and global context; 3) an adaptive point blending module to search a novel multi-level post-processing refinement strategy and allow more robust prediction over the shape variances and remote lanes. We consider a simple yet effective multi-objective search algorithm to push the Pareto front towards an optimal trade-off between efficiency and accuracy while the post-processing search can be naturally fit in our NAS formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Elastic Backbone Search Module</head><p>The common backbone of a lane detection is ImageNet pretrained ResNet <ref type="bibr" target="#b8">[9]</ref>, MoblieNet <ref type="bibr" target="#b23">[24]</ref> and GoogLeNet <ref type="bibr" target="#b24">[25]</ref>, which is neither task-specific nor dataspecific. The backbone is the most important part to extract relevant feature and handcrafting backbone may not be optimal for curve lane detection. Thus, we can resort to task-specific architecture search here to explore novel feature extractor for a better trade-off with good semantics and latency.</p><p>… Images e.g. Arch encoding: BB_64_13_ <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Objective Search on both Architecture and Post-processing</head><p>Evolutionary algorithm sampling and mutating to push the Pareto Front towards a better Acc/Flops trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BB -Bottleneck Block</head><p>Search the positions of down-sampling for each stage Search the positions of doubling the channels ？ ？ ？ <ref type="figure">Fig. 3</ref>. An overview of our NAS for lane detection pipeline. Our unified search frameworks has three modules: 1) an elastic backbone search module to explore an efficient feature extractor with an optimal setting of network width, depth and when to raise channels/down-sampling, 2) a feature fusion search module to find a suitable fusion of several feature levels; 3) an adaptive point blending module to automatically highlight the most important regions by an adaptive masking and allow a more robust refinement over the shape variances and remote lanes. A unified multi-objective search algorithm is applied to generate a Pareto front with the optimal accuracy/FLOPS trade-off.</p><p>A common backbone aims to generate intermediate-level features with increasing down-sampling rates, which can be regarded as 4 stages. The blocks in the same stage share the same spatial resolution. Note that the early-stage usually has higher computational cost with more low-level features. The late-stage feature maps are smaller thus the computational cost is relatively smaller but losing a lot of spatial details. How to leverage the computation cost over different stages for an optimal lane network design? Inside the backbone, we design a flexible search space to find the optimal base channel size, when to down-sample and when to raise the channels as follows:</p><p>We build up the backbone by several stacked ResNet blocks <ref type="bibr" target="#b8">[9]</ref>: basic residual block and bottleneck residual block. The backbone has the choice of 3 or 4 stages. We allow different base channel size 48, 64, 80, 96, 128 and different number of blocks in each stage corresponding to different computational budget. The number of total blocks variates from 10 to 45. To further allow a flexible allocation of computation, we also search for where to raise the channels. Note that in the original ResNet18/50, the position of doubling the channel size block is fixed at the beginning of each stage. For example, as shown in <ref type="figure">Figure 3</ref>, the backbone architecture encoding string looks like "BB 64 13 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>  <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>" where the first placeholder encodes the block setting, 64 is the base channel size, 13 is the total number of blocks and <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref> are the position of down-sampling blocks and <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> are the position of doubling channel size.</p><p>The total search space of the backbone search module has about 5 × 10 12 possible choices. During searching, the models can be trained well from scratch with a large batch size without using the pre-trained ImageNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Fusion Search Module</head><p>As mentioned in DetNAS <ref type="bibr" target="#b4">[5]</ref>, neurons in the later stage of the backbone strongly respond to entire objects while other neurons are more likely to be activated by local textures and patterns. In the lane detection context, features in the later stage can capture long-range coherent lane information while the features in the early stage contain more accurate short-range curve information by its local patterns. In order to fuse different information across multi-level features, we propose a feature fusion search module to find a better fusion of the high-level and low-level features. We also allow predictions on different feature maps since the detailed lane shapes are usually captured by a large feature map. We consider the following search space for a lane-sensitive model:</p><p>Let F 1,...,t denote the output feature maps from different stages of the backbone (t can be 3 or 4 depending on the choice of the backbone). From F 1 to F t , the spatial size is gradually down-sampled with factor 2. Our feature fusion search module consists of M fusion layers {O i }. For each fusion layer O i , we pick two output feature levels with {F 1 , ... , F 4 } as input features and one target output resolution. The two input features F i first go through one 1x1 convolution layer to become one output channels c, then both features will do up-sampling/down-sampling to the target output resolution and be concatenated together. The output of each O i will go through another 1x1 convolution with output channels c and to concatenate to the final output. Thus our search space is flexible enough to fuse different features and select the output feature layer to feed into the heads. For each level of the feature map, we also decide whether a prediction head should be added (at least one). The total search space of the feature fusion search module is relatively small (about 10 3 possible choices).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive Point Blending Search Module</head><p>Inspired by PointLaneNet <ref type="bibr" target="#b5">[6]</ref>, each head proposes many anchors on its feature map and predicts their corresponding offsets to generate line proposals. A lane line can be determined in the image by line points and one ending point. We first divide each feature map into a w f × h f grid G. If the center of the grid g ij is near to a ground truth lane, g ij is responsible for detecting that lane. Since a lane will go across several grids, multiple grids can be assigned to that lane and their confidence scores s ij reflect how confident the grid contains a part of the lane. For each g ij , the model will predict a set of offsets ∆x ijz and one ending point position, where ∆x ijz is the horizontal distance between the ground truth lane and a pre-defined vertical anchor points x ijz as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. With the predicted ∆x ijz and the ending point position, each grid g ij can forecast one potential lane l ij . <ref type="bibr" target="#b3">4</ref> Post-processing is required to summarize and filtering all line proposals and generate final results.</p><p>In PointLaneNet <ref type="bibr" target="#b5">[6]</ref>, a Line-NMS is adopted on one feature map to filter out lower confidence and a non-maximum suppression (NMS) algorithm is used to filter out occluded lanes according to their confidence score. <ref type="bibr" target="#b4">5</ref> However, it cannot fit in our multi-level situation. First, predictions from different levels of features cannot be treated equally. For example, predictions on the low-level feature map </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Lane</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ending point</head><p>Point Blending Technique to replace some of the prediction points by the good local points More consistent and long-ranged but less detail predictions on high-level feature Short-ranged but more accurate predictions on low-level feature Adaptive Score Masking By Searching</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X Better Details Consistent Curve Lane Results</head><p>Line-NMS and Clustering anchors into groups with searched score thresholds are more accurate in a short-range while predictions on high-level feature map are more consistent in a long-range but losing a lot of details. Moreover, we found that each grid can only predict the offsets precisely around its center and the ∆x ijz far away from the anchor is inaccurate. Using plain Line-NMS is not sensitive enough to capture the curve or remote part of the lanes.</p><p>An adaptive Point Blending Search Module is proposed to unify the postprocessing into the NAS framework for lane-sensitive detection as shown in <ref type="figure" target="#fig_3">Figure 4 (Right)</ref>. In order to allow a different emphasize regions in multi-level prediction, we use an adaptive score masking m f on the original score prediction for each feature map f . Let c x and c y denote the center position of each grid g ij on certain feature map f . We consider a very simple score masking for each map as follow:</p><formula xml:id="formula_0">logit(m f ) = α 1f (c y ) + β 1f + α 2f (c x − u xf ) 2 + (c y − u yf ) 2 1 2 .<label>(1)</label></formula><p>There are two main terms in the Eq. (1): one term linearly related to the vertical position of each prediction, and another term is related to the distance from [u xf , u yf ]. We use the above masking because we conjecture that low-level features may perform better in the remote part of the lane (near the center of the image) and such formulation allows flexible masking across feature maps. Note that each grid with a good confidence score has precise local information about the lane near the center of the grid. Filtering out all other occluded lanes and only using the lane with the highest confidence score in Line-NMS might not capture the long range curvature for the remote part, since a low score may be assigned. Thus, we further use a point blending technique for a lane-sensitive prediction. After modification of the original confidence score on each feature map, we first filter out those low score lanes by a suitable threshold and apply NMS to group the remaining lanes into several groups according to their mutual distance. In each group of lines, we iteratively swap the good local points in the lower score anchors with those remote points in the highest score anchors. For each high confidence anchor, some of its points are then replaced by the good local points to become the final prediction. The remote parts of lanes and curve shape can be amended by the local points and the details of lanes are better. The computation overhead is very small by adding a loop within each group of limited lines. The detailed algorithm can be found in the Appendix.</p><p>Thus, α 1f , β 1f , α 2f , [u xf , u yf ], the score thresholds and the mutual distance thresholds in NMS form the final search space of this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unified Multi-objective Search</head><p>We consider a simple but effective multi-objective search algorithm that can generate a Pareto front with the optimal trade-off between accuracy and different computation constraints. Non-dominate sorting is used to determine whether one model dominates another model in terms of both FLOPS and accuracy. During the search, we sample a candidate architecture by mutating the best architecture along the current Pareto front. We considering following mutation: for the backbone search space, we randomly swap the position of downsampling and double-channel to their neighboring position; for the feature fusion search space, we randomly change the input feature level of each fusion layer; for the adaptive point blending search module, we disturb the best hyper-parameters. Note that the search of the post-processing parameters does not involve training thus the mutation can be more frequent on this module. Our algorithm can be run on multiple parallel computation nodes and can lift the Pareto front simultaneously. As a result, the search algorithm will automatically allocate computation with reasonable receptive fields and spatial resolution towards an effecient and lane-sensitive detection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">New CurveLanes Benchmark</head><p>We have released a new dataset named CurveLanes consisting of 150K lanes images with 650K carefully annotated lane labels for bench-marking difficult scenarios such as curves and multi-lanes in traffic lane detection. <ref type="table" target="#tab_2">Table 1</ref> shows a comparison between the existing lane detection datasets TuSimple <ref type="bibr" target="#b27">[28]</ref>, CULane <ref type="bibr" target="#b19">[20]</ref> and BDD <ref type="bibr" target="#b31">[32]</ref>. It can be found that our new CurveLanes benchmark has more images and a higher resolution. Moreover, our dataset has more lanes per image and more curves lanes. Thus, the new benchmark is suitable to compare the performance in the difficult situation for the community. <ref type="figure" target="#fig_4">Figure 5</ref> shows some typical examples of the CurveLanes. More difficult cases such as S-curves, Y-lanes can be found in this dataset. <ref type="figure" target="#fig_5">Figure 6</ref> further shows the comparison of the distribution of the degree of curvature between common dataset and CurveLanes. The new CurveLanes has more turns and difficult curves CurveLanes also has more muti-lanes scenes thus more difficult. The whole dataset 150K is divided into: train:100K, val: 20K and testing 30K. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Other Datasets and Evaluation Metrics</head><p>We conduct neural architecture search on two large lane detection datasets: the CULane <ref type="bibr" target="#b19">[20]</ref>, and the new CurveLanes dataset. We also transfer the searched architectures to the TuSimple <ref type="bibr" target="#b27">[28]</ref> and test the generalization power of the proposed approach. Evaluation metrics. Evaluation metrics is important since it is also the target of our architecture search. We follow the literature <ref type="bibr" target="#b19">[20]</ref> and use the corresponding evaluation metrics for each particular dataset. 1) CULane and Curve-Lanes. Following the official implementation of the evaluation <ref type="bibr" target="#b19">[20]</ref>, we compute the intersection-over-union (IoU) between GT labels and predictions, where each lane has 30 pixel width. Predictions whose IoUs are larger than 0.5 are considered as true positives (TP). The F1 measure is used as the evaluation metric: F 1 = 2×P recision×Recall P recision+Recall , where P recision = T P T P +F P and Recall = T P T P +F N . 2) TuSimple. We also use the official metric as the evaluation metrics: Accuracy = N pred N GT , where where N pred is the number of correctly predicted lane points and N GT is the number of ground-truth lane points.</p><p>NAS Implementation Details. During the search, we directly trained the model without ImageNet pretraining since the architecture of the backbone is changed. We found that for a large dataset like CULane and CurveLanes(more than 80K training images), ImageNet pretraining is not necessary and the resulting model converges well with only about 3˜5% accuracy loss. We use FLOPS to measure the computational complexity and construct the Pareto front. During <ref type="table">Table 3</ref>. Comparison of different algorithms on the new dataset CurveLanes. CurveLane-S, CurveLane-M, and CurveLane-L are the searched architectures of our method. The SOTA methods such as SCNN and SAD suffer substantial performance drop (20%˜30% F1 score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method F1</head><p>Precision Recall FLOPS(G) SCNN <ref type="bibr" target="#b19">[20]</ref> 65.02% 76.13% 56.74% 328.4 Enet-SAD <ref type="bibr" target="#b10">[11]</ref>  search, we use SGD with cosine decay learning rate 0.04 to 0.0001, momentum 0.9. We train each candidate for 12 epochs. Empirically, we found that training with 12 epochs can well separate good models from bad models. We train and test the new architecture in parallel on four computation nodes, and each has 8 Nvidia V100 GPU cards. The batch size is 256 and the input size is 512 × 288. It takes about 1 hour to complete evaluating one architecture for both datasets and it only takes 5 minutes to evaluate one setting of post-processing parameters. The total search cost is about 5000 GPU hours for one dataset. We set the number of blocks from 10-45 in order to get a complete Pareto front with different FLOPS. We set two random fusions (M = 2) with 128 output channels for the fusion search module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Lane Detection Results</head><p>After identifying the optimal architecture on each dataset, we fully train those models. We first pre-train those searched backbones on ImageNet following com-  mon practice <ref type="bibr" target="#b8">[9]</ref> for fair comparison with other methods. We train 50 epochs with bs = 256, 40 epochs with bs = 32 and 30 epochs with bs =32 for TuSimple, CULane and CurveLanes, respectively. SGD is used with initial learning rate 0.04 and a cosine decay learning rate 0.04 to 0.0001, momentum 0.9. The input size of both training and testing is 512 × 288 for three datasets. We consider three kinds of computational constraints thus the resulting models are denoted as CurveLane-S, CurveLane-M, and CurveLane-L picked from the Pareto front of each dataset. A detailed description of the hyper-parameter can be found in the supplementary materials. For other methods, we report the accuracy numbers of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref> directly from the original papers for TuSimple and CULane. For the CurveLanes, we re-implement the official code from the <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref>. For Point-LaneNet <ref type="bibr" target="#b5">[6]</ref>, we use ResNet101 as the backbone.</p><p>Comparison with the state-of-the-art on CULane. <ref type="table" target="#tab_3">Table 2</ref> shows the performance of the searched architectures on the CULane. Our CurveLane-L achieves a new SOTA result on CULane dataset with a 74.8 F1 score comparing to all the competing methods. Our CurveLane-M model is 1.9 higher F1-core, and 9x fewer FLOPS than SCNN; 1.5 higher F1-score, 4.5x fewer FLOPS than R101-SAD. <ref type="figure" target="#fig_6">Figure 8</ref> (Left) shows a comparison between ours and other SOTA methods. Although our method is specifically designed for curve lane detection, it dominates most SOTA methods which proves the effectiveness of the proposed multi-objective search framework.</p><p>Results on new CurveLanes. The comparison is shown in <ref type="table">Table 3</ref>. It can be found the SOTA methods such as SCNN and SAD suffer substantial performance drop (20%˜30% F1 score). Our method is lane sensitive and performs far better than all the competing methods e.g. CurveLane-S can reach 81.12% with an F1 measure which is 16% higher than the SCNN. We also show some qualitative results on CurveLanes in <ref type="figure">Figure 7</ref>. Better performance of CurveLane-NAS <ref type="table">Table 4</ref>. Ablative study with the F1-measure on the CULane dataset. CurveLane S to L denote our searched backbone architectures. The performance of models combined with all the modules are listed in the final column. can be found in the difficult scenarios such as large-curves, night and wet roads. More qualitative comparisons can be found in the Appendix. The searched architectures also show strong transferability in dealing with other lane detection tasks. We transfer the searched architecture to the TuSimple dataset. With the optimal architecture searched in CUlane, our method reached a comparable performance with SCNN but much faster. The detailed tables can be found in the Appendix.</p><p>Searched Architectures. The detailed searched architectures of CurveLane-NAS can be found in the supplementary materials. The architecture is quite different from the hand-craft design such as ResNet50. The backbone of the found architectures usually down-samples twice (only 3 stages). The positions of doubling channels are usually in the later stages to control the total FLOPS since larger channels in the early stage which will result in a great computational burden. Larger models use more heads. Most results of fusion module tend to select output features from the first stage and the second stage with the output one to enable more spatial information.</p><p>Ablative Study. We conduct ablation analysis of our proposed model modifications in <ref type="table">Table 4</ref>. The study is based on different backbones (searched architectures and the ResNet101) on the CULane dataset. It can be found that multi-level heads are more useful for small models. Adaptive Points Blending modules can boost the performance more the larger models. <ref type="figure" target="#fig_6">Figure 8</ref> (Right) further shows some qualitative comparisons, it can be found that our adaptive points blending can yield significantly better performance than Plain-NMS for the curve lanes and remote part of the lanes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose CurveLane-NAS, a NAS pipeline unifying lane-sensitive architecture search and adaptive point blending for curve lane detection. The new framework can automatically fuse and capture both long-ranged coherent and accurate curve information and enable a more efficient computational allocation. The searched networks achieve state-of-the-art speed/FLOPS trade-off comparing to existing methods. Furthermore, we release a new largest lane detection dataset named CurveLanes for the community to establish a more challenging benchmark with more curve lanes/lanes per image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(b) TuSimple Dataset (only ~30% images contains a curve lane) (a) CULane Dataset (only ~2% images contains a curve lane) (c) Our New CurveLanes Benchmark (More than 90% images contains a curve lane) Examples of curve lane detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of the Lane Detection frameworks: (a) dense prediction based (SCNN<ref type="bibr" target="#b19">[20]</ref>), (b) proposal based (Line-CNN<ref type="bibr" target="#b13">[14]</ref>), and (c) our CurveLane-NAS. CurveLane-NAS is a unified neural architecture search framework to discover novel holistic network architectures for more robust lane predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>(Left) For each prediction head, we predict the offsets ∆xijz for each grid gij and their corresponding scores. The offset ∆xijz is the horizontal distance between the ground truth and a pre-defined vertical anchor xijz. Each lane prediction can be recovered by the offset and the point anchors positions. (Right) We propose an adaptive Point Blending Search Module for a lane-sensitive detection. Adaptive score masking allows different regions of interest in multi-level prediction. A point blending technique is used to replace some of the prediction points in high confidence lines by the accurate local points. The remote and detailed curve shape can be amended by the local points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of our new released CurveLanes dataset. All images are carefully selected so that each image contains at least one curve lane. It is the largest lane detection dataset so far and establishes a more challenging benchmark for the community. More difficult scenarios such as S-curves and Y-lanes can be found in this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>(Left) Comparison of the distribution of the degree of curvature between common datasets and our CurveLanes. Ours has more proportion of various curvatures comparing to the natural distribution of lanes.(Right)The histogram of the average number of lanes per image in our CurveLanes. CurveLanes also has more number of lanes per image than CULane (&lt;5) and TuSimple (&lt;5) thus more challenging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>(Left) Comparison between our methods and other SOTA methods on CULane. Although specifically designed for curve lane detection, our method still dominates most SOTA methods on the widely used CULane benchmark. (Right) Performance of the post-processing algorithms on TuSimple dataset: Ground-truth, Plain-NMS, and our Adaptive Point Blending. Our methods are more sensitive to the curve lanes and remote part of the lanes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Elastic Backbone Search ModuleFeature Fusion Search Module Multi-level Prediction Heads Lane Results Adaptive Point Blending Search Module</head><label></label><figDesc>_<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> </figDesc><table><row><cell>Down Sampling</cell><cell></cell><cell></cell><cell>Pareto front</cell></row><row><cell>@5thBB</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Down</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sampling</cell><cell></cell><cell></cell><cell></cell></row><row><cell>@9thBB</cell><cell></cell><cell>Accuracy</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Arch candidates</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pareto Front</cell></row><row><cell>Double @7thBB Channel</cell><cell>Double Channel @12thBB</cell><cell>: Feature Fusion BB: Bottle Neck Block</cell><cell>FLOPS</cell></row><row><cell></cell><cell></cell><cell>: Feature Maps</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the three largest lane detection datasets and our new Curve-Lanes. Our new CurveLanes benchmark has substantially more images, bigger resolution, more average number of lanes and more curves lanes.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Total amount of images Resolution</cell><cell>Road type</cell><cell># Lane&gt;5 Curves</cell></row><row><cell>TuSimple [28]</cell><cell>13.2K</cell><cell>1280x720</cell><cell>Highway</cell><cell>×˜30%</cell></row><row><cell>CULane [20]</cell><cell>133.2K</cell><cell cols="2">1640x590 Urban &amp; Highway</cell><cell>×˜2%</cell></row><row><cell>BDD100K [32]</cell><cell>100K</cell><cell cols="2">1280x720 Urban &amp; Highway</cell><cell>˜10%</cell></row><row><cell>Our CurveLanes</cell><cell>150K</cell><cell cols="2">2650x1440 Urban &amp; Highway</cell><cell>&gt;90%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison of F1-measure of the state-of-the-art models on CULane test set. CurveLane-S, CurveLane-M, and CurveLane-L are the searched architectures of our method. Our method outperforms the SOTA models by a large margin with a small computational overhead.</figDesc><table><row><cell cols="8">Methods SCNN[20] SAD[11] SAD PointLane[6] CurveLane-S CurveLane-M CurveLane-L</cell></row><row><cell>Backbone</cell><cell>SCNN</cell><cell cols="2">ENet R101</cell><cell>R101</cell><cell>Searched</cell><cell>Searched</cell><cell>Searched</cell></row><row><cell>Normal</cell><cell>90.6</cell><cell>90.1</cell><cell>90.7</cell><cell>88.0</cell><cell>88.3</cell><cell>90.2</cell><cell>90.7</cell></row><row><cell>Crowded</cell><cell>69.7</cell><cell>68.8</cell><cell>70.0</cell><cell>68.1</cell><cell>68.6</cell><cell>70.5</cell><cell>72.3</cell></row><row><cell>Dazzle light</cell><cell>58.5</cell><cell>60.2</cell><cell>59.9</cell><cell>61.5</cell><cell>63.2</cell><cell>65.9</cell><cell>67.7</cell></row><row><cell>Shadow</cell><cell>66.9</cell><cell>65.9</cell><cell>67.0</cell><cell>63.3</cell><cell>68.0</cell><cell>69.3</cell><cell>70.1</cell></row><row><cell>No line</cell><cell>43.4</cell><cell>41.6</cell><cell>43.5</cell><cell>44.0</cell><cell>47.9</cell><cell>48.8</cell><cell>49.4</cell></row><row><cell>Arrow</cell><cell>84.1</cell><cell>84.0</cell><cell>84.4</cell><cell>80.9</cell><cell>82.5</cell><cell>85.7</cell><cell>85.8</cell></row><row><cell>Curve</cell><cell>64.4</cell><cell>65.7</cell><cell>65.7</cell><cell>65.2</cell><cell>66.0</cell><cell>67.5</cell><cell>68.4</cell></row><row><cell>Night</cell><cell>66.1</cell><cell>66.0</cell><cell>66.3</cell><cell>63.2</cell><cell>66.2</cell><cell>68.2</cell><cell>68.9</cell></row><row><cell>Crossroad</cell><cell>1990</cell><cell cols="2">1998 2052</cell><cell>1640</cell><cell>2817</cell><cell>2359</cell><cell>1746</cell></row><row><cell>FLOPS (G)</cell><cell>328.4</cell><cell>3.9</cell><cell>162.2</cell><cell>25.1</cell><cell>9.0</cell><cell>35.7</cell><cell>86.5</cell></row><row><cell>Total</cell><cell>71.6</cell><cell>70.8</cell><cell>71.8</cell><cell>70.2</cell><cell>71.4</cell><cell>73.5</cell><cell>74.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>CULane [20] is a large scale dataset on traffic lane detection which is collected by cameras in Beijing, China. The CULane dataset includes 88,880 training images, 9675 verification images, and 34,680 test images. The test dataset is divided into 1 normal and 8 challenging categories. TuSimple [28] is created by TuSimple specifically focuses on real highway scenarios. It includes 3626 training images and 2782 test images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Qualitative result comparison on CurveLanes. Our method CurveLane-L performs better in the difficult scenarios such as curves, night and wet roads.</figDesc><table><row><cell>50.31% 63.6% 41.6%</cell><cell>3.9</cell></row><row><cell>PointLaneNet [6] 78.47% 86.33% 72.91%</cell><cell>14.8</cell></row><row><cell>CurveLane-S 81.12% 93.58% 71.59%</cell><cell>7.4</cell></row><row><cell>CurveLane-M 81.80% 93.49% 72.71%</cell><cell>11.6</cell></row><row><cell>CurveLane-L 82.29% 91.11% 75.03%</cell><cell>20.7</cell></row><row><cell>GT</cell><cell></cell></row><row><cell>SCNN</cell><cell></cell></row><row><cell>SAD</cell><cell></cell></row><row><cell>Ours</cell><cell></cell></row><row><cell>Fig. 7.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Backbone Backbone Only +Feature Fusion +Multi Level Heads +Adaptive Points Blending</figDesc><table><row><cell>ResNet101</cell><cell>70.2</cell><cell>70.4 +0.2</cell><cell>71.9 +1.5</cell><cell>72.4 +0.5</cell></row><row><cell>CurveLane S</cell><cell>69.5</cell><cell>70.1 +0.6</cell><cell>70.9 +0.8</cell><cell>71.5 +0.6</cell></row><row><cell>CurveLane M</cell><cell>71.7</cell><cell>72.0 +0.3</cell><cell>72.2 +0.2</cell><cell>73.5 +1.3</cell></row><row><cell>CurveLane L</cell><cell>72.6</cell><cell>73.1 +0.5</cell><cell>73.5 +0.4</cell><cell>74.8 +1.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The description of the loss function can be found in the Appendix.<ref type="bibr" target="#b4">5</ref> The detailed Line-NMS algorithm can be found in the Appendix.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Detnas: Neural architecture search on object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pointlanenet: Efficient end-to-end cnns for accurate real-time lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<editor>IV</editor>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2563" to="2568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lane detection using color-based segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Lin</surname></persName>
		</author>
		<editor>IV.</editor>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="706" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lane detection using histogram-based segmentation and decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ozguner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC2000. 2000 IEEE Intelligent Transportation Systems. Proceedings (Cat. No. 00TH8493)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="346" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Agnostic lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.03704" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning lightweight lane detection cnns by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sp-nas: Serial-to-parallel backbone search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11863" to="11872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective lane detection and tracking method using statistical modeling of color and lane edge-orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on Computer Sciences and Convergence Information Technology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1586" to="1591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Line-cnn: End-to-end traffic line detection with line proposal unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dynamic approach for lane detection using google street view and CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Mamidala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Uthkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Antony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Narasimhadhan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.00798" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lane detection and classification using cascaded cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pizzati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>García</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.01294" />
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks. In: ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TuSimple: Tusimple lane detection challenge</title>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Snas: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Auto-fpn: Automatic network architecture adaptation for object detection beyond classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sm-nas: Structural-to-modular neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Robust lane detection from continuous driving scenes using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02193</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
