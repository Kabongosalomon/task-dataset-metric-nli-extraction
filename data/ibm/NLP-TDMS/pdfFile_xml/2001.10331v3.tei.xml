<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">f-BRS: Rethinking Backpropagating Refinement for Interactive Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
							<email>k.sofiiuk@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Petrov</surname></persName>
							<email>ilia.petrov@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Barinova</surname></persName>
							<email>o.barinova@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
							<email>a.konushin@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center -Moscow</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">f-BRS: Rethinking Backpropagating Refinement for Interactive Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have become a mainstream approach to interactive segmentation. As we show in our experiments, while for some images a trained network provides accurate segmentation result with just a few clicks, for some unknown objects it cannot achieve satisfactory result even with a large amount of user input. Recently proposed backpropagating refinement scheme (BRS) <ref type="bibr" target="#b14">[15]</ref> introduces an optimization problem for interactive segmentation that results in significantly better performance for the hard cases. At the same time, BRS requires running forward and backward pass through a deep network several times that leads to significantly increased computational budget per click compared to other methods. We propose f-BRS (feature backpropagating refinement scheme) that solves an optimization problem with respect to auxiliary variables instead of the network inputs, and requires running forward and backward passes just for a small part of a network. Experiments on GrabCut, Berkeley, DAVIS and SBD datasets set new state-of-theart at an order of magnitude lower time per click compared to original BRS <ref type="bibr" target="#b14">[15]</ref>. The code and trained models are available at https://github.com/saic-vul/ fbrs_interactive_segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The development of robust models for visual understanding is tightly coupled with data annotation. For instance, one self-driving car can produce about 1Tb of data every day. Due to constant changes in environment new data should be annotated regularly.</p><p>Object segmentation provides fine-grained scene representation and can be useful in many applications, e.g. autonomous driving, robotics, medical image analysis, etc. However, practical use of object segmentation is now limited due to extremely high annotation costs. Several large segmentation benchmarks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> with millions of annotated object instances came out recently. Annotation of these datasets became feasible with the use of automated inter-active segmentation methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Interactive segmentation has been a topic of research for a long time <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b14">15]</ref>. The main scenario considered in the papers is click-based segmentation when the user provides input in a form of positive and negative clicks. Classical approaches formulate this task as an optimization problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2]</ref>. These methods have many built-in heuristics and do not use semantic priors to full extent, thus requiring a large amount of input from the user. On the other hand, deep learning-based methods <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b23">23]</ref> tend to overuse image semantics. While showing great results on the objects that were present in the training set, they tend to perform poorly on unseen object classes. Recent works propose different solutions to these problems <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b22">22]</ref>. Still, state-of-the-art networks for interactive segmentation are either able to accurately segment the object of interest after a few clicks or do not provide satisfactory result after any reasonable number of clicks (see Section 5.1 for experiments).</p><p>The recently proposed backpropagating refinement scheme (BRS) <ref type="bibr" target="#b14">[15]</ref> brings together optimization-based and deep learning-based approaches to interactive segmentation. BRS enforces the consistency of the resulting object mask with user-provided clicks. The effect of BRS is based on the fact that small perturbations of the inputs for a deep network can cause massive changes in the network output <ref type="bibr" target="#b31">[31]</ref>. Thus, BRS requires running forward and backward pass multiple times through the whole model, which substantially increases computational budget per click compared to other methods and is not practical for many enduser scenarios.</p><p>In this work we propose f-BRS (feature backpropagating refinement scheme) that reparameterizes the optimization problem and thus requires running forward and backward passes only through a small part of the network (i.e. last several layers). Straightforward optimization for activations in a small sub-network would not lead to the desired effect because the receptive field of the convolutions in the last layers relative to the output is too small. Thus we introduce a set of auxiliary parameters for optimization that are invariant to the position in the image. We show that op- timization with respect to these parameters leads to a similar effect as the original BRS, without the need to compute backward pass through the whole network.</p><p>We perform experiments on standard datasets: GrabCut <ref type="bibr" target="#b27">[27]</ref>, Berkeley <ref type="bibr" target="#b24">[24]</ref>, DAVIS <ref type="bibr" target="#b26">[26]</ref> and SBD <ref type="bibr" target="#b12">[13]</ref>, and show state-of-the-art results, improving over existing approaches in terms of speed and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The goal of interactive image segmentation is to obtain an accurate mask of an object using minimal user input. Most of the methods assume such interface where a user can provide positive and negative clicks (seeds) several times until the desired object mask is obtained.</p><p>Optimization-based methods. Before deep learning, interactive segmentation was usually posed as an optimization problem. Li et al. <ref type="bibr" target="#b17">[17]</ref> used graph-cut algorithm to separate foreground pixels from the background using distances from each pixel to foreground and background seeds in color space. Grady et al. <ref type="bibr" target="#b9">[10]</ref> proposed a method based on random walks, where each pixel is marked according to the label of the first seed that the walker reaches. Later, <ref type="bibr" target="#b10">[11]</ref> compute geodesic distances from the clicked points to every image pixel and use them in energy minimisation. In <ref type="bibr" target="#b15">[16]</ref>, several segmentation maps are first generated for an image. Then optimization algorithm is applied to the cost function that enforces pixels of the same segment to have the same label in the resulting segmentation mask.</p><p>Optimization-based methods usually demonstrate predictable behaviour and allow obtaining detailed segmentation masks with enough user input. Since no learning is involved, the amount of input required from a user does not depend on the type of an object of interest. The main drawback of this approach is insufficient use of semantic priors. This requires additional user effort to obtain accurate object masks for known objects compared to recently proposed learning-based methods.</p><p>Learning-based methods. The first deep learning-based interactive segmentation method was proposed in <ref type="bibr" target="#b34">[33]</ref>. They calculate distance maps from positive and negative clicks, stack them together with an input image and pass into a network that predicts an object mask. This approach was later used in most of the following works. Liew et al. <ref type="bibr" target="#b19">[19]</ref> propose to combine local predictions on patches containing user clicks and thus refine network output. Li et al. <ref type="bibr" target="#b18">[18]</ref> notice that learnt models tend to be overconfident in their predictions. In order to improve diversity of the outputs, they generate multiple masks and then select one among them. In <ref type="bibr" target="#b30">[30]</ref>, user annotations are multiplied automatically by locating foreground and background clicks.</p><p>The common problem of all deep-learning-based methods for interactive segmentation is overweighting semantics and making little use of user-provided clicks. This happens because during training user clicks are in perfect correspondence with the semantics of the image and add little information, therefore can be easily downweighted during the training process.</p><p>Optimization for activations. Optimization schemes that update activation responses while keeping weights of a neural network fixed have been used for different problems <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Szegedy et al. <ref type="bibr" target="#b31">[31]</ref> formulated an optimization problem for generating adversarial examples, i.e. images that are visually indistinguishable from the natural ones, though are incorrectly classified by the network with high confidence. They demonstrated that in deep networks small perturbation of an input signal can cause large changes in activations of the last layers. In <ref type="bibr" target="#b14">[15]</ref>, the authors apply this idea to the problem of interactive segmentation. They find minimal edits to the input distance maps that result in an object mask consistent with user-provided annotation.</p><p>In this work, we also formulate an optimization problem for interactive segmentation. In contrast to <ref type="bibr" target="#b14">[15]</ref>, here we do not perform optimization over the network inputs but introduce an auxiliary set of parameters for optimization. After such reparameterization, we do not need to run forward and backward passes through the whole network. We evaluate different reparameterizations and the speed and accuracy of the resulting methods. The derived optimization algorithm f-BRS is an order of magnitude faster than BRS from <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>First, let us recall optimization problems from the literature, where the fixed network weights were used. Below we use a unified notation. We denote the space of input images by R m and a function that a deep neural network implements by f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>Adversarial examples generation. Szegedy et al. <ref type="bibr" target="#b31">[31]</ref> formulate an optimization problem for generating adversarial examples for an image classification task. They find images that are visually indistinguishable from the natural ones, which are incorrectly classified by the network. Let L denote a continuous loss function that penalizes for incorrect classification of an image. For a given image x ∈ R m and target label l ∈ {1 . . . k}, they aim to find x + ∆x, which is the closest image to x classified as l by f . For that they solve the following optimization problem:</p><formula xml:id="formula_0">||∆x|| 2 → min subject to 1. f (x + ∆x) = l 2. x + ∆x ∈ [0, 1] m (1)</formula><p>This problem in (1) is reduced to minimisation of the following energy function:</p><formula xml:id="formula_1">λ||∆x|| 2 + L(f (x + ∆x), l) → min ∆x (2)</formula><p>The variable λ in later works is usually assumed a constant and serves as a trade-off between the two energy terms.</p><p>Backpropagating refinement scheme for interactive segmentation. Jang et al. <ref type="bibr" target="#b14">[15]</ref> propose a backpropagating refinement scheme that applies a similar optimization technique to the problem of interactive image segmentation. In their work, a network takes as input an image stacked together with distance maps for user-provided clicks. They find minimal edits to the distance maps that result in an object mask consistent with user-provided annotation. For that, they minimise a sum of two energy functions, i.e. corrective energy and inertial energy. Corrective energy function enforces consistency of the resulting mask with userprovided annotation, and inertial energy prevents excessive perturbations in the network inputs.</p><p>Let us denote the coordinates of user-provided click by (u, v) and its label (positive or negative) as l ∈ {0, 1}. Let us denote the output of a network f for an image x in posi-tion (u, v) as f (x) u,v and the set of all user-provided clicks as</p><formula xml:id="formula_2">{(u i , v i , l i )} n i=1 .</formula><p>The optimization problem in <ref type="bibr" target="#b14">[15]</ref> is formulated as follows:</p><formula xml:id="formula_3">λ||∆x|| 2 + n i=1 f (x + ∆x) ui,vi − l i 2 → min ∆x ,<label>(3)</label></formula><p>where the first term represents inertial energy, the second term represents corrective energy and λ is a constant that regulates trade-off between the two energy terms. This optimization problem resembles the one from (2) with classification loss for one particular label replaced by a sum of losses for the labels of all user-provided clicks. Here we do not need to ensure that the result of optimization is a valid image, so the energy (3) can be minimised by unconstrained L-BFGS.</p><p>The main drawback of this approach is that L-BFGS requires computation of gradients with respect to network inputs, i.e. backpropagating through the whole network. It is computationally expensive and results in significant computational overhead.</p><p>We also notice that since the first layer of a network f is a convolution, i.e. a linear combination of the inputs, one can minimise the energy (3) with respect to input image instead of distance maps and obtain equivalent solution. Moreover, if we minimise it with respect to RGB image which is invariant to an interactive input, we can use the result as an initialisation for optimization of (3) with new clicks. Thus, we set the BRS with respect to an input image as a baseline in our experiments and denote it as RGB-BRS. For a fair comparison, we also implement the optimization with respect to the input distance maps (DistMap-BRS), that was originally introduced in [15].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature backpropagating refinement</head><p>In order to speed-up the optimization process, we want to compute backpropagation not for the whole network, but for some part of it. This can be achieved by optimizing some intermediate parameters in the network instead of the input. A naive approach would be to simply optimize the outputs of some of the last layers and thus compute backpropagation only through the head of a network. However, such a naive approach would not lead to the desired result. The convolutions in the last layers have a very small receptive field with respect to the network outputs. Therefore, an optimization target can be easily achieved by changing just a few components of a feature tensor which would cause only minor localized changes around the clicked points in the resulting object mask.</p><p>Let us reparameterize the function f and introduce auxiliary variables for optimization. Letf (x, z) denote the function that depends both on the input x and on the introduced variables z. With auxiliary parameters fixed z = p the reparameterized function is equivalent to the original onef (x, p) ≡ f (x). Thus, we aim to find a small value of ∆p, which would bring the values off (x, p + ∆p) in the clicked points close to the user-provided labels. We formulate the optimization problem as follows:</p><formula xml:id="formula_4">λ||∆p|| 2 + n i=1 f (x, p + ∆p) ui,vi − l i 2 → min ∆p .<label>(4)</label></formula><p>We call this optimization task f-BRS (feature backpropagating refinement) and use unconstrained L-BFGS optimizer for minimization. For f-BRS to be efficient, we need to choose the reparameterization that a) does not have a localized effect on the outputs, b) does not require a backward pass through the whole network for optimization.</p><p>One of the options for such reparameterization may be channel-wise scaling and bias for the activations of the last layers in the network. Scale and bias are invariant to the position in the image, thus changes in this parameters would affect the results globally. Compared to optimization with respect to activations, optimization with respect to scale and bias cannot result in degenerate solutions (i.e. minor localized changes around the clicked points).</p><p>Let us denote the output of some intermediate layer of the network for an image x by F (x), the number of its channels by h, a function that the network head implements by g. Thus, f can be represented by f (x) ≡ g(F (x)). Then the reparameterized functionf looks as follows:</p><formula xml:id="formula_5">f (x, s, b) = g s · F (x) + b ,<label>(5)</label></formula><p>where b ∈ R h is a vector of biases, s ∈ R h is a vector of scaling coefficients and · denotes a channel-wise multiplication. For s = 1 and b = 0 we havef (x) ≡ f , thus we take these values as initial values for optimization. By varying the part of the network to which auxiliary scale and bias are applied, we achieve a natural trade-off between accuracy and speed. <ref type="figure" target="#fig_1">Figure 2</ref> shows the architecture of the network that we used in this work and illustrates different options for optimization. Surprisingly, we found that applying f-BRS to the last several layers causes just a small drop of accuracy compared to full-network BRS, and leads to significant speed-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Zoom-In for interactive segmentation</head><p>Previous works on interactive segmentation often used inference on image crops to achieve speed-up and preserve fine details in the segmentation mask. Cropping helps to infer the masks of small objects, but it also may degrade results in cases when an object of interest is too large to fit into one crop.</p><p>In this work, we use an alternative technique (we call it Zoom-In), which is quite simple but improves both quality and speed of the interactive segmentation. It is based on the ideas from object detection <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b6">7]</ref>. We have not found any mentions of this exact technique in the literature in the context of interactive segmentation, so we describe it below.</p><p>We noticed that the first 1-3 clicks are enough for the network to achieve around 80% IoU with ground truth mask in most cases. It allows us to obtain a rough crop around the region of interest. Therefore, starting from the third click we crop an image according to the bounding box of the inferred object mask and apply the interactive segmentation only to this Zoom-In area. We extend the bounding box by 40% along sides in order to preserve the context and not miss fine details on the boundary. If a user provides a click outside the bounding box, we expand or narrow down the zoom-in area. Then we resize the bounding box so that its longest side matches 400 pixels. <ref type="figure" target="#fig_2">Figure 3</ref> shows an example of Zoom-In.</p><p>This technique helps the network to predict more accurate masks for small objects. In our experiments, Zoom-In consistently improved the results, therefore we used it by default in all experiments in this work. <ref type="table" target="#tab_0">Table 1</ref> shows a quantitative comparison of the results with and without Zoom-In on GrabCut and Berkeley datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Following the standard experimental protocol, we evaluate proposed method on the following datasets: SBD <ref type="bibr" target="#b12">[13]</ref>, GrabCut <ref type="bibr" target="#b27">[27]</ref>, Berkeley <ref type="bibr" target="#b24">[24]</ref> and DAVIS <ref type="bibr" target="#b26">[26]</ref>.</p><p>GrabCut. The GrabCut dataset contains 50 images with a single object mask for each image.</p><p>Berkeley. For the Berkeley dataset, we use the same test set as in <ref type="bibr" target="#b25">[25]</ref>, which includes 96 images with 100 object masks for testing.</p><p>DAVIS. The DAVIS dataset is used for evaluating video segmentation algorithms. To evaluate interactive segmentation algorithms one can sample random frames from the videos. We use the same 345 individual frames from video sequences as <ref type="bibr" target="#b14">[15]</ref> for evaluation. To follow the evaluation protocol we combine instance-level object masks into one semantic segmentation mask for each image.</p><p>SBD. The SBD dataset was first used for evaluating object segmentation techniques in <ref type="bibr" target="#b34">[33]</ref>. The dataset contains 8,498 training images and 2,820 test images. As in previous works, we train the models on the training part and use the validation set, which includes 6,671 instance-level object masks, for the performance evaluation.</p><p>Evaluation protocol. We report the Number of Clicks (NoC) measure which counts the average number of clicks required to achieve a target intersection over union (IoU) with ground truth mask. We set the target IoU score to 85% or 90% for different datasets, denoting the corresponding measures as NoC@85 and NoC@90 respectively. For a fair comparison, we use the same click generation strategy as in <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b34">33]</ref> that operates as follows. It finds the dominant type of prediction errors (false positives or false negatives) and generates the next negative or positive click respectively at the point farthest from the boundaries of the corresponding error region.</p><p>Network architecture. In this work, we do not focus on network architecture improvements, so in all our experiments we use the standard DeepLabV3+ <ref type="bibr" target="#b4">[5]</ref> which is a state-of-the-art model for semantic segmentation. The architecture of our network is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The model contains Distance Maps Fusion (DMF) block for adaptive fusion of RGB image and distance maps. It takes a concatenation of RGB image and 2 distance maps (one for positive clicks and one for negative clicks) as an input. The DMF block processes the 5-channel input with 1 × 1 convolutions followed by LeakyReLU and outputs a 3-channel tensor which can be passed into the backbone that was pre-trained on RGB images. Implementation details. We formulate the training task as a binary segmentation problem and use normalized focal loss (NFL) introduced in <ref type="bibr" target="#b29">[29]</ref> for training. We compare results of training with NFL and binary cross entropy in Appendix E. We train all the models on image crops of size 320 × 480 with horizontal and vertical flips as augmentations. We randomly resize images from 0.75 to 1.25 of original size before cropping.</p><p>We sample clicks during training following the standard procedure first proposed in <ref type="bibr" target="#b34">[33]</ref>. We set the maximum number of positive and negative clicks to 10, resulting in a maximum of 20 clicks per image.</p><p>In all experiments, we used Adam with β 1 = 0.9, β 2 = 0.999 and trained the networks for 120 epochs (100 epochs with learning rate 5 × 10 −4 , last 20 epochs with learning rate 5 × 10 −5 ). The batch size was set to <ref type="bibr" target="#b28">28</ref>    learning rate for the pre-trained ResNet backbone was 10 times lower than the learning rate for the rest of the network. We set the value of λ to 10 −3 for RGB-BRS and to 10 −4 for all variations of f-BRS.</p><p>We use MXNet Gluon <ref type="bibr" target="#b5">[6]</ref> with GluonCV <ref type="bibr" target="#b13">[14]</ref> framework for training and inference of our models. We take pretrained models for ResNet-34, ResNet-50 and ResNet-101 from GluonCV Model Zoo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Convergence analysis</head><p>An ideal interactive segmentation method should demonstrate predictable performance even for unseen object categories or unusual demand from the user. Moreover, the hard cases that require a significant amount of user input are the most interesting in the data annotation scenario. Thus, the desired property of an interactive segmentation method is convergence, i.e. we expect the result to improve with adding more clicks and finally achieve satisfactory accuracy.</p><p>However, neither the training procedure nor the inference in feed-forward networks for interactive segmentation guarantee convergence. Therefore, we noticed that when using feed-forward networks, the result does not converge for a significant number of images, i.e. additional user clicks do not improve the resulting segmentation mask. An example of such behaviour can be found in <ref type="figure" target="#fig_3">Figure 4</ref>. We observe very similar behaviour with different network architectures, namely with an architecture from <ref type="bibr" target="#b14">[15]</ref> and with DeepLabV3+. Below we describe our experiments.</p><p>Motivation for using NoC 100 metric. Previous works usually report NoC with the maximum number of generated clicks limited to 20 (we simply call this metric NoC). However, for a large portion of images in the standard datasets, this limit is exceeded. In terms of NoC, images that require 20 clicks and 2000 clicks to obtain accurate masks will get the same penalty. Therefore, NoC does not distinguish between the cases where an interactive segmentation method requires slightly more user input to converge and the cases where it fails to converge (i.e. unable to achieve satisfactory results after any reasonable number of user clicks).</p><p>In the experiments below we analyse NoC with the maximum number of clicks limited to 100 (let us call this metric NoC 100 ). NoC 100 is better for the convergence analysis, allowing us to identify the images where interactive segmentation fails. We believe that NoC 100 is substantially more adequate for comparison of interactive segmentation methods than NoC.</p><p>Experiments and discussion. In <ref type="table" target="#tab_1">Table 2</ref> we report the number of images that were not correctly segmented even after 20 and 100 clicks, and NoC 100 for the target IoU=90% (NoC 100 @90).</p><p>One can see that both DeepLabV3+ and the network architecture from <ref type="bibr" target="#b14">[15]</ref> without BRS were unable to produce accurate segmentation results on a relatively large portion of images from all datasets even with 100 user clicks provided. Interestingly, this percentage is also high for the SBD dataset which has the closest distribution to the training set. The images that could not be segmented with 100 user clicks are clear failure cases for the method. The use of both original BRS and proposed f-BRS allows to reduce the number of such cases by several times and results in significant improvement in terms of NoC 100 . We believe that the use of optimization-based backpropagating refinement results not just in metrics improvement, but more importantly, it changes the behaviour of the interactive segmentation system and its convergence properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation of the importance of clicks passed to the network</head><p>We have noticed that the results do not always improve with the increasing number of clicks passed to the network. Moreover, too many clicks can cause unpredictable behaviour of the network. On the other hand, the formulation of the optimization task for backpropagating refinement enforces the consistency of the resulting mask with user-provided annotation.</p><p>One may notice that we can handle user clicks only as a target for BRS loss function without passing them to the network through distance maps. We initialise the state of the network by making a prediction with the first few clicks. Then we iteratively refine the resulting segmentation mask only with BRS according to the new clicks.</p><p>We studied the relation between the number of consecutive clicks passed to the network and resulting NoC@90 on GrabCut and Berkeley datasets. The results of this study for RGB-BRS and f-BRS-B are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. The results show that providing all clicks to the network is not an optimal strategy. It is clear that for RGB-BRS, the optimum is achieved by limiting the number of clicks to 4, and for f-BRS-B -by 8 clicks. This shows that both BRS and f-BRS can adapt the network output to user input, without explicitly passing clicks to the network.</p><p>In all other experiments, we have limited the number of clicks passed to the network to 8 for f-BRS algorithms and to 4 for RGB-BRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with previous works</head><p>Comparison using the standard protocol. <ref type="table">Table 3</ref> compares with previous works across the standard protocol and report the average NoC with two IoU thresholds: 85% and 90%.</p><p>The proposed f-BRS algorithm requires fewer clicks than conventional algorithms, which indicates that the proposed algorithm yields accurate object masks with less user effort.</p><p>We tested three backbones on all datasets. Surprisingly, there is no significant difference in performance between these models. The smallest ResNet-34 model shows the best quality on GrabCut dataset outperforming much heavier models such as ResNet-101. However, during training there was a significant difference in the values of the target loss function on the validation set between these models. This shows that the target loss function is poorly correlated with the NoC metric.</p><p>Running time analysis. We measure the average running time of the proposed algorithm in seconds per click (SPC) and measure the total running time to process a dataset. The SPC shows the delay between placing a click and getting the updated result. The second metric indicates the total time a user needs to spend to obtain a satisfactory image annotation. In these experiments, we set the threshold on the number of clicks per image to 20. We test it on Berkeley and DAVIS datasets using a PC with an AMD Ryzen Threadripper 1900X CPU and a GTX 1080 Ti GPU. <ref type="table">Table 4</ref> shows the results for different versions of the proposed method and for our implemented baselines: without BRS and with RGB-BRS. The running time of f-BRS is an order of magnitude lower compared to RGB-BRS and adds just a small overhead with respect to a pure feedforward model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison of different versions of f-BRS.</head><p>The choice of a layer where to introduce auxiliary variables provides a trade-off between speed and accuracy of f-BRS. We compare three options of applying scale and bias to intermediate outputs in different parts of the network: af-  ter the backbone (f-BRS-A), before the first separable convolutions block in DeepLabV3+ (f-BRS-B), and before the second separable convolutions block in DeepLabV3+ (f-BRS-C). As a baseline for our experiments, we report the results for a feed-forward network without BRS. We also implement RGB-BRS, employing the optimization with respect to an input image. In these experiments, we used the ResNet-50 backbone.</p><p>We report NoC@90 and the number of images where the satisfactory result was not obtained after 20 user clicks. We also measure SPC (seconds per click) and Time (total time to process a dataset). Notice that direct comparison of the timings with the numbers reported in previous works is not valid due to differences in used frameworks and hardware. Therefore, only relative comparison makes sense.</p><p>The results of the evaluation for Berkeley and DAVIS datasets are shown in <ref type="table">Table 4</ref>. One can notice that all versions of f-BRS perform better than the baseline without BRS. The f-BRS-B is about 8 times faster than the RGB-BRS while showing very close results in terms of NoC. Therefore, we chose it for the comparative experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We proposed a novel backpropagating refinement scheme (f-BRS) that operates on intermediate features in the network and requires running forward and backward passes just for a small part of a network. Our approach was evaluated on four standard interactive segmentation benchmarks and set new state-of-the-art results in terms of both accuracy and speed. The conducted experiments demonstrated a better convergence of backpropagating refinement schemes compared to pure feed-forward approaches. We analysed the importance of first clicks passed to the network and showed that both BRS and f-BRS can successfully adapt the network output to user input, without explicitly passing clicks to the network.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Results of interactive segmentation on an image from DAVIS dataset. First row: using proposed f-BRS-B (Section 3), second row: without BRS. Green dots denote positive clicks, red dots denote negative clicks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the proposed method described in Section 3. f-BRS-A optimizes scale and bias for the features after pre-trained backbone, f-BRS-B optimizes scale and bias for the features after ASPP, f-BRS-C optimizes scale and bias for the features after the first separable convblock. The number of channels is provided for ResNet-50 backbone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Example of applying zoom-in technique described in Section 4. See how cropping an image allows recovering fine details in the segmentation mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>IoU with respect to the number of clicks added by a user for one of the most difficult image from GrabCut dataset (scissors). All results are obtained using the same model with ResNet-50. One can see that without BRS the model does not converge to the correct results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Evaluation of different click-processing strategies on GrabCut and Berkeley datasets. The plots show NoC@90 with respect to the number of consecutive clicks passed to the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 . 5 .</head><label>65</label><figDesc>Comparison of the average IoU scores according to the number of clicks on GrabCut, Berkeley, DAVIS and SBD datasets. The dashed horizontal line shows the average IoU limit that can theoretically be reached by f-BRS-B method (for more details see Section B). Evaluation results on GrabCut, Berkeley, SBD and DAVIS datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Examples of good convergence of the proposed f-BRS-B method with ResNet-50 backbone on Berkeley dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Examples of good convergence of the proposed f-BRS-B method with ResNet-50 backbone on Berkeley dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Some challenging examples from Berkeley dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Some of the worst examples from DAVIS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of the proposed methods with ResNet-50 backbone with and without Zoom-In (ZI) on GrabCut and Berkeley datasets using NoC@90 (seeSection 5).</figDesc><table><row><cell>and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Convergence analysis on Berkeley, SBD and DAVIS datasets. We report the number of images that were not correctly segmented after 20 and 100 clicks and the NoC100@90 performance measure.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Evaluation results of GrabCut, Berkeley, SBD and DAVIS datasets. The best and the second best results are written in bold and underlined respectively. Comparison of the results without BRS and with f-BRS types A, B and C with ResNet-50 backbone.</figDesc><table><row><cell>Method</cell><cell>NoC@90</cell><cell cols="2">Berkeley #images ≥20</cell><cell>SPC</cell><cell cols="2">Time, s NoC@90</cell><cell>Davis #images ≥20</cell><cell cols="2">SPC Time, s</cell></row><row><cell>Ours w/o BRS</cell><cell>5.18</cell><cell>12</cell><cell cols="2">0.091</cell><cell>49.9</cell><cell>8.18</cell><cell>92</cell><cell>0.21</cell><cell>585.9</cell></row><row><cell>Ours RGB-BRS</cell><cell>4.08</cell><cell>4</cell><cell cols="2">1.117</cell><cell>455.7</cell><cell>7.58</cell><cell>72</cell><cell cols="2">2.89 7480.8</cell></row><row><cell>Ours DistMap-BRS</cell><cell>4.17</cell><cell>4</cell><cell cols="2">0.669</cell><cell>276.4</cell><cell>7.93</cell><cell>73</cell><cell cols="2">1.47 4028.4</cell></row><row><cell>Ours f-BRS-A</cell><cell>4.36</cell><cell>3</cell><cell cols="2">0.281</cell><cell>119.3</cell><cell>7.54</cell><cell>72</cell><cell cols="2">0.75 1980.5</cell></row><row><cell>Ours f-BRS-B</cell><cell>4.34</cell><cell>2</cell><cell cols="2">0.132</cell><cell>55.07</cell><cell>7.81</cell><cell>78</cell><cell>0.32</cell><cell>889.4</cell></row><row><cell>Ours f-BRS-C</cell><cell>4.91</cell><cell>8</cell><cell cols="2">0.138</cell><cell>61.4</cell><cell>7.91</cell><cell>84</cell><cell>0.31</cell><cell>848.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/saic-vul/fbrs_interactive_ segmentation</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Analysis of the average IoU according to the number of clicks</head><p>We computed the mean IoU score according to the number of clicks for GrabCut, Berkeley, SBD and DAVIS datasets (see <ref type="figure">Figure 6</ref>). We also evaluated the BRS <ref type="bibr" target="#b14">[15]</ref> model from authors' public repository for a fair comparison.</p><p>On the plots you can see that f-BRS-B has drops on DAVIS and SBD datasets at the number of clicks 9. This is due to the fact that f-BRS can sometimes fall into a bad local minimum. This issue can be solved by setting a higher regularization coefficient λ in the BRS loss function. However, with the increase of the λ, the convergence of the method at a large number of clicks becomes worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Measuring the limitation of f-BRS</head><p>We decided to find out the limit of accuracy that can be obtained using only f-BRS, adjusting scales and biases for an intermediate layer in the DeepLabV3+ head. For this, we first evaluated the model for 20 clicks using the standard protocol. Then we continued with L-BFGS-B optimization for scales and biases using ground truth mask as loss target instead of interactive clicks. It equals to using all pixels of the image as input clicks (positive click for each foreground pixel and negative for each background pixel). We estimated the mean IoU score for each dataset which is shown in <ref type="figure">Figure 6</ref> (f-BRS-B Oracle).</p><p>The figure illustrates that the accuracy limit the algorithm can reach is highly dependent on the dataset. DAVIS and SBD datasets are much harder than GrabCut and Berkeley. DAVIS has many complex masks labeled with pixel perfect precision, which is closer to the task of image matting. On the contrary, SBD has many masks with rough or inaccurate annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Full evaluation results for all our methods</head><p>We report the NoC@85 and NoC@90 metrics for Grab-Cut, Berkeley, SBD and DAVIS datasets for all BRS variations with different backbones (ResNet-34, ResNet-50 and ResNet-101). The use of BRS leads to consistent improvement in accuracy. All these results are presented in <ref type="table">Table 5</ref>.</p><p>Overall, the choice of a backbone only slightly affects the methods' accuracy on GrabCut and Berkeley datasets. However, we noticed a significant difference between ResNet-34 and ResNet-101 while testing on SBD validation dataset, which has the closest distribution to the training one. In most cases, DistMap-BRS shows slightly worse NoC compared to RGB-BRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Additional interactive segmentation results</head><p>We also provide more results of our interactive segmentation algorithm (f-BRS-B with ResNet-50) on different images. <ref type="figure">Figure 7</ref> and 8 represent good cases, while <ref type="figure">Figure 9</ref> represents bad cases when testing on Berkeley dataset. <ref type="figure">Figure 10</ref> shows some of the worst results of testing on DAVIS dataset. The algorithm does not even match 85% IoU in 20 clicks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Loss function ablation</head><p>We use normalized focal loss (NFL) introduced in <ref type="bibr" target="#b29">[29]</ref> as an alternative to binary cross entropy (BCE) in our experiments. NFL retains the advantages of focal loss <ref type="bibr" target="#b20">[20]</ref> and allows to concentrate the training process on erroneous regions and at the same time, the total gradient of NFL does not fade over time and remains equal to the total gradient of BCE. Thus training with NFL leads to faster convergence and better accuracy compared to training with BCE. We provide an ablation study for all our models trained with NFL and BCE loss functions in <ref type="table">Table 6</ref> (NFL * denotes results that were obtained with the latest code from our public GitHub repository 1 with minor technical improvements compared to the original code).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive full image segmentation by considering all regions jointly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11622" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Error-tolerant scribbles based interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale interactive object segmentation with human annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11700" to="11709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-P</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings eighth IEEE international conference on computer vision</title>
		<meeting>eighth IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic zoom-in network for fast object detection in large images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6926" to="6935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geodesic star convexity for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3129" to="3136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interactive image segmentation via backpropagating refinement scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonparametric higher-order learning for interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tae Hoon Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Uk</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="3201" to="3208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lazy snapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="308" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interactive image segmentation with latent diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regional interactive image segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sim-Heng</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive object detection using adjacency and zoom prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2351" to="2359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Content-aware multilevel guidance for interactive instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumajit</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11602" to="11611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="616" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparative evaluation of interactive segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><forename type="middle">E</forename><surname>Oconnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="434" to="444" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">grabcut&quot; interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptis: Adaptive instance selection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Seednet: Automatic seed generation with deep reinforcement learning for robust interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwangmo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesoo</forename><surname>Myeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1760" to="1768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Growcut: Interactive multi-label nd image segmentation by cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Konouchine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In proc. of Graphicon</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="150" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Citeseer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1084" to="1102" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sbd</forename><surname>Berkeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Datasets</surname></persName>
		</author>
		<title level="m">Table 6. Comparison between NFL and CEL losses on GrabCut</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
