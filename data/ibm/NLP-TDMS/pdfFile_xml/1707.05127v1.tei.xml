<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Reranking for Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
							<email>feidong@mymail.sutd.edu.sgyuezhang@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Reranking for Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a neural reranking system for named entity recognition (NER). The basic idea is to leverage recurrent neural network models to learn sentence-level patterns that involve named entity mentions. In particular, given an output sentence produced by a baseline NER model, we replace all entity mentions, such as Barack Obama, into their entity types, such as PER. The resulting sentence patterns contain direct output information, yet is less sparse without specific named entities. For example, "PER was born in LOC" can be such a pattern. LSTM and CNN structures are utilised for learning deep representations of such sentences for reranking. Results show that our system can significantly improve the NER accuracies over two different baselines, giving the best reported results on a standard benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Shown in <ref type="figure" target="#fig_0">Figure 1</ref>, named entity recognition aims to detect the entity mentions in a sentence and classify each entity mention into one out of a given set of categories. NER is typically solved as a sequence labeling problem. Traditional NER systems use Hidden Markov Models (HMM) <ref type="bibr" target="#b30">(Zhou and Su, 2002)</ref> and Conditional Random Fields (CRF) <ref type="bibr" target="#b14">(Lafferty et al., 2001)</ref> with manually defined discrete features. External resources such as gazetteers and human defined complex global features are also incorporated to improve system performance <ref type="bibr" target="#b21">(Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b2">Che et al., 2013)</ref>. Recently, deep neural network models have shown the ability of learning more abstract features compared with traditional statistical models with indicator features for NER <ref type="bibr">(Zhang et</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2015).</head><p>Recurrent Neural Network (RNN), in particular Long Short-Term Memory (LSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref>, shows the ability to automatically capture history information over input sequences, which makes LSTM a proper automatic feature extractor for sequence labeling tasks. Different methods have been proposed by stacking CRF over LSTM in NER task <ref type="bibr" target="#b4">(Chiu and Nichols, 2015;</ref><ref type="bibr" target="#b15">Lample et al., 2016;</ref><ref type="bibr" target="#b18">Ma and Hovy, 2016)</ref>. In addition, it is possible to combine discrete and neural features for enriched information, which helps improve sequence labeling preformance .</p><p>Reranking is a framework to improve system performance by utilizing more abstract features. A reranking system can take full advantage of global features, which are intractable in baseline sequence labelling systems that use exact decoding. The reranking method has been used in many NLP tasks, such as parsing <ref type="bibr" target="#b6">(Collins and Koo, 2005)</ref>, QAs <ref type="bibr" target="#b3">(Chen et al., 2006)</ref> and machine translation <ref type="bibr" target="#b25">(Wang et al., 2007;</ref><ref type="bibr" target="#b22">Shen et al., 2004)</ref>.</p><p>Some work has adopted the reranking strategy for NER. <ref type="bibr" target="#b5">Collins (2002)</ref>   task, also obtaining slight improvemts. All the above methods use sparse manual features. To the best of our knowledge, there has been no neural reranking model for NER task.</p><p>In this paper, we propose a simple neural reranking model for NER. The model learns sentence patterns that involve output named entities automatically, using neural network. Take the sentence "Barack Obama was born in hawaii ." as an example, <ref type="figure" target="#fig_1">Figure 2</ref> illustrates several candidate sentence patterns such as "PER was born in LOC ." (C 3 ) and "LOC was born in hawaii ." (C 2 ), where PER represents entity type persons and LOC means locations. It is obvious that C 3 is a much more reasonable sentence pattern compared to C 2 . To generate the sentence patterns above, we replace predicted entities in candidate sequences with their entity type names. This can effectively reduce the sparsity of candidate sequences, as each entity type contains open vocabulary names (e.g. PER can be Donald Trump, Hillary Clinton etc.), which can bring noise when learning the sentence patterns. In addition, since the learned sentence patterns are global over output structures, it is difficult for baseline sequence labeling systems to capture such patterns.</p><p>We develop a neural reranking model which captures candidate pattern features using LSTM and auxilliary neural structures, including Convolution Neural Network (CNN) <ref type="bibr" target="#b12">(Kim, 2014;</ref><ref type="bibr" target="#b10">Kalchbrenner et al., 2014)</ref> and character based neural features. The learned global sentence pattern representations are then used as features for scoring by the reranker. Results over a state-of-the-art discrete baseline using CRF and a state-of-the-art neural baseline using LSTM-CRF show significant improvements. On CoNLL 2003 test data, our model achieves the best reported result.</p><p>Our main contributions include (a) leveraging global sentence patterns that involve entity type information for NER raranking, (b) exploiting aux-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description</head><p>Feature Template word grams </p><formula xml:id="formula_0">w i , w i w i+1 shape, capital Sh(w i ), Ca(w i ) capital + word Ca(w i )w i connect word Co(w i ) capital + connect Ca(w i )Co(w i ) cluster grams Cl(w i ), Cl(w i w i+1 ) prefix, suffix P r(w i ), Su(w i ) POS grams P (w i , w i w i+1 , w i−1 w 1 w i+1 ) POS + word P (w 0 )w 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baselines</head><p>Formally, given a sentence S with t words: S = {w 1 , w 2 , ..., w t }, the task of NER is to find out all the named entity mentions from S. The dominate approach takes the task as a sequence labelling problem, where the goal is to generate a label sequence L = {l 1 , l 2 , ..., l t }, where l i = p i e i . Here p i is an entity label, p i ∈ {B, I, O}, where B indicates the beginning of an entity mention, I denotes a non-beginning word of a named entity mention and O denotes a non-named-entity word 1 . e i indicates the entity type. In the CoNLL dataset that we use for our experiments, e i ∈ {PER, ORG, LOC , MISC }, where "P ER" indicates a person name; "LOC", "ORG", "M ISC" represent location, organization and miscellaneous, respectively. We choose two baseline systems, one using dicrete CRF with handcrafted features and one using neural CRF model with bidirectional LSTM struc-ture, both baselines giving the state-of-the-art accuracies among their respective category of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Discrete CRF</head><p>We choose a basic discrete CRF model as our baseline tagger. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>(a), discrete word features are first extracted as binary vectors (black and white circles) and then fed into a CRF layer. Taking those discrete features as input, the CRF layer can give n-best predicted sequences as well as their probabilities. <ref type="table" target="#tab_2">Table 1</ref> shows the discrete features that we used, which follow the definition of . Here shape means whether characters in word are belonging to number, English character or not. capital is the indication if word starts with upper-case English character, connect words include five types: "of", "and", "for", "-" and other. Prefix and suffix include the 4-level prefixes and suffixes of each words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural CRF</head><p>A neural CRF with bidirectional LSTM structure is used as our second baseline, which is shown in <ref type="figure" target="#fig_3">Figure 3</ref>(b). Word representations are represented with continious vectors (gray circles), which are fed into a bidirectional LSTM layer to extract neural features. A CRF layer with n-best output is stacked on top of the LSTM layer to decode the label sequences based on the neural features. We use the neural structure of <ref type="bibr" target="#b18">Ma and Hovy (2016)</ref>, where the word representation is the concatenation of word embedding and a CNN output on the character sequence of the word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reranking Algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Collapsed Sentence Representation</head><p>Given the n-best output label sequences of a base-</p><formula xml:id="formula_1">line system {L 1 , L 2 , ..., L i , ..., L n }, where L i = {l i1 , l i2 , .</formula><p>.., l it }, we learn a reranking score s(L i ) for L i by firsting converting L i into a sequence pattern C i , and then learning a representation h(C i ) as its dense representation. To convert candidate sequence L i to collapsed sequence C i . We use the following rules to convert each label sequence L i into a collapsed sentence pattern C i .</p><p>If the L i include entity labels (e.g. l i1 =B-PER, l i2 =I-PER), then the entity labels are replaced with the corresponding entity type name (e.g. {l i1 , l i2 } → PER, C i1 = PER), else labels are replaced by its corresponding words (C ix = w x ). In the example  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Features</head><p>Given a collapsed sentence representation C i , we use neural network to learn its overall representation vector h(C i ), which is used for the scoring of C i . Word Representation: We use SENNA (Collobert et al., 2011) embedding to initialize the word embedding of our reranking system. For out of vocabulary words , embeddings are randomly</p><formula xml:id="formula_2">initialized within (− 3.0 wordDim , 3.0 wordDim ),</formula><p>where wordDim is the word dimension size <ref type="bibr" target="#b18">(Ma and Hovy, 2016)</ref>.</p><p>Character features are proved useful in capturing morphological features, such as word similarity and dealing with the out-of-vocabulary problem <ref type="bibr" target="#b16">(Ling et al., 2015)</ref>. As shown in <ref type="figure">Figure 4</ref>(a), we follow <ref type="bibr" target="#b18">Ma and Hovy (2016)</ref>   the embedding layer. Then we use a max-pooling layer to map varying length vectors into a fixed size output vector. Finally, word representation is the concatenation of character CNN output vectors and word embeddings. LSTM features: We choose a word-based LSTM as the main network, using it for capturing global sentence pattern information. For input sequence vectors {x 1 , x 2 , ..., x t }, our LSTM model is implemented as follows:</p><formula xml:id="formula_3">h t = tanh(M t ) o t i t = σ(W 1 h t−1 + W 2 x t + µ 1 M t−1 + b 1 ) f t = σ(W 3 h t−1 + W 4 x t + µ 2 M t−1 + b 2 ) M i = tanh(W 5 y t−1 + W 6 x i + b 3 ) M t = i i M i + f i M t−1 o t = σ(W 7 h t−1 + W 8 x t + b 4 ),</formula><p>where is the element-wise multiply operator, σ is the sigmoid function, and {W, b, µ} ∈ Θ are parameters. i t , f t , M t and o t are the input gate, forget gate, memory cell and output gate, respectively. h t is the hidden vector at step t in the input sentence. As shown in <ref type="figure">Figure 4(b)</ref>, word representations are the concatenation of word embeddings and character CNN output (red block). We choose the hidden vector in last word h LST M as the representation of the input sequence.</p><p>CNN features: We introduce CNN to capture local features of the candidate sequences. It con-sists of a filter W ∈ R h×k which operates on a context of k words to produce local order features. Max pooling layer is employed over the convolutional layer to extract the most salient features. Assume u j is the concatenation of word representations in Eq. (1) centralized in the embedding z j in a given sequence u 1 , u 2 , ..., u L , CNN applies a matrix-vector operation to each window of size k successive window along the sequence in Eq. <ref type="formula" target="#formula_5">(2)</ref>.</p><formula xml:id="formula_4">u j = (z j−(k−1)/2 , ..., z j+(k−1)/2 )<label>(1)</label></formula><formula xml:id="formula_5">r i = max 1&lt;j&lt;L (W u j + b) i , i = 1, ..., d,<label>(2)</label></formula><p>where z j is the j-th word embedding in the given sequence, d is the output dimension of the CNN. h = [r 1 , ..., r i , ..., r d ] is the fixed-size feature representation for the sequence after pooling. The CNN representation structure is similar to <ref type="figure">Figure 4</ref>(a) but its input is word representations rather than character embeddings. We define the CNN features of word sequence as h CN N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Score Calculation</head><p>After the LSTM and CNN features of collapsed sequence C i are extracted, we concatenate them together and feed the result into a softmax layer.</p><formula xml:id="formula_6">h(C i ) = h LST M ⊕ h CN N s(C i ) = σ(W h(C i ) + b),<label>(3)</label></formula><p>where ⊕ represents the concatenating operation, h(C i ) is the final representation of collapsed sequence C i and s(C i ) is the output score of C i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoding</head><p>We use a mixture reranking strategy during decoding. Denote the candidate label sequence set on sentence S as C(S) = {C 1 , C 2 , ..., C n } . We take advantage of both the reranker prediction score and the baseline tagger's output probability, using the scorê</p><formula xml:id="formula_7">y i = arg max C i ∈C(S) (αs(C i ) + (1 − α)p(L i )),<label>(4)</label></formula><p>where α ∈ [0, 1] is an interpolation weight, which is a hyperparameter tuned on the development set. p(L i ) is the probability of label sequence L i in the baseline tagger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training</head><p>For each training triplet {S, L i , C i }, given the golden sequence L golden , we calculate the tag accuracy y i ∈ [0, 1] of each candidate sequence  based on L i and L golden . The same decoding process is applied to each collapsed sequence (C i , y i ).</p><p>We use a logistic regression model with mean square error (MSE) as the loss function, with a l 2regulation term 3 :</p><formula xml:id="formula_8">J(Θ) = 1 |D| (C i ,y i )∈D (y i −s(C i )) 2 + λ 2 ||Θ|| 2 2 (5)</formula><p>where Θ are all the parameters to be trained, D is the training set and λ is the regulation factor. Adam <ref type="bibr" target="#b13">(Kingma and Ba, 2014)</ref> is used to update model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>We use CRF++ 4 as our discrete baseline CRF implementation and default parameters are used. For neural baseline, we follow the same structure and settings of the state-of-the-art system <ref type="bibr" target="#b18">(Ma and Hovy, 2016)</ref>. When building the neural reranking system, SENNA embedding with 50 dimensions is used to initialize word embeddings. Hyperparameters of reranking system are listed in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>As we use the mixture strategy in Eq. (4) during decoding, we search the ideal interpolation weight α within [0, 1] in a step of 0.005 based on the preformance under the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reranking Data</head><p>All of our experiments are evaluated on the standard CoNLL 2003 English dataset (Tjong <ref type="bibr" target="#b24">Kim Sang and De Meulder, 2003)</ref>, which is a collection of Reuters newswire articles. The To construct the reranking training data, we conduct five-fold jackknifing, spliting the training set into 5 equal parts. In each case, the baseline tagger trains the model with 4/5 of the data and decode the remaining 1/5 to generate n-best candidate label sequences. For the reranking development and test data, the full training set is used to encode baseline tagger and decode development/test sentences with n-best output. All the n-best candidate sequences are converted into collapsed sequences following Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Oracle Results</head><p>The discrete baseline achieves 92.13% of F1measure in development set and 88.15% in test set. Our neural baseline gives 94.58% and 91.25% on development and test data, respectively. The discrete baseline for example. <ref type="figure" target="#fig_5">Figure 5</ref> shows different oracle scores varying with n-best in discrete baseline. The oracle best is obtained by always chooseing the best sequence in the n-best candidates, and vice versa for the oracle worst. The orcale best sentence accuracy (OBA) 5 represents the accuracy of the sequence choice within the n-best candidates under oracle best assumption, and the orcale best F1-value (OBF) is the corresponding F1-value similarly. Orcale worst F1-value (OWF) is the F1-value under the worst choice situation.</p><p>As the figure shows, the larger n is, the better is the OBA, which means that a potentially better reranking result is possible. On the other hand, the OWF also drops, which means that the reranking task is more difficult. In our experiments, n-best is set as 10, the orcale best F1-value of test set achieves 97.13% (+8.98%) while its orcale worst F1-value drops 49.07% to 39.08%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Influence of Sentence Length</head><p>We perform development experiments to evaluate model performance on various sentence lengths. <ref type="figure" target="#fig_6">Figure 6</ref> shows results by reranking the discrete baseline. Here sentence select accuracy (SSA) is calculated using the corrected number of sentences divided by the total number of sentences. The x-axis is the sentence length range (e.g. 10 means sentence length range from 5 to 10), while the y-axis corresponds to SSA within 10-best candidates before the mixture strategy (without mixing baseline output probability). As shown in the <ref type="figure" target="#fig_6">Figure 6</ref>, the accuracies of all model settings drop as the size of the sentence increases, which demonstrates that longer sentences are more challenging to our neural rerankers as they are to the baseline models. This is because for longer sentences, candidate collapsed sequences have higher overlapped proportion and hence are hard to be distinguished by reranker. Both character information and CNN local features are useful for enhancing the SSA over a LSTM-only baseline. With the integration of character information and CNN features, our full model reranker can improve its performance on all sentence length ranges, especially for long sentences.   We believe that our model will benifit more from the NER corpus with fine-grained entity type. <ref type="table" target="#tab_7">Table 3</ref> shows the F1-value and SSA (after mixing baseline output probability) of our reranker on test data with different neural features on the discrete baseline. The word based LSTM reranker achieves the F1-value of 88.75%, with 0.6% absolute improvement over the baseline tagger. Cooperating with CNN features on word only does not make much improvement, while character CNN features are more effective (+0.78%). However, the full combination of character representation and word CNN features improves the F-value to 89.25% (+1.10%) with the significance level of p &lt; 0.05 with t-test. The trend of SSA is the same as the F1-value, the accuracy is improved from the baseline 83.31% to 85.12% using the full model reranker, with an absolute improvement of 1.82%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Influence of Entity Type</head><p>Discrete Model (%) F1 <ref type="bibr" target="#b11">Kazama and Torisawa (2007)</ref> 88.02 <ref type="bibr" target="#b23">Suzuki and</ref><ref type="bibr">Isozaki (2008) 89.92 Nguyen et al. (2010)</ref> 88.16 <ref type="bibr" target="#b21">Ratinov and Roth (2009)</ref> 88.55 <ref type="bibr" target="#b21">Ratinov and</ref><ref type="bibr">Roth (2009)* 90.57 Luo et al. (2015)</ref> 91.20 Discrete baseline 88.13 Our reranker 89.25 Neural Model (%) F1 <ref type="bibr" target="#b7">Collobert et al. (2011)</ref> 89.59 <ref type="bibr" target="#b20">Passos et al. (2014)</ref> 90.90  90.10 Chiu and Nichols <ref type="formula" target="#formula_4">(2015)</ref> 90.77 <ref type="bibr" target="#b15">Lample et al. (2016)</ref> 90.94 <ref type="bibr" target="#b18">Ma and Hovy (2016)</ref> 91.21 Neural baseline 91.25 Our reranker 91.62  <ref type="table" target="#tab_8">Table 4</ref> shows our rerank results on two baselines and the comparison with state-of-the-art systems. Our reranker on discrete baseline compares favourably to the best discrete models, including the use of external corpus <ref type="bibr" target="#b11">(Kazama and Torisawa, 2007;</ref><ref type="bibr" target="#b23">Suzuki and Isozaki, 2008</ref>  . <ref type="bibr" target="#b15">Lample et al. (2016)</ref> and <ref type="bibr" target="#b18">Ma and Hovy (2016)</ref>   91.25% in F-value. Our reranker on this baseline outperforms all the previous models with the F-value of 91.62%, which is the best reported Fscore on CoNLL 2003. <ref type="figure" target="#fig_8">Figure 8</ref> gives some example outputs on the development dataset for which discrete baseline gives incorrect outputs yet the reranker corrects the mistake. Our reranker learns better sentence patterns by correcting both named entity boundary errors and named entity type errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effectiveness of Reranking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Examples</head><p>In the first case, example 1 shows that "U.N. Ambassador Albright" in sentence "U.N. Ambassador Albright arrives in Chile ." is incorrectly tagged as a organization by the baseline and the entity boundary is incorrect either. By building the collapsed sentences as the input of our reranker, entities such as "U.N. Ambassador Albright" are replaced as a single entity name "ORG". Our reranking model learns that "... PER arrives in LOC ..." is more possible compared to "... ORG arrives in LOC ...", thereby the candidate with the reasonable entity boundary and type is picked by our reranker.</p><p>For the second case, the entity type of "EL SAL-VADOR" in example 3 "SOCCER -U.S. BEAT EL SALVADOR 3-1 ." is incorrectly recognized as organization by baseline. Our reranker corrects this entity type error by giving higher score to sentence pattern "... LOC BEAT LOC ..." rather than pattern "... LOC BEAT ORG ...".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a neural reranking architecture for NER by exploiting neural structure to learn sentence patterns. Given the candidate label sequences generated from a baseline tagger, we replace the predicted entity words with the corre-sponding entity type names to build collapsed sentences, which are used as inputs of a neural reranking model. A mixture reranking strategy is used to combine both the knowledge of the probability from the baseline tagger and the reranker score. Experiments on both discrete and neural baselines show our reranking system improves NER performance significantly, obtaining the best results on CoNLL 2003 English task .</p><p>One problem of current method is that all the candidates share the same non-entity words, which lead their neural representations quite similar and hard to distinguish, especially for long sentences. In the future work, we will develop the neural tree structures based on entity position which can enlarge the difference between candidate sequences. Intuitively, we believe the entities contribute more than non-entity when modeling the sequence vector, attention model (Bahdanau et al., 2014) may help collect more information from the intermediate vector of sentences. Besides, Using semisupervised methods to construct a bigger training data can help reranker learn more sentence patterns. Moreover, we also want to bring in an auxilliary classifier of predicting the probability of the replaced words being a real entity, this insideentity information may be an important compensation for the outside-entity sentence patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Named Entity Recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example of generating collapsed sentence patterns from baseline NER output label sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Baselines. shown in Figure 2, S = {Barack Obama was born in hawaii .} and L 3 = {B-PER I-PER O O O B-LOC O}. The corresponding collapsed sequence is C 3 = {PER was born in LOC .}, Barack Obama and hawaii are regarded as entities and hence are replaced by the entity names, i.e. PER and LOC, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>Figure 4: Representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Oracle scores in baseline outputs. and 3,684 test sentences, annotated into 4 entity types, i.e. persons(PER), locations(LOC), organizations(ORG) and miscellaneous(MISC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>SSA with sentence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 Figure 7 :</head><label>77</label><figDesc>shows the comparision of models on different entity types. Compared with the baseline, entities with type of PER and ORG receive the F1-value comparision by entity types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Output examples. The first two examples illustrate the correction of entity boundary errors and the followings show the correction of entity type errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>al., [Barack Obama] P ER was born in [hawaii] LOC . Rare [Hendrix] P ER song draft sells for almost $ 17,000 . [Volkswagen AG] ORG won 77,719 registrations . [Burundi] LOC disqualification from [African Cup] M ISC confirmed . The bank is a division of [First Union Corp] ORG .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Features of discrete CRF for NER, i ∈</cell></row><row><cell>{−1, 0}.</cell></row><row><cell>illiary neural features to enrich basic LSTM se-</cell></row><row><cell>quence representation and (c) achieving the best</cell></row><row><cell>F1 result on CoNLL 2003 data. The source codes</cell></row><row><cell>of this paper are released under GPL at https:</cell></row><row><cell>//github.com/jiesutd/RerankNER.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>by utilizing CNN to extract character-level representation 2 . Input character sequences are firstly passed through the embedding layer to lookup the character embeddings. To extract local features, a convolution layer with a fixed window-size is applied on top of</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>h c</cell><cell></cell><cell></cell></row><row><cell cols="3">Convolution:</cell><cell></cell><cell cols="2">Pooling</cell><cell></cell><cell></cell></row><row><cell cols="3">Embedding:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lookup:</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Embedding layer</cell><cell></cell></row><row><cell>Sequence:</cell><cell cols="3">. . . Pad Pad</cell><cell>O</cell><cell>b</cell><cell>a</cell><cell>m</cell><cell>a</cell></row><row><cell cols="9">(a) CNN character sequence representation for word.</cell></row><row><cell cols="2">LSTM:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>h LSTM</cell></row><row><cell cols="2">Embedding:</cell><cell>hc</cell><cell>hc</cell><cell>hc</cell><cell>hc</cell><cell>hc</cell><cell>hc</cell><cell>hc</cell></row><row><cell cols="2">Lookup:</cell><cell></cell><cell></cell><cell cols="2">Embedding layer</cell><cell></cell><cell></cell></row><row><cell cols="2">Sequence:</cell><cell cols="3">PER Obama was born</cell><cell cols="3">in LOC</cell><cell>.</cell></row><row><cell cols="9">(b) LSTM word sequence representation for sentence.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Hyperparameters of reranker.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>F1-value and SSA on test set.</figDesc><table><row><cell>most improvements, showing that sentence pat-</cell></row><row><cell>terns are useful for those types. While the per-</cell></row><row><cell>formance on entities with type of MISC decreases</cell></row><row><cell>slightly, since the MISC includes various entity</cell></row><row><cell>types which bring noise on learning sentence pat-</cell></row><row><cell>terns.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparison of state-of-the-art systems.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When pi = O, ei equals to NULL.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Characters are padded into a fixed length by using a special token Pad.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We also tried max-margin criterion like<ref type="bibr" target="#b31">(Zhu et al., 2015)</ref>, while the results are similar with regression model. 4 https://taku910.github.io/crfpp/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Notice this is different with accuracy which represents the correct rate of tags, OBA represents the correct rate in sentence level.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insightful comments. Yue Zhang is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Reranker 2 [West Indian] M ISC all-rounder [Phil Simmons] P ER took four</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<editor>Kyunghyun Cho, and Yoshua Bengio</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Reranker 4 ... prisoners are held in [Rangoon] LOC &apos;s [Insein Prison] LOC . Baseline 5 [PAKISTAN] LOC WIN TOSS , PUT [ENGLAND] ORG INTO BAT. Reranker 5 [PAKISTAN] LOC WIN TOSS , PUT [ENGLAND] LOC INTO BAT. References Dzmitry Bahdanau</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Named entity recognition with bilingual constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reranking answers for definitional qa using language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
		<title level="m">Named entity recognition with bidirectional lstm-cnns</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ranking algorithms for namedentity extraction: Boosting and the voted perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting wikipedia as external knowledge for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classification</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth international conference on machine learning</title>
		<meeting>the eighteenth international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02096</idno>
		<title level="m">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint named entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01354</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernel-based reranking for named-entity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Truc-Vien T Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">Riccardi</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mc-Callum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5367</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative reranking for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reranking machine translation hypotheses with structured and web-based language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition &amp; Understanding</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining discrete and neural features forsequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reranking for biomedical named-entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiro</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Libn3l: a lightweight package for neural nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural networks for open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on EMNLP</title>
		<meeting>the 2015 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="612" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Named entity recognition using an hmm-based chunk tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05667</idno>
		<title level="m">A re-ranking model for dependency parser with recursive convolutional neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
