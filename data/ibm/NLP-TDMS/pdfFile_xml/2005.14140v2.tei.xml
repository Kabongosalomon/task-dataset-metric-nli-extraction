<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling the Distribution of Normal Data in Pre-Trained Deep Features for Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Rippel</surname></persName>
							<email>oliver.rippel@lfb.rwth-aachen.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mertens</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Merhof</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Imaging &amp; Computer Vision</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Imaging &amp; Computer Vision</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Imaging &amp; Computer Vision</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling the Distribution of Normal Data in Pre-Trained Deep Features for Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly Detection (AD) in images is a fundamental computer vision problem and refers to identifying images and/or image substructures that deviate significantly from the norm. Popular AD algorithms commonly try to learn a model of normality from scratch using task specific datasets, but are limited to semi-supervised approaches employing mostly normal data due to the inaccessibility of anomalies on a large scale combined with the ambiguous nature of anomaly appearance.</p><p>We follow an alternative approach and demonstrate that deep feature representations learned by discriminative models on large natural image datasets are well suited to describe normality and detect even subtle anomalies in a transfer learning setting. Our model of normality is established by fitting a multivariate Gaussian (MVG) to deep feature representations of classification networks trained on ImageNet using normal data only. By subsequently applying the Mahalanobis distance as the anomaly score we outperform the current state of the art on the public MVTec AD dataset, achieving an Area Under the Receiver Operating Characteristic curve of 95.8 ± 1.2% (mean ± SEM) over all 15 classes. We further investigate why the learned representations are discriminative to the AD task using Principal Component Analysis. We find that the principal components containing little variance in normal data are the ones crucial for discriminating between normal and anomalous instances. This gives a possible explanation to the often subpar performance of AD approaches trained from scratch using normal data only. By selectively fitting a MVG to these most relevant components only, we are able to further reduce model complexity while retaining AD performance. We also investigate setting the working point by selecting acceptable False Positive Rate thresholds based on the MVG assumption. Code is publicly available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Anomaly Detection (AD) relates to identifying instances in data that are significantly different to the norm <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Correspondingly, AD in images aims at finding irregularities in images and poses a fundamental computer vision problem with various application domains ranging from industrial quality control <ref type="bibr" target="#b2">[3]</ref> to medical image analysis <ref type="bibr" target="#b3">[4]</ref>. In general, AD tasks are defined by the following two characteristics:</p><p>• Anomalies are rare events, i.e. their prevalence in the application domain is low. • Anomaly appearance is not well-defined (i.e. anomalies types are ambiguous). * Authors contributed equally to this work Together, these characteristics result in AD datasets that are heavily imbalanced, often containing only few anomalies for model verification and testing.</p><p>As a consequence, AD algorithms often focus on semisupervised learning approaches, where a model of normality is established based on normal data only <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>. While small dataset sizes predestine the capitalization of pre-training on large-scale databases such as ImageNet <ref type="bibr" target="#b5">[6]</ref>, only little research is performed to explore this potential <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. Instead, methods focus on learning feature representations from scratch, often in reconstruction-based approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>As our main contribution, we demonstrate the effectiveness of pre-trained deep feature representations transferred to the AD task. By fitting a multivariate Gaussian (MVG) to normal data of deep features learned by ImageNet training and using the Mahalanobis distance <ref type="bibr" target="#b10">[11]</ref> as the anomaly score, we are able to outperform the prior state of the art on the public MVTec AD dataset <ref type="bibr" target="#b2">[3]</ref>. We additionally gain insight into and explain the discriminative nature of pre-trained deep features by means of Principal Component Analysis (PCA). Here, we find that principal components that retain little variance in normal data are highly discriminative to the AD task, indicating that learning these features from scratch may be difficult using normal data only. We further show that the working point can be sensibly set based on choosing an acceptable False Positive Rate (FPR) under the MVG assumption. Here, retaining only highly variant principal components decreases FPR at the cost of AD performance. These results demonstrate that there should be a clear focus on leveraging pre-trained deep feature representations in future AD research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In recent years, a large body of research has been published in the field of AD. Therefore we provide an extensive overview of AD techniques in the following, focussing on methods applied to image data. We further categorize the approaches into whether they leverage pre-trained deep feature representations in a transfer learning approach or are learned from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning AD from Scratch</head><p>Learning useful representations from scratch in a semi-or unsupervised manner is a major research field on its own.</p><p>Out of the multitude of ways of learning such representations, autoencoder-based approaches are the most popular in AD.</p><p>Here, autoencoders (AEs) try to learn the identity function in a semi-supervised manner using a given set of exclusively normal training images. The learned identity function is constrained, whereas the model first has to compress the input image to a low dimensional embedding, and subsequently has to reconstruct the input image based on this embedding. It is argued that the overall model cannot represent anomalous image structures, reconstructing a plausible normal image instead. An AE trained until convergence can then be used for AD in different ways:</p><p>Anomalous images can be detected by comparing the input test image with its reconstruction yielded by the model. There have been various proposals employing this reconstruction for AD <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. While the results of reconstructionbased AD approaches are intuitive to understand, they suffer from two drawbacks: (I) The reconstruction has to be postprocessed in order to yield an image-level anomaly score, thus increasing the complexity of the method and (II) the decoder part introduces additional computational overhead.</p><p>Embeddings learned by AEs are also utilized in many AD frameworks. Common approaches try to model the distribution of normal data in the AE embedding in a generative way using variational AEs <ref type="bibr" target="#b13">[14]</ref> that are oftentimes trained using an adversarial objective <ref type="bibr" target="#b14">[15]</ref>. Alternatively, classical shallow ML methods such as k-Nearest Neighbor (k-NN) or one class Support Vector Machine (oc-SVM) <ref type="bibr" target="#b15">[16]</ref> are also applied to embeddings learned by an AE <ref type="bibr" target="#b16">[17]</ref>. More recently, Ruff et al. have initialized their proposed Deep Support Vector Data Description (Deep SVDD) using pre-trained AEs <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>Hybrid approaches also exist, where anomaly scores are generated by combining measures proposed on the embeddings with reconstruction errors <ref type="bibr" target="#b19">[19]</ref>- <ref type="bibr" target="#b21">[21]</ref>, enhancing model performance at cost of further increased complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transfer Learning AD with Deep Feature Representations</head><p>Less extensively studied than semi-supervised feature learning methods, AD has also been performed by using deep representations learned by large-scale ImageNet training in a transfer learning setting for both anomaly segmentation and image-level AD.</p><p>While there has been recent success in adapting deep feature representations for anomaly segmentation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, these proposals compute features patch-wise to yield the pixelwise output. As a consequence, receptive fields are limited, feature complexity is rather low and there is an implicit assumption that the anomalies fit inside one patch. Further, segmentations have to be aggregated to yield image-level AD. Regarding image-level AD, Christiansen et al. <ref type="bibr" target="#b24">[24]</ref> repurpose deep AlexNet <ref type="bibr" target="#b25">[25]</ref> and VGG <ref type="bibr" target="#b26">[26]</ref> features for agricultural anomalous object detection. While they also fit a MVG to deep feature representations and use the Mahalanobis distance as an anomaly measure, they evaluate their model using a small in-house dataset only. Further, in their use-case anomalous instances deviate significantly in appearance from the normal class (as opposed to the subtle deviations present in the MVTec AD dataset), and benchmarking against other AD approaches is not performed. Also, they do not investigate the properties of the pre-trained feature representations that make them suitable to AD. Andrews et al. successfully fit an oc-SVM to deep representations learned by VGG on ImageNet for AD in X-Ray scans of containers <ref type="bibr" target="#b7">[8]</ref>. Bergman et al. <ref type="bibr" target="#b27">[27]</ref> and Cohen et al. <ref type="bibr" target="#b8">[9]</ref> evaluate a k-NN using L2-distance on ResNet <ref type="bibr" target="#b28">[28]</ref> features. Here, Cohen et al. <ref type="bibr" target="#b8">[9]</ref> report an average Area Under the Receiver Operating Characteristic curve (AUROC) of 85.5% on the public MVTec AD dataset with k = 50 and average-pooled features extracted from the last convolutional layer of a Wide-ResNet50-2. Except for these and a 1-NN approach with different normalizations in surveillance videos <ref type="bibr" target="#b29">[29]</ref>, little notice has been given to employing deep features in AD for classifying full images.</p><p>While not directly used as an AD algorithm, Lee et al. also model the data distribution of in-distribution data by means of MVG for Out-Of-Distribution (OOD) detection <ref type="bibr" target="#b30">[30]</ref>. Contrary to AD, OOD determines whether a given query image is part of the in-distribution dataset (i.e. the dataset used for training) or OOD. Using a small subset of anomalies for fine-tuning, they apply the linear combination of Mahalanobis distances computed at various depths of a pre-trained ResNet <ref type="bibr" target="#b28">[28]</ref> to a test image. The test image is additionally pre-processed to maximize Mahalanobis distance by means of performing a single gradient ascent step to implicitly evaluate Probability Density Function (PDF) around the test image. They further show that discriminative deep classifiers employing softmax learn the same posterior distribution as generative classifiers under a Linear Discriminant Analysis (LDA) assumption. They expand on this and argue that the pre-trained features of the deep softmax classifier may also follow the class-conditional Gaussian distribution of the generative classifier. While they do not give a theoretical proof for this, the performance of a generative classifier based on pre-trained features is verified experimentally. They also show that prior unseen classes may be easily integrated into the generative classifier by introducing a new class to the LDA (compute new mean and update joint covariance). This finding is the motivation for our work, where we apply pre-trained deep feature representations to the AD task in a transfer learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODELING NORMAL DATA DISTRIBUTION IN DEEP FEATURE REPRESENTATIONS</head><p>Based on the findings of Lee et al. <ref type="bibr" target="#b30">[30]</ref>, we hypothesize that pre-trained deep representations can also be successfully applied to the AD task. Similar to the class-incremental learning approach, we directly model the PDF of each "normal" class in the pre-trained features, using normal data only and omitting fine-tuning of the pre-trained model.</p><p>We model the PDF using the MVG, defined as</p><formula xml:id="formula_0">ϕ µ,Σ (x) := 1 (2π) D |det Σ| e − 1 2 (x−µ) Σ −1 (x−µ) . (1)</formula><p>Here, D is the number of dimensions, µ ∈ R D is the mean vector and Σ ∈ R D×D the symmetric covariance matrix of the distribution. Σ must be positive definite. Under a Gaussian distribution with mean µ and covariance Σ, a distance measure between a particular point x ∈ R D and the distribution is called the Mahalanobis distance and defined as</p><formula xml:id="formula_1">M (x) = (x − µ) Σ −1 (x − µ).<label>(2)</label></formula><p>Introduced by Mahalanobis in 1936, M (x) is a useful measure of a sample's uncertainty <ref type="bibr" target="#b10">[11]</ref>. This interpretation stems from the fact that the Mahalanobis distance uniquely determines the probability density ϕ µ,Σ (x) of an observation. When x is sampled from the Gaussian distribution, M (x) 2 is chisquared distributed with k = D degrees of freedom. This χ 2 -distribution with k degrees of freedom is the sum of k conditionally independent standard normal random variables. Its PDF is given as</p><formula xml:id="formula_2">f k (x) =      x k 2 −1 e − x 2 2 k 2 Γ k 2 , x &gt; 0; 0, otherwise.<label>(3)</label></formula><p>Here, Γ(s) is the gamma function for s &gt; 0. The Cumulative Distribution Function (CDF) of the χ 2 -distribution is calculated as</p><formula xml:id="formula_3">F k (x) = γ( k 2 , x 2 ) Γ( k 2 )<label>(4)</label></formula><p>with the lower incomplete gamma function γ(s, x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Covariance Estimation</head><p>As the true distribution of the novel data in the deep feature spaces is unknown, the covariance matrix Σ needs to be approximated from observations x 1 , . . . , x n ∈ R D with the sample covariancê</p><formula xml:id="formula_4">Σ = 1 n − 1 n i=1 (x i −x) (x i −x) .<label>(5)</label></formula><p>Here,x denotes the empirical mean of the observations. However, the sample covariance matrix is only wellconditioned when the number of dimensions D is much lower than the number of samples n. If D n is non-negligible, the covariance estimate becomes unstable, and when D &gt; n, Σ becomes singular and hence is not invertible. To solve this problem the concept of shrinkage has been proposed for sample covariance estimation and will be used to estimate the sample covariance in this work.</p><p>Shrinkage is defined as a linear combination of empirically estimatedΣ and the (scaled) identity matrix I D ,</p><formula xml:id="formula_5">Σ shrunk = (1 − ρ)Σ + ρ tr(Σ) D I D<label>(6)</label></formula><p>with shrinkage intensity ρ. Thus, ρ regulates the influence of the empirical estimator on the final matrix, preferring the wellconditioned identity matrix for larger ρ. It can be seen as a bias-variance tradeoff between the biased, invariant identity estimate and the unbiased high-variance empirical covariance. By minimizing the expected squared error E[ Σ shrunk −Σ 2 ] to the true covariance, Ledoit, Wolf et al. obtain a closed form solution for the amount of shrinkage that allows optimal selection of ρ given an unstable estimate ofΣ <ref type="bibr" target="#b31">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Setting the Working Point</head><p>In the case of assuming an underlying MVG, the working point can be estimated based on probabilities. The idea is that if a specific Mahalanobis distance corresponds to a probability p of seeing a normal sample, this matches the expected True Negative Rate (TNR) of a detector thresholded at that distance. 1 − p can be seen as the allowed probability of falsely-labeled normal instances, i.e. the FPR.</p><p>For a (multivariate) Gaussian the probability of seeing a sample with a Mahalanobis score less than t with t &gt; 0 is given by the CDF F D of the chi-square distribution as</p><formula xml:id="formula_6">1−FPR = P (M &lt; t) = P (M 2 &lt; t 2 ) = F D (t 2 ) = γ D 2 , t 2 2 Γ D 2 .</formula><p>(7) Solving for t, the AD threshold is obtainable using the inverse CDF for any desired FPR.</p><formula xml:id="formula_7">t = F −1 D (1 − FPR).<label>(8)</label></formula><p>IV. EXPERIMENTS AND RESULTS First, we assess the suitability of deep features extracted at various stages of a pre-trained classifier model for AD. We employ EfficientNet, which achieves state-of-the-art accuracy on ImageNet classification <ref type="bibr" target="#b32">[32]</ref>, as well as ResNet <ref type="bibr" target="#b28">[28]</ref>, a commonly applied model in research, as architecture variants. We extract features at the end of every model block "level" to assess which feature level gives the best performance. Here, "level" is defined as in <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b32">[32]</ref> (cf. Appendix <ref type="table" target="#tab_1">Table VI</ref> for EfficientNet-B0). We argue that class probabilities are too application-specific and therefore make use of the features before the final mapping in the highest level. As feature probability maps may contain spatial dimensions in earlier levels, aggregation is necessary. We choose simple average pooling to reduce the complexity of our approach, but it should be noted that dedicated aggregation procedures may be an avenue of future research, especially for smaller anomalies. To guarantee reproducibility of our work, we make our code publicly available 1 . The overall approach is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>We now compare our approach to two different assumptions: (I) When assuming a fixed-variance univariate Gaussian distribution, the anomaly score reduces to the simple L 2 -distance to the mean of the training set. (II) When assuming a featureindependent univariate Gaussian, an anomaly score can be defined with the standardized Euclidean distance (SED):  Here, s d is the (empirical) standard deviation of the d-th feature in the training set, andf d its empirical mean.</p><formula xml:id="formula_8">S(x) := D d=0 f d (x) −f d 2 s 2 d .<label>(9)</label></formula><formula xml:id="formula_9">M 1 M 2 M 3 M 4 M 5 M 6 M 7 M 8 M 9 9 i=1 M i</formula><p>To increase robustness of our evaluation, we perform a 5-fold evaluation over the original training dataset of each MVTec category, where we compute the necessary characteristics for each fold, respectively, and apply the scores to the test set of MVTec AD. We refer to the original publication <ref type="bibr" target="#b2">[3]</ref> for an overview of the MVTec AD dataset and to Appendix <ref type="figure">Figure 3</ref> for additional, representative anomalies. To fully assess the capability of the approach, we compute and compare the AUROC, a commonly employed measure for binary classification problems <ref type="bibr" target="#b33">[33]</ref>. We report mean ± Standard Error of the Mean (SEM) AUROC performance over all categories and folds in percent. In addition to the feature level performances, we also report the AUROC performance yielded by summing distance scores over all levels. Further, note that no feature reduction is performed in this first evaluation.</p><p>Assessing the performance of the three different normal distributions with their respected anomaly scores for EfficientNet-B4 in <ref type="table" target="#tab_1">Table I</ref>, the following two observations can be made: (I) The MVG is best suited for AD due to its high and robust performance (with an AUROC of 96.7% ± 1.0% for level 7). (II) Deeper feature representations are more suitable for AD in a transfer learning setting. This is congruent with findings reported by <ref type="bibr" target="#b7">[8]</ref>, and reasons for this may be found in the increased abstraction level that is necessary to conclusively describe the distribution of normality. However, performance saturates (and even starts to decline) in higher levels (level 8 and 9) in case of the multivariate approach. Comparing model architectures, features extracted from ResNet models yield worse performance as indicated by the lower average AUROC of 90.4% ± 3.6% for the best level 4 and 88.2% ± 4.0% for the sum predictor in ResNet-34 (cf. Appendix <ref type="table" target="#tab_1">Table VII</ref>). The increased AD performance of EfficientNet may be attibuted to its efficient architecture (i.e. higher ImageNet accuracy per trainable weight) and the output range of the Swish activation function <ref type="bibr" target="#b34">[34]</ref>. In fact, SED score calculation with features extracted after the ReLU activation used in ResNet often failed as the activations for normal data are clipped to zero for some features.</p><p>Compared to OOD <ref type="bibr" target="#b30">[30]</ref>, no learned, linearly weighted sum of feature-level anomaly score is required to achieve strong performance. In fact, average AUROC of 94.8% ± 1.6% is achieved by simple equal weighting. While Hsu et al. <ref type="bibr" target="#b35">[35]</ref> show that this linear weighting of feature level distributions is also not strictly necessary for OOD, their OOD approach still relies on input preprocessing by means of gradient ascent. It should also be noted that OOD, although similar to AD, still ultimately pursues a slightly different objective.</p><p>We also evaluate the influence of model complexity on AD performance of deep features in a transfer learning setting and apply the proposed method to all EfficientNet variants. Analyzing performance across model complexities, it can be seen that features learned by less complex variants of EfficientNet (i.e. B0-B3, cf. <ref type="table" target="#tab_1">Table II</ref>) perform worse in a transfer learning AD setting. Further, it can be seen that the performance saturates eventually, and even degrades for EfficientNet-B7. This could indicate that more complex Effi-cientNet variants start to overfit on ImageNet and no longer learn features that generalize well to new domains/use cases. A similar effect is observed in our evaluations with Mahalanobis distance on ResNet architectures (cf. Appendix <ref type="table" target="#tab_1">Table VII)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Why Pre-Trained Deep Features Work so Well</head><p>Next, we investigate possible reasons for the oustanding performance of the MVG enacted in a transfer learning setting. We hypothesize that features discriminative to the AD task do not necessarily vary strongly within the normal dataset, as implicitly presumed by semi-supervised approaches that employ normal data only. Therefore, we perform PCA on feature levels of a pre-trained EfficientNet-B4 and keep only principal components accounting for most of the variance before fitting the MVG to the dataset. Vice versa, we retain principal components with the least amount of variance (i.e. those with smallest eigenvalues), which we denote as negated PCA (NPCA) in the following. When assessing effects of PCA-based dimensionality reduction two observations can be made. First, keeping only principal components that retain high variance in the training datasets reduces AD performance. In fact, removing principal components that account for a total of 1% of variance leads to a reduction in AD performance across all levels (cf. PCA 99% in <ref type="table" target="#tab_1">Table III</ref>). Conversely, when retaining only principal components with small eigenvalues, no considerable performance is lost and performance for the sum mode even increases <ref type="table" target="#tab_1">(Table III)</ref>. Therefore, NPCA offers an elegant way to reduce dimensionality of the pre-trained feature spaces. For reference, NPCA 0.01% reduces dimensionality of the feature space in level 7 from 272 to 15.6 features on average across all folds and categories. This alleviates the curse of dimensionality and makes it easier to fit the MVG even with little training data.</p><p>Furthermore, this finding supports the hypothesis that AD algorithms which learn features from scratch utilizing only normal data perform worse than AD approaches using pretrained features. The reason is that feature combinations that retain little variance in normal data (i.e. do not occur in normal data and can thus not be learned effectively) are ultimately those that can be used to discriminate between normal and anomalous images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Choosing a Working Point Solely on FPR</head><p>While our evaluation has focussed on AUROC, neglecting the issue of choosing a working point, the MVG assumption also offers a theoretical framework for selecting the working point by choosing an acceptable FPR out of the box (cf. <ref type="bibr" target="#b7">(8)</ref>). Note that a target FPR cannot be easily set for the sum mode where the Mahalanobis distances of feature-level MVGs are added. Therefore, we restrict our evaluations to level 7 features of two different EfficientNet variants, choosing EfficientNet-B0 for its low model complexity and EfficientNet-B4 for its high AD performance at medium complexity. We assess effects of performing no compression, 99% PCA and 0.01% NPCA compression. Here, we compare target FPR based on <ref type="bibr" target="#b7">(8)</ref> to the FPR achieved on the test set, also reporting the TPR yielded by that working point and overall AUROC. We also assess the potentially beneficial effect of augmentations in order to to artificially enlarge small datasets for more robust covariance estimation. Augmentations are selected per MVTec category to avoid accidental transformation of normal to anomalous data (details can be found in the Appendix <ref type="figure">Fig. 2</ref>). We artifically increase each dataset's size by aggregating over 100 epochs.</p><p>Performing the experiments, we observsed that augmentations were essential to enable setting the working point that completely failed otherwise (e.g. FPR of 99.8% and TPR of 99.9% were achieved 3σ for EfficientNet-B0 at no compression). Looking at Table IV, it can be observed that PCA decreases FPRs yielded on the test set, whereas NPCA increases the FPRs. Therefore, PCA and NPCA behave inverse to each other, and PCA compression may prove useful in providing robust estimates of achieved FPRs at the cost of reduced AD performance. Furthermore, even with artificially enlarged datasets, a sensible setting of the FPR based on training data is possible only for the smallest model, EfficientNet-B0. This indicates the curse of dimensionality, as complex models require increasingly more data to avoid overfitting on noise present in the training data (bias-variance tradeoff). Thus, experiments should be reevaluated on larger AD datasets to confirm our findings. Still, setting the working point by means of a FPR can be realized via this theoretical framework. This is novel, as purely empirical approaches dominate the current literature (i.e. setting working point based on a holdout validation set before applying to the test set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State of the Art on MVTec AD</head><p>Finanlly, we compare the performance of our proposed AD approach with state-of-the-art AD algorithms on the MVTec AD dataset. Our evaluation comprises a semi-supervised reconstruction approach using a convolutional AE, a fullysupervised AD classifier as well as an oc-SVM fit to the  pre-trained feature representations of EfficientNet. We further compare to the current state-of-the-art performance reported in literature on MVTec, taking the correspoding values directly from the linked sources. While the fully-supervised classifier can not be deployed in practice to AD problems, it serves as an upper bound of what can be achieved by AD algorithms. Here, we fine-tune a pretrained EfficientNet-B4, EfficientNet-B2 as well as ResNet-18 and ResNet-34 variants per category. For data splits, we still perform a 5-fold evaluation, but no longer adhere to the original MVTec splits, as there are no anomalies present in the train datasets. Instead, we pool both train and test set and stratify splits to maintain identical anomaly prevalence in all folds. We compute AUROC on a val set split from the train set to select the best model state. The best model is then applied to the unused test set. For training, we select and apply the same augmentations per-category as used to artficially enlarge dataset size (Appendix <ref type="figure">Fig. 2</ref>). We use a batch-size of 64 for ResNet, 16 for EfficientNet and train using the Adam <ref type="bibr" target="#b36">[36]</ref> optimizer with an initial learning rate of 0.0001 employing the binary cross-entropy loss function.</p><p>For the AE, we choose the ResNet-18 for the encoder and an inverted ResNet-18 for the decoder part (i.e. every operation of the encoder should be inverted by the decoder). For the upsampling operations we employ pixel shuffle operations as introduced by Shi et al. <ref type="bibr" target="#b37">[37]</ref> to reduce checkerboard artifacts which would be present otherwise. The latent dimension of the bottleneck is set to 32 and yields proper reconstruction of the normal class in all categories. We also generally employ the same augmentations as before, but disable noise augmentations (cf. <ref type="figure">Appendix Fig. 2</ref>). Batch-size, learning rate and optimizer are the same as for the supervised classifier, and based on preliminary experiments the L 2 -distance is chosen for the reconstruction error. As stated before, an aggregation of the residual image to image-level information is necessary for reconstruction-based approaches. While the threshold employed for ROC calculation is set on the pixel level, we perform connected component analysis and label a test image as defective only if it contains a connected component at least as big as the smallest anomaly present in the test dataset. Note that by extracting the minimal anomaly size from the test dataset, knowledge about the process is introduced to the AE approach, increasing complexity of the procedure.</p><p>To enhance comparability, we also apply augmentation to covariance estimation and compute features across 100 epochs of normal training data per split. To demonstrate the general applicability of our approach, we evaluate the proposed approach in sum mode over all feature levels of EfficientNet-B4, as we can achieve comparable performance and further reduce complexity by omitting feature level selection.</p><p>For the oc-SVM, we fit a RBF-kernel model to every feature level using normal data only and aggregate the predicted anomaly score over all levels to yield a sum score similar to the proposed pipeline.</p><p>Assessing performance results, it becomes apparent that MVG estimation on pre-trained deep features vastly outperforms the state of the art on MVTec, achieving 10% higher average AUROC than the next best model SPADE (cf <ref type="table" target="#tab_5">.  Table V)</ref>. Notably, SPADE as proposed by Cohen et al. <ref type="bibr" target="#b8">[9]</ref> also leverages pre-trained deep feature spaces in combination with deep k-NN and L 2 loss. Also, NPCA slightly improves robustness of the method, leading to an increased average AU-ROC score and a decreased SEM. Furthermore, performance is mostly comparable to fully-supervised fine-tuning of pretrained classifiers but at times slightly worse (cf. per-category results reported in Appendix <ref type="table" target="#tab_1">Table VIII</ref>). This is especially the case for the texture categories, whereas for some object categories (e.g. pill, screw) better performance is achieved by fine-tuning. We stress again that fully-supervised AD can not be realistically applied to most AD problems, and only serves as an upper limit of achievable AUROC performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>We have demonstrated the benefits of using ImageNet pretraining for general-purpose AD in images. In particular, we showed that the MVG assumption of high correlation in pre-trained deep features is crucial to attain state-of-theart AD performance (cf <ref type="table" target="#tab_1">. Table I</ref>). Therefore, the generative assumption proposed by Lee et al. <ref type="bibr" target="#b30">[30]</ref> holds also in a transfer learning setting for AD in images semantically different to the training dataset and can be leveraged to improve AD. Our PCA analysis revealed that discriminative components of the transferred deep feature representations contain little overall variance in normal data and should thus be difficult to learn. This finding is in agreement with literature on shallow AD <ref type="bibr" target="#b41">[41]</ref> and explains the poor performance of OC-SVM when applied to deep feature representations <ref type="bibr" target="#b42">[42]</ref>. We therefore agree with Bergman et al. <ref type="bibr" target="#b27">[27]</ref> that pre-trained feature spaces should be adopted for AD in a transfer learning approach instead of learning features from scratch using normal data only. We further expand upon this and conclude that the MVG assumption is crucial to fully recapitulate the characteristics of pre-trained deep feature representations and to realize their potential.</p><p>While good performance has been achieved on the MVTec AD dataset, we expect that with increasing semantic distance to natural images (e.g. images of the medical domain), outof-the-box AD performance will decrease. In such scenarios, pre-trained models could be used as starting points for finetuning on target domains, e.g. by employing the deep SVDD proposed by Ruff et al. <ref type="bibr" target="#b4">[5]</ref>. The MVG assumption can be easily integrated here, as the hypersphere could be initialized by transforming the features with the inverse Cholesky decomposition of the estimated covariance.</p><p>Apart from the unimodal setting, AD may also occur in a multi-modal context <ref type="bibr" target="#b43">[43]</ref>. We therefore will extend the presented AD approach to multi-modal distributions on a complex in-house fabric dataset (e.g. by fitting Gaussian Mixture Models).</p><p>The MVG further offers a framework to set the working point by determining an acceptable FPR. However, while augmentations and low model complexities were shown to alleviate the mismatch between desired and achieved FPR, an evaluation on even larger AD datasets is required. Further, modifications to the model architecture may prove beneficial to enforce a normal distribution in deep feature representations, as we have seen from the difference in performance between ResNet and EfficientNet features. Here, Self-Normalizing Neural Networks (SNNs) provide a basis for learning Gaussian-distributed features <ref type="bibr" target="#b44">[44]</ref>. Also, classwise distributions may be explicitly constrained to follow Gaussians in deep feature spaces during ImageNet pretraining. It should be noted that while the FPR may be selected, Probably Approximately Correct (PAC) style TPR guarantees, as required for security-critical use-cases, cannot be given by this approach and are a different field of research. Here, Liu et al. <ref type="bibr" target="#b45">[45]</ref> achieved PAC-style guarantees for the TPR, requiring well-defined anomaly distributions to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have achieved state-of-the-art performance on the public MVTec AD dataset using deep feature representations extracted from classifiers pre-trained on ImageNet. Our approach is simple yet effective and consists of fitting a MVG to normal data in the pre-trained deep feature representations, using Mahalanobis distance as anomaly score. We have further investigated the reason behind the effectiveness of our approach. Using PCA, we reveal that principal components containing only little variance in normal data are ultimately those necessary for discriminating between normal and anomalous images. We argue that these features are difficult to learn from scratch using normal data only, and propose to instead use feature representations generated by large-scale discriminative training in a transfer learning setting assuming MVG distributions. Future research in image AD should focus on (I) increasing the generalizability of pre-trained features, (II) fine-tuning of transferred representations using the small available datasets as well as (III) extending the approach to AD tasks with multi-modal normal data distributions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Anomaly Detection using pre-trained deep feature representations. Fitting a multivariate Gaussian to features extracted from every level of an ImageNet pre-trained model and subsequently applying the Mahalanobis distance as anomaly score followed by their unweighted summation yields a simple yet effective Anomaly Detection algorithm. The figure depicts this procedure for the EfficientNet-B0 architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I FEATURE</head><label>I</label><figDesc>LEVEL AUROC (± SEM) SCORES IN PERCENT FOR EFFICIENTNET-B4 USING DIFFERENT NORMAL DISTRIBUTIONS.</figDesc><table><row><cell>Level</cell><cell>L 2</cell><cell></cell><cell>SED</cell><cell></cell><cell cols="2">Mahalanobis</cell></row><row><cell></cell><cell cols="6">Mean SEM Mean SEM Mean SEM</cell></row><row><cell>1</cell><cell>44.5</cell><cell>4.8</cell><cell>51.6</cell><cell>5.7</cell><cell>60.3</cell><cell>6.1</cell></row><row><cell>2</cell><cell>47.3</cell><cell>5.3</cell><cell>48.1</cell><cell>5.1</cell><cell>62.0</cell><cell>6.4</cell></row><row><cell>3</cell><cell>58.1</cell><cell>5.9</cell><cell>59.2</cell><cell>6.3</cell><cell>71.1</cell><cell>5.4</cell></row><row><cell>4</cell><cell>59.7</cell><cell>4.7</cell><cell>61.5</cell><cell>5.1</cell><cell>75.6</cell><cell>5.5</cell></row><row><cell>5</cell><cell>62.6</cell><cell>4.8</cell><cell>66.1</cell><cell>5.0</cell><cell>82.1</cell><cell>4.6</cell></row><row><cell>6</cell><cell>71.7</cell><cell>4.4</cell><cell>74.3</cell><cell>4.3</cell><cell>89.1</cell><cell>3.1</cell></row><row><cell>7</cell><cell>82.9</cell><cell>4.3</cell><cell>85.1</cell><cell>4.0</cell><cell>96.7</cell><cell>1.0</cell></row><row><cell>8</cell><cell>83.2</cell><cell>3.7</cell><cell>85.2</cell><cell>3.4</cell><cell>95.5</cell><cell>1.1</cell></row><row><cell>9</cell><cell>83.3</cell><cell>3.7</cell><cell>87.8</cell><cell>3.0</cell><cell>93.1</cell><cell>1.7</cell></row><row><cell>Sum</cell><cell>75.3</cell><cell>4.5</cell><cell>79.5</cell><cell>6.6</cell><cell>94.8</cell><cell>1.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II AUROC</head><label>II</label><figDesc>(± SEM) SCORES IN PERCENT FOR EFFICIENTNET FEATURES WITH MAHALANOBIS DISTANCE</figDesc><table><row><cell>Level</cell><cell cols="16">EfficientNet-B0 EfficientNet-B1 EfficientNet-B2 EfficientNet-B3 EfficientNet-B4 EfficientNet-B5 EfficientNet-B6 EfficientNet-B7</cell></row><row><cell></cell><cell>Mean</cell><cell cols="2">SEM Mean</cell><cell cols="2">SEM Mean</cell><cell cols="2">SEM Mean</cell><cell cols="2">SEM Mean</cell><cell cols="2">SEM Mean</cell><cell cols="2">SEM Mean</cell><cell cols="2">SEM Mean</cell><cell>SEM</cell></row><row><cell>1</cell><cell>56.8</cell><cell>6.0</cell><cell>56.7</cell><cell>6.0</cell><cell>59.9</cell><cell>6.1</cell><cell>60.1</cell><cell>6.3</cell><cell>60.3</cell><cell>6.1</cell><cell>61.5</cell><cell>6.2</cell><cell>61.3</cell><cell>6.3</cell><cell>60.4</cell><cell>6.2</cell></row><row><cell>2</cell><cell>62.3</cell><cell>5.7</cell><cell>58.2</cell><cell>6.0</cell><cell>59.5</cell><cell>5.7</cell><cell>62.0</cell><cell>6.2</cell><cell>62.0</cell><cell>6.4</cell><cell>63.7</cell><cell>6.5</cell><cell>63.2</cell><cell>6.4</cell><cell>61.6</cell><cell>7.0</cell></row><row><cell>3</cell><cell>68.4</cell><cell>6.0</cell><cell>67.8</cell><cell>6.1</cell><cell>68.4</cell><cell>5.9</cell><cell>70.1</cell><cell>6.2</cell><cell>71.1</cell><cell>5.4</cell><cell>69.5</cell><cell>6.2</cell><cell>70.6</cell><cell>5.7</cell><cell>71.2</cell><cell>5.8</cell></row><row><cell>4</cell><cell>73.8</cell><cell>5.4</cell><cell>73.6</cell><cell>5.7</cell><cell>75.2</cell><cell>5.2</cell><cell>73.5</cell><cell>5.6</cell><cell>75.6</cell><cell>5.5</cell><cell>76.5</cell><cell>5.4</cell><cell>75.1</cell><cell>5.9</cell><cell>76.8</cell><cell>5.2</cell></row><row><cell>5</cell><cell>79.1</cell><cell>5.3</cell><cell>81.0</cell><cell>4.8</cell><cell>82.7</cell><cell>4.9</cell><cell>82.1</cell><cell>5.1</cell><cell>82.1</cell><cell>4.6</cell><cell>83.9</cell><cell>4.3</cell><cell>81.7</cell><cell>4.8</cell><cell>82.4</cell><cell>4.8</cell></row><row><cell>6</cell><cell>86.1</cell><cell>4.1</cell><cell>87.1</cell><cell>3.9</cell><cell>89.1</cell><cell>3.6</cell><cell>91.2</cell><cell>2.8</cell><cell>89.1</cell><cell>3.1</cell><cell>89.0</cell><cell>3.0</cell><cell>88.1</cell><cell>3.1</cell><cell>87.5</cell><cell>3.6</cell></row><row><cell>7</cell><cell>92.5</cell><cell>2.3</cell><cell>95.3</cell><cell>1.4</cell><cell>95.5</cell><cell>1.4</cell><cell>96.4</cell><cell>1.2</cell><cell>96.7</cell><cell>1.0</cell><cell>96.9</cell><cell>1.2</cell><cell>96.7</cell><cell>1.1</cell><cell>96.3</cell><cell>1.6</cell></row><row><cell>8</cell><cell>92.2</cell><cell>2.5</cell><cell>94.7</cell><cell>1.5</cell><cell>94.7</cell><cell>1.6</cell><cell>94.8</cell><cell>1.7</cell><cell>95.5</cell><cell>1.1</cell><cell>96.2</cell><cell>1.2</cell><cell>95.7</cell><cell>1.0</cell><cell>95.7</cell><cell>1.3</cell></row><row><cell>9</cell><cell>91.3</cell><cell>3.0</cell><cell>93.4</cell><cell>2.0</cell><cell>93.1</cell><cell>2.0</cell><cell>92.8</cell><cell>2.1</cell><cell>93.1</cell><cell>1.7</cell><cell>93.0</cell><cell>1.9</cell><cell>93.4</cell><cell>1.5</cell><cell>92.2</cell><cell>1.8</cell></row><row><cell>Sum</cell><cell>90.6</cell><cell>3.2</cell><cell>93.3</cell><cell>2.2</cell><cell>93.6</cell><cell>2.1</cell><cell>94.0</cell><cell>2.2</cell><cell>94.8</cell><cell>1.6</cell><cell>95.2</cell><cell>1.6</cell><cell>95.3</cell><cell>1.2</cell><cell>94.2</cell><cell>1.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III AUROC</head><label>III</label><figDesc>(±SEM) SCORES IN PERCENT FOR NEGATED PCA COMPRESSED EFFICIENTNET-B4 FEATURES WITH MAHALANOBIS DISTANCE</figDesc><table><row><cell>Level</cell><cell cols="2">No Compression</cell><cell cols="2">PCA 99%</cell><cell cols="2">PCA 95%</cell><cell cols="2">NPCA 1%</cell><cell cols="2">NPCA 0.1%</cell><cell cols="2">NPCA 0.01%</cell></row><row><cell></cell><cell>Mean</cell><cell cols="11">SEM Mean SEM Mean SEM Mean SEM Mean SEM Mean SEM</cell></row><row><cell>1</cell><cell>60.3</cell><cell>6.1</cell><cell>50.8</cell><cell>6.1</cell><cell>45.8</cell><cell>5.4</cell><cell>64.1</cell><cell>6.4</cell><cell>67.8</cell><cell>6.6</cell><cell>69.9</cell><cell>6.2</cell></row><row><cell>2</cell><cell>62.0</cell><cell>6.4</cell><cell>53.3</cell><cell>5.9</cell><cell>48.6</cell><cell>5.7</cell><cell>67.6</cell><cell>6.3</cell><cell>68.0</cell><cell>5.9</cell><cell>67.3</cell><cell>5.7</cell></row><row><cell>3</cell><cell>71.1</cell><cell>5.4</cell><cell>65.5</cell><cell>6.5</cell><cell>59.7</cell><cell>6.2</cell><cell>71.6</cell><cell>4.9</cell><cell>68.1</cell><cell>4.2</cell><cell>65.7</cell><cell>3.7</cell></row><row><cell>4</cell><cell>75.6</cell><cell>5.5</cell><cell>69.5</cell><cell>6.1</cell><cell>63.2</cell><cell>6.4</cell><cell>76.1</cell><cell>5.1</cell><cell>73.1</cell><cell>4.6</cell><cell>69.4</cell><cell>4.0</cell></row><row><cell>5</cell><cell>82.1</cell><cell>4.6</cell><cell>76.2</cell><cell>5.3</cell><cell>66.6</cell><cell>6.5</cell><cell>82.5</cell><cell>4.0</cell><cell>78.7</cell><cell>3.6</cell><cell>72.3</cell><cell>3.7</cell></row><row><cell>6</cell><cell>89.1</cell><cell>3.1</cell><cell>83.3</cell><cell>4.8</cell><cell>77.3</cell><cell>5.7</cell><cell>90.2</cell><cell>2.5</cell><cell>88.2</cell><cell>2.4</cell><cell>83.6</cell><cell>2.9</cell></row><row><cell>7</cell><cell>96.7</cell><cell>1.0</cell><cell>93.4</cell><cell>2.1</cell><cell>87.1</cell><cell>4.0</cell><cell>96.1</cell><cell>1.0</cell><cell>94.5</cell><cell>1.3</cell><cell>89.6</cell><cell>2.5</cell></row><row><cell>8</cell><cell>95.5</cell><cell>1.1</cell><cell>91.9</cell><cell>2.1</cell><cell>88.6</cell><cell>3.1</cell><cell>94.8</cell><cell>1.2</cell><cell>93.8</cell><cell>1.4</cell><cell>90.6</cell><cell>2.3</cell></row><row><cell>9</cell><cell>93.1</cell><cell>1.7</cell><cell>91.3</cell><cell>2.1</cell><cell>88.6</cell><cell>2.9</cell><cell>93.3</cell><cell>1.6</cell><cell>91.2</cell><cell>2.1</cell><cell>86.3</cell><cell>3.0</cell></row><row><cell>Sum</cell><cell>94.8</cell><cell>1.6</cell><cell>89.6</cell><cell>3.4</cell><cell>82.2</cell><cell>6.0</cell><cell>95.6</cell><cell>1.3</cell><cell>95.5</cell><cell>1.2</cell><cell>94.0</cell><cell>1.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV EXPECTED</head><label>IV</label><figDesc>FPR AND ACHIEVED FPR / TPR IN PERCENT PER MULTIPLE n · σ FOR EFFICIENTNET LEVEL 7 FEATURES UNDER DIFFERENT COMPRESSION MODES. AUROC VALUES ARE ALSO REPORTED.</figDesc><table><row><cell>n</cell><cell>Target FPR</cell><cell>PCA 99%</cell><cell>EN-B0 All Features</cell><cell>NPCA 1%</cell><cell>EN-B4 All Features</cell></row><row><cell></cell><cell></cell><cell cols="4">FPR TPR FPR TPR FPR TPR FPR TPR</cell></row><row><cell cols="2">1 31.7</cell><cell cols="4">17.9 77.8 30.4 89.1 47.4 94.0 66.1 98.4</cell></row><row><cell>2</cell><cell>4.6</cell><cell cols="4">10.1 71.2 19.3 84.2 29.8 88.9 56.0 97.3</cell></row><row><cell>3</cell><cell>0.3</cell><cell cols="4">5.8 66.1 13.5 80.2 18.9 83.2 44.7 95.9</cell></row><row><cell>4</cell><cell>6×10 −3</cell><cell>1.9 61.3</cell><cell cols="3">9.3 76.3 11.3 78.5 35.7 94.0</cell></row><row><cell>5</cell><cell>6×10 −5</cell><cell>0.7 57.0</cell><cell>6.5 72.7</cell><cell cols="2">6.8 74.4 28.2 91.6</cell></row><row><cell cols="2">AUROC</cell><cell>90.3</cell><cell>93.2</cell><cell>93.7</cell><cell>97.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V COMPARISON</head><label>V</label><figDesc>TO THE STATE OF THE ART. VALUES FOR STATE-OF-THE-ART METHODS ARE DIRECTLY TAKEN FROM THE CORRESPONDING SOURCES. WE REPORT AUROC (±SEM) SCORES IN PERCENT. THE AE APPROACHES MAP-MEAN AND CCA STAND FOR SCORE MAP MEAN AND CONNECTED COMPONENT ANALYSIS, RESPECTIVELY. MAHALANOBIS AND OC-SVM APPROACHES ARE SUMMED OVER ALL FEATURE LEVELS. THE HIGHEST AUROC AMONGST NON-FULLY SUPERVISED METHODS IS BOLDFACED.</figDesc><table><row><cell>Approach</cell><cell>Architecture</cell><cell cols="2">Mean SEM</cell></row><row><cell>GeoTrans [38] (source: [39])</cell><cell>Wide-ResNet</cell><cell>67.2</cell><cell>4.7</cell></row><row><cell cols="2">GANomaly [40] (source: [39]) DCGAN</cell><cell>76.1</cell><cell>1.6</cell></row><row><cell>ITAE [39]</cell><cell>Custom</cell><cell>83.9</cell><cell>2.8</cell></row><row><cell>SPADE [9]</cell><cell>Wide-ResNet50-2</cell><cell>85.5</cell><cell>-</cell></row><row><cell>MSE AE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Map-Mean</cell><cell>ResNet-18</cell><cell>78.8</cell><cell>4.1</cell></row><row><cell>CCA</cell><cell>ResNet-18</cell><cell>81.8</cell><cell>3.4</cell></row><row><cell>Pre-Trained Classifier</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fully-Supervised Fine-tune</cell><cell>ResNet-18</cell><cell>93.3</cell><cell>1.4</cell></row><row><cell>Fully-Supervised Fine-tune</cell><cell>ResNet-34</cell><cell>93.4</cell><cell>1.3</cell></row><row><cell>Fully-Supervised Fine-tune</cell><cell>EfficientNet-B0</cell><cell>94.1</cell><cell>1.4</cell></row><row><cell>Fully-Supervised Fine-tune</cell><cell>EfficientNet-B4</cell><cell>96.3</cell><cell>1.0</cell></row><row><cell>Oc-SVM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>All Features</cell><cell>EfficientNet-B0</cell><cell>73.0</cell><cell>6.1</cell></row><row><cell>All Features</cell><cell>EfficientNet-B4</cell><cell>78.1</cell><cell>4.7</cell></row><row><cell>Mahalanobis (ours)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>All Features</cell><cell>EfficientNet-B4</cell><cell>95.2</cell><cell>1.5</cell></row><row><cell>NPCA 1%</cell><cell>EfficientNet-B4</cell><cell>95.8</cell><cell>1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI EFFICIENTNET</head><label>VI</label><figDesc>-B0 BASELINE NETWORK (SOURCE:<ref type="bibr" target="#b32">[32]</ref>)</figDesc><table><row><cell cols="3">Stage Operator</cell><cell></cell><cell cols="2">Resolution</cell><cell cols="2">#Channels #Layers</cell></row><row><cell>i</cell><cell>F i</cell><cell></cell><cell></cell><cell cols="2">H i × W i</cell><cell>C i</cell><cell>L i</cell></row><row><cell>1</cell><cell cols="2">Conv3x3</cell><cell></cell><cell cols="2">224 × 224</cell><cell>32</cell><cell>1</cell></row><row><cell>2</cell><cell cols="2">MBConv1, k3x3</cell><cell></cell><cell cols="2">112 × 112</cell><cell>16</cell><cell>1</cell></row><row><cell>3</cell><cell cols="2">MBConv6, k3x3</cell><cell></cell><cell cols="2">112 × 112</cell><cell>24</cell><cell>2</cell></row><row><cell>4</cell><cell cols="2">MBConv6, k5x5</cell><cell></cell><cell cols="2">56 × 56</cell><cell>40</cell><cell>2</cell></row><row><cell>5</cell><cell cols="2">MBConv6, k3x3</cell><cell></cell><cell cols="2">28 × 28</cell><cell>80</cell><cell>3</cell></row><row><cell>6</cell><cell cols="2">MBConv6, k5x5</cell><cell></cell><cell cols="2">14 × 14</cell><cell>112</cell><cell>3</cell></row><row><cell>7</cell><cell cols="2">MBConv6, k5x5</cell><cell></cell><cell cols="2">14 × 14</cell><cell>192</cell><cell>4</cell></row><row><cell>8</cell><cell cols="2">MBConv6, k3x3</cell><cell></cell><cell>7 × 7</cell><cell></cell><cell>320</cell><cell>1</cell></row><row><cell>9</cell><cell cols="3">Conv1x1 &amp; pool &amp; FC</cell><cell>7 × 7</cell><cell></cell><cell>1280</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell></row><row><cell cols="8">FEATURE LEVEL AUROC (± SEM) SCORES IN PERCENT FOR RESNET</cell></row><row><cell></cell><cell cols="6">ARCHITECTURES WITH MAHALANOBIS DISTANCE</cell></row><row><cell></cell><cell>Level</cell><cell cols="2">ResNet-18</cell><cell cols="2">ResNet-34</cell><cell cols="2">ResNet-50</cell></row><row><cell></cell><cell></cell><cell cols="6">Mean SEM Mean SEM Mean SEM</cell></row><row><cell></cell><cell>1</cell><cell>66.6</cell><cell>6.4</cell><cell>67.5</cell><cell>6.8</cell><cell>68.0</cell><cell>5.9</cell></row><row><cell></cell><cell>2</cell><cell>71.6</cell><cell>6.4</cell><cell>71.9</cell><cell>6.1</cell><cell>73.7</cell><cell>6.4</cell></row><row><cell></cell><cell>3</cell><cell>78.6</cell><cell>5.0</cell><cell>79.4</cell><cell>4.8</cell><cell>81.3</cell><cell>4.9</cell></row><row><cell></cell><cell>4</cell><cell>86.7</cell><cell>4.1</cell><cell>90.4</cell><cell>3.6</cell><cell>89.0</cell><cell>4.3</cell></row><row><cell></cell><cell>5</cell><cell>88.3</cell><cell>2.7</cell><cell>89.0</cell><cell>3.0</cell><cell>86.9</cell><cell>3.8</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel-wise</head><p>Average Blur Median Blur Motion Blur 1 1 2 p = 0.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLAHE Sharpen Emboss</head><p>Brightness, Contrast 1 1 1 1 p = 0.3 HSV p = 0.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additive Gaussian Noise</head><p>Per Color Same for all Colors 1 1 p = 0.2 Shift, Scale, Rotate p = 0.2</p><p>Rotate n · 90 • p = 0.5</p><p>Horizontal Flip p = 0.5   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-wise</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A review of novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="215" to="249" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MVTec AD -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<editor>J. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning, ser. Machine Learning Research<address><addrLine>Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Stockholmsmässan</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anomaly detection in nanofibrous materials by CNN-based self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Napoletano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">209</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Transfer representation-learning for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Griffin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>JMLR. Multidisciplinary Digital Publishing Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the generalized distance in statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Mahalanobis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1936" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Science of India</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02011</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anomaly detection using deep learning based image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tabatabai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep variational semi-supervised novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurutach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04971</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative probabilistic novelty detection with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Almohsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6822" to="6833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fast distance-based anomaly detection in images using an inception-like autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafijanovic-Djukic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<editor>Discovery Science, P. Kralj Novak, T.Šmuc, and S. Džeroski</editor>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="493" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Görnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vasilev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meissner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sgarlata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tomassini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02997</idno>
		<title level="m">q-space novelty detection with variational autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepanomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepanomaly: Combining background subtraction and deep learning for detecting obstacles and anomalies in an agricultural field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Steen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Jørgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karstoft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1904</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep nearest neighbor anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10445</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Are pre-trained CNNs good feature extractors for anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Nazare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ponti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08495</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A well-conditioned estimator for largedimensional covariance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ledoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="411" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Research, K. Chaudhuri and R. Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA, ser</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A coherent interpretation of AUC as a measure of aggregated classification performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Is the deconvolution layer the same as a convolutional layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07009</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9758" to="9769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Inverse-transform autoencoder for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10676</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GANomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akçay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Feature extraction for one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Neural Information Processing-ICANN/ICONIP 2003</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="342" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Covariance-guided one-class support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ksantini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Shafiq</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2165" to="2177" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Detecting semantic anomalies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04388</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selfnormalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Open category detection with PAC guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garrepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3169" to="3178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Representative anomalies for all categories of the MVTec AD dataset. Red lines are boundaries of the segmentation ground truth. For additional information about the MVTec AD dataset</title>
		<imprint/>
	</monogr>
	<note>we refer to the original publication [3</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
