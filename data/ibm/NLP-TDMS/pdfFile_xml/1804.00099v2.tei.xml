<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Convolutional Neural Networks via Scattering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11-20">November 20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmian</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Lerman</surname></persName>
						</author>
						<title level="a" type="main">Graph Convolutional Neural Networks via Scattering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-11-20">November 20, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We generalize the scattering transform to graphs and consequently construct a convolutional neural network on graphs. We show that under certain conditions, any feature generated by such a network is approximately invariant to permutations and stable to graph manipulations. Numerical results demonstrate competitive performance on relevant datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many interesting and modern datasets can be described by graphs. Examples include social <ref type="bibr" target="#b0">[1]</ref>, physical <ref type="bibr" target="#b1">[2]</ref>, and transportation <ref type="bibr" target="#b2">[3]</ref> networks. The recent survey paper of Bronstein et al. <ref type="bibr" target="#b3">[4]</ref> on geometric deep learning emphasizes the need to develop deep learning tools for such datasets and even more importantly to understand the mathematical properties of these tools, in particular, their invariances.</p><p>They also mention two types of problems that may be addressed by such tools. The first problem is signal analysis on graphs with applications such as classification, prediction and inference on graphs. The second problem is learning the graph structure with applications such as graph clustering and graph matching.</p><p>Several recent works address the first problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. In these works, the filters of the networks are designed to be parametric functions of graph operators, such as the graph adjacency and Laplacian, and the parameters of those functions have to be trained.</p><p>The second problem is often explored with random graphs generated according to two common models: Erdős-Rényi, which is used for graph matching, and the Stochastic Block Model (SBM), which is used for community detection. Some recent graph neural networks have obtained state-of-the-art performance for graph matching <ref type="bibr" target="#b8">[9]</ref> and community detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> with synthetic data generated from the respective graph models. As above, the filters in these works are parametric functions of either the graph adjacency or Laplacian, where the parameters are trained.</p><p>Despite the impressive progress in developing graph neural networks for solving these two problems, the performance of these methods is poorly understood. Of main interest is their invariance or stability to basic signal and graph manipulations. In the Euclidean case, the stability of a convolutional neural network <ref type="bibr" target="#b11">[12]</ref> to rigid transformations and deformations is best understood in view of the scattering transform <ref type="bibr" target="#b12">[13]</ref>. The scattering transform has a multilayer structure and uses wavelet filters to propagate signals. It can be viewed as a convolutional neural network where no training is required to design the filters. Training is only required for the classifiers given the transformed data. Nevertheless, there is freedom in the selection and design of the wavelets. The scattering transform is approximately invariant to translation and rotation. More precisely, under strong assumptions on the wavelet and scaling functions and as the coarsest scale −J approaches −∞, the scattering transform becomes invariant to translations and rotations. Moreover, it is Lipschitz continuous with respect to smooth deformation. These properties are shown in <ref type="bibr" target="#b12">[13]</ref> for signals in L 2 (R d ) and L 2 (H), where H is a compact Lie group.</p><p>It is interesting to note that the design of filters in existing graph neural networks is related to the design of wavelets on graphs in the signal processing literature. Indeed, the construction of wavelets on graphs use special operators on graphs such as the graph adjacency and Laplacian. As mentioned above, these operators are commonly used in graph neural networks. The earliest works on graph wavelets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> apply the normalized graph Laplacian to define the diffusion wavelets on graphs and use them to study multiresolution decomposition of graph signals. Hammond et al. <ref type="bibr" target="#b15">[16]</ref> use the unnormalized graph Laplacian to define analogous graph wavelets and study properties of these wavelets such as reconstructibility and locality. One can easily construct a graph scattering transform by using any of these wavelets. A main question is whether this scattering transform enjoys the desired invariance and stability properties.</p><p>In this work, we use a special instance of the graph wavelets of <ref type="bibr" target="#b15">[16]</ref> to form a graph scattering network and establish its covariance and approximate invariance to permutations and stability to graph manipulations. We also demonstrate the practical effectiveness of this transform in solving the two types of problems discussed above.</p><p>The rest of the paper is organized as follows. The scattering transform on graphs is defined in Section 2. Section 3 shows that the full scattering transform preserves the energy of the input signal. This section also provides an absolute bound on the energy decay rate of components of the transform at each layer. Section 4 proves the permutation covariance and approximate invariance of the graph scattering transform. It also briefly discusses previously suggested candidates for the notion of translation or localization on graphs and the possible covariance and approximate invariance of the scattering transform with respect to them. Furthermore, it clarifies why some special permutations are good substitutes for Euclidean rigid transformations. Section 5 establishes the stability of the scattering transform with respect to graph manipulations. Section 6 demonstrates competitive performance of the proposed graph neural network in solving the two types of problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Wavelet graph convolutional neural network</head><p>We first review the graph wavelets of <ref type="bibr" target="#b15">[16]</ref> in Section 2.1. We then use these wavelets and ideas of <ref type="bibr" target="#b12">[13]</ref> to construct a graph scattering transform in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Wavelets on graphs</head><p>We review the wavelet construction of Hammond et al. <ref type="bibr" target="#b15">[16]</ref> and adapt it to our setting. Our general theory applies to what we call simple graphs, that is, weighted, undirected and connected graphs with no self-loops. We remark that we may also address self-loops, but for simplicity we exclude them. Throughout the paper we fix an arbitrary simple graph G = (V, E) with N vertices. We also consistently use uppercase boldface letters to denote matrices and lowercase boldface letters to denote vectors or vector-valued functions.</p><p>The weight matrix of G is an N × N symmetric matrix W with zero diagonal, where W (n, m) denotes the weight assigned to the edge {n, m} of G. The degree matrix of G is an N × N diagonal matrix with</p><formula xml:id="formula_0">D(n, n) = N m=1 W (n, m) , 1 ≤ n ≤ N .<label>(1)</label></formula><p>The (unnormalized) Laplacian of G is the N × N matrix</p><formula xml:id="formula_1">L = D − W .<label>(2)</label></formula><p>The eigenvalues of L are non-negative and the smallest one is 0. Since the graph is connected, the eigenspace of 0 (that is, the kernel of L) has dimension one. It is spanned by a vector with equal nonzero entries for all vertices. This vector represents a signal of the lowest possible "frequency".</p><p>The graph Laplacian L is symmetric and can be represented as</p><formula xml:id="formula_2">L = N −1 l=0 λ l u l u * l ,<label>(3)</label></formula><p>where 0 = λ 0 &lt; λ 1 ≤ · · · ≤ λ N −1 are the eigenvalues of L, u 0 , · · · , u N −1 are the corresponding eigenvectors, and * denotes the conjugate transpose. We remark that the phases of the eigenvectors of L and their order within any eigenspace of dimension larger than 1 can be arbitrarily chosen without affecting our theory for the graph scattering transform formulated below. Let f ∈ L 2 (G) be a graph signal. Note that in our setting we can regard L 2 (G) L 2 (V ) C N , and without further specification we shall consider f ∈ C N . We define the Fourier transform F :</p><formula xml:id="formula_3">C N → C N by F f =f := (u * l f ) N −1 l=0 ,<label>(4)</label></formula><p>and the inverse Fourier transform F −1 : C N → C N by</p><formula xml:id="formula_4">F −1f := N −1 l=0f (l)u l .<label>(5)</label></formula><p>Let denote the Hadamard product, that is, for g 1 , g 2 ∈ C N , g 1 g 2 (l) = g 1 (l)g 2 (l), l = 0, · · · , N − 1. Define the convolution of f 1 and f 2 in L 2 (G) as the inverse Fourier transform off 1 f 2 , that is,</p><formula xml:id="formula_5">f 1 * f 2 = F −1 f 1 f 2 = N −1 l=0 u lf 1 (l)f 2 (l) = N −1 l=0 u l u * l f 1f 2 (l) = N −1 l=0 u l u * l f 1 u * l f 2 .<label>(6)</label></formula><p>When emphasizing the dependence of * on the graph G, we denote it by * G . Euclidean wavelets use shift and scale in Euclidean space. For signals defined on graphs, which are discrete, the notions of translation and dilation need to be defined in the spectral domain. Hammond et al. <ref type="bibr" target="#b15">[16]</ref> view R as the spectral domain since it contains the eigenvalues of L. Their procedure assumes a scaling function φ and a wavelet functions ψ <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> with corresponding Fourier transformsφ andψ. They have minimal assumptions on φ and ψ. In our construction, we consider dyadic wavelets, that is,</p><formula xml:id="formula_6">ψ j (ω) =ψ(2 −j ω), j ∈ Z .<label>(7)</label></formula><p>Also, we fix a scale −J ∈ Z of coarsest resolution and assume that φ and ψ can be constructed from multiresolution analysis, that is,</p><formula xml:id="formula_7">φ −J 2 + j&gt;−J ψ j 2 = 1 .<label>(8)</label></formula><p>The graph wavelets of <ref type="bibr" target="#b15">[16]</ref> are constructed as follows in our setting. For j &gt; −J, denote byψ j the vector in C N with the following entries:</p><formula xml:id="formula_8">ψ j (l) =ψ j (λ l ) =ψ(2 −j λ l ), l = 0, · · · , N − 1. Similarly,φ −J (l) =φ −J (λ l ) = φ(2 −J λ l ).</formula><p>In view of (6),</p><formula xml:id="formula_9">f * ψ j = N −1 l=0 u l u * l fψ(2 −j λ l ) for j &gt; −J and f * φ −J = N −1 l=0 u l u * l fφ(2 J λ l ) .<label>(9)</label></formula><p>Note that f * ψ j and f * φ −J are both in C N . The graph wavelet coefficients of f ∈ L 2 (G) are defined by</p><formula xml:id="formula_10">Q J f := f * φ −J and Q j f := f * ψ j , j &gt; −J.<label>(10)</label></formula><p>We use boldface notation for {Q j } j≥−J to emphasize that they are operators even though the wavelet coefficients Q j f (n), n = 1, · · · , N , are scalars. At last, we note that (8) implies thatψ(0) = 0. Combining this fact and (9) results in u * 0 Q j = 0 for all j &gt; −J .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scattering on graphs</head><p>Our construction of convolutional neural networks on graphs is inspired by Mallat's scattering network <ref type="bibr" target="#b12">[13]</ref>. As a feature extractor, the scattering transform defined in <ref type="bibr" target="#b12">[13]</ref> is translation and rotation invariant when the coarsest scale approaches −∞ and the wavelet and scaling functions satisfy some strong admissibility conditions. It is also Lipschitz continuous with respect to small deformations. The neural network representation of the scattering transform is the scattering network. It has been successfully used in image and audio classification problems <ref type="bibr" target="#b18">[19]</ref>.</p><p>We form a scattering network on graphs in a similar way, while using the graph wavelets defined above and the following definitions. A path p = (j 1 , · · · , j m ) is an ordering of the scales of wavelets j 1 , · · · , j m &gt; −J. The length of the path p is |p| = m. The length of an empty path p = ∅ is zero. For a path p = (j 1 , · · · , j m ) as above and a scale j m+1 &gt; −J, we define the path p + j m+1 as p + j m+1 = (j 1 , · · · , j m , j m+1 ). For a vector v ∈ R n we denote |v| = (|v(n)|) N n=1 and note that the vectors v and |v| have the same norm.</p><p>For a scale j &gt; −J, the one-step propagator U [j] : R N → R N is defined by</p><formula xml:id="formula_12">U [j]f = Q j f = f * ψ j = f * ψ j (n) N n=1 , ∀f ∈ R N .<label>(12)</label></formula><p>For p = ∅, the scattering propagator U [p] : R N → R N is defined by</p><formula xml:id="formula_13">U [p] = U [j m ]U [j m−1 ] · · · U [j 1 ] .<label>(13)</label></formula><p>For the empty path, we define U [∅]f = f . We note that for any path p and any scale j m+1 &gt; −J</p><formula xml:id="formula_14">U [p + j m+1 ] = U [j m+1 ]U [p] .<label>(14)</label></formula><p>The windowed scattering transform for a path p is defined by</p><formula xml:id="formula_15">S[p]f (n) = Q J U [p]f (n) = U [p]f * φ −J (n) = l u l u * l U [p]f (n)φ(2 J λ l ) .<label>(15)</label></formula><p>Let Λ m denote the set of all paths of length m ∈ N ∪ {0}, i.e., Λ m = {p : |p| = m}. The collection of all paths of finite length is denoted by P J := ∞ m=0 Λ m . The scattering propagator and the scattering transform with respect to P J , which we denote by U [P J ] and S[P J ] : C N → (C N ) |P| respectively, are defined as </p><formula xml:id="formula_16">U [P J ]f = (U [p]f ) p∈P J and S[P J ]f = (S[p]f ) p∈P J , ∀f ∈ C N .<label>(16)</label></formula><formula xml:id="formula_17">U [P J ]f =   p∈P J U [p]f 2   1 2 and S[P J ]f =   p∈P J S[p]f 2   1 2 ,<label>(17)</label></formula><p>where · = · 2 denotes the l 2 -norm on C N . In the terminology of deep learning, the scattering transform acts as a convolutional neural network on  In a similar way, we can define the scattering transform for matrices of signals on graphs. Let F = (f 1 , f 2 , · · · , f D ) ∈ C N ×D , where for each 1 ≤ d ≤ D, f d is a complex signal of length N on the same underlying graph. We define</p><formula xml:id="formula_18">S[P J ]F := (S[P J ]f d ) D d=1<label>(18)</label></formula><p>and . Here and throughout the rest of the paper we denote by A F the Frobenius norm of a matrix A.</p><formula xml:id="formula_19">S[P J ]F F := D d=1 S[P J ]f d 2 1 2 .<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Energy preservation</head><p>We discuss the preservation of energy of a given signal by the scattering transform. The signal is either f ∈ C N with the energy f 2 or F ∈ C N ×D with the energy F 2 F . We first formulate our main result. Theorem 3.1. The scattering transform is norm preserving. That is, for f ∈ C N or F ∈ C N ×D ,</p><formula xml:id="formula_20">S[P J ]f 2 = f 2 and S[P J ]F 2 F = F 2 F .<label>(20)</label></formula><p>The analog of Theorem 3.1 in the Euclidean case appears in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">Theorem 2.6</ref>]. However, the proof is different for the graph case. One basic observation analogous to the Euclidean case is the following.</p><formula xml:id="formula_21">Proposition 3.2. For f ∈ C N and m ∈ N, p∈Λ m U [p]f 2 = p∈Λ m+1 U [p]f 2 + p∈Λ m S[p]f 2 .<label>(21)</label></formula><p>This proposition can be rephrased as follows: the propagated energy at the m-th layer splits into the propagated energy at the next layer and the output energy at the current layer. In order to conclude Theorem 3.1 from Proposition 3.2, we quantify the decay rate of the propagated energy, which may be of independent interest. Fast decay rate means that few layers are sufficient to extract most of the energy of the signal. We define the decay rate of the scattering transform at a given layer as follows. </p><formula xml:id="formula_22">p∈Λ m+1 U [p]f 2 ≤ r p∈Λ m U [p]f 2 .<label>(22)</label></formula><p>In practice, different choices of graph G and scale J lead to different energy decay rates. Nevertheless, we establish the following generic result that applies to all graph scattering transforms under the construction in Section 2.2. Proposition 3.3. The scattering transform S[P J ] has energy decay rate of at least 1 − 2/N at all layers but the first one. This is the sharpest generic decay rate, though a better one can be obtained with additional assumptions on J, φ, ψ and L.</p><p>Note that in the Euclidean domain, no such generic result exists. Therefore, one has to choose the wavelets very carefully (see the admissibility condition in [13, Theorem 2.6]). Numerical results illustrating the energy decay in the Euclidean domain are given in <ref type="bibr" target="#b18">[19]</ref>. Furthermore, theoretical rates are provided in <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b20">[21]</ref>, where <ref type="bibr" target="#b19">[20]</ref> introduces additional assumptions on the smoothness of input signals and the bandwidth of filters and <ref type="bibr" target="#b20">[21]</ref> studies time-frequency frames instead of wavelets.</p><p>In practice, the propagated energy seems to decrease much faster than the generic rate stated in Proposition 3.3. <ref type="figure">Figure 2</ref> illustrates this claim. It considers 100 randomly selected images from the MNIST database <ref type="bibr" target="#b21">[22]</ref>. A graph that represents a grid of pixels shared by these images is used. Details of the graph and the dataset are described in Section 6.1. The figure reports the box plots of the cumulative percentage of the output energy of the scattering transform with J = 3 for the first four layers and the 100 input images. That is, at layer 1 ≤ m ≤ 4 the cumulative percentage for an image f is m k=1</p><formula xml:id="formula_23">p∈Λ k−1 S[p]f 2 / f 2 .</formula><p>We see that in the third layer, the scattering transform already extracts almost all the energy of the signal. Therefore, in practice we can estimate the graph scattering transform with a small number of layers, which is also evident in practice for the Euclidean scattering transform <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Demonstration of fast energy decay rate for the graph scattering transform on MNIST. One hundred random images are drawn from the MNIST database, and the scattering transform is applied with the graph described in Section 6.1. The box plots summarize the distribution of the cumulative energy percentages for the random images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proof of Proposition 3.2</head><p>Application of (9) and later <ref type="bibr" target="#b7">(8)</ref> implies that for any</p><formula xml:id="formula_24">f ∈ C N j&gt;−J f * ψ j 2 + f * φ −J 2 = j&gt;−J N −1 l=0 u l u * l fψ(2 −j λ l ) 2 + N −1 l=0 u l u * l fψ(2 J λ l ) 2 = j&gt;−J N −1 l=0 u * l fψ(2 −j λ l ) 2 + N −1 l=0 u * l fφ(2 J λ l ) 2 = N −1 l=0 |u * l f | 2   j&gt;−J ψ (2 −j λ l ) 2 + φ (2 J λ l ) 2   = N −1 l=0 |u * l f | 2 = f 2 .<label>(23)</label></formula><p>Replacing f with U [p]f , summing over all paths with length m and applying <ref type="bibr" target="#b13">(14)</ref> yields</p><formula xml:id="formula_25">p∈Λ m U [p]f 2 = p∈Λ m   j&gt;−J U [p]f * ψ j 2 + U [p]f * φ J 2   = p∈Λ m   j&gt;−J Q j U [p]f 2 + S[p]f 2   = p∈Λ m j&gt;−J U [p + j]f 2 + p∈Λ m S[p]f 2 = p∈Λ m+1 U [p]f 2 + p∈Λ m S[p]f 2 .<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proof of Proposition 3.3</head><p>Recall that λ 0 = 0 and u 0 = α/ √ N (1, · · · , 1) * where α ∈ C with |α| = 1. Note that <ref type="bibr" target="#b7">(8)</ref> implies that φ (0) = 1. Note further that for any p ∈ Λ m , m ≥ 1, the entries of U [p]f are non-negative due to the absolute value in <ref type="bibr" target="#b11">(12)</ref>, and thus |u *</p><formula xml:id="formula_26">0 U [p]f | = U [p]f 1 / √ N . Consequently, S[p]f 2 = Q J U [p]f 2 = N −1 l=0φ (2 J λ l )u l u * l U [p]f 2 = N −1 l=0 φ (2 J λ l )u * l U [p]f 2 ≥ φ (2 J λ 0 )u * 0 U [p]f 2 = 1 N U [p]f 2 1 .<label>(25)</label></formula><p>Furthermore, we claim that</p><formula xml:id="formula_27">U [p]f 2 1 ≥ 2 U [p]f 2 .<label>(26)</label></formula><p>Indeed, in view of (11)-(13) and the form of u 0 , U [p]f = |g|, where g ∈ C N satisfies (1, · · · , 1) * g = 0. One can easily show that the minimal value of g 2 1 , over all g ∈ C N satisfying g 2 = 1 and (1, · · · , 1) * g = 0, equals 2 and this concludes <ref type="bibr" target="#b25">(26)</ref>.</p><p>Combining <ref type="formula" target="#formula_1">(25)</ref> and <ref type="bibr" target="#b25">(26)</ref> and summing the resulting inequality over p ∈ Λ m yields</p><formula xml:id="formula_28">p∈Λ m S[p]f 2 ≥ 2 N p∈Λ m U [p]f 2 .<label>(27)</label></formula><p>The combination of <ref type="formula" target="#formula_0">(21)</ref> and <ref type="formula" target="#formula_1">(27)</ref> concludes the proof as follows</p><formula xml:id="formula_29">p∈Λ m+1 U [p]f 2 ≤ 1 − 2 N p∈Λ m U [p]f 2 .<label>(28)</label></formula><p>An improvement of this decay rate is possible if and only if one may strengthen the single inequality in <ref type="bibr" target="#b24">(25)</ref> and the inequality in <ref type="bibr" target="#b25">(26)</ref>. We show that these inequalities can be equalities for special cases and thus the stated generic decay rate is sharp. We first note that equality occurs in the inequality of (25) if, for example,φ is the indicator function of [0, 2 J λ 1 ). Equality occurs in the second inequality when U [p]f has exactly two non-zero elements, for example, when N = 2. These two cases can be simultaneously satisfied. We comment that certain choices of J, φ, ψ and L imply different inequalities with stronger decay rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proof of Theorem 3.1</head><p>We write <ref type="bibr" target="#b20">(21)</ref> as</p><formula xml:id="formula_30">p∈Λ m S[p]f 2 = p∈Λ m U [p]f 2 − p∈Λ m+1 U [p]f 2<label>(29)</label></formula><p>and sum over m ≥ 0, while recalling that U [∅]f := f , to obtain that</p><formula xml:id="formula_31">S[P J ]f 2 = m≥0 p∈Λ m S[p]f 2 = m≥0   p∈Λ m U [p]f 2 − p∈Λ m+1 U [p]f 2   = lim m→∞   p∈Λ 0 U [p]f 2 − p∈Λ m+1 U [p]f 2   = f 2 − lim m→∞ p∈Λ m+1 U [p]f 2 .<label>(30)</label></formula><p>Combining Proposition 3.3 and <ref type="formula" target="#formula_1">(23)</ref> yields</p><formula xml:id="formula_32">p∈Λ m+1 U [p]f 2 ≤ 1 − 2 N m p∈Λ 1 U [p]f 2 ≤ 1 − 2 N m f 2 → 0, as m → ∞ .<label>(31)</label></formula><p>The first equality in <ref type="bibr" target="#b19">(20)</ref> clearly follows from <ref type="formula" target="#formula_2">(30)</ref> and <ref type="bibr" target="#b30">(31)</ref>. The second equality in <ref type="formula" target="#formula_1">(20)</ref> is an immediate consequence of the first equality and the observation that for</p><formula xml:id="formula_33">F = (f 1 , · · · , f D ), F 2 F = D d=1 f d 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Permutation covariance and invariance</head><p>When applying a transformation Φ[G] to a graph signal it is natural to expect that relabeling the graph vertices and the corresponding signal's indices before applying the transformation has the same effect as relabeling the corresponding indices after applying the transformation. More precisely, let P ∈ S N be a permutation, where S N denotes the symmetric group on N letters, then it is natural to ask whether</p><formula xml:id="formula_34">Φ[P G](P f ) = P Φ[G](f ).<label>(32)</label></formula><p>In deep learning, the property expressed in (32) is referred to as covariance to permutations. On the other hand, invariance to permutations means that</p><formula xml:id="formula_35">Φ[P G](P f ) = Φ[G](f ).<label>(33)</label></formula><p>Ideally, a graph-based classifier should not be sensitive to "graph-consistent relabeling" of the signal coordinates. The analog of this ideal request in the Euclidean setting is that a classifier of signals defined on R D should not be sensitive to their rigid transformations. In the case of classifying graph signals by first applying a feature-extracting transformation and then a standard classifier, this ideal request translates to permutation invariance of the initial transformation. However, permutation invariance is a very strong property that often contradicts the necessary permutation covariance. We show here that the scattering transform is permutation covariant and if the scaling function is sufficiently smooth and J approaches infinity, then it becomes permutation invariant.</p><p>We first exemplify the basic notions of covariance and invariance in Section 4.1. Section 4.2 reviews the few existing results on permutation covariance and invariance of graph neural networks and then presents our results for the scattering network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Basic examples of graph permutations, covariance and invariance</head><p>For demonstration, we focus on the graph G depicted in <ref type="figure" target="#fig_4">Figure 3a</ref>. In this graph, each drawn edge has weight one and thus the double edge between the first two nodes has the total weight 2. The weight matrix of the graph is</p><formula xml:id="formula_36">W =     0 2 1 1 2 0 1 0 1 1 0 0 1 0 0 0     .<label>(34)</label></formula><p>The signal f = (2, 1, 0, 0) * is depicted on the graph with different colors corresponding to different values. The following permutation is applied to the graph in <ref type="figure" target="#fig_4">Figure 3b</ref>:</p><formula xml:id="formula_37">P =     0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0     .</formula><p>(35) <ref type="figure" target="#fig_4">Figure 3c</ref> applies the permutation both to the signal and the graph.  <ref type="bibr" target="#b33">(34)</ref>. This transformation is also independent of the labeling of the graph and thus permutation covariant. This property can also be formally verified as follows:</p><formula xml:id="formula_38">(a) (G, f ) (b) (P G, f ) (c) (P G, P f )</formula><formula xml:id="formula_39">(f ) = (f 2 , f 1 , f 3 , f 4 ) * and Φ[P G](P f ) = (f 3 , f 2 , f 4 , f 1 ) * . One can readily check that indeed P Φ[G](f ) = (f 3 , f 2 , f 4 , f 1 ) * = Φ[P G](P f ). Another example is the transformation Φ[G](f ) = W [G]f , where W [G] ≡ W is the weight matrix in</formula><formula xml:id="formula_40">Φ[P G](P f ) = W [P G]P f = P W [G]P * P f = P W [G]f = P Φ[G](f ) .<label>(36)</label></formula><formula xml:id="formula_41">Similarly, Φ[G](f ) = L[G]f , where L[G]</formula><p>is the graph Laplacian, is permutation covariant. The above three examples of permutation covariant transformations are not permutation invariant. An example of a permutation invariant transformation Φ[G], but not permutation covariant, maps the signal</p><formula xml:id="formula_42">f = (f 1 , . . . , f 4 ) * to the signal Φ[G]f = (max 4 i=1 f i , 0, 0, 0) * .</formula><p>Clearly the output Φ[G]f is not affected by permutation of the input signal and is thus permutation invariant. On the other hand, zeroing out three specified signal coordinates, instead of three vertices with unique graph properties (e.g., the vertices connected by at least two edges), violates permutation covariance.</p><p>The latter example demonstrates in a very simplistic way the value of invariance for classification. Indeed, assume that there are two types of signals with low and high values and a classifier tries to distinguish between the two classes according to the first coordinate of Φ[G](f ) by checking whether it is larger than a certain threshold or not. Then this procedure can distinguish the two types of signals without getting confused with signal relabeling. Permutation covariance does not play any role in this simplistic setting, since the classifier only considers the first coordinate of Φ[G](f ) and ignores the rest of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Permutation covariance and invariance of graph neural networks</head><p>The recent works of Gilmer et al. <ref type="bibr" target="#b22">[23]</ref> and Kondor et al. <ref type="bibr" target="#b23">[24]</ref> discuss permutation covariance and invariance for composition schemes on graphs, where message passing is a special case. Composition schemes are covariant to permutations since they do not depend on any labeling of the graph vertices. Moreover, if the aggregation function of the composition scheme is invariant to permutations, so is the whole scheme [24, Proposition 2]. However, aggregation leads to loss of local information, which might weaken the performance of the scheme.</p><p>Methods based on graph operators, such as the graph adjacency, weight or Laplacian, are not invariant to permutations (see demonstration in Section 4.1). Nevertheless, the scattering transform is approximately permutation invariant when the wavelet scaling function is sufficiently smooth. Furthermore, when J approaches infinity it becomes invariant to permutations. We first formulate its permutation covariance and then its approximate permutation invariance. </p><formula xml:id="formula_43">φ(ω) ≤ C φ /|ω|, where C φ is a constant depending on φ. For any f ∈ C N and P ∈ S N S[P G][P J ]P f − S[G][P J ]f ≤ C φ 2 −(J+0.5) λ −1 1 √ N + 2 P − I f .<label>(38)</label></formula><p>In particular, the scattering transform is invariant as J approaches infinity. The result also holds if f ∈ C N is replaced with F ∈ C N ×D and the Euclidean norm is replaced with the Frobenius norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generalized graph translations</head><p>Permutation invariance on graphs is an important notion, which is motivated by concrete applications <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. It can be seen as an analog of translation invariance in Euclidean domains, which is also essential for applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>. A different line of research asks for the most natural notion of translation on a graph <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>. We show here that very special permutations of signals on graphs naturally generalize the notion of translation or rigid transformation of a signal in a Euclidean domain. More precisely, there is a planar representation of the graph on which the permutation acts like a rigid transformation. However, in general, there are many permutations that act very differently than translations or rigid transformations in a Euclidean domain, though, they still preserve the graph topology. Indeed, the underlying geometry of general graphs is richer than that of the Euclidean domain. We later discuss previously suggested generalized notions of "translations" on graphs and the possible covariance and invariance of a modified graph scattering transform with respect to these. We first present two examples of permutations of graphs that can be viewed as Euclidean translations or rigid transformations. We later provide examples of permutations of the same graphs that are different than rigid transformations. The first example, demonstrated in <ref type="figure">Figure 4a</ref>, shows a periodic lattice graph G and signal f with two values denoted by white and blue. Note that the periodic graph can be embedded in a torus, whereas the figure only shows the projection of its 25 vertices into a 5 × 5 grid in the plane. The edges are not depicted in the figure, but they connect points to their four nearest neighbors on the torus. That is, including "periodic padding" for the 5 × 5 grid of vertices, each vertex in the plane is connected with its four nearest neighbors. For example vertex 21 is connected with vertices 1, 16, 22, 25. The graph signal obtains a non-zero constant value on the four vertices colored in blue <ref type="bibr">(3, 4, 7 and 8)</ref> and is zero on the rest of them. <ref type="figure">Figure 4b</ref> demonstrates an application of a permutation P to both the graph and the signal. At last, <ref type="figure">Figure 4c</ref> depicts the permuted graph and signal of <ref type="figure">Figure 4b</ref> when the indices are rearranged so that the representation of the lattice in the plane is the same as that in <ref type="figure">Figure 4a</ref> (this is necessary as the lattice lives in the torus and may have more than one representation in the plane). The relation between the consistent representations of (G,f ) in <ref type="figure">Figure 4a</ref> and (P G, P f ) in <ref type="figure">Figure 4c</ref> is obviously a translation. That is, graph and signal permutation in this example corresponds to translation. We remark that the fact</p><formula xml:id="formula_44">(a) (G, f ) relabel −−−−→ (b) (P G, P f ) rearrange − −−−− → (c)</formula><p>Indices of (P G, P f ) rearranged as in (a) <ref type="figure">Figure 4</ref>: Demonstration of graph permutation as Euclidean translation. <ref type="figure">Figure 4a</ref> shows a signal lying on a lattice in the torus embedded onto a 5 × 5 planar grid. <ref type="figure">Figure 4b</ref> demonstrates a permutation of the graph and signal. <ref type="figure">Figure 4c</ref> shows a planar representation of the permuted graph and signal that is consistent with the one of <ref type="figure">Figure 4a</ref>. The permutation clearly corresponds to translations in a Euclidean space.</p><p>that <ref type="figure">Figure 4c</ref> coincides with the description of (G,P f ) is incidental for this particular example and does not occur in the next example. <ref type="figure" target="#fig_6">Figure 5</ref> depicts a different example where a permutation of a graph signal can be viewed as a variant of a Euclidean rigid transformation. The graph G and the signal f are shown in <ref type="figure" target="#fig_6">Figure 5a</ref>, where f is supported on the vertices marked in blue (indexed by 1, 2 and 3). <ref type="figure" target="#fig_6">Figure 5b</ref> demonstrates an application of a permutation P (mapping (1, 2, 3, 4, 5) to (5, 4, 3, 2, 1)) to the graph and signal. <ref type="figure" target="#fig_6">Figure 5c</ref> shows a different representation of (P G, P f ), which is consistent with the one of (G, f ) presented in <ref type="figure" target="#fig_6">Figure 5a</ref>. The comparison between Figures 5a and 5c makes it clear that the graph and signal permutation corresponds to a Euclidean rigid transformation in the planar representation of the graph. At last, <ref type="figure" target="#fig_6">Figure 5d</ref> demonstrates that unlike the example in <ref type="figure">Figure 4</ref>, the rearrangement of (P G, P f ) is generally different than the graph (G, P f ). Indeed, the subgraph associated with the blue values of the signal is not a triangle and thus the topology is different.</p><p>We remark that many permutations on graphs do not act like translations or rigid transformations. We demonstrate this claim using the graphs of the previous two examples. In <ref type="figure">Figure 6</ref>, we consider the same graph as in <ref type="figure">Figure 4</ref>, but with a different permutation. The difference in permutations can be noticed by comparing the second columns of the grids in <ref type="figure">Figures 4b and 6b</ref>. We note that the rearrangement of the vertices in <ref type="figure">Figure 6c</ref> does not yield an analog of a Euclidean translation. The reason is that the rearranged vertices do not form a grid. To demonstrate this claim, note that in <ref type="figure">Figure 6a</ref>, label 17 is connected to 22, but in <ref type="figure">Figure 6b</ref>, and consequently in the rearranged representation in <ref type="figure">Figure 6c</ref>, they are disconnected. <ref type="figure">Figure 7</ref> demonstrates a permutation that does not act like a rigid transformation with respect to the graph of <ref type="figure" target="#fig_6">Figure 5</ref>. Clearly, the rearranged graph and signal in <ref type="figure">Figure 7c</ref> have different planar geometry. We remark that while the permutations demonstrated in <ref type="figure">Figures 6 and 7</ref> do not preserve the planar geometry, they still preserve the topology of the graphs. Indeed, the notion of permutation invariance is richer than invariance to rigid transformations in the Euclidean domain.</p><p>In the signal processing community, some candidates were proposed for translating signals on graphs. Shuman et al. <ref type="bibr" target="#b2">[3]</ref> defined a "graph translation" (or in retrospect, a graph localization procedure) as follows</p><formula xml:id="formula_45">T c f = √ N (f * δ c ) = √ N N −1 l=0 u l (c)u l u * l f .<label>(39)</label></formula><p>They established useful localization properties of T c , which justify a corresponding construction of a windowed graph Fourier transform. They also demonstrated the applicability of this tool for the Minnesota road network [3, <ref type="figure">Figure 7</ref>]. We remark that in their definition u l (c) may not be well-defined. To make it well-defined one needs to assume fixed choices of the phases of u l (c), 0 ≤ l ≤ N − 1, and that the algebraic multiplicities of all eigenvalues equal one.  <ref type="figure" target="#fig_6">Figure 5d</ref> shows (G, P f ), which is different from the rearrangement procedure depicted in 5c.  <ref type="figure">Figure 6</ref>: A different permutation of <ref type="figure">Figure 4a</ref>, which is not similar to rigid motion, but still preserves the graph topology. Note that 6c does not maintain the planar geometry of the graph: for instance, the vertices 19 and 24 are not connected by an edge.</p><p>Sandryhaila and Moura <ref type="bibr" target="#b26">[27]</ref> define a "shift" of a graph signal f by T s f = W f , where W is the weight matrix of the graph. This definition is motivated by the example of a directed cyclic graph, where an application of the weight matrix is equivalent to a shift by one vertex. Note that in this special case, the graph signal permutation (P G, P f ) advocated in this section also results in a vertex shift. We remark that it is unclear to us why this notion of shift is useful for general graphs.</p><p>If one needs covariance and approximate invariance of a graph scattering transform to the graph localization procedure defined in <ref type="bibr" target="#b2">[3]</ref>, then one may modify the nonlinearity of the scattering transform as  <ref type="figure">Figure 7</ref>: A different permuation of <ref type="figure" target="#fig_6">Figure 5a</ref>, which is not similar to rigid motion, but still preserves the graph topology.</p><formula xml:id="formula_46">σ(f ) = N −1 l=0 |u * l f | u l and redefine U j f = σ(Q j f ) for j &gt; −J. Note that σ(T c f ) = σ √ N N −1 l=0 u l (c)(u * l f )u l = √ N N −1 l=0 u l (c) |u * l f | u l (40) and T c σ(f ) = σ N −1 l=0 u l (c) u * l N −1 l =0 |u * l f | u l u l = √ N N −1 l=0 u l (c) |u * l f | u l .<label>(41)</label></formula><p>Therefore, the nonlinearity σ and the modified scattering transform are covariant to the localization operator T c . Similarly, by following the proof for Theorem 4.2 one can show that the modified scattering transform is approximately invariant to T c as long as its energy decay is sufficiently fast. The scattering transform cannot be adjusted to be covariant and approximately invariant to the "shift" defined by T s f = W f . The reason is that unlike L, W does not commute in general with the eigenvectors {u l } N l=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Proof of Proposition 4.1</head><p>We need to show that for each path p = (j 1 , · · · , j m ) ∈ P J , where j 1 , · · · , j m &gt; −J,</p><formula xml:id="formula_47">S[P G][p]P f = P S[G][p]f .<label>(42)</label></formula><p>Note that the Laplacian of P G isL = P LP * , which has the same eigenvalues as L and has eigenvectors u l = P u l , l = 0, · · · , N − 1. Equation <ref type="formula" target="#formula_9">(9)</ref> implies that for j &gt; −J</p><formula xml:id="formula_48">f * P G ψ j = N −1 l=0 P u l u * l P * fψ(2 −j λ l ) .<label>(43)</label></formula><p>Therefore, for j &gt; −J</p><formula xml:id="formula_49">(P f ) * P G ψ j = N −1 l=0 P u l u * l P * P fψ(2 −j λ l ) = N −1 l=0 P u l u * l fψ(2 −j λ l ) = P N −1 l=0 u l u * l fψ(2 −j λ l ) = P (f * G ψ j ).<label>(44)</label></formula><p>Consequently, applying the absolute value pointwise,</p><formula xml:id="formula_50">(P f ) * P G ψ j = P (f * G ψ j ) = P f * G ψ j<label>(45)</label></formula><p>Similarly,</p><formula xml:id="formula_51">(P f ) * P G φ −J = P (f * G φ −J ) .<label>(46)</label></formula><p>Application of (45) and (46) results in the identity</p><formula xml:id="formula_52">(P f ) * P G ψ j1 * P G · · · * P G ψ jm * P G φ −J = P f * G ψ j1 * G · · · * G ψ jm * G φ −J .<label>(47)</label></formula><p>In view of (13) -(15), (42) is equivalent to (47), and the proof is thus concluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Proof of Theorem 4.2</head><p>According to <ref type="bibr" target="#b14">(15)</ref> and <ref type="formula" target="#formula_2">(37)</ref>,</p><formula xml:id="formula_53">S[P G][P J ]P f − S[G][P J ]f = P Q J U [G][P J ]f − Q J U [G][P J ]f ≤ P Q J − Q J U [P]f .<label>(48)</label></formula><p>We bound the right-hand-side of (48) by a function that approaches zero as J → ∞. We first bound P Q J − Q J . We apply <ref type="bibr" target="#b14">(15)</ref> as well as the following facts: λ 0 = 0,φ(0) = 0 and λ 1 &gt; 0 (since G is connected) to obtain that for f ∈ C N</p><formula xml:id="formula_54">(P Q J − Q J )f 2 = P N −1 l=0φ (2 J λ l )u l u * l f − N −1 l=0φ (2 J λ l )u l u * l f 2 = N −1 l=0φ (2 J λ l )u l u * l (P f − f ) 2 = N −1 l=1φ (2 J λ l )u l u * l (P f − f ) 2 = N −1 l=1 φ (2 J λ l )u * l (P f − f ) 2 ≤ max l=1,··· ,N −1 φ (2 J λ l ) 2 P − I 2 f 2 ≤ C 2 φ 2 −2J λ −2 1 P − I 2 f 2 .<label>(49)</label></formula><p>Hence</p><formula xml:id="formula_55">P Q J − Q J ≤ C φ 2 −J λ −1 1 P − I .<label>(50)</label></formula><p>It remains to bound U [P J ]f . The application of (17), Proposition 3.3 and (23) results in</p><formula xml:id="formula_56">U [P J ]f 2 = f 2 + m≥1 p∈Λ m U [p]f 2 ≤ f 2 + m≥1 1 − 2 N m−1 p∈Λ 1 U [p]f 2 = f 2 + N 2 p∈Λ 1 U [p]f 2 ≤ f 2 + N 2 f 2 = N + 2 2 f 2 .<label>(51)</label></formula><p>At last, the combination of (48), (50) and (51) implies (38). The generalization to F ∈ C N ×D is immediate since F = (f 1 , · · · , f D ) and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Stability to signal and graph manipulations</head><p>We establish the stability of the graph scattering transform to both signal and graph manipulations. The stability to signal manipulation is an immediate corrolary of the energy preservation established in Theorem 3.1. It states that the graph scattering transform is Lipschitz continuous with respect to the graph signal in the following way.</p><formula xml:id="formula_57">Proposition 5.1. For two signals f ∈ C N andf ∈ C N , S[P J ]f − S[P J ]f ≤ f −f . (52)</formula><p>Similarly, for two signals F ∈ C N ×D andF ∈ C N ×D ,</p><formula xml:id="formula_58">S[P J ]F − S[P J ]F F ≤ F −F F .<label>(53)</label></formula><p>In order to motivate the stability to graph manipulation, we discuss the problem of community detection <ref type="bibr" target="#b27">[28]</ref>. Its setting assumes different groups of vertices that communicate more significantly with each other than with other groups. The goal is to identify these underlying groups. In some cases, such as for data generated by the stochastic-block model <ref type="bibr" target="#b28">[29]</ref>, the edge set of the graph is the only information one can work with. For other cases, such as bibliographic datasets <ref type="bibr" target="#b29">[30]</ref>, in addition to the edge set (the citations), information of features of vertices is provided. For graph convolutional neural networks, if vertex-wise features are not given, it is natural to choose an artificial feature for each vertex. For instance, Kipf &amp; Welling <ref type="bibr" target="#b10">[11]</ref> use f = (1, 1, · · · , 1) * and Bruna &amp; Li <ref type="bibr" target="#b9">[10]</ref> use F = I.</p><p>In this problem, stability to graph manipulations can be formulated as follows when the number of vertices is sufficiently large: small changes of the edge weights should not affect the community structure. More specifically, one may consider graph manipulation as modification of edge weights and ask for the effect on such manipulation on important features. In the following, we establish such stability to graph manipulations, where the features are expressed by the output of the graph scattering transform. This result is conditioned on sufficiently fast decay rate of the energy as well as of φ and ψ (equivalently, their Fourier transforms are sufficiently smooth).</p><p>Theorem 5.2. Let G be a simple graph with N vertices and weights {W (n, m)} N n,m=1 , and let δ &gt; 0 denote the smallest gap of eigenvalues of its Laplacian:</p><formula xml:id="formula_59">δ = min l1 =l2 |λ l1 − λ l2 | .</formula><p>LetG be a perturbation of G with weights {W (n, m)} N n,m=1 , such that for some 0 &lt; C ≤ N δ/2 W (n, m) −W (n, m) ≤ C N −2 .</p><p>(54)</p><p>Let f ∈ C N be a fixed input signal for which the energy of the scattering transform decays fast in the sense that for some M &gt; 0 and C 0 &gt; 0</p><formula xml:id="formula_60">m≥M p∈Λ m S[p]f 2 ≤ C 0 N f 2 .<label>(55)</label></formula><p>Also, supposeφ andψ are both Lipschitz continuous functions with Lipschitz constant C 1 . Then there exists a constant C depending on C , C 0 , C 1 , such that</p><formula xml:id="formula_61">S[G][P J ]f − S[G][P J ]f ≤ C √ N f .<label>(56)</label></formula><p>The same result holds if f ∈ C N is replaced with F ∈ C N ×D .</p><p>We remark that the assumption δ &gt; 0 in the above theorem implies that all eigenvalues have algebraic multiplicity one. In general, it is impossible to extend this theorem to higher multiplicity of eigenvalues. Indeed, assume for example that there are two zero eigenvalues, so the graph is disconnected. Then it is possible to make the graph connected by changing a certain edge weight from zero to an arbitrarily small positive number. Such a small change completely deform the topology of the graph and we thus do not expect a general theorem that includes higher multiplicities.</p><p>We also remark that (54) only allows a very small change of weights and generally does not allow one to remove or add an edge. The latter more general graph manipulation is natural in some applications.</p><p>For example, for the CORA dataset <ref type="bibr" target="#b29">[30]</ref> of publications and citations, the lack of knowledge of the mutual citation between two specific publications should not significantly affect the detection of communities. We are unaware of previous theoretical results for stability with respect to this more general graph manipulation. In some cases, removing an edge from a graph can completely change the topology, no matter how large the graph is. For example, one can make some graphs disconnected by removing a single edge. Therefore, it is difficult to have a general result that can handle stability to removal or addition of edges. Nevertheless, the following theorem generalizes Theorem 5.2 by restricting the perturbation of the spectral decomposition of the graph Laplacian.</p><p>Theorem 5.3. Let G andG be two simple graphs with the same set of N vertices. Let f ∈ C N be a fixed input signal for which the energy of the scattering transform decays fast in the sense that for some M &gt; 0 and C 0 &gt; 0</p><formula xml:id="formula_62">m≥M p∈Λ m S[p]f 2 ≤ C 0 N f 2 .<label>(57)</label></formula><p>Also, supposeφ andψ are both Lipschitz continuous functions with Lipschitz constant C 1 and the Laplacian eigenpairs of G andG satisfy</p><formula xml:id="formula_63">λ l −λ l ≤ C 2 N and sin ∠(u l ,ũ l ) ≤ C 3 N , l = 0, · · · , N − 1 .<label>(58)</label></formula><p>Then there exists a constant C depending on C 0 , C 1 , C 2 , C 3 and M , for which</p><formula xml:id="formula_64">S[G][P J ]f − S[G][P J ]f ≤ C √ N f .<label>(59)</label></formula><p>The same result holds if f ∈ C N is replaced with F ∈ C N ×D .</p><p>Condition (58) might be strong for some practical applications, but we are unable to relax it. As mentioned above, we do not expect a general stability result with respect to addition or removal of edges. Nevertheless, for some stochastic models, it is rather unlikely that adding or removing an edge leads to a significant change in the graph topology. To demonstrate this claim, we numerically test the stability of the scattering transform in practice for a synthetic setting produced by SBM with arbitrary edge corruption. In this setting, the condition of (58) may not hold. For each N = 5, 10, 20, 50, 100, 200, 400, 600, 800, 1, 000, we randomly sample 20 graphs from an SBM with two classes both containing N vertices. The probability of connecting two vertices within one class is p = max{1, 6 log N/N } and the probability of connecting two vertices from different classes is q = log N/N . For each model, we randomly choose two vertices of the graph G: if they are connected by an edge, we remove the edge to formG; if they are not connected by an edge, we add an edge to formG. We compute the relative error S[G]f − S[G]f / f and average it over the 20 random samples. The results of the experiments are shown in <ref type="figure" target="#fig_10">Figure 8</ref>. We note that the ratio decays fast and the relative error is negligible for sufficiently large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Proof of Proposition 5.1</head><p>Similarly to establishing <ref type="bibr" target="#b23">(24)</ref>, f ||/ f for a graph G and its perturbationG generated by SBM. The perturbationG is formed by randomly selecting two vertices of G and reversing the connectivity by adding/deleting an edge between them. The average ratio from 20 randomly generated graphs is plotted for each N (number of vertices in each class).</p><formula xml:id="formula_65">p∈Λ m U [p]f − U [p]f 2 = p∈Λ m   j&gt;−J (U [p]f − U [p]f ) * ψ j 2 + (U [p]f − U [p]f ) * φ J 2   = p∈Λ m   j&gt;−J U [p]f * ψ j − U [p]f * ψ j 2 + U [p]f * φ J − U [p]f * φ J 2   ≥ p∈Λ m j&gt;−J U [p + j]f − U [p + j]f 2 + p∈Λ m S[p]f − S[p]f 2 (60) = p∈Λ m+1 U [p]f − U [p]f 2 + p∈Λ m S[p]f − S[p]f 2 .</formula><p>We remark that the inequality of (60) follows from the triangle inequality x − y ≥ | x − y |. By summing the terms on the left and right hand sides of (60) over m ≥ 0, one obtains, similarly to <ref type="bibr" target="#b29">(30)</ref>, that</p><formula xml:id="formula_66">S[P J ]f − S[P J ]f 2 ≤ f −f 2 − lim m→∞ p∈Λ m+1 U [p]f − U [p]f 2 .<label>(61)</label></formula><p>Application of (31) to (61) yields (52). The proof of (53) is the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Proof of Theorem 5.3</head><p>Let p ∈ Λ m be an arbitrary path of length m ≥ 0. We bound the squared norm of the difference of wavelet coefficients of U [p]f with respect to G andG. Recall that λ 0 =λ 0 = 0 and u 0 =ũ 0 = (1/ √ N , · · · , 1/ √ N ) * .</p><p>The required bound for the J-th coefficient is as follows</p><formula xml:id="formula_67">Q J [G]U [G][p]f − Q J [G]U [G][p]f 2 = N −1 l=0φ (2 J λ l )u l u * l U [G][p]f − N −1 l=0φ (2 Jλ l )ũ lũ * l U [G][p]f 2 = N −1 l=0 φ (2 J λ l ) −φ(2 Jλ l ) u l u * l U [G][p]f + N −1 l=0φ (2 J λ l ) (u l u * l −ũ lũ * l ) U [G][p]f + N −1 l=0φ (2 Jλ l )ũ lũ * l U [G][p]f − U [G][p]f 2 = N −1 l=1 φ (2 J λ l ) −φ(2 Jλ l ) u l u * l U [G][p]f + N −1 l=1φ (2 J λ l ) (u l u * l −ũ lũ * l ) U [G][p]f + N −1 l=0φ (2 Jλ l )ũ lũ * l U [G][p]f − U [G][p]f 2 ≤ 3 N −1 l=1 φ (2 J λ l ) −φ(2 Jλ l ) u l u * l U [G][p]f 2 + N −1 l=1φ (2 J λ l ) (u l u * l −ũ lũ * l ) U [G][p]f 2 + N −1 l=0φ (2 Jλ l )ũ lũ * l U [G][p]f − U [G][p]f 2 .<label>(62)</label></formula><p>Similarly, for j &gt; −J,</p><formula xml:id="formula_68">Q j [G]U [G][p]f − Q j [G]U [G][p]f 2 ≤ 3 N −1 l=1 ψ (2 −j λ l ) −ψ(2 −jλ l ) u l u * l U [G][p]f 2 + N −1 l=1ψ (2 −j λ l ) (u l u * l −ũ lũ * l ) U [G][p]f 2 + N −1 l=0ψ (2 −jλ l )ũ lũ * l U [G][p]f − U [G][p]f 2 .<label>(63)</label></formula><p>Both (62) and (63) bound the energy by the sum of three terms. We next bound each of these terms. In order the bound the first term, note that sinceφ andψ are both C 1 -Lipschitz</p><formula xml:id="formula_69">φ (2 J λ l ) −φ(2 Jλ l ) ≤ C 1 2 J λ l −λ l ,<label>(64)</label></formula><p>and</p><formula xml:id="formula_70">ψ (2 −j λ l ) −ψ(2 −jλ l ) ≤ C 1 2 −j λ l −λ l , for all j &gt; −J .<label>(65)</label></formula><p>Therefore,</p><formula xml:id="formula_71">N −1 l=1 φ (2 J λ l ) −φ(2 Jλ l ) u l u * l U [G][p]f 2 ≤ max l=1,··· ,N −1 φ (2 J λ l ) −φ(2 Jλ l ) 2 N −1 l=1 u l u * l U [G][p]f 2 ≤ C 1 2 2J C 2 N U [G][p]f 2 = C 1 C 2 2 2J N U [G][p]f 2 .<label>(66)</label></formula><p>Similarly,</p><formula xml:id="formula_72">N −1 l=1 ψ (2 −j λ l ) −ψ(2 −jλ l ) u l u * l U [G][p]f 2 ≤ C 1 C 2 2 −2j N U [G][p]f 2 .<label>(67)</label></formula><p>The second term for J and j &gt; −J is bounded as follows</p><formula xml:id="formula_73">N −1 l=1φ (2 J λ l )(u l u * l −ũ lũ N −1 l=0ψ (2 −jλ l )ũ lũ * l U [G][p]f − U [G][p]f 2 = N −1 l=0 ψ (2 −jλ l ) 2 ũ * l (U [G][p]f − U [G][p]f ) 2 .</formula><p>(71)</p><p>Applying (66) -(71),</p><formula xml:id="formula_74">Q J [G]U [G][p]f − Q J [G]U [G][p]f 2 + j&gt;−J Q j [G]U [G][p]f − Q j [G]U [G][p]f 2 ≤ 3 j≥−J C 1 C 2 2 −2j N U [G][p]f 2 + C 2 3 N 2 N −1 l=1 φ (2 J λ l ) 2 + j&gt;−J ψ (2 −j λ l ) 2 U [G][p]f 2 + N −1 l=0 φ (2 J λ l ) 2 + j&gt;−J ψ (2 −j λ l ) 2 ũ * l U [G][p]f − U [G][p]f 2 = 3 C 1 C 2 2 2J+1 N U [G][p]f 2 + (N − 1)C 2 3 N 2 U [G][p]f 2 + U [G][p]f − U [G][p]f 2 ≤ 3 C N U [G][p]f 2 + U [G][p]f − U [G][p]f 2 ,<label>(72)</label></formula><p>where</p><formula xml:id="formula_75">C = C 1 C 2 2 2J+1 + C 2 3 . Summing over p ∈ Λ m yields p∈Λ m Q J [G]U [G][p]f − Q J [G]U [G][p]f 2 + j&gt;−J Q j [G]U [G][p]f − Q j [G]U [G][p]f 2 ≤ 3 p∈Λ m C N U [G][p]f 2 + U [G][p]f − U [G][p]f 2 .<label>(73)</label></formula><p>That is,</p><formula xml:id="formula_76">p∈Λ m Q J [G]U [G][p]f − Q J [G]U [G][p]f 2 + p∈Λ m+1 U [G][p]f − U [G][p]f 2 ≤ 3C N p∈Λ m U [G][p]f 2 + 3 p∈Λ m U [G][p]f − U [G][p]f 2 .<label>(74)</label></formula><p>To make the following estimation clear, we denote for m ≥ 1</p><formula xml:id="formula_77">a m = p∈Λ m−1 Q J [G]U [G][p]f − Q J [G]U [G][p]f 2 , b m = p∈Λ m U [G][p]f 2 , d m = p∈Λ m U [G][p]f − U [G][p]f 2 .</formula><p>Also, we denote b 0 = f 2 and d 0 = 0. Note that b m ≤ b 0 for all m ∈ N ∪ {0}. Now (74) can be written as</p><formula xml:id="formula_78">a m + d m ≤ 3C N b m−1 + 3d m−1 , m ≥ 1.<label>(75)</label></formula><formula xml:id="formula_79">Summing over m = 1, · · · , M yields M m=1 a m = 3C N M m=1 b m−1 + 3 M m=1 d m−1 − M m=1 d m ≤ 3CM N b 0 + 2 M −1 m=1 d m − d M ≤ 3CM N b 0 + 2 M −1 m=1 d m .<label>(76)</label></formula><p>Note that d m ≤ 3CN −1 b 0 + 3d m−1 for m ≥ 1, and d 0 = 0, and hence</p><formula xml:id="formula_80">d m ≤ 1 2 − 1 2 · 3 m 3CM N b 0 .<label>(77)</label></formula><p>Therefore,</p><formula xml:id="formula_81">M m=1 a m ≤ 3CM N b 0 + 2 M −1 m=1 1 2 − 1 2 · 3 m 3CM N b 0 = 3CM N b 0 + 3CM (M − 1) N b 0 − M −1 m=1 1 3 m 3CM N b 0 = 3CM 2 N b 0 − 3CM N b 0 1 2 − 1 2 · 3 M −1 = 3CM N M − 1 2 + 1 2 · 3 M −1 b 0 = C N b 0 ,<label>(78)</label></formula><p>where C = 3CM M − 2 −1 + 2 −1 3 1−M . That is, for P M = ∪ M −1 m=0 Λ m , the collection of all paths of length smaller than M ,</p><formula xml:id="formula_82">p∈P M S[G][p]f − S[G][p]f 2 ≤ C N f 2 .<label>(79)</label></formula><p>On the other hand, summation over paths in P J \P M results in</p><formula xml:id="formula_83">p∈P J \P M S[G][p]f − S[G][p]f 2 ≤ 2 p∈P J \P M S[G][p]f 2 + S[G][p]f 2 ≤ 4C 0 N f 2 .<label>(80)</label></formula><p>Therefore,</p><formula xml:id="formula_84">S[G][P J ]f − S[G][P J ]f 2 ≤ C + 4C 0 N f 2 .<label>(81)</label></formula><p>The generalization to F ∈ C N ×D is immediate since F = (f 1 , · · · , f D ) and</p><formula xml:id="formula_85">F 2 F = D d=1 f d 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Proof of Theorem 5.2</head><p>This theorem is actually a corollary of Theorem 5.3. In order to prove it, we only need to show that <ref type="formula">(</ref> To derive a bound for the perturbation of the eigenvalues, we need to control E = max</p><p>x =0</p><p>x * Ex</p><formula xml:id="formula_86">x 2 .</formula><p>By applying basic algebraic manipulations as well as the two parts of (82), We obtain that</p><formula xml:id="formula_87">x * Ex = − N n=1 |x n | 2 m =n E(n, m) + N n=1 m =nx n E(n, m)x m = − 1 2 N n=1 m =n |x n − x m | 2 E(n, m) ≤ 1 2 max 1≤n,m≤N, n =m |E(n, m)| · 2 1≤n,m≤N, n =m |x n | 2 + |x m | 2 ≤ C N −2 (N − 1) x 2 ≤ C N −1 x 2 .</formula><p>Therefore, E ≤ C N −1 . By Weyl's inequality,</p><formula xml:id="formula_88">λ l −λ l ≤ E ≤ C N −1 , l = 0, · · · , N − 1 .<label>(83)</label></formula><p>Next, we establish bounds for the perturbation of eigenvectors. First note that</p><formula xml:id="formula_89">min l1 =l2 λ l1 −λ l2 ≥ min l1 =l2 |λ l1 − λ l2 | − |λ l2 −λ l2 | ≥ δ − C N −1 ≥ δ/2 .</formula><p>Let P V l and PṼ l be the orthogonal projections onto the eigenspaces corresponding to λ l andλ l , respectively. According to the Davis-Kahan Theorem <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>,</p><formula xml:id="formula_90">P V l − PṼ l ≤ E min λ l 1 =λ l 2 λ l1 −λ l2 ≤ 2C N δ .<label>(84)</label></formula><p>As a result,</p><formula xml:id="formula_91">sin ∠(u l ,ũ l ) = uu * −ũũ * ≤ P V l − PṼ l ≤ 2C N −1 δ −1 .<label>(85)</label></formula><p>We thus note that (58) is satisfied with C 2 = C and C 3 = 2C /δ and consequently conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Numerical results</head><p>We demonstrate the effectiveness of the graph wavelet scattering transform by using the MNIST dataset for the problem of image classification and the CORA citation network for the problem of community detection. All tests in this section are executed on a PC with Intel i7-6700 CPU, 8GB RAM and GTX1060 6GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Image Classification using MNIST</head><p>The MNIST dataset <ref type="bibr" target="#b21">[22]</ref> contains 28 × 28 gray-scaled images of digits from 0 to 9. There are 60,000 training images and 10,000 testing images in total, where the task is to classify the images according to the digits. In order to test a graph-based method on this dataset, we follow <ref type="bibr" target="#b6">[7]</ref> and construct a graph representing the underlying grid of the images. More specifically, the vertices of this graph correspond to the 28 × 28 pixels of each image. For vertices v i and v j we let dist(v i , v j ) denote the scaled Euclidean distance between the centers of the corresponding pixels so that if v i and v j are nearest pixels then dist(v i , v j ) = 1. Edges are drawn between any vertices v i and v j satisfying dist</p><formula xml:id="formula_92">(v i , v j ) ≤ √ 2.</formula><p>That is, each pixel is connected to the nearest pixels in horizontal, vertical and diagonal directions. The weight e −dist(vi,vj ) 2 is assigned to any pair of vertices v i and v j connected by an edge. The rest of the pairs have zero weight.</p><p>Using this graph, we apply our proposed graph scattering transform with J = 3, either two or three layers and the Shannon wavelets. The dimension of the output of the scattering transform is 28 × 28 × (1 + 3 + 9) = 10, 192. It is reduced by PCA to 1,000. <ref type="figure" target="#fig_13">Figure 9</ref> illustrates an input image of the digit 7 with the features obtained at the first few layers of the scattering transform with J = 3 applied to this image. Note that the "frequency" of these features increases with the number of layers.  We use three different classifiers on the features generated via scattering: (1) support vector machine (SVM), (2) softmax layer and (3) fully-connected network (FCN). We use 6,000 images of the training set for validation (simple holdout validation). The accuracies of all three classifiers with and without the scattering transform with 2 and 3 layers are shown in <ref type="table">Table 1</ref>. Note that the scattering transform is able to generate features that improve the classification results for all classifiers. Moreover, a three-layer network performs better than a two-layer network. Three layers almost exhaust the energy of the input signal, so a deeper network is not necessary. Indeed, we did not notice any improvement when using a fourth-layer.  <ref type="table">Table 1</ref>: Classification results on MNIST with and without graph scattering. The first row shows percentage of correct classification by direct application of three common classifiers. The next two lines show classification percentages after preprocessing the data by the graph scattering transform with 2 and 3 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM</head><p>Our best result does not compete with the state-of-the-art result for the MNIST dataset that obtains 99.75% accuracy rate <ref type="bibr" target="#b32">[33]</ref>. While the network structure of the method in <ref type="bibr" target="#b32">[33]</ref> is very carefully designed and architected, the graph model used here is only able to encode the information for neighboring pixels. On the other hand, for a convolutional neural network, the convolution at the lowest level collects local information that is not restricted to direct neighbors and is thus able to learn more meaningful local relations.</p><p>Although the grid graph is not the best way to fully represent the image information, it is still a common benchmark for sanity check of a graph neural network. <ref type="table">Table 2</ref> lists classification results of other graphbased methods. Results of the first three methods from <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref> are indicated in parenthesis since they are copied from their original works (codes were not available for the methods of <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b7">[8]</ref> and the code for Spline filters <ref type="bibr" target="#b6">[7]</ref> did not converge on our computer). We remark that codes for the methods of <ref type="bibr" target="#b6">[7]</ref> in these experiments and the ones below were obtained from https://github.com/mdeff/cnn_graph. It is evident that in terms of accuracy the scattering transform is comparable with the best graph-based performer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy Laplacian eigenvalues <ref type="bibr" target="#b5">[6]</ref> (94.96%) Intuitive convolution <ref type="bibr" target="#b7">[8]</ref> (98.55%) Spline filters <ref type="bibr" target="#b6">[7]</ref> (97.15%) Chebyshev filters <ref type="bibr" target="#b6">[7]</ref> 99.12% Scattering transform 99.09% <ref type="table">Table 2</ref>: Percentages of correct classification of different graph-based methods on the MNIST database. Results in parenthesis are copied from their original publications as explained in the main text.</p><p>To further compare the methods in <ref type="bibr" target="#b6">[7]</ref> and our method, we list the running time for each method on our machine. We note that although the method that uses Chebyshev filters is accurate, it is not computationally efficient. Furthermore, the method that uses spline filters did not converge (DNC) on our computer. On the other hand, the scattering transform achieves a competitive accuracy with high efficiency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Community detection using CORA</head><p>The CORA dataset <ref type="bibr" target="#b29">[30]</ref> contains 2,708 research papers with 1,433 features describing each paper. There are also 5,429 citation links of the different papers. This dataset gives rise to a graph whose vertices correspond to the research papers and edges correspond to citations. We assume an undirected graph, where the weight between two papers is one if at least one of them cite the other, and zero otherwise. There are 7 communities of papers and the problem is to detect them. The dataset in <ref type="bibr" target="#b29">[30]</ref> provides labels (in {1, 2, · · · , 7}) of 140 vertices for training, 500 vertices for validation, and 1,000 vertices for testing. Due to the small fraction of training samples, the community detection problem in this setting can be considered as semi-supervised learning. The graph scattering transform is applied to the 2, 708 × 1, 433 feature matrix with J = 3, three layers and the Shannon wavelets. The dimension of the output of the scattering transform is 1, 433 × (1 + 3 + 9) = 18, 629. The communities are detected by applying FCN to the features obtained by the scattering transform. We remark that since training with only 140 samples is fast, there is no need to reduce dimension. <ref type="table">Table 4</ref> lists the accuracy of the graph scattering transform compared with the state-of-the-art graphbased neural network methods. Note that they are comparable, where the scattering transform demonstrates a slight improvement. All of them outperform the traditional methods listed in [11, <ref type="table">Table 2</ref>] (the accuracies of those methods are in the range 59% -75%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head><p>Chebyshev filters <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> 79.5% Renormalization <ref type="bibr" target="#b10">[11]</ref> 81.5% Graph scattering + FCN 81.9% <ref type="table">Table 4</ref>: Percentages of correct labels on CORA for graph scattering and two state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We constructed a graph convolutional neural network by adapting the scattering transform to graphs. We showed that, with the proper choice of graph wavelets, the graph scattering transform is invariant to permutations and stable to signal and graph manipulations. These invariance and stability properties make the graph scattering transform effective for classification and community detection tasks. Although we exemplified the performance of the graph scattering transform in only two particular instances, where one is a bit artificial, it is a generic tool for feature extraction on graphs. Its potential use is thus not limited to the discriminative tasks illustrated in these two examples. Furthermore, the graph scattering transform does not require training. However, it can be adapted to different datasets and choosing different kinds of wavelets. In the numerical experiments of this paper we only used the simple Shannon wavelets.</p><p>In addition to our work, there are other models that try to use the idea of the scattering transform for graphs. For example, the deep Haar scattering <ref type="bibr" target="#b33">[34]</ref>. We believe that our proposed graph scattering network has a more flexible design. Its established permutation invariance and stability to signal and graph manipulations makes it a robust feature extractor that is natural for graph representation. The convolutions with the wavelets of <ref type="bibr" target="#b15">[16]</ref> used in our graph transform are somewhat similar to the ones used in trained graph convolutional neural networks such as <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b10">[11]</ref>. Our work thus suggest some conceptual understanding of invariance and stability properties for other graph convolutional networks.</p><p>Despite the advocated properties of the graph scattering transform, it has some limitations. First of all, it is based on the full spectral decomposition of the graph Laplacian and for very large graphs, its computation is demanding. In order to improve efficiency for the training component, dimension reduction techniques can be used after computing the graph scattering transform. Second of all, the "high frequency" information for the graph Laplacian is not as clear as the high-frequency information in the Euclidean case. Therefore, we do not sufficiently understand the kind of information being processed at deeper layers of the graph scattering transform. At last, the graph scattering transform is a basic generic tool and it may take some time for practitioners to evaluate its potential use. The examples demonstrated here are very simple and the stylized application of classification of images via graph neural networks cannot result in sufficiently competitive results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>graphs. At the m-th layer, where m ≥ 0, the propagated signal is {U [p]f : p ∈ Λ m } and the extracted feature is {S[p]f : p ∈ Λ m }. This network is illustrated in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Network representation of the scattering transform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 1 .</head><label>1</label><figDesc>For J ∈ N, m ∈ N and r &gt; 0, the energy decay rate of S[P J ] at the m-th layer is r if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Section 4.3 explains why some permutations are natural generalizations of rigid transformations and then discusses previous broad generalizations of the notion of "graph translation" and their possible covariance and invariance properties. Sections 4.4 and 4.5 prove the main results formulated in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of permutation for a particular example of a graph and signal discussed in this section. An example of a transformation Φ[G] can be the replacement of the signal values in the two vertices connected by the edge of weight 2. This transformation is independent of the labeling of the graph and is thus permutation covariant. This can be formally verified as follows, while using for simplicity the permutation P defined in (35). For a signal f = (f 1 , f 2 , f 3 , f 4 ) * , P f = (f 3 , f 1 , f 4 , f 2 ) * . Furthermore, Φ[G] swaps the first two entries of a signal, while Φ[P G] swaps the second and the fourth entries (the second claim is obvious from Figure 3b). Accordingly, Φ[G]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Proposition 4 . 1 .</head><label>41</label><figDesc>Let G be a simple graph and S[G][P J ] be the graph scattering transform with respect to G. For any f ∈ C N and P ∈ S N ,S[P G][P J ]P f = P S[G][P J ]f .(37) Theorem 4.2. Let G be a simple graph and S[G][P J ] be the graph scattering transform with respect to G. Assume that the Fourier transform of the scaling function φ of S[G][P J ] decays as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>(P G, P f ) (c) (P G, P f ) with indices of P G embedded the same way as in (a) (d) (G, P f ) relabel rearrange Another example where a graph signal permutation corresponds to Euclidean translation. Figures 5a-5c are created analogously to Figures 4a-4c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Indices of (P G, P f ) rearranged as in (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Indices of (P G, P f ) rearranged as in (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>The relative error ||S[G]f − S[G]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>58) holds under the assumptions of Theorem 5.2. We define E := L−L and conclude from (54) and the definition of the graph Laplacian that |E(n, m)| ≤ C N −2 , for n = m, 1 ≤ n, m ≤ N and E(n, n) = − m =n E(n, m), for 1 ≤ n ≤ N . (82)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) original image (b) 1st layer feature (c) 2nd layer feature (d) 3rd layer feature (e) 4th layer feature</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Examples of features obtained in each layer of the graph scattering network. The input image f is shown in 9a. The scattering transform is applied with J = 3. The features at the four different layers, that is, S[∅]f , S[(−2)]f , S[(−2, −2)]f and S[(−2, −2, −2)]f , are demonstrated in Subfigures 9b-9e. The pixels of the output images representing the features are arranged in the same manner as in the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Note that S[p]F F is the Frobenious norm of the matrix S[p]F = (S[p]f d )</figDesc><table /><note>D d=1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>= 2 layers 95.68% 94.31% 99.02% Graph scattering transform with M = 3 layers 96.59% 94.62% 99.09%</figDesc><table><row><cell></cell><cell>Softmax</cell><cell>FCN</cell></row><row><cell>No initial data processing</cell><cell cols="2">94.16% 91.78% 98.10%</cell></row><row><cell>Graph scattering transform with M</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy and time needed for training MNIST on our machine.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was partially supported by NSF awards DMS-14-18386, DMS-18-21266 and DMS-18-30418. We thank Radu Balan, Addison Bohannon and Maneesh Singh for helpful references and Loren Anderson, Vahan Huroyan and Tyler Maunu for commenting on an earlier version of this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Life in the network: the coming age of computational social science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Christakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">5915</biblScope>
			<biblScope unit="page">721</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vertex-frequency analysis on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ricaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="291" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond Euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08965</idno>
		<title level="m">Graph based convolutional neural network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A generalization of convolutional neural networks to graphstructured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hechtlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08165</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A note on learning algorithms for quadratic assignment with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Supervised community detection with hierarchical graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08415</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Group invariant scattering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1331" to="1398" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diffusion wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="94" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Value function approximation with diffusion wavelets and Laplacian eigenfunctions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="843" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<title level="m">Ten lectures on wavelets. SIAM</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A wavelet tour of signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Energy propagation in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiatowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grohs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bölcskei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03636</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analysis of time-frequency scattering transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02144</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Permutation-equivariant neural networks applied to dynamics prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guttenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Virgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Witkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kanai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs: Frequency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="3042" to="3054" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Community detection in graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fortunato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<biblScope unit="issue">3-5</biblScope>
			<biblScope unit="page" from="75" to="174" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: Recent developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbe</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">177</biblScope>
			<biblScope unit="page" from="1" to="86" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The rotation of eigenvectors by a perturbation. III</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Kahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A useful variant of the Davis-Kahan theorem for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Samworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="323" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised deep Haar scattering on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1709" to="1717" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
