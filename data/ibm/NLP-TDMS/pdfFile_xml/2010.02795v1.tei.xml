<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COSMIC: COmmonSense knowledge for eMotion Identification in Conversations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
							<email>deepanwayghosal@mymail.</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore CIC</orgName>
								<orgName type="institution" key="instit3">Instituto Politécnico Nacional</orgName>
								<orgName type="institution" key="instit4">Mexico University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
							<email>navonilmajumder@</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore CIC</orgName>
								<orgName type="institution" key="instit3">Instituto Politécnico Nacional</orgName>
								<orgName type="institution" key="instit4">Mexico University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
							<email>gelbukh@cic.ipn.mx</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore CIC</orgName>
								<orgName type="institution" key="instit3">Instituto Politécnico Nacional</orgName>
								<orgName type="institution" key="instit4">Mexico University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<email>mihalcea@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore CIC</orgName>
								<orgName type="institution" key="instit3">Instituto Politécnico Nacional</orgName>
								<orgName type="institution" key="instit4">Mexico University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<email>sporia@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit2">Singapore CIC</orgName>
								<orgName type="institution" key="instit3">Instituto Politécnico Nacional</orgName>
								<orgName type="institution" key="instit4">Mexico University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">COSMIC: COmmonSense knowledge for eMotion Identification in Conversations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the task of utterance level emotion recognition in conversations using commonsense knowledge. We propose COSMIC, a new framework that incorporates different elements of commonsense such as mental states, events, and causal relations, and build upon them to learn interactions between interlocutors participating in a conversation. Current state-of-theart methods often encounter difficulties in context propagation, emotion shift detection, and differentiating between related emotion classes. By learning distinct commonsense representations, COSMIC addresses these challenges and achieves new state-of-the-art results for emotion recognition on four different benchmark conversational datasets. Our code is available at https://github.com/ declare-lab/conv-emotion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotion recognition is a long-standing research problem in Artificial Intelligence (AI). With the growing popularity of conversational AI research, the topic of emotion recognition in conversations has received significant attention from the research community <ref type="bibr" target="#b12">Ghosal et al., 2019;</ref><ref type="bibr" target="#b39">Zhang et al., 2019)</ref>. Identifying emotions in conversations is a core step toward fine-grained conversation understanding, which in turn is essential for downstream tasks such as emotion-aware chat agents , visual question answering <ref type="bibr" target="#b33">(Tapaswi et al., 2016;</ref><ref type="bibr" target="#b1">Azab, 2019)</ref>, health conversations <ref type="bibr" target="#b0">(Althoff et al., 2016;</ref><ref type="bibr" target="#b23">Pérez-Rosas et al., 2017)</ref> and others.</p><p>Natural conversations are complex as they are governed by several distinct variables that affect the flow of a conversation and the emotional dynamics of the participants. These variables include <ref type="figure">Figure 1</ref>: Commonsense knowledge can lead to explainable dialogue understanding. It will help models to understand, reason, and explain events and situations. In this particular example, commonsense inference is applied to a sequence of utterances in a twoparty conversation. Person A's first utterance indicates that he/she is tired of arguing with person B. The tone of the utterance also implies that person B is getting yelled at by person A, which invokes a reaction of irritation in person B. Person B then asks what he/she can do to help and says this while being angry. This again makes person A annoyed and influences him/her to respond with anger. This kind of inferred commonsense knowledge about the reaction, effect, and intent of the speaker and the listener helps in predicting the emotional dynamics of the participants. topic, viewpoint, speaker personality, argumentation logic, intent, and so on <ref type="bibr" target="#b27">(Poria et al., 2019b)</ref>. Additionally, individual utterances are also governed by the mental state, intent, and emotional state of the participants at the time when they are uttered. In this conversation model, only the utterances can be observed as the conversation unfolds, while other variables such as speaker state and intent remain latent as they are not directly observed by the other participants. Similarly, the emotional state of the speakers cannot be directly observed, but it can be inferred from the utterances that are observable. <ref type="bibr">1</ref> The commonsense knowledge of the participants in a conversation plays a central role in inferring the latent variables of a conversation. It is used to guide the participants through their reasoning about the content of the conversation, dialog planning, decision making, and many other reasoning tasks. It is also used to recognize other finer-grained elements of a conversation, such as avoiding repetition, asking questions, refraining from giving unrelated responses, and so on -all of which control aspects of the conversation such as fluency, interestingness, inquisitiveness, or empathy. Commonsense knowledge is thus necessary to model the nature and flow of the dialogue and the emotional dynamics of the participants. In <ref type="figure">Figure 1</ref>, we illustrate one such scenario where commonsense knowledge is utilized to infer emotions of the utterances in a dialogue.</p><p>Natural language is often indicative of one's emotion. Hence, emotion recognition has been enjoying popularity in the field of NLP <ref type="bibr" target="#b16">(Kratzwald et al., 2018;</ref><ref type="bibr" target="#b7">Colneriĉ and Demsar, 2018)</ref>, due to its widespread applications in opinion mining, recommender systems, healthcare, and so on. Only in the past few years has emotion recognition in conversation (ERC) gained attention from the NLP community <ref type="bibr" target="#b35">(Yeh et al., 2019;</ref><ref type="bibr" target="#b5">Chen et al., 2018;</ref><ref type="bibr" target="#b41">Zhou et al., 2018)</ref> due to the growing availability of public conversational data. ERC can be used to analyze conversations that take place on social media. It can also aid in analyzing conversations in real time, which can be instrumental in legal trials, interviews, ehealth services, and more. Unlike vanilla emotion recognition of sentences/utterances, ERC ideally requires context modeling of the individual utterances. This context can be attributed to the preceding utterances, and relies on the temporal sequence of utterances. Compared to the recently published works on ERC <ref type="bibr" target="#b5">(Chen et al., 2018;</ref><ref type="bibr" target="#b41">Zhou et al., 2018;</ref><ref type="bibr" target="#b28">Qin et al., 2020;</ref><ref type="bibr" target="#b40">Zhong et al., 2019;</ref><ref type="bibr" target="#b39">Zhang et al., 2019)</ref>, both lexicon-based <ref type="bibr" target="#b34">(Wu et al., 2006;</ref><ref type="bibr" target="#b22">Mohammad and Turney, 2010;</ref><ref type="bibr" target="#b32">Shaheen et al., 2014)</ref> and modern deep learning-based <ref type="bibr" target="#b16">(Kratzwald et al., 2018;</ref><ref type="bibr" target="#b7">Colneriĉ and Demsar, 2018)</ref> vanilla emotion recognition approaches fail to work well on ERC datasets as this work ignores the conversation specific factors such as the presence of contextual cues, the temporality in speakers' turns, or speaker-specific information.</p><p>In this paper, we introduce COSMIC, a commonsense-guided framework for emotion identification in conversations. By building upon a very large commonsense knowledge base, our proposed framework captures some of the complex interactions between personality, events, mental states, intents, and emotions leading towards a better understanding of the emotional dynamics and other aspects of conversation. Through extensive evaluations on four different conversation datasets and comparisons with several baselines and stateof-the-art models, we show the effectiveness of a model that explicitly accounts for commonsense. Moreover, feature ablation experiments highlight the role that such knowledge plays in identifying emotion in conversations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Emotion recognition has been an active area of research for many years and has been explored across inter-disciplinary fields such as machine learning, signal processing, social and cognitive psychology, etc <ref type="bibr" target="#b24">(Picard, 2010)</ref>. The seminal work from <ref type="bibr" target="#b10">Ekman (1993)</ref> presented findings on facial expressions, methods to measure facial expression and their relation with human emotion. Acoustic information and visual cues were later used for emotion recognition by <ref type="bibr" target="#b8">Datcu and Rothkrantz (2014)</ref>.</p><p>However, emotion recognition in conversations has gained popularity only recently due to the emergence of publicly available conversational datasets collected from social media platforms and scripted situations such as movies and tv-shows <ref type="bibr" target="#b26">(Poria et al., 2019a;</ref><ref type="bibr" target="#b38">Zahiri and Choi, 2018)</ref>. The main approach towards conversational emotion recognition is to perform contextual modeling in either textual or multimodal setting with deep-learning based algorithms. <ref type="bibr" target="#b25">Poria et al. (2017)</ref> used recurrent neural networks for multimodal emotion recognition followed by , where party and global states were used for modeling the emotional dynamics. An external knowledge base was used in <ref type="bibr" target="#b40">(Zhong et al., 2019)</ref> with transformer networks to perform emotion recognition. Some of the other important works include <ref type="bibr">(Hazarika et al., 2018a,b;</ref><ref type="bibr" target="#b4">Chen et al., 2017;</ref><ref type="bibr" target="#b36">Zadeh et al., 2018a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task definition</head><p>Given the transcript of a conversation along with speaker information for each constituent utterance, the ERC task aims to identify the emotion of each utterance from a set of pre-defined emotions. <ref type="figure">Figure 1</ref> illustrates one such conversation between two people, where each utterance is labeled by the underlying emotion. Formally, given an input sequence of N utterances [(u 1 , p 1 ), (u 2 , p 2 ), . . . , (u N , p N )], where each utterance u i = [u i,1 , u i,2 , . . . , u i,T ] consists of T words u i,j spoken by party p i , the task is to predict the emotion label e i of each utterance u i . In conversational emotion recognition, the task is to classify each of the constituting utterances into its appropriate emotion category. In literature, the main approach towards this problem has been to first produce context independent representations and then perform contextual modeling. We identify these two distinct modeling phases and aim to improve both of them through the proposed COSMIC framework. Our framework consists of three main stages:</p><p>1. Context independent feature extraction from pretrained transformer language models.</p><p>2. Commonsense feature extraction from a commonsense knowledge graph.</p><p>3. Incorporating commonsense knowledge to design better contextual representations and using it for the final emotion classification.</p><p>The overall architecture of the COSMIC framework is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context Independent Feature Extraction</head><p>We employ the RoBERTa model <ref type="bibr" target="#b20">(Liu et al., 2019)</ref> to extract context independent utterance level feature vectors. We first fine-tune the RoBERTa Large model for emotion label prediction from the transcript of the utterances. RoBERTa Large follows the original BERT Large <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref> architecture having 24 layers, 16 self-attention heads in each block and a hidden dimension of 1024, resulting in a total of 355M parameters. Let an utterance</p><p>x consists of a sequence of BPE tokenized tokens x 1 , x 2 , . . . , x N , with emotion label E x . In this setting, the fine-tuning of the pretrained RoBERTa model is realized through a sentence classification task. A special token [CLS] is appended at the beginning of the utterance to create the input sequence for the model:</p><formula xml:id="formula_0">[CLS], x 1 , x 2 , . . . , x N .</formula><p>This sequence is passed through the model, and the activation from the last layer corresponding to the [CLS] token is then used in a small feedforward network to classify it into its emotion class E x .</p><p>Once the model has been fine-tuned for emotion label classification, we pass the [CLS] appended BPE tokenized utterances to it and extract out activations from the final four layers corresponding to the [CLS] token. These four vectors are then averaged to obtain the context independent utterance feature vector with a dimension of 1024.  <ref type="table">Table 1</ref>: Functional notations of commonsense knowledge used in COMET. The functions take as input the utterance u and returns the feature indicated in the leftmost column. Intent and effect on speaker and listeners can be categorized into mental states, whereas their reactions are events. Intent is also a causal variable whereas the rest are effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Commonsense Feature Extraction</head><p>In this work, we use the commonsense transformer model COMET <ref type="bibr" target="#b2">(Bosselut et al., 2019)</ref> to extract the commonsense features. COMET is trained on several commonsense knowledge graphs to perform automatic knowledge base construction. The model is given a triplet {s, r, o} from the graph and is trained to generate the object phrase o from concatenated subject phrase s and relation phrase r. COMET is an encoder-decoder model that uses the pretrained autoregressive language model GPT <ref type="bibr" target="#b29">(Radford et al., 2018)</ref> as the base generative model.</p><p>To perform the task of generative commonsense knowledge construction, COMET is trained on ATOMIC (The Atlas of Machine Commonsense) , a collection of everyday inferential if-then commonsense knowledge organized through textual descriptions. ATOMIC consists of nine different if-then relation types to distinguish agents vs themes, causes vs effects, voluntary vs non-voluntary events, and actions vs mental states. Given an event in which X participates, the nine relation types (r) are inferred as follows: i) intent of X, ii) need of X, iii) attribute of X, iv) effect on X, v) wanted by X, vi) reaction of X, vii) effect on others, viii) wanted by others, and ix) reaction of others. As an example, given an event or subject phrase (s): "Person X gives Person Y a compliment", the inference from COMET for relation phrase (r): intent of X and reaction of others would be "X wanted to be nice" and "Y will feel flattered" respectively.</p><p>COMET is a generative model and as illustrated in the above example it produces a discrete sequence of commonsense knowledge conditioned on the subject and relation phrase. In our model however, we make use of continuous vectors of commonsense representations. For that, we take the pretrained COMET model on ATOMIC knowledge graph and discard the phrase generating decoder module. We treat utterance U as the subject phrase and concatenate it with the relation phrase r. Next, we pass the concatenated {U ⊕ r} through the encoder of COMET and extract out the activations from the final time-step. In particular we use the relations presented in <ref type="table">Table 1</ref>: intent of X, effect on X, reaction of X, effect on others and reaction of others (where X is the speaker and others are listeners). Performing this feature extraction operation results in five different vectors (respective to the five different relations) for each utterance in the conversation. These vectors are 768 dimensional.</p><p>The nature of the various relation types in ATOMIC allows us to extend it naturally to conversational frameworks. The relations enable the modeling of phenomenons such as content (event, persona, mental states) and causal relations (cause, effect, stative) which are essential elements for understanding conversational context. These different relations are of key importance because generally there is a major interplay between virtually all of them throughout the course of a conversation. For instance, the relations i) -vi) are all intrinsically related to the speaker and vii) -ix) are all akin to the listener. On a more fine-grained level, the intent, effect and react components of the speaker and listener are all elemental for understanding the nature of the conversation. We surmise that adopting these relational variables in a unified framework would be highly useful to create enhanced representations of the conversation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Commonsense Conversational Model</head><p>We first introduce our notations and present a high level view of the main architecture of our COSMIC model. A conversation consists of N utterances u 1 , u 2 , . . . , u N , in which M distinct speakers/participants p 1 , p 2 , . . . , p M take part. Utterance u t is spoken by participant p s(ut) . For every t ∈ {1, 2, . . . , N }, we denote context independent RoBERTa vectors by x t . Commonsense vectors corresponding to intent of X, effect on X, reaction of X, effect on others and reaction of others are denoted by IS cs (u t ), ES cs (u t ), RS cs (u t ), EL cs (u t ), and RL cs (u t ) respectively. X is assumed to be the speaker and others are assumed to be the listeners.</p><p>Since conversations are highly sequential in nature and contextual information flows along a sequence, a context state c t and attention vector a t are formulated that model the sequential dependency between utterances. The context state and attention vector are always shared between all the participants of the conversation.</p><p>An internal state, external state and intent state are used to model different mental states, actions and events for the participants. These are represented by q k,t , r k,t and i k,t for the participants k ∈ [1, 2, . . . , M ]. The internal state and the external state can be collectively considered as the speaker state. This states are necessary to capture the complex mental and emotional dynamics of the participants. The emotion state e t is then modelled from a combination of the three states and the immediate preceding emotion state. Finally the appropriate emotion class for the utterance is inferred from the emotion state.</p><p>In our framework, context and commonsense modeling is performed using GRU cells (Chung   <ref type="bibr">, 2014)</ref>. GRU cells take as input y t and update its hidden state from h t−1 to h t using the transformation: h t = GRU (h t−1 , y t ). New hidden state h t also serves as the output of the current step. The cell is parameterized by weights W and biases b of appropriate sizes depending upon the input y t and output h t . We use five Bidirectional GRU cells GRU C , GRU Q , GRU R , GRU I , and GRU E for modeling context state, internal state, external state, intent state, and emotion state respectively. For ease of representation we formulate the different states with unidirectional GRU cells here.</p><formula xml:id="formula_1">c t−1 c 1 GRU R GRU I CSK r A, t−1 i A, t−1 q A, t r A, t i A, t x t Utterance r A, t−1 q A, t e t−1 x t ℰ c s ( . ) ℛ cs ( . ) I cs( . ) q A, t−1 r A, t−1 i A, t−1 Speaker (A) Listener (B) q A, t r A, t i A, t c t c t−1 i A, t−1 q B, t−1 r B, t−1 i B, t−1 GRU Q q B, t−1 GRU R r B, t−1 i B, t−1 q B, t r B, t i B, t Attention a t c t−1 c 1 q B, t r B, t i B, t CSK x t ℰℒcs( . ) ℛ ℒ cs ( . )</formula><p>Context State: The context state stores and propagates the overall utterance-level information along the sequence of the conversation flow. This state is updated using context GRU cell GRU C after each time-step t when the utterance is uttered by some participant p s(ut) . RoBERTa feature vector x t , internal state q s(ut),t−1 , and external state r s(ut),t−1 of the speaker from the immediate previous timestep (just before uttering the utterance) are concatenated and serve as the input vector for GRU C .</p><p>c t = GRU C (c t−1 , (x t ⊕ q s(ut),t−1 ⊕ r s(ut),t−1 )) (1) We also pool attention vector a t from the history of context [c 1 , c 2 , . . . , c t−1 ] using soft-attention. This attention vector is later used to perform updates on internal and external states.</p><formula xml:id="formula_2">u i = tanh(W s c i + b s ), i ∈ [1, t − 1] α i = exp(u T i x i ) t−1 i=1 exp(u T i x i ) a t = t−1 i=1 α i c i (2)</formula><p>Internal State: The internal state of the participants is conditioned on how the individual is feeling and what is the effect perceived from other participants. This state may remain concealed, as participants may not always express explicitly their feeling or outlook through external stance or reactions. Apart from feelings, this state can also be considered to include aspects that the participant actively tries not to express or features that are considered common knowledge and don't require explicit communication. The effect on oneself is thus elemental to represent the internal state of the participants. We model the internal state of the participants using GRU Q . For time-step t, the internal state of the speaker p s(ut) is updated by taking into account the attention vector a t and commonsense vector effect on speaker ES cs (u t ) q s(ut),t = GRU Q (q s(ut),t−1 , (a t ⊕ ES cs (u t )))</p><p>For all the other participants apart from the speaker, this update is performed using effect on listeners EL cs (u t ).</p><p>q j,t = GRU Q (q j,t−1 , (a t ⊕EL cs (u t ))); ∀j = s(u t ) (4)</p><p>External State: Unlike the internal state, the external state of the participants is all about the expressions, reactions, and responses. Naturally, this state can be easily seen, felt, or understood by the other participants. For instance, the actual utterance, the manner of articulation, the speech, and other acoustic features, the visual expression, gestures, and stance can all be loosely considered to fall under the regime of external state. GRU R updates the external state of the speaker p s(ut) by taking as input the concatenation of attention vector a t , utterance vector x t and commonsense vector reaction of speaker RS cs (u t ) r s(ut),t = GRU R (r s(ut),t−1 , (a t ⊕x t ⊕RS cs (u t ))) (5) For listeners, this update is performed using reaction of listeners RL cs (u t ).</p><formula xml:id="formula_4">r j,t = GRU R (r j,t−1 , (a t ⊕ x t ⊕ RL cs (u t ))); ∀j = s(u t )<label>(6)</label></formula><p>Intent State: Intent is a mental state that represents the commitment to carry out a particular set of actions. The intent of the speaker always plays a crucial role in determining the emotional dynamics of a conversation. The intent of the speaker changes from i s(ut),t−1 to i s(ut),t at time-step t. This change is invoked by the commonsense intent of speaker vector IS cs (u t ) and internal speaker state q s(ut),t at that respective time-step t. The intent states are captured by GRU cell GRU I :</p><formula xml:id="formula_5">i s(ut),t = GRU I (i s(ut),t−1 , (IS cs (u t ) ⊕ q s(ut),t )) (7)</formula><p>The intent of the listener(s), however, is kept unchanged. This is because the intent of a participant who is silent should not change. The change should occur only when the particular participant speaks again.</p><formula xml:id="formula_6">i j,t = i j,t−1 ; ∀j = s(u t )<label>(8)</label></formula><p>Emotion State: The emotional state determines the emotional mood of the speaker and the emotion class of the utterance. We posit that the emotional state depends upon the utterance and composite state of the speaker that takes into account the internal, external, and intent state. Naturally the current emotion state also depends on the previous emotion state of the speaker. GRU E captures the emotion state by combining all of the factors as following, e t = GRU E (e t−1 , (x t ⊕q s(ut),t ⊕r s(ut),t ⊕i s(ut),t )) (9)</p><p>Emotion Classification: Finally all the utterances in the conversation are classified with a fully connected network from e t P t = sof tmax(W smax e t + b smax ); ∀t ∈ [1, N ]  We benchmark COSMIC on four different conversational emotion recognition datasets: i) IEMO-CAP <ref type="bibr" target="#b3">(Busso et al., 2008)</ref> ii) MELD <ref type="bibr" target="#b26">(Poria et al., 2019a)</ref> iii) DailyDialog <ref type="bibr" target="#b18">(Li et al., 2017)</ref>, and iv) EmoryNLP (Zahiri and Choi, 2018). IEMOCAP and DailyDialog are two-party datasets, whereas MELD and EmoryNLP are multi-party datasets. We report experimental results for conversational emotion recognition from the textual information for all four datasets. Information about the datasets is shown in <ref type="table" target="#tab_5">Table 3</ref>.</p><formula xml:id="formula_7">y t = argmax k (P t [k])<label>(10</label></formula><p>IEMOCAP <ref type="bibr" target="#b3">(Busso et al., 2008</ref>) is a dataset of two person conversations among ten different unique speakers. The train set dialogues come from the first eight speakers, whereas the test set dialogues are from the last two. Each utterance is annotated with one of the following six emotions: happy, sad, neutral, angry, excited, and frustrated.</p><p>DailyDialog <ref type="bibr" target="#b18">(Li et al., 2017)</ref> covers various topics about our daily life and follows the natural human communication approach. All utterances are labeled with both emotion categories and dialogue acts. The emotion can belong to one of the following seven labels: anger, disgust, fear, joy, neutral, sadness, and surprise. The dataset has over 83% neutral labels and these are excluded during Micro-F1 evaluation.</p><p>MELD <ref type="bibr" target="#b26">(Poria et al., 2019a</ref>) is a multimodal dataset extended from the EmotionLines dataset <ref type="bibr" target="#b5">(Chen et al., 2018)</ref>. MELD is collected from the TV show Friends and has more than 1400 dialogues and 13000 utterances. Utterances are labeled with emotion and sentiment classes. The emotion classes belong to anger, disgust, sadness, joy, surprise, fear, or neutral, and the sentiment classes belong to positive, negative or neutral.</p><p>EmoryNLP (Zahiri and Choi, 2018) is another dataset also based on the show Friends. Utterances in this dataset are annotated on seven and three emotion classes. The seven emotion classes are neutral, joyful, peaceful, powerful, scared, mad and sad. To create three emotion classes: joyful, peaceful, and powerful are grouped together to form the positive class; scared, mad and sad are grouped together to form the negative class; and the neutral class is kept unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Setup</head><p>For context independent feature extraction, the RoBERTa model is fine-tuned on the set of all utterances and their emotion labels in the training data. We fine-tune the RoBERTa model for a batch size of 32 utterances with Adam optimizer with learning rate of 1e-5. In the case of MELD and EmoryNLP datasets, we use a residual connection between the first and the penultimate layer which brings more stability in the training in the emotion recognition model. The emotion recognition model is trained with Adam optimizer having a learning rate of 1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline and State-of-the art Methods</head><p>For a comprehensive evaluation of COSMIC, we compare it against the following methods: CNN (Kim, 2014) is a convolutional neural network model trained on top of pretrained GloVe embeddings. Standard configurations of filter sizes are used. The model is trained at the utterance level to predict the emotion classes. ICON (Hazarika et al., 2018b) uses two GRU networks to learn the utterance representations for dialogues between two-participants. The output of the two speaker GRUs is then connected using another GRU that helps in performing explicit inter-speaker modeling. ICON is limited to conversations with only two participants only. KET <ref type="bibr" target="#b40">(Zhong et al., 2019)</ref> or Knowledge enriched transformers dynamically leverages external commonsense knowledge using hierarchical self-attention and context aware graph attention. ConGCN <ref type="bibr" target="#b39">(Zhang et al., 2019)</ref> considers utterances and participants of a conversation as nodes of graph network and models both context and speaker sensitive dependence for emotion detection. BERT DCR-Net <ref type="bibr" target="#b28">(Qin et al., 2020</ref>) is a deep co-interactive relation network that uses BERT based features for joint dialogue act recognition and emotion (sentiment) classification. A relation layer learns to explicitly model the relation and interaction between these two tasks in a multi-task setting. BERT+MTL ) is a multi-task learning framework where features extracted from BERT are used in a recurrent neural network for emotion recognition and speaker identification. DialogueRNN  models the emotion of utterances in a conversation with speaker, context and emotion information from neighbour utterances. These factors are modeled using three separate GRU networks to keep track of the individual speaker states.</p><p>We report and compare the performance of COSMIC on test data in <ref type="table" target="#tab_7">Table 4</ref>. State-of-the-art models use GloVe embeddings to extract contextindependent features. As features extracted from transformer based networks such as BERT and RoBERTa generally outperform traditional word embeddings such as word2vec and GloVe, we also report results of the models when used with BERT or RoBERTa features.  MELD and EmoryNLP: These two datasets have been annotated from the TV show Friends, and utterances are often very short. Although dialogues occasionally contain emotion specific words, this does not happen very often at the utterance level. Naturally, emotion dynamics are highly contextual in nature and almost always depend on surrounding utterances. It has been observed in previous work that emotion modeling in MELD is difficult because often there are a lot of speakers in each conversation but they utter only a small number of utterances. Sophisticated models such as Dia-logueRNN do not bring as much improvement over CNN as they do on IEMOCAP. We observe that, COSMIC brings a large improvement over other models on the fine-grained (7 class) classification setup for both datasets. It achieves new state-ofthe-art weighted F1 scores of 73.20 and 56.51 on three class classification; 65.21 and 38.11 on seven class classification on MELD and EmoryNLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Role of Commonsense</head><p>In <ref type="table" target="#tab_7">Table 4</ref>, we also report results of ablation studies by removing listener-specific and speaker-specific commonsense components. For speaker ablation, we discard IS cs (u t ), ES cs (u t ), RS cs (u t ), and observe a sharp drop in performance in most cases. For listener ablation, we discard EL cs (u t ), and RL cs (u t ) and find that the performance also drops but not as much as the speaker ablation. In fact, listener ablation leads to slight improvement in performance in EmoryNLP. The results suggest that speaker-specific commonsense has a greater impact in the overall performance of COSMIC, which is expected because we are predicting the emotion class of the speaker at each utterance. Finally, ablation with respect to both components at the same time naturally leads to higher drop in overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Study</head><p>We illustrate a case study on a test conversation instance from the IEMOCAP dataset in <ref type="figure">Figure 3</ref>. The conversation begins with a couple of neutral utterances, but then the situation quickly escalates, and finally, it ends with a lot of angry and frustrated utterances from both the speakers. State-of-the-art models like DialogueRNN often find this kind of scenarios difficult, when there is a couple of sudden emotions shifts in between (neutral to frustrated and then neutral again). These models also tend to misclassify utterances that have subtle differences in emotion classes such as frustrated and angry. In COSMIC, the propagation of commonsense knowledge makes it easier for the model to handle the  <ref type="figure">Figure 3</ref>: Case study from the IEMOCAP dataset. Discrete commonsense sequences are shown for more interpretability. Commonsense knowledge helps in predicting emotion shifts and understanding difference between closely related emotion classes such as angry and frustrated.</p><p>sudden transitions and to understand the subtle difference between closely related emotion classes. In <ref type="figure">Figure 3</ref>, for the first utterance, the commonsense model predicts that the reaction of speaker is annoyed and propagation of this information helps in predicting that the speaker's next utterance actually belongs to the frustrated class. Similarly for the rest of the illustrated utterances, the commonsense knowledge from effect on speaker and reaction of listener helps the model in distinguishing and predicting the anger and frustrated classes correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Strategies to Incorporate Commonsense</head><p>Apart from the five commonsense features that we use in COSMIC <ref type="table">(Table 1)</ref>, there are four other features that can be extracted from COMET: attribute of speaker, need of speaker, wanted by speaker, and wanted by listeners. We incorporate them using different strategies that add extra complexity in our framework but ultimately do not improve the performance by a significant margin. We experimented along the following directions:</p><p>• Attribute of speaker is loosely considered as a personality trait. This latent variable influenced the internal, external and intent states. We find that the discrete attribute features from COMET are mostly a single word like 'stubborn', 'patient', 'argumentative', 'calm', etc and they change quite abruptly for the same participant in continuing utterances. Hence, we find that their vectorized representations do not help much.</p><p>• Need of speaker, wanted by speaker, and wanted by listeners are considered as output variables that are to be predicted from the input utterance and the five basic commonsense features <ref type="table">(Table 1)</ref>. We add auxiliary output functions and jointly optimize the emotion classification loss with mean-squared loss between predictions and reference commonsense vectors. This strategy also does not help much in improving the emotion classification performance.</p><p>Although the performance improvement is observed using commonsense knowledge across the datasets, this improvement is not very substantial. In the future, we plan to identify better commonsense knowledge sources and develop models that can infuse this knowledge into deep learning models more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we presented COSMIC, a framework that models various aspects of commonsense knowledge by considering mental states, events, actions, and cause-effect relations for emotion recognition in conversations. Using commonsense representations, our model alleviates issues such as difficulty in detecting emotion shifts and misclassification between related emotion classes that are often present in current RNN and GCN based methods. COSMIC achieves new state-of-the-art results for emotion recognition across several benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of COSMIC framework. CSK: Commonsense knowledge from COMET. In practice we use Bidirectional GRU cells. However, for clarity unidirectional cells are shown in the sketch. et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Different states and the respective variables they are influenced by. Italic variables are forms of commonsense knowledge fromTable 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Statistics of splits and evaluation metrics used</cell></row><row><cell>in different datasets. In MELD and EmoryNLP evalu-</cell></row><row><cell>ation is performed for 3 class (broad) and 7 class (fine-</cell></row><row><cell>grained) classification. Neutral* classes constitutes to</cell></row><row><cell>83% of the DailyDialog dataset. These are excluded</cell></row><row><cell>when calculating the Micro F1 score.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison of results against various methods. Scores are average of five runs. Test scores are computed at best validation scores. COSMIC achieves new state-of-the-art results across all the datasets. CSK refers to commonsense knowledge components from COMET. We report the average score of the 10 runs for RoBERTa DialogueRNN and COSMIC. The CNN and DialogueRNN scores using Glove embeddings are obtained from<ref type="bibr" target="#b11">(Ghosal et al., 2020)</ref>.</figDesc><table><row><cell>however the improvement on IEMOCAP is not as</cell></row><row><cell>large as it is on DailyDialog. COSMIC achieves</cell></row><row><cell>new state-of-the-art scores of 65.28 on IEMOCAP;</cell></row><row><cell>51.05 and 58.48 in DailyDialog for the two differ-</cell></row><row><cell>ent evaluation metrics.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In multimodal conversations, there are other variables that can be observed, such as facial expressions, gestures, pitch, and acoustic indicators.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported by A*STAR under its RIE 2020 AME programmatic grant RGAST2003, by the National Science Foundation (grant #1815291), and by the John Templeton Foundation (grant #61156). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of A*STAR, the National Science Foundation, or the John Templeton Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale analysis of counseling conversations: An application of natural language processing to mental health</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multimodal Character Representation for Visual Story Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Azab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Comet: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05317</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with wordlevel fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Emotionlines: An emotion corpus of multi-party conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yeh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Chun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08379</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotion recognition on twitter: comparative study and training a unison model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Colneriĉ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic audio-visual data fusion for automatic emotion recognition. Emotion recognition: a pattern analysis approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rothkrantz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="411" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Facial expression and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">384</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Utterance-level dialogue understanding: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13902</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rada Mihalcea, and Soujanya Poria</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyati</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="154" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Icon: Interactive conversational memory network for multimodal emotion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decision support with text-based emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kratzwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzana</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Kraus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06397</idno>
	</analytic>
	<monogr>
		<title level="m">Deep learning for affective computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Stefan Feuerriegel, and Helmut Prendinger</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task learning with auxiliary speaker identification for conversational emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijiang</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/2003.01478</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dailydialog: A manually labelled multi-turn dialogue dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="986" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
	<note>MoEL: Mixture of empathetic listeners</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DialogueRNN: An Attentive RNN for Emotion Detection in Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016818</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6818" to="6825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text</title>
		<meeting>the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="26" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting counselor behaviors in motivational interviewing encounters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Resnicow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Goggin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delwyn</forename><surname>Catley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Association for Computational Linguistics</title>
		<meeting>the European Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Affective computing: from laughter to ieee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rosalind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Context-Dependent Sentiment Analysis in User-Generated Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MELD: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Emotion recognition in conversation: Research challenges, datasets, and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2929050</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="100943" to="100953" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dcr-net: A deep co-interactive relation network for joint dialog act recognition and sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards empathetic opendomain conversation models: a new benchmark and dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computatinal Linguistics</title>
		<meeting>the Association for Computatinal Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Atomic: An atlas of machine commonsense for ifthen reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3027" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Emotion recognition from text based on automatically generated rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Shaheen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wassim</forename><surname>El-Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shady</forename><surname>Elbassuoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Data Mining Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MovieQA: Understanding Stories in Movies through Question-Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Raquel Urtasun, and Sanja Fidler</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Emotion recognition from text using semantic labels and separable mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Hsien</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze-Jing</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chung</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on Asian language information processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="183" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An interaction-aware attention network for speech emotion recognition in spoken dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Lin</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Shao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6685" to="6689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Memory Fusion Network for Multi-view Sequential Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5634" to="5641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5642" to="5649" />
		</imprint>
	</monogr>
	<note>Prateek Vij, Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Emotion detection on tv show transcripts with sequence-based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho D</forename><surname>Zahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5415" to="5421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Knowledge-enriched transformer for emotion detection in textual conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixiang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emotional chatting machine: Emotional conversation generation with internal and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
