<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2018 CGANS WITH PROJECTION DISCRIMINATOR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
							<email>miyato@preferred.jpkoyama.masanori@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ritsumeikan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2018 CGANS WITH PROJECTION DISCRIMINATOR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (Im-ageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator. The code with Chainer <ref type="bibr" target="#b26">(Tokui et al., 2015)</ref>, generated images and pretrained models are available at https://github.com/pfnet-research/sngan_projection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b5">(Goodfellow et al., 2014)</ref> are a framework to construct a generative model that can mimic the target distribution, and in recent years it has given birth to arrays of state-of-the-art algorithms of generative models on image domain <ref type="bibr" target="#b23">Salimans et al., 2016;</ref><ref type="bibr" target="#b11">Ledig et al., 2017;</ref><ref type="bibr" target="#b30">Zhang et al., 2017;</ref><ref type="bibr" target="#b20">Reed et al., 2016)</ref>. The most distinctive feature of GANs is the discriminator D(x) that evaluates the divergence between the current generative distribution p G (x) and the target distribution q(x) <ref type="bibr" target="#b5">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b16">Nowozin et al., 2016;</ref>. The algorithm of GANs trains the generator model by iteratively training the discriminator and generator in turn, with the discriminator acting as an increasingly meticulous critic of the current generator.</p><p>Conditional GANs (cGANs) are a type of GANs that use conditional information <ref type="bibr" target="#b13">(Mirza &amp; Osindero, 2014)</ref> for the discriminator and generator, and they have been drawing attention as a promising tool for class conditional image generation <ref type="bibr" target="#b17">(Odena et al., 2017)</ref>, the generation of the images from text <ref type="bibr" target="#b20">(Reed et al., 2016;</ref><ref type="bibr" target="#b30">Zhang et al., 2017)</ref>, and image to image translation <ref type="bibr" target="#b10">(Kim et al., 2017;</ref><ref type="bibr" target="#b31">Zhu et al., 2017)</ref>. Unlike in standard GANs, the discriminator of cGANs discriminates between the generator distribution and the target distribution on the set of the pairs of generated samples x and its intended conditional variable y. To the authors' knowledge, most frameworks of discriminators in cGANs at the time of writing feeds the pair the conditional information y into the discriminator by naively concatenating (embedded) y to the input or to the feature vector at some middle layer <ref type="bibr" target="#b13">(Mirza &amp; Osindero, 2014;</ref><ref type="bibr" target="#b2">Denton et al., 2015;</ref><ref type="bibr" target="#b20">Reed et al., 2016;</ref><ref type="bibr" target="#b30">Zhang et al., 2017;</ref><ref type="bibr" target="#b18">Perarnau et al., 2016;</ref><ref type="bibr" target="#b22">Saito et al., 2017;</ref><ref type="bibr" target="#b3">Dumoulin et al., 2017a;</ref><ref type="bibr" target="#b24">Sricharan et al., 2017)</ref>. We would like to however, take into account the structure of the assumed conditional probabilistic models underlined by the structure of the discriminator, which is a function that measures the information theoretic distance between the generative distribution and the target distribution.</p><p>By construction, any assumption about the form of the distribution would act as a regularization on the choice of the discriminator. In this paper, we propose a specific form of the discriminator, a form motivated by a probabilistic model in which the distribution of the conditional variable y given x is Published as a conference paper at ICLR 2018 Concat (b) cGANs, hidden concat <ref type="bibr" target="#b20">(Reed et al., 2016)</ref> 0 y x Adversarial loss Concat (a) cGANs, input concat <ref type="bibr" target="#b13">(Mirza &amp; Osindero, 2014)</ref> x y</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial loss</head><p>Classificaition loss (c) AC-GANs <ref type="bibr" target="#b17">(Odena et al., 2017)</ref> x y Adversarial loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(d) (ours) Projection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial loss</head><p>Inner product x y  discrete or uni-modal continuous distributions. This model assumption is in fact common in many real world applications, including class-conditional image generation and super-resolution.</p><p>As we will explain in the next section, adhering to this assumption will give rise to a structure of the discriminator that requires us to take an inner product between the embedded condition vector y and the feature vector ( <ref type="figure" target="#fig_0">Figure 1d</ref>). With this modification, we were able to significantly improve the quality of the class conditional image generation on 1000-class ILSVRC2012 dataset <ref type="bibr" target="#b21">(Russakovsky et al., 2015)</ref> with a single pair of a discriminator and generator (see the generated examples in <ref type="figure" target="#fig_1">Figure 2</ref>). Also, when we applied our model of cGANs to a super-resolution task, we were able to produce high quality super-resolution images that are more discriminative in terms of the accuracy of the label classifier than the cGANs based on concatenation, as well as the bilinear and the bicubic method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE ARCHITECTURE OF THE CGAN DISCRIMINATOR WITH A PROBABILISTIC MODEL ASSUMPTIONS</head><p>Let us denote the input vector by x and the conditional information by y 1 . We also denote the cGAN discriminator by D(x, y; θ) := A(f (x, y; θ)), where f is a function of x and y, θ is the parameters of f , and A is an activation function of the users' choice. Using q and p to designate the true distributions and the generator model respectively, the standard adversarial loss for the discriminator is given by:</p><formula xml:id="formula_0">L(D) = −E q(y) E q(x|y) [log(D(x, y))] − E p(y) E p(x|y) [log(1 − D(x, y))] ,<label>(1)</label></formula><p>with A in D representing the sigmoid function. By construction, the nature of the 'critic' D significantly affects the performance of G. A conventional way of feeding y to D until now has been to concatenate the vector y to the feature vector x, either at the input layer <ref type="bibr" target="#b13">(Mirza &amp; Osindero, 2014;</ref><ref type="bibr" target="#b2">Denton et al., 2015;</ref><ref type="bibr" target="#b22">Saito et al., 2017)</ref>, or at some hidden layer <ref type="bibr" target="#b20">(Reed et al., 2016;</ref><ref type="bibr" target="#b30">Zhang et al., 2017;</ref><ref type="bibr" target="#b18">Perarnau et al., 2016;</ref><ref type="bibr" target="#b3">Dumoulin et al., 2017a;</ref><ref type="bibr" target="#b24">Sricharan et al., 2017</ref>) (see <ref type="figure" target="#fig_0">Figure 1a</ref> and <ref type="figure" target="#fig_0">Figure 1b</ref>). We would like to propose an alternative to this approach by observing the form of the optimal solution <ref type="bibr" target="#b5">(Goodfellow et al., 2014)</ref> for the loss function, Eq. (1), can be decomposed into the sum of two log likelihood ratios:</p><formula xml:id="formula_1">f * (x, y) = log q(x|y)q(y) p(x|y)p(y) = log q(y|x) p(y|x) + log q(x) p(x) := r(y|x) + r(x).<label>(2)</label></formula><p>Now, we can model the log likelihood ratio r(y|x) and r(x) by some parametric functions f 1 and f 2 respectively. If we make a standing assumption that p(y|x) and q(y|x) are simple distributions like those that are Gaussian or discrete log linear on the feature space, then, as we will show, the parametrization of the following form becomes natural:</p><formula xml:id="formula_2">f (x, y; θ) := f 1 (x, y; θ) + f 2 (x; θ) = y T V φ(x; θ Φ ) + ψ(φ(x; θ Φ ); θ Ψ ),<label>(3)</label></formula><p>where V is the embedding matrix of y, φ(·, θ Φ ) is a vector output function of x, and ψ(·, θ Ψ ) is a scalar function of the same φ(x; θ Φ ) that appears in f 1 (see <ref type="figure" target="#fig_0">Figure 1d</ref>). The learned parameters θ = {V, θ Φ , θ Ψ } are to be trained to optimize the adversarial loss. From this point on, we will refer to this model of the discriminator as projection for short. In the next section, we would like to elaborate on how we can arrive at this form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION BEHIND THE projection DISCRIMINATOR</head><p>In this section, we will begin from specific, often recurring models and show that, with certain regularity assumption, we can write the optimal solution of the discriminator objective function in the form of (3). Let us first consider the a case of categorical variable. Assume that y is a categorical variable taking a value in {1, . . . , C}, which is often common for a class conditional image generation task. The most popular model for p(y|x) is the following log linear model:</p><formula xml:id="formula_3">log p(y = c|x) := v pT c φ(x) − log Z(φ(x)),<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">Z(φ(x)) := C j=1 exp v pT j φ(x)</formula><p>is the partition function, and φ : x → R d L is the input to the final layer of the network model. Now, we assume that the target distribution q can also be parametrized in this form, with the same choice of φ. This way, the log likelihood ratio would take the following form;</p><formula xml:id="formula_5">log q(y = c|x) p(y = c|x) = (v q c − v p c ) T φ(x) − (log Z q (φ(x)) − log Z p (φ(x))) .<label>(5)</label></formula><p>If we make the values of (v q c , v p c ) implicit and put v c :</p><formula xml:id="formula_6">= (v q c − v p c ), we can write f 1 (x, y = c) = v T c φ(x)</formula><p>. Now, if we can put together the normalization constant − (log Z q (φ(x)) − log Z p (φ(x))) and r(x) into one expression ψ(φ(x)), we can rewrite the equation above as</p><formula xml:id="formula_7">f (x, y = c) := v T c φ(x) + ψ(φ(x)).<label>(6)</label></formula><p>Here, if we use y to denote a one-hot vector of the label y and use V to denote the matrix consisting of the row vectors v c , we can rewrite the above model by:</p><formula xml:id="formula_8">f (x, y) := y T V φ(x) + ψ(φ(x)).<label>(7)</label></formula><p>Most notably, this formulation introduces the label information via an inner product, as opposed to concatenation. The form <ref type="formula" target="#formula_8">(7)</ref> is indeed the form we proposed in (3).</p><p>We can also arrive at the form (3) for unimodal continuous distributions p(y|x) as well. Let y ∈ R d be a d-dimensional continuous variable, and let us assume that conditional q(y|x) and p(y|x) are both given by Gaussian distributions, so that q(y|x) = N (y|µ q (x), Λ −1 q ) and p(y|x) = N (y|µ p (x), Λ −1 p ) where µ q (x) := W q φ(x) and µ p (x) := W p φ(x). Then the log density ratio r(y|x) = log(q(y|x)/p(y|x)) is given by:</p><formula xml:id="formula_9">r(y|x) = log |Λ q | |Λ p | exp(−(1/2)(y − µ q (x)) T Λ q (y − µ q (x))) exp(−(1/2)(y − µ p (x)) T Λ p (y − µ p (x))) (8) = − 1 2 y T (Λ q − Λ p ) y + y T (Λ q µ q (x) − Λ p µ p (x)) + ψ(φ(x)) (9) = − 1 2 y T (Λ q − Λ p ) y + y T (Λ q W q − Λ p W p )φ(x) + ψ(φ(x)),<label>(10)</label></formula><p>where ψ(φ(x)) represents the terms independent of y. Now, if we assume that Λ q = Λ p := Λ, we can ignore the quadratic term. If we further express Λ q W q − Λ p W p in the form V , we can arrive at the form (3) again.</p><p>Indeed, however, the way that this regularization affects the training of the generator G is a little unclear in its formulation. As we have repeatedly explained, our discriminator measures the divergence between the generator distribution p and the target distribution q on the assumption that p(y|x) and q(y|x) are relatively simple, and it is highly possible that we are gaining stability in the training process by imposing a regularity condition on the divergence measure. Meanwhile, however, the actual p(y|x) can only be implicitly derived from p(x, y) in computation, and can possibly take numerous forms other than the ones we have considered here. We must admit that there is a room here for an important theoretical work to be done in order to assess the relationship between the choice of the function space for the discriminator and training process of the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">COMPARISON WITH OTHER METHODS</head><p>As described above, (3) is a form that is true for frequently occurring situations. In contrast, incorporation of the conditional information by concatenation is rather arbitrary and can possibly include into the pool of candidate functions some sets of functions for which it is difficult to find a logical basis. Indeed, if the situation calls for multimodal p(y|x), it might be smart not to use the model that we suggest here. Otherwise, however, we expect our model to perform better; in general, it is preferable to use a discriminator that respects the presumed form of the probabilistic model.</p><p>Still another way to incorporate the conditional information into the training procedure is to directly manipulate the loss function. The algorithm of AC-GANs <ref type="bibr" target="#b17">(Odena et al., 2017</ref>) use a discriminator (D 1 ) that shares a part of its structure with the classifier(D 2 ), and incorporates the label information into the objective function by augmenting the original discriminator objective with the likelihood score of the classifier on both the generated and training dataset (see <ref type="figure" target="#fig_0">Figure 1c</ref>). Plug and Play Generative models (PPGNs) <ref type="bibr" target="#b15">(Nguyen et al., 2017)</ref> is another approach for the generative model that uses an auxiliary classifier function. It is a method that endeavors to make samples from p(x|y) using an MCMC sampler based on the Langevin equation with drift terms consisting of the gradient of an autoencoder prior p(x) and a pretrained auxiliary classifier p(y|x). With these method, one can generate a high quality image. However, these ways of using auxiliary classifier may unwittingly encourage the generator to produce images that are particularly easy for the auxiliary classifier to classify, and deviate the final p(x|y) from the true q(x|y). In fact, <ref type="bibr" target="#b17">Odena et al. (2017)</ref> reports that this problem has a tendency to exacerbate with increasing number of labels. We were able to reproduce this phenomena in our experiments; when we implemented their algorithm on a dataset with 1000 class categories, the final trained model was able to generate only one image for most classes. <ref type="bibr">Nguyen et al.'</ref>s PPGNs is also likely to suffer from the same problem because they are using an order of magnitude greater coefficient for the term corresponding to p(y|x) than for the other terms in the Langevin equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In order to evaluate the effectiveness of our newly proposed architecture for the discriminator, we conducted two sets of experiments: class conditional image generation and super-resolution on ILSVRC2012 (ImageNet) dataset <ref type="bibr" target="#b21">(Russakovsky et al., 2015)</ref>. For both tasks, we used the ResNet <ref type="bibr" target="#b8">(He et al., 2016b)</ref> based discriminator and the generator used in <ref type="bibr" target="#b6">Gulrajani et al. (2017)</ref>, and applied spectral normalization <ref type="bibr" target="#b14">(Miyato et al., 2018)</ref> to the all of the weights of the discriminator to regularize the Lipschitz constant. For the objective function, we used the following hinge version of the standard adversarial loss (1) <ref type="bibr" target="#b12">(Lim &amp; Ye, 2017;</ref><ref type="bibr" target="#b28">Tran et al., 2017</ref>)</p><formula xml:id="formula_10">L(Ĝ, D) = E q(y) E q(x|y) [max(0, 1 − D(x, y)] + E q(y) E p(z) max(0, 1 + D Ĝ (z, y), y) , L(G,D) = −E q(y) E p(z) D (G(z, y), y)) ,<label>(11)</label></formula><p>where the last activation function A of D is identity function. p(z) is standard Gaussian distribution and G(z, y) is the generator network. For all experiments, we used Adam optimizer <ref type="bibr" target="#b11">(Kingma &amp; Ba, 2015)</ref> with hyper-parameters set to α = 0.0002, β 1 = 0, β 2 = 0.9. We updated the discriminator five times per each update of the generator. We will use concat to designate the models <ref type="figure" target="#fig_0">(Figure 1b</ref>) 2 , and use projection to designate the proposed model <ref type="figure" target="#fig_0">(Figure 1d</ref>) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CLASS-CONDITIONAL IMAGE GENERATION</head><p>The ImageNet dataset used in the experiment of class conditional image generation consisted of 1,000 image classes of approximately 1,300 pictures each. We compressed each images to 128×128 pixels. Unlike for AC-GANs 3 we used a single pair of a ResNet-based generator and a discriminator. Also, we used conditional batch normalization <ref type="bibr" target="#b4">(Dumoulin et al., 2017b;</ref><ref type="bibr" target="#b1">de Vries et al., 2017)</ref> for the generator. As for the architecture of the generator network used in the experiment, please see <ref type="figure" target="#fig_0">Figure 14</ref> for more detail. Our proposed projection model discriminator is equipped with a 'projection layer' that takes inner product between the embedded one-hot vector y and the intermediate output ( <ref type="figure" target="#fig_0">Figure 14a</ref>). As for the structure of the the concat model discriminator to be compared against, we used the identical bulk architecture as the projection model discriminator, except that we removed the projection layer from the structure and concatenated the spatially replicated embedded conditional vector y to the output of third ResBlock. We also experimented with AC-GANs as the current state of the art model. For AC-GANs, we placed the softmax layer classifier to the same structure shared by concat and projection. For each method, we updated the generator 450K times, and applied linear decay for the learning rate after 400K iterations so that the rate would be 0 at the end. For the comparative experiments, we trained the model for 450K iterations, which was ample for the training of concat to stabilize. AC-GANs collapsed prematurely before the completion of 450K iterations, so we reported the result from the peak of its performance ( 80K iterations). For all experiments throughout, we used the training over 450K iterations for comparing the performances. On a separate note, our method continued to improve even after 450K. We therefore also reported the inception score and FID of the extended training (850K iterations) for our method exclusively. See the table 1 for the exact figures.</p><p>We used inception score <ref type="bibr" target="#b23">(Salimans et al., 2016)</ref> for the evaluation of the visual appearance of the generated images. It is in general difficult to evaluate how 'good' the generative model is. Indeed, however, either subjective or objective, some definite measures of 'goodness' exists, and essential two of them are 'diversity' and the sheer visual quality of the images. One possible candidate for quantitative measure of diversity and visual appearance is FID <ref type="bibr" target="#b9">(Heusel et al., 2017)</ref>. We computed FID between the generated images and dataset images within each class, and designated the values as intra FIDs. More precisely, FID <ref type="bibr" target="#b9">(Heusel et al., 2017)</ref> measures the 2-Wasserstein distance between the two distributions q y and p y , and is given by F (q y , p y ) = µ qy − µ py 2 2 + trace C qy + C py − 2(C qy C py ) 1/2 , where {µ qy , C qy }, {µ py , C py } are respectively the mean and the covariance of the final feature vectors produced by the inception model <ref type="bibr" target="#b25">(Szegedy et al., 2015)</ref> from the true samples and generated samples of class y. When the set of generated examples have collapsed modes, the trace of C py becomes small and the trace term itself becomes large. In order to compute C qy we used all samples in the training data belonging to the class of concern, and used 5000 generated samples for the computation of C py . We empirically observed in our experiments that intra FID is, to a certain extent, serving its purpose well in measuring the diversity and the visual quality.</p><p>2 in the preliminary experiments of the image geneation task on CIFAR-10 <ref type="bibr" target="#b27">(Torralba et al., 2008)</ref> and CIFAR-100 <ref type="bibr" target="#b27">(Torralba et al., 2008)</ref>, we confirmed that hidden concatenation is better than input concatenation in terms of the inception scores. For more details, please see <ref type="table" target="#tab_2">Table 3</ref> in the appendix section.</p><p>3 For AC-GANs, the authors prepared a pair of discriminator and generator for each set classes of size 10.  To highlight the effectiveness of our inner-product based approach (projection) of introducing the conditional information into the model, we compared our method against the state of the art AC-GANs as well as the conventional incorporation of the conditional information via concatenation at hidden layer (concat). As we can see in the training curves <ref type="figure">Figure 3</ref>, projection outperforms inception score than concat throughout the training. <ref type="table" target="#tab_0">Table 1</ref> compares the intra class FIDs and the inception Score of the images generated by each method. The result shown here for the AC-GANs is that of the model at its prime in terms of the inception score, because the training collapsed at the end. We see that the images generated by projection have lower intra FID scores than both adversaries, indicating that the Wasserstein distance between the generative distribution by projection to the target distribution is smaller. For the record, our model performed better than other models on the CIFAR10 and CIFAR 100 as well (See Appendix A). <ref type="figure" target="#fig_0">Figure 10a</ref> and 10b shows the set of classes for which (a) projection yielded results with better intra FIDs than the concat and (b) the reverse. From the top, the figures are listed in descending order of the ratio between the intra FID score between the two methods. Note that when the concat outperforms projection it only wins by a slight margin, whereas the projection outperforms concat by large margin in the opposite case. A quick glance on the cases in which the concat outperforms the projection suggests that the FID is in fact measuring the visual quality, because both sets looks similar to the human eyes in terms of appearance. <ref type="figure" target="#fig_3">Figure 5</ref> shows an arbitrarily selected set of results yielded by AC-GANs from variety of zs. We can clearly observe the mode-collapse on this batch. This is indeed a tendency reported by the inventors themselves <ref type="bibr" target="#b17">(Odena et al., 2017)</ref>. AC-GANs can generate easily recognizable (i.e classifiable) images, but at the cost of losing diversity and hence at the cost of constructing a generative distribution that is significantly different from the target distribution as a whole. We can also assess the low FID score of projection from different perspective. By construction, the trace term of intra FID measures the degree of diversity within the class. Thus, our result on the intra FID scores also indicates that that our projection is doing better in reproducing the diversity of the original. The GANs with the concat discriminator also suffered from mode-collapse for some classes (see <ref type="figure" target="#fig_4">Figure 6</ref>). For the set of images generated by projection, we were not able to detect any notable mode-collapse. <ref type="figure" target="#fig_5">Figure 7a</ref> shows the samples generated with the projection model for the classes on which the cGAN achieved lowest intra FID scores (that is the classes on which the generative distribution were particularly close to target conditional distribution), and <ref type="figure" target="#fig_5">Figure 7b</ref> the reverse. While most of the images listed in <ref type="figure" target="#fig_5">Figure 7a</ref> are of relatively high quality, we still observe some degree of mode-collapse. Note that the images in the classes with high FID are featuring complex objects like human; that is, one can expect the diversity within the class to be wide. However, we note that  we did not use the most complicated neural network available for the experiments presented on this paper, because we prioritized the completion of the training within a reasonable time frame. It is very possible that, by increasing the complexity of the model, we will be able to further improve the visual quality of the images and the diversity of the distribution.</p><p>Category Morphing With our new architecture, we were also able to successfully perform category morphism. When there are classes y 1 and y 2 , we can create an interpolated generator by simply mixing the parameters of conditional batch normalization layers of the conditional generator corresponding to these two classes. <ref type="figure" target="#fig_6">Figure 8</ref> shows the output of the interpolated generator with the same z. Interestingly, the combination is also yielding meaningful images when y 1 and y 2 are significantly different.</p><p>Fine-tuning with the pretrained model on the ILSVRC2012 classification task. As we mentioned in Section 4, the authors of Plug and Play Generative model (PPGNs) <ref type="bibr" target="#b15">(Nguyen et al., 2017)</ref> were able to improve the visual appearance of the model by augmenting the cost function with that of the label classifier. We also followed their footstep and augmented the original generator loss with an additional auxiliary classifier loss. As warned earlier regarding this type of approach, however, this type of modification tends to only improve the visual performance of the images that are easy for the pretrained model to classify. In fact, as we can see in Appendix B, we were able to improve the visual appearance the images with the augmentation, but at the cost of diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SUPER-RESOLUTION</head><p>We also evaluated the effectiveness of (3) in its application to the super-resolution task. Put formally, the super-resolution task is to infer the high resolution RGB image of dimension x ∈ R R H ×R H ×3 from the low resolution RGB image of dimension y ∈ R R L ×R L ×3 ; R H &gt; R L . This task is very much the case that we presumed in our model construction, because p(y|x) is most likely unimodal even if p(x|y) is multimodal. For the super-resolution task, we used the following formulation for discriminator function:</p><formula xml:id="formula_11">f (x, y; θ) = i,j,k (y ijk F ijk (φ(x; θ Φ ))) + ψ(φ(x; θ Φ ); θ Ψ ),<label>(12)</label></formula><p>(a) Generated images on the class with 'low' FID scores.</p><p>(b) generated images on the class with 'high' FID scores. where</p><formula xml:id="formula_12">F (φ(x; θ Φ )) = V * φ(x; θ Φ )</formula><p>where V is a convolutional kernel and * stands for convolution operator. Please see <ref type="figure" target="#fig_0">Figure 15</ref> in the appendix section for the actual network architectures we used for this task. For this set of experiments, we constructed the concat model by removing the module in the projection model containing the the inner product layer and the accompanying convolution layer altogether, and simply concatenated y to the output of the ResBlock preceding the inner product module in the original. As for the resolutions of the image datasets, we chose R H = 128 and R L = 32, and created the low resolution images by applying bilinear downsampling on high resolution images. We updated the generators 150K times for all methods, and applied linear decay for the learning rate after 100K iterations so that the final learning rate was 0 at 150K-th iteration. <ref type="figure">Figure 9</ref> shows the result of our super-resolution. The bicubic super-resolution is very blurry, and concat result is suffering from excessively sharp and rough edges. On the other hand, the edges of the images generated by our projection method are much clearer and smoother, and the image itself is much more faithful to the original high resolution images. In order to qualitatively compare the performances of the models, we checked MS-SSIM <ref type="bibr" target="#b29">(Wang et al., 2003)</ref> and the classification accuracy of the inception model on the generated images using the validation set of the ILSVRC2012 dataset. As we can see in <ref type="table" target="#tab_1">Table 2</ref>, our projection model was able to achieve high inception accuracy  and high MS-SSIM when compared to bicubic and concat. Note that the performance of superresolution with concat model even falls behind those of the bilinear and bicubic super-resolutions in terms of the inception accuracy. Also, we used projection model to generate multiple batches of images with different random values of z to be fed to the generator and computed the average of the logits of the inception model on these batches (MC samples). We then used the so-computed average logits to make prediction of the labels. With an ensemble over 10 seeds (10 MC in <ref type="table" target="#tab_1">Table 2</ref>), we were able to improve the inception accuracy even further. This result indicates that our GANs are learning the super-resolution as an distribution, as opposed to deterministic function. Also, the success with the ensemble also suggests a room for a new way to improve the accuracy of classification task on low resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Any specification on the form of the discriminator imposes a regularity condition for the choice for the generator distribution and the target distribution. In this research, we proposed a model for the discriminator of cGANs that is motivated by a commonly occurring family of probabilistic models. This simple modification was able to significantly improve the performance of the trained generator Published as a conference paper at ICLR 2018 <ref type="figure">Figure 9</ref>: 32x32 to 128x128 super-resolution by different methods on conditional image generation task and super-resolution task. The result presented in this paper is strongly suggestive of the importance of the choice of the form of the discriminator and the design of the distributional metric. We plan to extend this approach to other applications of cGANs, such as semantic segmentation tasks and image to image translation tasks.</p><p>(a) Images better with projection than concat.</p><p>(b) Images better with concat than projection. A RESULTS OF CLASS CONDITIONAL IMAGE GENERATION ON CIFAR-10 AND CIFAR-100</p><p>As a preliminary experiment, we compared the performance of conditional image generation on CIFAR-10 and CIFAR-100 3. For the discriminator and the generator, we reused the same architecture used in <ref type="bibr" target="#b14">Miyato et al. (2018)</ref> for the task on CIFAR-10. For the adversarial objective functions, we used (11), and trained both machine learners with the same optimizer with same hyper parameters we used in Section 5. For our projection model, we added the projection layer to the discriminator in the same way we did in the ImageNet experiment (before the last linear layer). Our projection model achieved better performance than other methods on both CIFAR-10 and CIFAR-100. Concatenation at hidden layer (hidden concat) was performed on the output of second ResBlock of the discriminator. We tested hidden concat as a comparative method in our main experiments on ImageNet, because the concatenation at hidden layer performed better than the concatenation at the input layer (input concat) when the number of classes was large (CIFAR-100).</p><p>To explore how the hyper-parameters affect the performance of our proposed architecture, we conducted hyper-parameter search on CIFAR-100 about the Adam hyper-parameters (learning rate α and 1st order momentum β 1 ) for both our proposed architecture and the baselines. Namely, we varied each one of these parameters while keeping the other constant, and reported the inception scores for all methods including several versions of concat architectures to compare. We tested with concat module introduced at (a) input layer, (b) hidden layer, and at (c) output layer. As we can see in <ref type="figure" target="#fig_0">Figure 11</ref>, our projection architecture excelled over all other architectures for all choice of the parameters, and achieved the inception score of 9.53. Meanwhile, concat architectures were able to achieve all 8.82 at most. The best concat model in term of the inception score on CIFAR-100 was the hidden concat with α = 0.0002 and β 1 = 0, which turns out to be the very choice of the parameters we picked for our ImageNet experiment.    In this experiment, we followed the footsteps of Plug and Play Generative model (PPGNs) <ref type="bibr" target="#b15">(Nguyen et al., 2017)</ref> and augmented the original generator loss with an additional auxiliary classifier loss. In particular, we used the losses given by :</p><formula xml:id="formula_13">L G,D,p pre (y|x) = −E q(y) E p(z) D (G(z, y), y) − L C (p pre (y|G(z, y))) ,<label>(13)</label></formula><p>wherep pre (y|x) is the fixed model pretrained for ILSVRC2012 classification task. For the actual experiment, we trained the generator with the original adversarial loss for the first 400K updates, and used the augmented loss for the last 50K updates. For the learning rate hyper parameter, we adopted the same values as other experiments we described above. For the pretrained classifier, we used ResNet50 model used in <ref type="bibr" target="#b7">He et al. (2016a)</ref>. <ref type="figure" target="#fig_0">Figure 12</ref> compares the results generated by vanilla objective function and the results generated by the augmented objective function. As we can see in <ref type="table" target="#tab_3">Table 4</ref>, we were able to significantly outperform PPGNs in terms of inception score. However, note that the images generated here are images that are easy to classify. The method with auxiliary classifier loss seems effective in improving the visual appearance, but not in training faithful generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C MODEL ARCHITECTURES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv 3x3</head><p>ReLU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv 3x3</head><p>ReLU (a) ResBlock architecture for the discriminator. Spectral normalization <ref type="bibr" target="#b14">(Miyato et al., 2018)</ref> was applied to each conv layer.  For the ResBlock in the generator for the super resolution tasks that implements the upsampling, the random vector z was fed to the model by concatenating the vector to the embedded low resolution image vector y prior to the first convolution layer within the block. For the procedure of downsampling and upsampling, we followed the implementation by <ref type="bibr" target="#b6">Gulrajani et al. (2017)</ref>. For the discriminator, we performed downsampling (average pool) after the second conv of the ResBlock. For the generator, we performed upsampling before the first conv of the ResBlock. For the ResBlock that is performing the downsampling, we replaced the identity mapping with 1x1 conv layer followed by downsampling to balance the dimension. We did the essentially same for the Resblock that is performing the upsampling, except that we applied the upsampling before the 1x1 conv.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BN</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Discriminator models for conditional GANs (a) Images generated with the projection model. (left) Tibetan terrier and (right) mushroom. (b) (left) Consecutive category morphing with fixed z. geyser → Tibetan terrier → mushroom → robin. (right) category morphing from Tibetan terrier to mushroom with different value of fixed z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The generator trained with the projection model can generate diverse set of images. For more results, see the experiment section and the appendix section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of intra FID scores for projection concat, and AC-GANs on ImageNet. Each dot corresponds to a class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>comparison of the images generated by (a) AC-GANs and (b) projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Collapsed images on the concat model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>128×128 pixel images generated by the projection method for the classes with (a) bottom five FID scores and (b) top five FID scores. The string and the value above each panel are respectively the name of the corresponding class and the FID score. The second row in each panel corresponds to the original dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Category morphing. More results are in the appendix section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Comparison of concat vs. projection. The value attached above each panel represents the achieved FID score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Varying α (fixing β1 = 0) Varying β1 (fixing α = 0.0002)Figure 11: Inception scores on CIFAR-100 with different discriminator models varying hyperparameters (α and β 1 ) of Adam optimizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Effect of the finetuning with auxiliary classifier loss. Same coordinate in panel (a) and (b) corresponds to same value of z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Architecture of the ResBlocks used in all experiments. For the generator generator's Resblock, conditional batch normalization layer (Dumoulin et al., 2017b; de Vries et al., 2017) was used in place of the standard batch normalization layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :Figure 16 :</head><label>1416</label><figDesc>The models we used for the conditional image generation task. Dog (Lhasa apso) to different categories(a) Hip to Yellow lady's slipper (b) Pirate ship to Yawl (c) Yurt to Castle (d) Chiffonier to Chinese cabinet (e) Valley to Sandbar Figure 17: Morphing between different categories E MORE RESULTS WITH SUPER-RESOLUTION Figure 18: 32x32 to 128x128 super-resolution results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Inception score and intra FIDs on ImageNet.</figDesc><table><row><cell>Method</cell><cell cols="2">Inception Score Intra FID</cell></row><row><cell>AC-GANs</cell><cell>28.5±.20</cell><cell>260.0</cell></row><row><cell>concat</cell><cell>21.1±.35</cell><cell>141.2</cell></row><row><cell>projection</cell><cell>29.7±.61</cell><cell>103.1</cell></row><row><cell>*projection (850K iteration)</cell><cell>36.8±.44</cell><cell>92.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Inception accuracy and MS-SSIM on different super-resolution methods. We picked up dataset images from the validation set.</figDesc><table><row><cell>Method</cell><cell cols="5">biliear bicubic concat projection projection (10 MC)</cell></row><row><cell>Inception Acc.(%)</cell><cell>23.1</cell><cell>31.4</cell><cell>11.0</cell><cell>35.2</cell><cell>36.4</cell></row><row><cell>MS-SSIM</cell><cell>0.835</cell><cell>0.859</cell><cell>0.829</cell><cell>0.878</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The performance of class conditional image generation on CIFAR-10 (C10) and CIFAR-100 (C100).</figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell cols="2">Inception score C10 C100 C10 C100 FID</cell></row><row><cell></cell><cell></cell><cell>(Real data)</cell><cell>11.24</cell><cell>14.79 7.60</cell><cell>8.94</cell></row><row><cell></cell><cell></cell><cell>AC-GAN</cell><cell>8.22</cell><cell>8.80 19.7</cell><cell>25.4</cell></row><row><cell></cell><cell></cell><cell>input concat</cell><cell>8.25</cell><cell>7.93 19.2</cell><cell>31.4</cell></row><row><cell></cell><cell></cell><cell>hidden concat</cell><cell>8.14</cell><cell>8.82 19.2</cell><cell>24.8</cell></row><row><cell></cell><cell></cell><cell>(ours) projection</cell><cell>8.62</cell><cell>9.04 17.5</cell><cell>23.2</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>9</cell><cell></cell><cell></cell></row><row><cell>Inception score</cell><cell>4 5 6 7 8 3</cell><cell>Projection Input concat Hidden concat Output concat</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>AC-GANs</cell><cell></cell></row><row><cell></cell><cell>0.0001 1</cell><cell>0.0002</cell><cell>0.0005</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Inception score and intra FIDs on ImageNet with a pretrained model on classification tasks for ILSVRC2012 dataset. ‡Nguyen et al. (2017)</figDesc><table><row><cell>Method</cell><cell cols="2">Inception Score Intra FID</cell></row><row><cell>PPGNs  ‡</cell><cell>47.4</cell><cell>N/A</cell></row><row><cell>projection(finetuned)</cell><cell>210</cell><cell>54.2</cell></row><row><cell cols="3">B OBJECTIVE FUNCTION WITH AN AUXILIARY CLASSIFIER COST</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When y is discrete label information, we can assume that it is encoded as a one-hot vector.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the members of Preferred Networks, Inc., especially Richard Calland, Sosuke Kobayashi and Crissman Loomis, for helpful comments. We would also like to thank Shoichiro Yamaguchi, a graduate student of Kyoto University, for helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 15</ref><p>: The models we used for the super resolution task.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6576" to="6586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein GANs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungkwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba ; Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometric GAN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka. F-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Invertible conditional gans for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guim</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Málvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Adversarial Training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Kumar Sricharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Shreve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Saketh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05789</idno>
		<title level="m">Semi-supervised conditional GANs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of workshop on machine learning systems (LearningSys) in the twentyninth annual conference on neural information processing systems (NIPS)</title>
		<meeting>workshop on machine learning systems (LearningSys) in the twentyninth annual conference on neural information processing systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08896</idno>
		<title level="m">Deep and hierarchical implicit models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conference on Signals, Systems and Computers</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
