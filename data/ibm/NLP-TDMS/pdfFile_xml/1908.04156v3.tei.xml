<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIP: Local Importance-based Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziteng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LIP: Local Importance-based Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatial downsampling layers are favored in convolutional neural networks (CNNs) to downscale feature maps for larger receptive fields and less memory consumption. However, for discriminative tasks, there is a possibility that these layers lose the discriminative details due to improper pooling strategies, which could hinder the learning process and eventually result in suboptimal models. In this paper, we present a unified framework over the existing downsampling layers (e.g., average pooling, max pooling, and strided convolution) from a local importance view. In this framework, we analyze the issues of these widely-used pooling layers and figure out the criteria for designing an effective downsampling layer. According to this analysis, we propose a conceptually simple, general, and effective pooling layer based on local importance modeling, termed as Local Importance-based Pooling (LIP). LIP can automatically enhance discriminative features during the downsampling procedure by learning adaptive importance weights based on inputs. Experiment results show that LIP consistently yields notable gains with different depths and different architectures on ImageNet classification. In the challenging MS COCO dataset, detectors with our LIP-ResNets as backbones obtain a consistent improvement (â‰¥ 1.4%) over the vanilla ResNets, and especially achieve the current state-of-the-art performance in detecting small objects under the single-scale testing scheme.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For discriminative tasks like image classification <ref type="bibr" target="#b7">[8]</ref> and object detection <ref type="bibr" target="#b26">[27]</ref>, the modern architectures of convolutional neural networks (CNNs) mostly utilize spatial downsampling (pooling) layers to reduce the spatial size of feature maps in the hidden layers. Such pooling layers are for larger receptive fields and less memory consumption, especially in extremely deep networks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15]</ref>. The widelyused max pooling, average pooling, and strided convolution use a sliding window whose stride is larger than 1 and pool Figure 1: Illustration of our motivation. (Left to right) the original image, some nearby patches, corresponding illustrative activations in the feature map to downsample, and last, the output activations that we need. Here, red-tone activations are caused by the foreground bird. Blue-tone activations are caused by the background clutter in the top patch or the representive blue feather in the bottom patch. We want to preserve the red-tone activations in the top patch window and the blue-tone activations in the bottom. The downsampling method should recognize discriminative features adaptively across sliding windows.</p><p>features by different strategies in each local window. But these layers might prevent discriminative details from being well preserved, which are crucial for recognition and detection task. This is especially undesirable for discriminative features of tiny objects, as such details might be diluted with clutter activations or even not be sampled by improper downsampling strategies.</p><p>In this paper, we aim to address these issues raised by the existing downsampling layers. To analyze their drawbacks, we present a unified framework from a local importance view. Under this new perspective, the existing pooling procedure could be seen as aggregating features with their local importance in each sliding window. To our best knowledge, we are the first to present a framework from the importance view for downsampling layers, which allows us to analyze and improve the pooling methods in a more principled way. As a result, we show that average and max pooling are both suboptimal due to the strong assumption or the invalid prior knowledge. Strided convolution adopts the improper interval sampling and also fails to model importance adaptively. To overcome their limitations, we present a new pooling method to learn importance weights automatically, coined as Local Importance-based Pooling (LIP).</p><p>Basically, we argue that not all nearby pixels contribute equally and some features are more discriminative than the others within a neighborhood in the downsampling procedure, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Therefore, it is expected to explicitly model the local importance and build a metric measure over pixels within local neighborhoods. From this analysis, we propose the LIP to meet the requirement of an ideal pooling operation. Specifically, LIP proposes to learn the metric of importance by a subnetwork based on the input features automatically. In this sense, LIP is able to adaptively determine which features are more important to be kept through downsampling. For instance, LIP enables the network to preserve features of tiny targets while discarding false activations of the background clutter when recognizing or detecting small objects. Moreover, LIP is a more generic pooling method than the existing methods, in sense that it is capable of mimicking the behavior of average pooling, max pooling and detail-preserving pooling <ref type="bibr" target="#b32">[33]</ref>. Experiments show LIP outperforms baseline methods by a large margin on ImageNet <ref type="bibr" target="#b7">[8]</ref> with different architectures. We also evaluate our LIP backbones on the challenging COCO detection task <ref type="bibr" target="#b26">[27]</ref>, where localizing small objects play an important role. The both one-and two-stage detectors with our LIP-ResNets as backbones obtain a consistent improvement over the vanilla ResNets, and in particular achieve a new state-of-the-art performance in detecting small objects under the single-scale testing scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Downsampling layers as basic layers in CNNs were proposed with LeNet-5 <ref type="bibr" target="#b19">[20]</ref> as a way to reduce spatial resolution by summing out values in a sliding window. Spatial downsampling procedures also exist in traditional methods. For example, HOG and SIFT <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref> aggregated the gradient descriptors within each spatial neighborhood. Bag of words (BoW) based models also used intensive pooling in object recognition as to obtain more robust representations against translation and scale variance <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Modern CNNs utilize pooling layers to downscale feature maps mainly for larger receptive field and less memory consumption. VGG <ref type="bibr" target="#b33">[34]</ref>, Inception <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">39]</ref> and DenseNet <ref type="bibr" target="#b16">[17]</ref> used average and max pooling as downsampling layers. ResNet <ref type="bibr" target="#b14">[15]</ref> adopted convolutions whose stride is not 1 to extract features at regular non-consecutive locations as downsampling layers.</p><p>Some pooling methods, including global average pooling <ref type="bibr" target="#b23">[24]</ref>, ROI pooling <ref type="bibr" target="#b8">[9]</ref>, and ROI align <ref type="bibr" target="#b13">[14]</ref>, aim to downscale feature maps of arbitrary size to a fixed size and therefore enable the network to cooperate with inputs of different sizes. We do not discuss these methods as they are designed to specific architectures. Here, we only focus on pooling layers inside networks, that is, the ones that gradually downscale feature maps by a fixed ratio.</p><p>There are some analysis on pooling methods before the widespread application of CNNs. Boureau et al. <ref type="bibr" target="#b1">[2]</ref> analyzed average and max pooling in traditional methods, and proved that max pooling can preserve more discriminative features than average pooling in terms of probability. The work <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b41">41]</ref> showed that pooling can be without specific forms and learning to pool features is beneficial. Our work mainly follows this research line and our results further support these conclusions. Recent work about pooling has focused on how to better downscale feature maps in CNNs through new pooling layers. Fractional pooling <ref type="bibr" target="#b10">[11]</ref> and S3pool <ref type="bibr" target="#b46">[46]</ref> tried to improve the way to perform spatial transformation of pooling, which is not the focus of our paper. Mixed and hybrid pooling <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b20">21]</ref> used various combinations of max and average pooling to perform downscaling. L p pooling <ref type="bibr" target="#b12">[13]</ref> aggregated activations in the L p norm way, which can be viewed as a continuum between max and average pooling controlled by the learned p. These methods can unify max and average pooling and further improve the performance of networks. However, they can simply learn better pooling method based on average pooling and max pooling, or the combination of them, but fails to provide more insights about general donwsampling methods. Saeedan et al. <ref type="bibr" target="#b32">[33]</ref> argued that details should be preserved and redundant features can be discarded by proposed detail-preserving pooling (DPP). The detail criterion of DPP is relatively handcrafted by calculating the deviation from statistics of pixels in sliding windows, which is heuristic and may be not optimal.</p><p>In this paper, we analyze widely-used pooling layers based on a local importance view, which has not been investigated in previous work. Our proposed LIP, naturally arisen from this concept, outperforms hand-crafted pooling layers by a large margin.</p><p>Attention-based methods are recently popular in computer vision community <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b49">49]</ref>. Our LIP can be also seen as a local attention approach designed for pooling, of which attention weights are in the softmax form. LIP mainly differs from other attention methods in two important aspects for the better compatibility with downsampling procedure: (1) attention weights are produced by local convolutions in logit modules and then normalized locally; (2) LIPs do not adopt the key-query schemes in attention modeling for achieving better shift invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Local Importance Modeling</head><p>In this section, we first present the framework for downsampling layers from local importance modeling view. We discuss some widely-used pooling layers in this framework. Next, we describe our proposed local importance-based pooling (LIP), which naturally arises from this analysis. Finally, we show how to equip popular architectures with LIP layers and then obtain LIP-ResNet and LIP-DenseNet.  For strided convolution, the window size is equivalent to the stride, which is 2 here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework and Analysis</head><p>To analyze the existing downsampling methods and well motivate our LIP, we present a unified framework for downsampling layers from the view of local importance, named Local Aggregation and Normalization (LAN). Specifically, given the input feature map I, the kernel indice set â„¦ consisting of relative sampling locations (âˆ†x, âˆ†y) in a sliding window, and the left-top location (x, y) corresponding to the sliding window in the input feature map with regrad to the output location (x , y ), the LAN framework is formulated as:</p><formula xml:id="formula_0">O x ,y = (âˆ†x,âˆ†y)âˆˆâ„¦ F (I) x+âˆ†x,y+âˆ†y I x+âˆ†x,y+âˆ†y (âˆ†x,âˆ†y)âˆˆâ„¦ F (I) x+âˆ†x,y+âˆ†y ,<label>(1)</label></formula><p>where F (I) is the importance map whose size is the same with I and F (I) â‰¥ 0 over space. The division (x/x , y/y ) stands for the stride factor, e.g., x = 2x , y = 2y for 2 Ã— 2 stride. We simply denote a stride 2 Ã— 2 as 2 in this paper. As the name of the framework implies, pooling in this view can be seen two steps: aggregate features with the importance F (I) and normalize them by importance within local sliding windows. This framework can be extended naturally to the multi-channel situation.</p><p>One can see pooling in this framework as weighted sum over each window where weights are locally normalized importance:</p><formula xml:id="formula_1">F (I) x+âˆ†x,y+âˆ†y (âˆ†x,âˆ†y)âˆˆâ„¦ F (I) x+âˆ†x,y+âˆ†y ,<label>(2)</label></formula><p>for I x+âˆ†x,y+âˆ†y , which we term simply as local importance. Therefore, local importance stands for weights of features within a sliding window. We can analyze which features in downsampling procedures are more important than others nearby by F (I).</p><p>Our motivation is that since the feature pooling procedure is intrinsically lossy as it squeezes large input into small output, it is necessary to carefully consider which features to sample and how to aggregate them in a small sliding window as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Sampled features should be discriminative enough for the target tasks. The LAN framework provides a principled way to understand and improve these pooling methods by studying the corresponding importance function F . Next, we analyze some widely-used downsampling layers in this framework and figure out the requirement of an ideal pooling operation. <ref type="figure" target="#fig_1">Figure 2</ref> shows some of these downsampling methods viewing in the framework.</p><p>Average and max pooling. As discussed in <ref type="bibr" target="#b1">[2]</ref>, given F (I) = exp(Î²I), Î² = 0 gives average pooling and Î² â†’ âˆž gives max pooling. Average pooling associates features with the same importance to all locations during aggregation in a small window, while max pooling put all attention on the largest activation within a neighborhood. We argue that both of them are suboptimal. Average pooling harms discriminative but small features and cause blurry downsampled features due to the strong assumption of the local equality of features. Max pooling as an improvement over average pooling on feature selection, however, assumes that the most discriminative feature should be of the maximum activation. This assumption mainly has two drawbacks. First, the prior knowledge that the maximum activation stands for the most discriminative detail, may not be always true. Second, the max operator over sliding windows hinders gradient-based optimization since in the backpropagation gradients are assigned only to the local maximums, as discussed in <ref type="bibr" target="#b32">[33]</ref>. These sparse gradients would further enhance this inconsistence, in sense that discriminative activations will never become maximums unless current maximums are suppressed.</p><p>Strided convolutions. Strided convolutions can be seen as dense convolutions whose stride is 1, followed by spatial subsampling <ref type="bibr" target="#b47">[47]</ref>. This spatial subsampling can be interpreted as downsampling in our framework with F (I) x,y = 1, if x and y are both multiples of s, 0, otherwise,</p><p>where I is densely convolved features and s is both the stride factor and sliding window size. From this perspective, the downsampling part of strided convolutions fails to model the importance in downsampling procedures adaptively. Moreover, it focuses only on one fixed location within each sliding window and discards the rest. This fixed interval sampling scheme will limit shift invariance, as convolutional patterns are required to appear at specific and non-consecutive locations to activate. In this sense, minor shifts and distortions can lead to great changes in downsampled features and thus disturb the shift invariance of CNNs <ref type="bibr" target="#b47">[47]</ref>. For the case of strided 1 Ã— 1 convolutions, it is even worse since the feature map are not fully utilized <ref type="bibr" target="#b15">[16]</ref> and it will incur gradient checkerboard problem <ref type="bibr" target="#b31">[32]</ref>.</p><p>Detail-preserving pooling. Recent proposed detailpreserving pooling (DPP) <ref type="bibr" target="#b32">[33]</ref> uses the detail criterion as importance function F , which is measured by the deviations of features from the activation statistics in sliding windows. DPP solves the problem of max pooling by designing more sophisticated importance function and ensuring the continuity for better gradient optimization. However, the assumption in DPP is heuristic and the more detailed feature might be the less discriminative ones. For example, the background clutter could be more detailed than a bird of solid color in foreground. Therefore, DPP might preserve the less discriminative details to outputs. Handcrafted importance functions in max pooling and DPP incorporate the general prior knowledge into downsampling procedure, which might lead to the inconsistence with the final target of discriminative tasks.</p><p>Requirements of ideal pooling. From the analysis above, we can figure out the requirement of an ideal pooling layer. First, the downsampling procedure is expected to handle minor shifts and distortions as much as possible, and thus should avoid adopting the fixed interval sampling scheme, i.e., F used by strided convolutions. Second, the importance function F should be selective to the discriminative features rather than manually designed based on prior knowledge, i.e., F used in max pooling and DPP. This discriminativeness measure should be adaptive to different tasks and automatically determined by the final objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Importance-based Pooling</head><p>To meet requirements of ideal pooling arisen from local importance view in the LAN framework, we propose local importance-based pooling (LIP). By using a learnable network G in F , the importance function now is not limited in hand-crafted forms and able to learn the criterion for the discriminativeness of features. Also, we restrict the window size of LIP to be not less than stride to fully utilize the feature map and avoid the issue of fixed interval sampling scheme. More specifically, the importance function in LIP is implemented by a tiny fully convolutional network (FCN) <ref type="bibr" target="#b27">[28]</ref>, which learns to produce the importance map based on inputs in an end-to-end manner. To make the importance weights non-negative and easy to optimize, we add exp(Â·) operation on top of G, that is:</p><formula xml:id="formula_3">F (I) = exp(G(I)),<label>(4)</label></formula><p>where G and G(I) are named the logit module and the logit respectively as G(I) is the logarithm of the importance. In contrast to the hand-crafted form specified by prior knowledge in max pooling or DPP, the logit module G is able to learn a better and more compatible importance criterion for both the network and target task. More concretely, according to Equation <ref type="formula" target="#formula_0">(1)</ref>, LIP is then written as: (5) With LIP, discriminative features can be automatically emphasized during downsampling procedure by learning a larger value of G(I) at the corresponding locations. In the current implementation of LIP, the logit is calculated in a channel wise manner. <ref type="figure" target="#fig_2">Figure 3</ref> shows the diagram and Py-Torch implementation of LIP.</p><p>Deformable modeling of LIP. At the macro level, learnable importance function F of LIP enables the network to model deformation of objects by learning a good effective spatial allocation of features into downsampling with adaptive importance weights. Different from deformable convolutions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b50">50]</ref> to sample features by bilinear interpolation with adaptive offsets, LIP explicitly performs spatially dynamic feature selection based on inputs and thus has deformable receptive fields. Empirical evidence of the deformable capacity of LIP is shown and discussed in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Exemplars: LIP-ResNet and LIP-DenseNet</head><p>ResNet <ref type="bibr" target="#b14">[15]</ref> and DenseNet <ref type="bibr" target="#b16">[17]</ref> are typical architectures among modern CNNs. ResNet mainly uses strided convo-lutions as downsampling layers except one max pooling in the bottom. DenseNet utilizes average pooling in transition blocks, and in the bottom a strided convolution layer and max pooling like ResNet to downscale feature maps.</p><p>Architectures with LIP. We adopt the revised ResNet <ref type="bibr" target="#b11">[12]</ref> as our plain ResNet baseline, where residual branches employ 3 Ã— 3 kernel for strided convolutions, shown in <ref type="figure" target="#fig_4">Figure 4a</ref>. To build LIP variants, we replace max pooling in the bottom and strided convolutions in downsampling blocks with LIP. As discussed in Section 3.1, strided convolutions in ResNet could be replaced by a dense convolution and a following LIP. However, this substitution is computational intensive and memory inefficient. We instead first downscale features and then perform convolution. In this sense, we use a LIP and a following convolution to replace strided convolutions in residual and shortcut branches, as shown in <ref type="figure" target="#fig_4">Figure 4b</ref>. To keep receptive fields the same and avoid the interval sampling problem, we set the window size of LIP to 3 Ã— 3 and the following convolution to 1 Ã— 1. We leave the global average pooling in the top of ResNet unchanged. Total 7 layers (1 for max pooling, 3 Ã— 2 for strided convolution) are replaced with LIP layers. We name this modified ResNet architecture as LIP-ResNet. For DenseNet, we replace 2 Ã— 2 average pooling layers in transition blocks and 3Ã—3 max pooling in the bottom by LIP layers of same configurations of window size. The global average pooling also remains unchanged like LIP-ResNet. Total 4 layers (1 max pooling and 3 average pooling) are replaced by LIP layers, and the resulted network is termed as LIP-DenseNet.</p><p>Design of logit modules. In the current implementation, we design two forms of logit modules for LIP layers, called the projection and the bottleneck form, respectively. Structures of logit modules are shown in <ref type="figure" target="#fig_4">Figure 4d</ref> and 4e. In projection form, the logit module in LIP is simply composed of a 1 Ã— 1 convolution layer. The logit module of bottleneck form is like residual branches in bottleneck blocks <ref type="bibr" target="#b14">[15]</ref>, which aims to capture spatial information in an efficient way. This form is denoted as Bottleneck-x, where x is number of channels in the the input and output of 3 Ã— 3 convolution. To further reduce computational complexity of bottleneck logit modules in LIP-ResNet, the first 1 Ã— 1 convolution and 3 Ã— 3 convolution are shared between the residual and shortcut branches in a building block. The input of logit modules here is changed to the feature map fed into the building block, i.e., the top cyan circle in <ref type="figure" target="#fig_4">Figure 4b</ref>, instead of the feature map to downsample. Bottleneck-x logit module in LIP substitution for replacing max pooling in ResNet and DenseNet is simply a 3 Ã— 3 convolution.</p><p>For more effective modeling and stable training, we apply affine instance normalization <ref type="bibr" target="#b40">[40]</ref> as spatial normalization and sigmoid function with a fixed amplification coefficient on the top of each logit module. Affine instance nor- malization make activations on each channel of each feature map follow normal distribution and then rescale it by learnable affine parameters. The spatial normalization and rescale operation aim to help learn extreme cases such as max pooling. The sigmoid function is used here to maintain numerical stability and the fixed amplification coefficient provides large enough range for logits, which is set to 12 throughout our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To validate the effectiveness of our LIP, we carry out experiments on the ImageNet 1K classification task <ref type="bibr" target="#b7">[8]</ref> and the MS COCO detection task <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet Classification Experiment Setup</head><p>ImageNet 1K classification task <ref type="bibr" target="#b7">[8]</ref> requires the methods to cope with high-resolution images to capture discriminative details. We use (LIP-)ResNet and (LIP-)DenseNet for our experiments on the ImageNet classification task. For (LIP-)ResNet training, we use 8 GPUs and a mini-batch of 256 inputs, 32 images per GPU. For (LIP-)DenseNet training, we use 4 GPUs and a mini-batch of 256, 64 images per GPU. Our training procedure is generally following the recipe <ref type="bibr" target="#b9">[10]</ref> with two minor modifications. One is that we use SGD optimizer to update parameters with the vanilla momentum rather than Nesterov one. The other is that weight decay of 10 âˆ’4 is applied to all learnable parameters including those of Batch Normalization. All LIP layers are initialized to behave like average pooling by initializing parameters of the last convolution in logit modules to 0. All results are reported on the validation set with single-crop testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on ImageNet and Analysis</head><p>Study on LIPs and different logit modules. To compare with other pooling methods, we replace all LIP layers in LIP-ResNet by other pooling layers, i.e., average pooling or DPP, and keep the same configuration of window size and stride for fair comparison. The building blocks of these baselines are shown in <ref type="figure" target="#fig_4">Figure 4c</ref>. Note that these baselines    eliminate other factors including receptive fields and nonlinearities to to be more consistent with LIP-ResNet. In this study, we resort to the ResNet-50 to perform comparison between different pooling layers. The results are reported in <ref type="table">Table 1</ref>. First, the ResNet-50 baseline with average pooling reduces both parameters and FLOPs, but still improves performance over the vanilla ResNet by around 0.5% in top-1 accuracy. This result may be ascribed to the fixed interval sampling issue in strided convolutions, and a similar result was found in <ref type="bibr" target="#b15">[16]</ref>. Second, for our downsampling method, LIP with the simplest projection logit modules gains a noticeable improvement over these baselines (&gt; 0.5% in top-1). This shows that the importance simply learned from the projection logit module is beneficial for downsampling procedure. Third, with a more powerful logit module Bottleneck-64, LIP-ResNet further improves accuracy over the projection one with fewer parameters and less computational cost. This demonstrates that spatial information is helpful for designing a better logit module. The performance would saturate when we stretch the bottleneck logit module wider, and the Bottleneck-128 is a good trade-off between computational complexity and recognition performance, improving by 1.79% in top-1 and 0.81% in top-5 over the plain net-  work. We adopt LIP with the Bottleneck-128 logit module as our default choice in the remaining experiments. Finally, we test the effectiveness of instance normalization and amplified sigmoid function. Results are shown in <ref type="table" target="#tab_3">Table 2</ref>. The combination of them improves accuracy by enabling LIP to approximate extreme cases such as max pooling stably. LIP layers at various locations. <ref type="table" target="#tab_5">Table 3</ref> shows the results by placing different numbers of LIPs at different locations. We can find more LIPs generally contributes to better result but LIPs at different locations may not improve performance equally. LIP as the max pooling substitution only improves the top-1 accuracy significantly. We suspect that a single convolution as the logit module at this layer fails to encode enough semantic information to provide powerful logits into LIP. Another possible reason is that high-resolution details may help fine-grained classification but not benefit coarse-grained one. We can also find that the LIP at Res 4 is the most effective one. This might be due to the fact that the feature at this layer contains more semantics and the feature map size is still relatively large for downscaling. For practical applications, we recommend the usage of Combination C in <ref type="table" target="#tab_5">Table 3</ref>    <ref type="table" target="#tab_7">Table 4</ref>. We find that LIP-ResNet-50 performs comparably to the vanilla ResNet-101 with only about half parameters and less FLOPs. LIP-ResNet-101 surpasses the vanilla ResNet-152 in both top-1 and top-5 accuracy by a notable margin (0.84% and 0.38%). For DenseNet and LIP-DenseNet, the result is also favorable, demonstrating the effectiveness of our method across different network architectures.</p><p>Visualizations. As discussed in Section 3.2, LIP enables the network to have capacity of deformable modeling. To show this, we perform some visualizations of LIP layers. We first compute class activation mappings (CAMs) <ref type="bibr" target="#b48">[48]</ref> of ResNet-50 models with LIP, average pooling, and strided convolution. Next, we backpropagate activation of specific locations in CAMs to get gradient maps, which are called effective receptive fields <ref type="bibr" target="#b30">[31]</ref> of specific locations in the original image context. Results are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. The CAMs are similar but the gradient maps differ much among three downsampling approaches. The effective receptive field of the model with LIP layers is more compact and mainly focuses on the foreground even when the backpropagated location moves out of the foreground (i.e., <ref type="figure" target="#fig_5">Figure 5d</ref>). Average pooling and strided convolution ones, however, are interfered more by the background clutter when backpropagating the activation out of the foreground. This comparison shows the deformable modeling capacity of LIP layers. Compared with the average pooling and strided convolution, the clutter and background without discriminative features contribute much less to final recognition results in LIP-ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MS COCO Detection Experiment Setup</head><p>After verifying the effectiveness of LIP on image classification, we now focus on the more challenging detection task. There exists the problem of invisibility of tiny objects   <ref type="bibr" target="#b44">[44]</ref>.</p><p>in most CNN architectures for detection <ref type="bibr" target="#b22">[23]</ref>. This issue is mainly caused by losing discriminative information of small objects during improper downsampling procedure, which is suitable to justify the design of our LIP. MS COCO detection <ref type="bibr" target="#b26">[27]</ref> is a challenging task where the object scale variation is very large and detecting small objects plays a crucial role in final detection performance <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. We adopt mmdetection codebase <ref type="bibr" target="#b3">[4]</ref> for our experiments. Our training strictly follows the default configuration of mmdetection, which includes setting shorter size of the image to 800, using standard horizontal flipping augmentation and ROI Align <ref type="bibr" target="#b13">[14]</ref>. In this experiment, we train two detection frameworks: Faster R-CNN with FPN <ref type="bibr" target="#b24">[25]</ref> and RetinaNet <ref type="bibr" target="#b25">[26]</ref>, on the COCO 2017 train set with the pre-trained backbone networks in Section 4.2. We adopt the typical 2Ã— training time scheme for all COCO experiments. The baseline results are reported by evaluating the released detectors in mmdetection model zoo <ref type="bibr" target="#b2">3</ref> . Detection performance is reported with single-scale testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on MS COCO and Analysis</head><p>The results of different backbones with Faster R-CNN and FPN are shown in <ref type="table" target="#tab_9">Table 5</ref>. LIP-ResNet-50 and LIP-ResNet-101 backbones with Faster R-CNN yield 1.5% and 2.3% gain in AP over baselines, showing the effectiveness of our LIP at capturing discriminative features for detection branch. Their improvement gap may be ascribed to the fact that the deeper backbone provides more semantic features to produce better logits for LIP downsampling. For small object detection, the deeper vanilla ResNet only results in 0.2% gain in AP s , while LIP-ResNet-101 is better than LIP-ResNet-50 by 1.2% in AP s . The improvement of LIP-ResNets in AP s over the vanilla ResNets (2.1% and 3.1%) is also notable. These results show that the LIP layers are able to better preserve discriminative features of tiny   <ref type="table">Table 6</ref>: Results on COCO test-dev set. 'Deformable' denotes deformable convolutions in <ref type="bibr" target="#b5">[6]</ref>. 'MD' denotes adding more deformable convolutions. The 1st and 2nd of each criterion are boldface and underlined respectively.</p><p>objects. The results with the single-stage RetinaNet also validate the effectiveness of the LIP layer.</p><p>To compare with the state-of-the-art detectors, we train the deformable backbone (following the placement of more deformable convolutions in <ref type="bibr" target="#b50">[50]</ref>, but without modulation and feature mimicking) with LIP in Faster R-CNN and FPN framework. The results are shown in <ref type="table">Table 6</ref>. The detectors with LIP-ResNet-101 are comparable to the stateof-the-art methods by simply using a standard detection pipeline without any specific design. The LIP-ResNet-101-MD backbone can further boosts AP to 43.9% and AP s to 25.4%, yielding a new state-of-the-art performance in detecting small objects under the single-scale testing scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we stress spatial importance modeling in pooling procedures. We have presented the Local Aggregation and Normalization (LAN) framework based on local importance to analyze the widely-used pooling layers. Under the framework, we figure out these layers might keep out discriminative features due to using improper downsampling importance maps. Based on this analysis, we have proposed the Local Importance-based Pooling (LIP), a conceptually simple, general, and effective donwsampling method, with a goal of learning adaptive and discriminative importance maps to aggregate features for downsampling. Networks with LIPs are able to better preserve the discriminative details, especially those of tiny objects. Experiments on the ImageNet classification task indicate that LIP can capture rich details for holistic image recognition. On the COCO detection task, LIPs enable both one-and twostage detection frameworks to yield better performance, especially that on small objects. Moreover, detectors with LIP-ResNet backbones reach a new state-of-the-art performance in detecting small objects by simply using a standard detection framework.</p><p>In the future, we plan to study more aspects of implementation of LIP, such as logit module design, adaptive pooling size exploration and so on. Meanwhile, we will verify the effectiveness of LIP to more tasks, e.g., pose estimation and image segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 1 ,</head><label>1</label><figDesc>xand y are even, 0, otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Different downsampling methods viewed in the LAN framework. Activations in the input feature map are blue colored, darker meaning larger. Only activations and corresponding importance within current sliding window are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>weight = torch.exp(logit) return F.avg pool2d(x * weight , kernel size, stride, padding)/F.avg pool2d( weight, kernel size, stride, padding) LIP operator and its PyTorch implementation. The logit module G is not shown in (b). The 'window sum's in (a) mean locally summing within sliding windows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>O</head><label></label><figDesc>x ,y = (âˆ†x,âˆ†y)âˆˆâ„¦ I x+âˆ†x,y+âˆ†y exp(G(I)) x+âˆ†x,y+âˆ†y (âˆ†x,âˆ†y)âˆˆâ„¦ exp(G(I)) x+âˆ†x,y+âˆ†y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Structures of ResNet building blocks for downsampling and logit modules. There are ResNet building blocks with strided convolutions (a), LIP (b), average pooling or DPP (c). (d) and (e) show the projection and bottleneck logit module. The first two 'Conv's in (e) mean convolutions and following affine Instance Normalization and ReLU function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualizations of ResNets with different downsampling techniques. For each subfigure, there are an original image and results of ResNet-50 with LIP, average pooling and strided convolution from left to right. (a) denotes the class activation mappings (CAMs)<ref type="bibr" target="#b48">[48]</ref> for koala. (b)âˆ¼(d) denote the effective receptive fields<ref type="bibr" target="#b30">[31]</ref> in the image context, namely, backpropagated gradients from specific locations in CAMs (red in original images). Contrast of visualizations is lowered for human vision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>93.35 22.8M 3.82G DPP (our baseline structure) 76.87 93.30 22.8M 3.83G DPP (original structure in [33]) 77.22 93.64 25.6M 6.59G</figDesc><table><row><cell>Method</cell><cell>Top-1 Top-5 #Params FLOPs</cell></row><row><cell>Strided convolution</cell><cell>76.40 93.15 25.6M 4.12G</cell></row><row><cell cols="2">Average pooling 76.96 LIP w Projection 77.49 93.86 24.7M 4.78G</cell></row><row><cell>LIP w Bottleneck-64</cell><cell>77.92 93.97 23.2M 4.65G</cell></row><row><cell>LIP w Bottleneck-128</cell><cell>78.19 93.96 23.9M 5.33G</cell></row><row><cell>LIP w Bottleneck-256</cell><cell>78.15 94.02 25.8M 7.61G</cell></row><row><cell cols="2">Table 1: ResNet-50 with different downsampling methods.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Different top layers on logit modules. Combination D is trained with average pooling within first 2000 iterations and then with LIP to avoid numerical overflow due to exp(Â·) operation along with noisy gradient during early iterations. Combination B falls in training although we tried various ways to avoid numerical problems.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Different LIP substitution locations. Combination A stands for the ResNet-50 with full 7 LIPs (LIP-ResNet w Bottleneck-128) and E stands for the vanilla ResNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>due to less parameters and only 3% extra FLOPs compared to the vanilla ResNet.</figDesc><table><row><cell>Architecture</cell><cell cols="2">Top-1 Top-5 #Params FLOPs</cell></row><row><cell>ResNet-50</cell><cell>76.40 93.15</cell><cell>25.6M 4.12G</cell></row><row><cell>LIP-ResNet-50</cell><cell>78.19 93.96</cell><cell>23.9M 5.33G</cell></row><row><cell>ResNet-101</cell><cell>77.98 93.98</cell><cell>44.5M 7.85G</cell></row><row><cell>LIP-ResNet-101</cell><cell>79.33 94.60</cell><cell>42.9M 9.06G</cell></row><row><cell>ResNet-152  *</cell><cell>78.49 94.22</cell><cell>60.2M 11.58G</cell></row><row><cell>DenseNet-BC-121</cell><cell>75.62 92.56</cell><cell>8.0M 2.88G</cell></row><row><cell cols="2">LIP-DenseNet-BC-121 76.64 93.16</cell><cell>8.7M 4.13G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>ResNets and DenseNets with and without LIP. For ResNet-152, we adopt the result trained by a similar recipe 2 .</figDesc><table><row><cell>But our default choice for the remaining experiments is the</cell></row><row><cell>full LIP model, i.e., Combination A.</cell></row><row><cell>Different network depth and architectures. We also</cell></row><row><cell>evaluate LIP-ResNet and LIP-DenseNet with the deeper</cell></row><row><cell>network, and the result is summarized in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Backbone AP AP50 AP75 APs APm AP l Faster R-CNN w FPN results ResNet-50 37.7 59.3 41.1 21.9 41.5 48.7 LIP-ResNet-50 39.2 61.2 42.5 24.0 43.1 50.3 ResNet-101 39.4 60.7 43.0 22.1 43.6 52.1 LIP-ResNet-101 41.7 63.6 45.6 25.2 45.8 54.0</figDesc><table><row><cell>ResNeXt-101</cell><cell>40.7 62.1 44.5 23.0 44.5 53.6</cell></row><row><cell>RetinaNet results</cell><cell></cell></row><row><cell>ResNet-50</cell><cell>36.6 56.6 38.9 19.6 40.3 48.9</cell></row><row><cell cols="2">LIP-ResNet-50 38.0 58.8 40.5 22.6 41.5 49.9</cell></row><row><cell>ResNet-101</cell><cell>38.1 58.1 40.6 20.2 41.8 50.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Faster R-CNN with FPN and RetinaNet with different backbones results on COCO 2017 val set. ResNeXt-101 stands for ResNeXt-64x4d-101 backbone in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tensorpack/tensorpack/tree/ master/examples/ResNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Evaluated when this paper was submitted and some baseline results are slightly higher than the officially reported ones in<ref type="bibr" target="#b3">[4]</ref>.(a) (b) (c) (d)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Science Foundation of China (No. 61921006, No. 61321491), and Collaborative Innovation Center of Novel Software Technology and Industrialization. The first author would like to thank Nan Wei and Qinshan Zeng for their comments and support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SOD-MTGAN: small object detection via multitask generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<idno>arXiv, 2019. 7</idno>
		<editor>Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Revisiting RCNN: on awakening the classification power of faster RCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RogÃ©rio</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fractional max-pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Training and investigating residual nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wilber</surname></persName>
		</author>
		<ptr target="https://github.com/facebook/fb.resnet.torch.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learned-norm pooling for deep feedforward and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Ã‡ Aglar GÃ¼lÃ§ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv, 2019. 8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detnet: A backbone network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr DollÃ¡r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Grid R-CNN. In arXiv</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">What do deep networks like to see</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Palacio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Folz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JÃ¶rn</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detail-preserving pooling in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faraz</forename><surname>Saeedan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SNIPER: efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<pubPlace>Vincent</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A novel learning-based frame pooling method for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Task-driven feature pooling for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Sen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu-Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mixed pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqiu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSKT</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">S3pool: Pooling with stochastic spatial sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RogÃ©rio</forename><surname>Schmidt Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ã€gata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">An empirical study of spatial attention mechanisms in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>arXiv, 2019. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
