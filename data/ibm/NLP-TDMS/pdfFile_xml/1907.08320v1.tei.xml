<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Task Regression-based Learning for Autonomous Unmanned Aerial Vehicle Flight Control within Unstructured Outdoor Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruna</forename><forename type="middle">G</forename><surname>Maciel-Pearson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Akçay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Holder</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
						</author>
						<title level="a" type="main">Multi-Task Regression-based Learning for Autonomous Unmanned Aerial Vehicle Flight Control within Unstructured Outdoor Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JULY, 2019 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Aerial Systems: Perception and Autonomy</term>
					<term>Au- tonomous Vehicle Navigation</term>
					<term>Computer Vision for Other Robotic Applications</term>
					<term>Deep Learning in Robotics and Automation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Increased growth in the global Unmanned Aerial Vehicles (UAV) (drone) industry has expanded possibilities for fully autonomous UAV applications. A particular application which has in part motivated this research is the use of UAV in wide area search and surveillance operations in unstructured outdoor environments. The critical issue with such environments is the lack of structured features that could aid in autonomous flight, such as road lines or paths. In this paper, we propose an End-to-End Multi-Task Regression-based Learning approach capable of defining flight commands for navigation and exploration under the forest canopy, regardless of the presence of trails or additional sensors (i.e. GPS). Training and testing are performed using a software in the loop pipeline which allows for a detailed evaluation against state-of-the-art pose estimation techniques.</p><p>Our extensive experiments demonstrate that our approach excels in performing dense exploration within the required search perimeter, is capable of covering wider search regions, generalises to previously unseen and unexplored environments and outperforms contemporary state-of-the-art techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A UTONOMOUS exploration and navigation in unstructured environments remains an unsolved challenge mainly because flights in such areas carry a higher risk of collision and are further aggravated by the power consumption/availability per battery, which is usually limited to less than 20 minutes. In this context, unstructured areas correspond to environments such as disaster areas <ref type="bibr" target="#b0">[1]</ref>, icebergs <ref type="bibr" target="#b1">[2]</ref>, snowy mountains <ref type="bibr" target="#b2">[3]</ref> and forests <ref type="bibr" target="#b3">[4]</ref>. These environments tend to have exceedingly variable nature ( <ref type="figure" target="#fig_1">Fig. 1</ref>) and are often affected by constant changes in local wind conditions. As a result, the mission planning process needs to take into consideration arbitrary, unknown environments and weather conditions <ref type="bibr" target="#b4">[5]</ref>. Since it is not feasible to preconceive large quantities of unforeseen events that may occur during a mission, an intelligent UAV control system with the capability to generalise to other domains is highly desirable. In this context, a new domain  can be defined as an area previously unexplored by the UAV, which differs from the original environment on which the deep neural network (DNN) is trained.</p><p>Currently, the existing literature on autonomous flight tends to focus either on mapping the environment, where obstacle avoidance is readily achievable or on deploying models approximated by DNN <ref type="bibr" target="#b5">[6]</ref>. The former is commonly achieved by techniques such as SLAM or Visual Odometry <ref type="bibr" target="#b6">[7]</ref>, which require prior knowledge of the camera intrinsic parameters, while the latter requires a substantial volume of data which is often intractable to obtain <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>The focus of our research is navigation within unstructured environments, which is primarily achieved by autonomous exploration under the forest canopy. In this paper, we present a Multi-Task Regression-based Learning (MTRL) approach that allows a UAV to perceive obstacle-free areas while simultaneously predicting the orientation quaternions and positional waypoints in NED (North, East, Down) coordinates, required to explore the environment safely. Due to the nature of our tests, all the experiments are carried out in a virtual environment using the AirSim simulator <ref type="bibr" target="#b9">[10]</ref>. As such, our approach uses the Software In The Loop (SITL) <ref type="bibr" target="#b7">[8]</ref> stack, commonly used to simulate the flight control of a single or multiagent UAV <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref>. The navigational approach presented in this paper is also independent of the Global Positioning System (GPS), mainly due to the low reliability of GPS signals under dense forest canopy <ref type="bibr" target="#b6">[7]</ref>. The proposed learningbased approach utilises a very simple and light-weight network architecture 1 and robustly generalises to new unstructured and unseen environments.</p><p>Extensive evaluations point to the superiority of the proposed approach compared to contemporary state-of-the-art techniques <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. In addition, flight behaviour is also assessed in a SITL environment with ground truth telemetry data gathered during a simulated flight. To the best of our knowledge, this is the first approach to autonomous flight and exploration under the forest canopy without path following or aid of additional sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Current research on autonomous navigation for UAV can be divided into two groups based on whether path planning or waypoint navigation is the main objective <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Path planning requires understanding the environment ahead and it is usually achievable by pre-mapping the environment or specifying a navigational area as the UAV flight takes place <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref>, which means the UAV can operate at a constant speed for a set duration in a specific direction <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Within the existing literature, various end-to-end learningbased approaches have been employed to derive a set of navigational parameters from a given image, allowing for obstacle avoidance <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Additionally, the recent advances made in multi-task systems partially focusing on depth estimation <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b25">[26]</ref> can also be potentially beneficial towards a successful obstacle avoidance and path planning approach. However, most existing approaches offers only three degrees of freedom navigation, which makes them unsuitable for autonomous flight in unstructured environments, where the UAV may require to change altitude to avoid certain obstacles. As such, varying height estimation and generalisation are fundamental and can best be achieved by defining waypoints.</p><p>Waypoints are usually dependant on the the Global Positioning System (GPS) and can be defined before or dynamically generated during the flight. Once the UAV reaches the first waypoint, usually positioned at a short distance away and in an obstacle-free area, the algorithm moves on to defining the next waypoint, and the flight controller makes the necessary adjustments in speed and direction to reach its goal <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> safely. Alternatively, the flight speed can be estimated by a neural network allowing more generalised and dynamic navigation with six degrees of freedom <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>. It is important to highlight that navigation using such GPS waypoints is usually limited in environments where the GPS signal is unreliable or unavailable <ref type="bibr" target="#b6">[7]</ref>.</p><p>Recently, significant results have been achieved by Deep Neural Networks (DNN) in the task of pose estimation based on monocular imagery. In this sense, the use of a Convolutional Neural Network (CNN) <ref type="bibr" target="#b5">[6]</ref> to learn and to match features, which aids in camera pose estimation, has become popular with the work of Kendall et al. <ref type="bibr" target="#b11">[12]</ref> and more recently the work of Mueller et al. <ref type="bibr" target="#b29">[30]</ref>. However, both approaches rely on prior environmental knowledge before yielding an estimation of the camera position. Further enhancements in <ref type="bibr" target="#b0">1</ref> Dataset and source code is available at https://github.com/brunapearson/mtrl-auto-uav predicting camera position have been made possible by integrating a Long-Short Term Memory (LSTM) network into the process <ref type="bibr" target="#b12">[13]</ref>. This recurrent neural network uses gates to handle the vanishing gradient problem, which is very common during back-propagation <ref type="bibr" target="#b30">[31]</ref>.</p><p>In contrast, our approach uses Multi-Task Regression-based Learning to individually learn the position of waypoints in NED (North, East, Down) coordinates within the scene in addition to learning the rotational quaternions. As a result, the flight controller can dynamically change position and speed based on the output of our resulting multi-task regression network. Such an approach does not rely on GPS readings to navigate, which makes it suitable for operation in weak or denied GPS signal areas. Additionally, it does not require any knowledge of the camera intrinsics and operates with lowresolution images, which makes it ideal for varying payload UAV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CONTROL SYSTEM INTEGRATION</head><p>In this work, the development as well as testing of the proposed approach and the state-of-the-art comparators <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> is performed using the open-source AirSim simulator <ref type="bibr" target="#b9">[10]</ref>. AirSim is built on the Unreal Engine <ref type="bibr" target="#b31">[32]</ref> and offers physically and visually realistic scenarios for data-driven autonomous and intelligent systems. Due to the closeness of the simulated environment and the real world, the control system, which integrates the simulated flight controller into an autonomous navigation approach, will follow the same structure as the control system in a real UAV with on-board processing capabilities. In our work, each approach tested receives two sets of parameters with distinct measurement units. The first is denoted by the NED values noted in meters, whereas the second one is the rotation and orientation of the UAV established in orientation quaternions. Although quaternions are often a standard representation of attitude in graphical engines, particularly for three-dimensional computations <ref type="bibr" target="#b32">[33]</ref>, our main motivation for the use of quaternions is attributed to the fact that calculating quaternions requires a significantly smaller amount of memory than calculating rotational matrices, thereby making them more suitable for on-board processing in drones deployed in the real world.</p><p>A quartenion is a hyper complex number of rank 4, commonly used to avoid the inherent geometrical singularity characteristic of the Euler's method <ref type="bibr" target="#b33">[34]</ref>, which leads to a loss of one degree of freedom in a three-dimensional space. This reduction happens when two of the rotational axes align and lock together <ref type="bibr" target="#b34">[35]</ref>. Formally, a quaternion q is defined as the sum of a scalar q 0 and a vector q = (q 1 , q 2 , q 3 ):</p><formula xml:id="formula_0">q = q 0 + q = q 0 + qî 1 + qĵ 2 + qk 3 ,<label>(1)</label></formula><p>where q 0 , q 1 , q 2 and q 3 denote real numbers, andî = (1, 0, 0), j = (0, 1, 0) andq = (0, 0, 1) refer to the fundamental quaternion unit vectors. During simulation, the position P k+1 (Eqn. 3) of the body at time k +1 is updated by integrating the linear velocity (Eqn. 2) and initial position p k , as shown in Eqn. <ref type="bibr" target="#b2">3</ref>.</p><formula xml:id="formula_1">v k+1 = vk + a k + a k+1 2 .dt,<label>(2)</label></formula><formula xml:id="formula_2">p k+1 = p k + vk · dt + 1 2 · a k · dt 2 ,<label>(3)</label></formula><p>where a represents the linear acceleration obtained by applying Newton's second law added to the gravity vector as illustrated in Eqn. 4 and a k is a function of acceleration over time k.</p><formula xml:id="formula_3">a = F net /m + g (4)</formula><p>The orientation is updated by computing the instantaneous axisω = ω/|ω| through the angle α dt = |ω|·dt, where ω refers to the angular velocity in the body frame concerning a fixed (world) frame and can be determined by Newton's equations for dynamics.</p><p>Flight stability is achieved by combining the rate and attitude control loops at each iteration T (Algorithm 1). The Rate Control Loop (RCL) has three independent PD (Proportional Derivative) controllers (PD roll rc,PD pitch rc,PD yaw rc) for controlling the body rates (φ D ,θ D andψ D ). The body rates (aka desired body rates) are derived from the target rates (φ T , θ T , ψ T , thrust cmd ), and current rates (φ,θ,ψ). The quartenion values outputted by the network (model output) are directly fed into the AirSim RCL. This generates the target rates. The Attitude Control Loop (ACL) uses IMU (Inertial Measurement Unit) readings (α x , α y , α z , µ x , µ y , µ z ) to estimate the current rate (φ,θ,ψ). Thereafter, the motion (φ cmd ,θ cmd ,ψ cmd ) commands are generated by the ACL by integrating (PID roll ac,PID pitch ac,PID yaw ac) the desired rates and angular speeds (φ,θ,ψ) acquired from the readings of a 3-axis gyro.</p><p>Algorithm 1: Implementation of Attitude and Rate Control </p><formula xml:id="formula_4">1 while True do 2 On each T ; 3 φ T ,θ T ,ψ T ,thrust cmd ← model output ; 4φ,θ,ψ ← read gyro() ; 5 α x ,α y ,α z ← read accelerometer() ; 6 µ x ,µ y ,µ z ← read magnetometer() ; 7φ,θ,ψ ← attitude(φ,θ,ψ,α x ,α y ,α z ,µ x ,µ y ,µ z ) ; / * calculate desired body rate * / 8φ D ← P D roll rc(φ T −φ) ; 9θ D ← P D pitch rc(θ T −θ) ; 10ψ D ← P D yaw rc(ψ T −ψ) ; / * motion commands * / 11φ cmd ← P ID roll ac(φ D −φ) ; 12θ cmd ← P ID pitch ac(θ D −θ) ; 13ψ cmd ← P ID yaw ac(ψ D −ψ) ; / *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NETWORK ARCHITECTURE</head><p>In our proposed approach, rather than explicitly following a trail, the objective is to identify clear flight areas and predict the flight behaviour while exploring an unknown environment. As such, to compare the effectiveness of the different approaches described in this work, each technique is required to be adapted in order to produce the same navigational output, which is subsequently mapped into the flight controller.</p><p>Originally, the approach by Wang et al. <ref type="bibr" target="#b12">[13]</ref> receives as its input a pair of images and two sets of navigational coordinates. The first is the (x, y, z) positional coordinates, and the second is the Euler angles. The distance between the ground truth pose and the predicted pose is minimised during training, resulting in the final output of three positional coordinates (x, y, z) and three Euler angles (pitch, roll and heading). By contrast, during this experiment, we feed to the network the NED positions and orientation quaternion. As such, the output vector is resized to accommodate seven quantities instead of the six initially specified by <ref type="bibr" target="#b12">[13]</ref>. The architecture of the network used in <ref type="bibr" target="#b12">[13]</ref> consists of nine convolutional layers which feed the first LSTM recurrent neural network. This LSTM then supplies its output to a second LSTM network. Each LSTM has 1000 hidden layers. For a detailed description of the architecture, we refer the reader to <ref type="bibr" target="#b12">[13]</ref>.</p><p>Similarly, the approach in <ref type="bibr" target="#b13">[14]</ref>, which receives one input image and processes three navigational directions, will now output a vector containing seven estimations. Since the approach in <ref type="bibr" target="#b11">[12]</ref> originally receives one input image and outputs the camera's NED position and orientation in unit quaternion, no changes are required. In all cases, the number of input images originally required for each network is maintained, and all images are resized to 224 × 224 × 3. In addition, all networks receive the coordinates referent to P k+1 , instead of P k . As such, the aim is to predict the next action instead of the current position. Image normalisation is performed according to the specification of each network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Our network is based on a Multi-Task Regression-based Learning (MTRL) approach <ref type="figure" target="#fig_3">(Figure 2)</ref>, where the features learned from the input are shared across two subnetworks to learn the NED position and rotational quaternions. To achieve this, the input image is first resized and pre-processed by applying channel mean subtraction. Next, the normalised image is shared between two identical subnetworks, each with a convolutional layer (222 × 222 × 16), followed by a max polling layer (111 × 111 × 16), a second convolutional layer (109 × 109 × 32), followed by a second max pooling layer (54 × 54 × 32), a third convolutional layer (52 × 52 × 64), a third max pooling layer (26 × 26 × 64), in which the output is flattened (43264) and fed into a dense layer (500), where dropout is applied. For the positional branch, we apply a dropout of 0.5 while none is applied to the rotational branch. The output is then fed into a second dense layer, to which we apply a dropout of 0.25 only to the rotational branch. The third dense layer (20) performs the linear regression and outputs the estimation for each task. Our cost function θ minimises  the Euclidean distance between the predicted outputx and the ground truth x. A scalar factor N with assigned value of 0.1 is inserted to reduce the difference between the positional and rotational errors.</p><formula xml:id="formula_5">θ = argmin 1 N i=1 ||x − x|| 2<label>(5)</label></formula><p>V. EXPERIMENTAL SETUP Training is performed using adaptive moment estimation <ref type="bibr" target="#b35">[36]</ref> with a learning rate of 0.0001 and default parameters following those provided in the original paper (β 1 = 0.9, β 2 = 0.999) <ref type="bibr" target="#b35">[36]</ref> using a GTX 1080Ti using 64,000 frames for training data and 17,674 frames for validation. During the test, AirSim is set up to use the GPU while all approaches are tested using an Intel Xeon processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Preparation</head><p>Data is obtained by manually flying the UAV through the Redwood Forest environment using a FrSky Taranis (Plus) Digital Telemetry Radio System. In total, 81,674 frames were captured together with the flight telemetry comprising of flights under and above the forest canopy, navigation inside caves and over river beds, lakes and mountains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Criteria</head><p>Our evaluation is presented across five metrics: repetition, generalisation, flight behaviour, distance travelled and reliability.</p><p>In search and exploration scenarios, exploring as many routes as possible is often the primary objective, which is why we investigate the capability of different approaches in alternating between different paths rather than continuously repeating the same route. Additionally, we aim to evaluate the capability of various methods to generalise to unseen environments. During flight behaviour analysis, we observe wherever the UAV is flying or hovering and note the flight duration and the distance traversed during the mission. Here, the goal is to identify the method capable of traversing greater distances in a shorter period. Finally, the reliability of the flight mission is measured by considering the distance the UAV can fly without any intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head><p>In this section, we evaluate the performance of the proposed approach in forecasting the next position and rotation of the UAV in a given environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Repetition</head><p>Our first set of tests assess the approach's behaviour when processing the test data ( <ref type="figure" target="#fig_4">Figure 3</ref>). For simplicity, we shall define motion in the N coordinate as y and E coordinate as x, while D will be depicted as z. Here, the flight commands are predicted but not sent to the flight controller. The objective is to observe the navigational patterns of each method and to compare the distribution of the predicted positional values for x and y directions. In order to create the flight pattern shown on the left <ref type="figure" target="#fig_4">(Figure 3)</ref>, we add the predicted values of (x k+1 , y k+1 , z k+1 ) to the current position (x k , y k , z k ), just as it would be in the flight controller, using a frame rate of 20fps.</p><p>The test set used to compare each network has a high density of repetitive positional values (dark blue areas) distributed over a small area ( <ref type="figure" target="#fig_4">Figure 3</ref>). Because this test set mostly contains scenes of low mobility and hovering behaviour, the endeavour is to observe if any of the approaches are capable of understanding the difference between hovering behaviour and slow motion. As illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>, the approaches of Bojarski et al. <ref type="bibr" target="#b13">[14]</ref> and Kendal et al. <ref type="bibr" target="#b11">[12]</ref> perform better at slow motion, given that most predicted positional values in x are in the range of −1.0 to 1.0. Similarly, the approach of Wang et al. <ref type="bibr" target="#b12">[13]</ref> showed a lower variance of predicted values for the x direction (range between −0.02 to 0.02). In contrast, the proposed approach covers more exploratory ground due to significantly higher predicted positional values (range of −20 to 40) as illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>.  Here, we also evaluate the consistency of each model by superposing each one of the resulting routes after four iterations ( <ref type="figure" target="#fig_4">Figure 3</ref>) and although none repeated the same route, the proposed approach shows greater consistency of decision when the same set of images are presented, as evidenced by the overlap/common ground (dark blue) areas in <ref type="figure" target="#fig_4">Figure 3</ref>. The approach of Bojarski et al. <ref type="bibr" target="#b13">[14]</ref> predicts similar positional values during three out of four iterations, while the approaches of Kendal et al. <ref type="bibr" target="#b11">[12]</ref> and Wang et al. <ref type="bibr" target="#b12">[13]</ref> choose different directions at every iteration. From <ref type="figure" target="#fig_5">Figure 4</ref>, it can be observed that the higher the variance in the y coordinate, the wider the FoV (Field of View) will be. At a later stage, the imagery gathered during navigation can be used to identify any object/person/animal of interest. As such, a wider FoV will undoubtedly be more favourable. Based on the qualitative results in <ref type="figure" target="#fig_4">Figure 3</ref>, we can observe that the Bojarski et al. <ref type="bibr" target="#b13">[14]</ref> and Kendal et al. <ref type="bibr" target="#b11">[12]</ref> approaches have a lower variance closer to zero for y; contrarily, the proposed approach tends to forecast positional values in y far from zero, which results in a wider angular rotation of the head.</p><p>In summary, it can be observed that the proposed approach performs better due to its ability to carry a high-density exploration of the search perimeter by predicting waypoints that leads to the navigation of different routes that are closer to each other. Besides, due to its angular rotation, the FoV of the proposed approach is significantly wider as compared to comparators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Behaviour</head><p>During autonomous flight tests using the SITL, we observe that although the approach by Wang et al. <ref type="bibr" target="#b12">[13]</ref> has the highest reliability rate, which makes it quantitatively better than the other approaches <ref type="table" target="#tab_1">(Table I)</ref>, the fact remains that it has, the worst performance when qualitatively analysing the flight mission in the dense forest environment ( <ref type="figure" target="#fig_7">Figure 5</ref>). The network fails to learn exploratory behaviour, which causes the model to predict very similar values, thus resulting in the UAV constantly moving forward in a path. Additionally, this approach <ref type="bibr" target="#b12">[13]</ref> produces significantly lower changes in the y direction, appointing to small FoV. Similar behaviour is also observed in the findings of the approaches adopted by Bojarski et al. <ref type="bibr" target="#b13">[14]</ref> and Kendal et al. <ref type="bibr" target="#b11">[12]</ref>.</p><p>A secondary behaviour observed in <ref type="bibr" target="#b13">[14]</ref> is the constant attempts to gain altitude above the canopy. In order to correct this behaviour, a function is created that regulates the altitude during the flight, which forces the UAV to remain under the canopy. Within real-world scenarios, UAV flights are  commonly monitored by a Geo-Fencing mechanism <ref type="bibr" target="#b36">[37]</ref>. In contrast, the proposed approach traverses the greatest distance (631.55m) with significant changes in y, which indicates a greater FoV. Additionally, <ref type="figure" target="#fig_7">Figure 5</ref>, illustrates that the proposed approach has a precise exploratory behaviour under the canopy, characteristic of low-altitude flight and constant changes in the y direction.</p><p>Conventionally, sensor filtering, mechanical dampers and dynamic g compensation are often used to reduce the effect of motor/propeller vibration before translating attitude commands to motor commands. Although a smooth navigation is desirable for the purpose of this work, we remove the sensor filtering from the RCL which results in a jittering motion. This allows us to observe anomalies, such as overshooting or drifting. Since drifting is caused by a minor increase in rotation rate in a pair of motors, when a low pass filter is applied to it, the rotation rate changes and the resulting total force equals to the gravitational force. This implies that the UAV changes its behaviour from drifting to hovering.</p><p>A clear distinction between behaviours is imperative since there is a strong relationship between generalisation and an increase in rotational rates, as depicted in <ref type="figure" target="#fig_7">Figure 5</ref>. Greater generalisation capabilities lead to a heightened rotational rate and mobility, while lower generalisation capabilities result in rotational rate values closer to zero and no generalisation results in hovering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generalisation</head><p>In assessing the ability of the approaches to generalise to unseen domains, we find that the approaches in <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> fail to generalise in the snowy mountain as well as the plain field ( <ref type="figure" target="#fig_7">Figure 5</ref>). Since the values predicted by these approaches are mostly close to zero, the behaviour observed during the UAV flight is hovering or slowly drifting due to the simulated air flow velocity rather than flying. The hovering behaviour is evidence in the results derived from the plain field, where values predicted using the approaches by Bojarski et al. <ref type="bibr" target="#b13">[14]</ref>, Kendal et al. <ref type="bibr" target="#b11">[12]</ref> and Wang et al. <ref type="bibr" target="#b12">[13]</ref> cause the UAV to remain at position (x : 0, y : 0); only change is in the altitude (z). In contrast, the proposed approach is capable of generalising in all tested environments and has the greatest flight distance <ref type="table" target="#tab_1">(Table I)</ref>.</p><p>The generalisation capability of the proposed approach can primarily be attributed to the shallow architecture of its network, in which learning is confined to local features that are commonly found in various obstacles from the global structure of the scene, such as edges and depth information ( <ref type="figure">Figure  6</ref>). Consequently, the proposed approach does not learn to differentiate between a tree and a rock or a branch; instead, it learns to differentiate salient objects that are to be avoided from any other elements within the scene. When tested in a new environment, our model will try to avoid areas rich in salient obstacles/object features, regardless of what this saliency may entail.</p><p>As observed by the heat map and activation map in <ref type="figure">Figures  Fig. 6</ref>. Heat and activation maps from the three last convolutional layers of the proposed approach, prior to flattening. Row (A) shows the results of testing the forest environment, while row (B) the plain field and (C) the snowy environment.   7-9, the deeper the network, the more specific is the knowledge acquired about the training environment. This phenomenon is mainly attributed to the fact later layers tend not only to retain spatial information but also to learn high-level semantic information about the scene <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, as is the case for the approaches by Kendall et al. <ref type="bibr" target="#b11">[12]</ref>  <ref type="figure" target="#fig_9">(Figure 8</ref>) and Bojarski et al. <ref type="bibr" target="#b13">[14]</ref>  <ref type="figure" target="#fig_8">(Figure 7)</ref>. In both these cases, the network highlights arbitrary regions within the heat map, which illustrate the origins of the features observed in the activation map. Here, blue areas signify lower certainty about the classification of the area of interest (ROI), while red areas denote higher certainty. Furthermore, all three approaches <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> suggest that, the last activation map for both the plain field and snowy mountain are smoother than the activation map for the forest environment, implying that the network is less certain regarding to its predictions in these environments than the forest environment <ref type="bibr" target="#b37">[38]</ref>. Contrarily, in our proposed approach, we can observe an equal representation of salient features across all three environments, which is indicative of the superior generalisation capabilities of our approach. Put succinctly, learning very specific details about the content of a given environment can prove to be very useful when navigating within the same environment or those with close similarity in their appearance. However, this can preclude generalisation to unseen environments, which are far more likely to be encountered in real-world applications. Although all comparators <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref> suffer from this predicament, our technique demonstrates significantly superior generalisation capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Distance and Reliability</head><p>Further experiments are also carried out to verify the validity of our approach when flying from different starting points within the forest and for a longer duration of time <ref type="table" target="#tab_1">(Table II)</ref>. The results of the experiments presented in <ref type="table" target="#tab_1">Table II</ref> provide a better numerical evaluation of the robustness of the proposed approach. The set of tests F1-F4 have a navigational loop of 150 iterations, while tests F5 and F6 have a navigational loop of 1000 and 2000 iterations respectively. Experiments F4-F6 are premised on the initial starting position with coordinates of (70,-450, -12), while F1-F3 use the default starting position of (0,0,0) within the map. The difference concerning the time taken to complete each one of the flight mission F1-F3 is caused by choice of the route and the amount of airflow to overcome. Regardless of the distance or initial starting position, the level of reliability for each test is above 88%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this work, we present a deep learning approach based on Multi-task Regression-based Learning, which takes advantage of the use of shallow networks by learning each task individually. Experiments indicate that our method is capable of generalising to unseen domains and has a larger coverage area than comparators <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Our method demonstrates a more aggressive exploratory behaviour due to a wider FoV in comparison with other approaches. Additionally, our techniques is particularly suitable for search and rescue operations in any above-ground environment, as it is does not require distinct pathways or GPS for navigation. An interesting direction for future research would be the investigation of the efficacy of the proposed approach when it is deployed indoors and in areas with limited navigational space, as well as boosting the performance of the approach by incorporating the task of depth estimation into overall multi-task model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received: February, 24, 2019; Revised April, 23, 2019; Accepted July, 1, 2019. This paper was recommended for publication by Editor Eric Marchand upon evaluation of the Associate Editor and Reviewers' comments. This work was supported by (EPSRC/1750094) 1 B.G. Pearson, S.Akçay, A.Atapour, C.J. Holder and T.P.Breckon are with Department of Computer Science, Durham University, Durham, UK (email: { b.g.maciel-pearson, samet.akcay, amir.atapour-abarghouei, c.j.holder toby.breckon }@durham.ac.uk). Digital Object Identifier (DOI): see top of this page.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Exemplar imagery for autonomous flight and exploration through the AirSim simulator<ref type="bibr" target="#b8">[9]</ref>. Images are from (A) the dense redwood forest, (B) snowy mountain and (C) the wind farm environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>The proposed Multi-Task Regression-based Learning approach. The network predicts 3 positional (N, E, D) and 4 rotational values (q 0 ,q 1 ,q 2 ,q 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>2D representation of flights using the test set. Left image shows the combined routes of four flights which are then superposed. Middle and right images show waypoint distributions for the pair (x, y) directions, while flying in the default mode of 1m/s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the angular rotation of the UAV when the values in the y position are close to zero (a), and far from zero (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0.0,-0.0,0.29 end:-350.61,-300.97,10.0.0,-0.0,-0.0 end:-0.0,-0.0,-0.7 Plain Field Our Approach Bojarski et al. Kendal et al. Wang et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of each approach when autonomously flying under the canopy of a dense forest, over a snowy mountain and over a plain field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Three last convolutional layers of Bojarski et al.<ref type="bibr" target="#b13">[14]</ref> approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Three last inception layers of Kendall et al. [12] approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Three last convolutional layers of Wang et al. [13] approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>← thrust cmd −ψ cmd +φ cmd +θ cmd ; 15 P W M 2 ← thrust cmd +ψ cmd −φ cmd +θ cmd ;</figDesc><table><row><cell></cell><cell cols="2">translate motion commands to PWM</cell></row><row><cell></cell><cell>signals</cell><cell>* /</cell></row><row><cell>14</cell><cell>P W M 1 / * Driving</cell><cell>* /</cell></row></table><note>16 P W M 3 ← thrust cmd −ψ cmd −φ cmd −θ cmd ;17 P W M 4 ← thrust cmd +ψ cmd +φ cmd −θ cmd ;18 drive motor(P W M 1 , P W M 2 , P W M 3 , P W M 4 ) ; 19 end</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>OF EACH APPROACH DURING AUTONOMOUS FLIGHT IN THE FOREST, SNOWY MOUNTAIN AND PLAIN FIELD ENVIRONMENTS. RELIABILITY IS THE PERCENTAGE OF THE DISTANCE TRAVELLED WITHOUT INTERVENTIONS N I .</figDesc><table><row><cell>Method</cell><cell cols="5">NI Reliability Duration Distance (m) Behaviour</cell></row><row><cell></cell><cell></cell><cell cols="2">Dense Forest</cell><cell></cell><cell></cell></row><row><cell>Bojarski et al. [14]</cell><cell>7</cell><cell>98.59</cell><cell>14min</cell><cell>496.61m</cell><cell>flying</cell></row><row><cell>Kendal et al. [12]</cell><cell>49</cell><cell>91.79</cell><cell>8min</cell><cell>596.7m</cell><cell>flying</cell></row><row><cell>Wang et al. [13]</cell><cell>0</cell><cell>100.0</cell><cell>5min</cell><cell>443.11m</cell><cell>flying</cell></row><row><cell>MTRL</cell><cell>31</cell><cell>95.09</cell><cell>10min</cell><cell>631.55m</cell><cell>flying</cell></row><row><cell></cell><cell></cell><cell cols="2">Snowy Mountain</cell><cell></cell><cell></cell></row><row><cell>Bojarski et al. [14]</cell><cell>0</cell><cell>-</cell><cell>6min</cell><cell>65.59m</cell><cell>hovering</cell></row><row><cell>Kendal et al. [12]</cell><cell>0</cell><cell>-</cell><cell>5min</cell><cell>125.62m</cell><cell>hovering</cell></row><row><cell>Wang et al. [13]</cell><cell>0</cell><cell>-</cell><cell>13min</cell><cell>6.02m</cell><cell>hovering</cell></row><row><cell>MTRL</cell><cell>27</cell><cell>97.25</cell><cell>20min</cell><cell>982.64m</cell><cell>flying</cell></row><row><cell></cell><cell></cell><cell cols="2">Plain Field</cell><cell></cell><cell></cell></row><row><cell>Bojarski et al. [14]</cell><cell>0</cell><cell>-</cell><cell>20min</cell><cell>0.0m</cell><cell>hovering</cell></row><row><cell>Kendal et al. [12]</cell><cell>0</cell><cell>-</cell><cell>20min</cell><cell>0.0m</cell><cell>hovering</cell></row><row><cell>Wang et al. [13]</cell><cell>0</cell><cell>-</cell><cell>15min</cell><cell>0.0m</cell><cell>hovering</cell></row><row><cell>MTRL</cell><cell>7</cell><cell>99.19</cell><cell>11min</cell><cell>867.45m</cell><cell>flying</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II THE</head><label>II</label><figDesc>MULTI-TASK APPROACH IN 6 DISTINCT FLIGHT MISSIONS.</figDesc><table><row><cell>Method</cell><cell>NI</cell><cell>Reliability</cell><cell>Duration</cell><cell>Distance (m)</cell></row><row><cell>MTRL (F1)</cell><cell>2</cell><cell>97.46</cell><cell>4min</cell><cell>78.88m</cell></row><row><cell>MTRL (F2)</cell><cell>21</cell><cell>98.07</cell><cell>9min</cell><cell>1086.62m</cell></row><row><cell>MTRL (F3)</cell><cell>13</cell><cell>98.01</cell><cell>6min</cell><cell>654.76m</cell></row><row><cell>MTRL (F4)</cell><cell>31</cell><cell>95.09</cell><cell>10min</cell><cell>631.55m</cell></row><row><cell>MTRL (F5)</cell><cell>197</cell><cell>90.55</cell><cell>2h40min</cell><cell>2086.61m</cell></row><row><cell>MTRL (F6)</cell><cell>399</cell><cell>88.55</cell><cell>5h37min</cell><cell>2287.37m</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of unmanned aerial vehicle (UAV) usage for imagery collection in disaster research and management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Friedland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Workshop on Remote Sensing for Disaster Response</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="8" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adapting open-source drone autopilots for real-time iceberg observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rysgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MethodsX</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1059" to="1072" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The potential use of unmanned aircraft systems (drones) in mountain search and rescue operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Karaca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tatli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pasli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Beser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Turedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American J. Emergency Medicine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="583" to="588" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Forestry applications of UAVs in Europe: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Carotenuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Di Gennaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gioli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Miglietta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vagnoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaldei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="2427" to="2447" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Intelligent Autonomy of UAVs: Advanced Missions and Future Use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Sebbane</surname></persName>
		</author>
		<idno>2018. 1</idno>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Survey on computer vision for UAVs: Current developments and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanellakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intelligent &amp; Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="168" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An architecture for robust UAV navigation in GPS-denied areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Perez-Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ragel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Viguria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ollero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Field Robotics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="145" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simulation tools, environments and frameworks for UAV systems performance analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Hentati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Krichen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fourati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Fourati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Wireless Communications &amp; Mobile Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1495" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AirSim: High-fidelity visual and physical simulation for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lovett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Field and Service Robotics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="621" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Airsim: High-fidelity visual and physical simulation for autonomous vehicles</title>
	</analytic>
	<monogr>
		<title level="m">Field and Service Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="621" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FlyMASTER: Multi-UAV control and supervision with ROS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Lamping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Ouwerkerk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">O</forename><surname>Stockton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Casbeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aviation Technology, Integration, and Operations Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-DOF camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DeepVO: Towards end-toend visual odometry with deep recurrent convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Review and analysis of solutions of the three point perspective pose estimation problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ottenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nölle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="356" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Vision and learning for deliberative monocular cluttered flight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Agcayazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daftry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.6326</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Uav flight experiments applied to the remote sensing of vegetated areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Salamí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pastor</surname></persName>
		</author>
		<idno>pp. 11 051-11 081</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward lowflying autonomous MAV trail navigation using deep neural networks for environmental awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smolyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamenev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Plato: Policy learning using adaptive trajectory optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3342" to="3349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Extending Deep Neural Network Trail Navigation for Unmanned Aerial Vehicle Operation within the Forest Canopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maciel-Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breckon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Aggressive deep driving: Model predictive control with a cnn cost model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldfain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05303</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cad2rl: Real single-image flight without a single real image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04201</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5038" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Veritatem dies aperittemporally consistent depth prediction enabled by a multi-task geometric and semantic scene understanding approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep drone racing: Learning agile flight in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast, autonomous flight in gps-denied and cluttered environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Watterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mulgaonkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delmerico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="120" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Perception, guidance, and navigation for indoor autonomous drone racing using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2539" to="2544" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">UAS navigation with SqueezePoseNetaccuracy boosting for pose regression by data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jutzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drones</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real shading in Unreal engine 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Games</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physically Based Shading Theory Practice</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representing attitude: Euler angles, unit quaternions, and rotation vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">15-16</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Novi commentarii academiae scientiarum petropolitanae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Euler</surname></persName>
		</author>
		<idno>1776. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Full quaternion based attitude control for a quadrotor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fresk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro. Control Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3864" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Geo-fencing for unmanned aerial vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratyusha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Naidu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Applications</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Safe visual navigation via deep learning and novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<idno>2017. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
