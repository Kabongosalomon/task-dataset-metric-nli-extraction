<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">INDUCTIVE MATRIX COMPLETION BASED ON GRAPH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
							<email>chen@cse.wustl.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Washington University in St. Louis</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Washington University</orgName>
								<address>
									<settlement>St. Louis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">INDUCTIVE MATRIX COMPLETION BASED ON GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020 *Now at Facebook</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user's age or movie's genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix completion model? In this paper, we propose an Inductive Graph-based Matrix Completion (IGMC) model to address this problem. IGMC trains a graph neural network (GNN) based purely on 1-hop subgraphs around (user, item) pairs generated from the rating matrix and maps these subgraphs to their corresponding ratings. It achieves highly competitive performance with state-of-the-art transductive baselines. In addition, IGMC is inductive -it can generalize to users/items unseen during the training (given that their interactions exist), and can even transfer to new tasks. Our transfer learning experiments show that a model trained out of the MovieLens dataset can be directly used to predict Douban movie ratings with surprisingly good performance. Our work demonstrates that: 1) it is possible to train inductive matrix completion models without using side information while achieving similar or better performances than state-of-the-art transductive methods; 2) local graph patterns around a (user, item) pair are effective predictors of the rating this user gives to the item; and 3) Long-range dependencies might not be necessary for modeling recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Matrix completion <ref type="bibr" target="#b4">(Candès &amp; Recht, 2009</ref>) is one common formulation of recommender systems, where rows and columns of a matrix represent users and items, respectively, and predicting users' interest in items corresponds to filling in the missing entries of the rating matrix. By assuming a low-rank rating matrix, many of the most popular matrix completion algorithms use factorization techniques that decompose a rating r ij into w i h j , the inner product of user i's and item j's latent feature vectors w i and h j , respectively, which have achieved great successes <ref type="bibr" target="#b0">(Adomavicius &amp; Tuzhilin, 2005;</ref><ref type="bibr" target="#b34">Schafer et al., 2007;</ref><ref type="bibr" target="#b20">Koren et al., 2009;</ref><ref type="bibr" target="#b2">Bobadilla et al., 2013)</ref> However, matrix factorization is intrinsically transductive, meaning that the learned latent features (embeddings) for users/items are not generalizable to users/items unseen during the training. When the rating matrix has changed values or has new rows/columns added, it often requires a complete retraining to get the new embeddings. To make matrix completion inductive, Inductive Matrix Completion (IMC) has been proposed, which leverages content (side information) of users and items <ref type="bibr" target="#b14">(Jain &amp; Dhillon, 2013;</ref><ref type="bibr" target="#b38">Xu et al., 2013)</ref>. In IMC, a rating is decomposed by r ij = x i Qy j , where x i and y j are content feature vectors of user i and item j, respectively, and Q is a learnable matrix modeling the feature interactions. To accurately predict missing entries, IMC methods have strong constraints on the content quality, which often leads to inferior performance when high-quality content is not available. Other content-based recommender systems <ref type="bibr" target="#b25">(Lops et al., 2011)</ref> face similar problems. In Published as a conference paper at ICLR 2020 Extract enclosing subgraphs user average rating = 3 item average rating = 5 item total ratings received = 2 similar users' average rating= 5 # of ( → 4 → 3 → 5 ) = 1 # of ( → 2 → 1 → 5 ) = 1 Learn graph patterns user average rating = 4 item average rating = 1 item total ratings received = 1 similar users' average rating= 1 # of ( → 4 → 4 → 1 ) = 1 ……  <ref type="figure">Figure 1</ref>: We extract a local enclosing subgraph around each rating (dark green), and train a GNN to map subgraphs to ratings. Each enclosing subgraph is induced by the user and item associated with the target rating as well as their h-hop neighbors (here h = 1). From the subgraphs, a GNN can learn mixed graph patterns (such as average ratings, paths, etc.) useful for rating prediction. We illustrate some possible patterns in the GNN box. We use the trained GNN to complete the missing entries. some extreme settings, there is not any content available for user, such as a website where users are completely anonymous. In these cases, inductive matrix completion seems impossible.</p><p>Recently, <ref type="bibr" target="#b13">Hartford et al. (2018)</ref> propose exchangeable matrix layers, which apply permutation equivariant operations on the rating matrix to reconstruct missing entries. The resulting models are inductive and do not rely on side information. However, these models take the whole rating matrix R as input and output another reconstructed matrix, which poses scalability challenges for practical datasets with millions of users/items. In this paper, we propose a novel inductive matrix completion method that does not use any content. Further, our method does not need to take the whole rating matrix as input, and can infer ratings for individual user-item pairs. The key that frees us from using content or whole rating matrix is local graph pattern. If for each observed rating we add an edge between the corresponding user and item, we can build a bipartite graph from the rating matrix. Subsequently, predicting unknown ratings converts equivalently to predicting labeled links in this bipartite graph. This transforms matrix completion into a link prediction problem <ref type="bibr" target="#b24">(Liben-Nowell &amp; Kleinberg, 2007)</ref>, where graph patterns play a major role in determining link existences.</p><p>A major class of link prediction methods are heuristic methods, which predict links based on some heuristic scores. For example, the common neighbors heuristic count the common neighbors between two nodes to predict links, while the Katz index <ref type="bibr" target="#b17">(Katz, 1953)</ref> uses a weighted sum of all the walks between two nodes. See <ref type="bibr" target="#b24">(Liben-Nowell &amp; Kleinberg, 2007)</ref> for an overview. These heuristics can be seen as some predefined graph structure features calculated based on the local or global graph patterns around links, which have achieved great successes due to their simplicity and effectiveness.</p><p>However, these traditional link prediction heuristics only work for simple graphs where nodes and edges both only have a single type. Can we find some heuristics for labeled link prediction in bipartite graph? Intuitively, such heuristics should exist. For example, if a user u 0 likes an item v 0 , we may expect to see very often that v 0 is also liked by some other user u 1 who shares a similar taste to u 0 . By similar taste, we mean u 1 and u 0 have together both liked some other item v 1 . In the bipartite graph, such a pattern is realized as a "like" path (u 0 → like v 1 → liked by u 1 → like v 0 ). If there are many such paths between u 0 and v 0 , we may infer that u 0 is highly likely to like v 0 . Thus, we may count the number of such paths as an indicator of how likely u 0 likes v 0 . In fact, many neighborhood-based recommender systems <ref type="bibr" target="#b7">(Desrosiers &amp; Karypis, 2011)</ref> rely on similar heuristics.</p><p>Of course we can try to manually define many such intuitive heuristics and test their effectiveness. In this work, however, we take a different approach that automatically learns suitable heuristics from the given bipartite graph. To do so, we first extract an h-hop enclosing subgraph for each training user-item pair <ref type="bibr">(u, v)</ref>, which is defined to be the subgraph induced from the bipartite graph by nodes u, v and their neighbors within h hops. Such local subgraphs contain rich graph pattern information about the rating that u may give to v. For example, all the (u 0 → like v 1 → liked by u 1 → like v 0 ) paths are included in the 1-hop enclosing subgraph around (u 0 , v 0 ). By feeding these enclosing subgraphs to a graph neural network (GNN), we train a graph regression model that maps each subgraph to the rating that its center user gives to its center item. <ref type="figure">Figure 1</ref> illustrates the overall framework.</p><p>Due to the superior graph learning ability, a GNN can learn highly expressive graph structure features useful for inferring the ratings without restricting the features to predefined heuristics. Given a trained GNN, we can also apply it to unseen users/items without retraining. Our resulting algorithm is inductive, and is named Inductive Graph-based Matrix Completion (IGMC). Note that IGMC does not address the extreme cold-start problem, as it still requires an unseen user-item pair's enclosing subgraph (i.e., the user and item should at least have some interactions with neighbors so that the enclosing subgraph is not empty). This scenario is very common in practice. For example, a newly registered YouTube user may quickly watch some videos without completing their personal information. In this case, if we cannot retrain user embeddings frequently, IGMC can be of great value by still making recommendations based purely on this user's interaction history with videos.</p><p>We compare IGMC with state-of-the-art matrix completion algorithms on five benchmark datasets. Without using any content, IGMC achieves the smallest RMSEs on four of them, even beating many transductive baselines augmented by side information. Our model is also equipped with excellent transfer learning ability. We show that an IGMC model trained on the MovieLens-100K dataset can be directly used to predict Douban movie ratings and even outperforms some baselines trained specifically on Douban. We also analyze IGMC's behavior on sparse rating matrices. We show that IGMC is more robust than transductive methods on sparse matrices. Under an extremely sparse case (only 0.1% of MovieLens-1M training ratings are kept), IGMC can still achieve less than 0.95 RMSE, beating a state-of-the-art transductive method GC-MC by more than 0.1 RMSE. Finally, our visualization confirms that local enclosing subgraphs are indeed strong predictors of ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph neural networks Graph neural networks (GNNs) are a new type of neural networks for learning over graphs <ref type="bibr" target="#b33">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b3">Bruna et al., 2013;</ref><ref type="bibr" target="#b9">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b23">Li et al., 2015;</ref><ref type="bibr" target="#b19">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b30">Niepert et al., 2016;</ref><ref type="bibr" target="#b6">Dai et al., 2016)</ref>. There are two types of GNNs: Nodelevel GNNs use message passing layers to iteratively pass messages between each node and its neighbors in order to extract a feature vector for each node encoding its local substructure. Graphlevel GNNs additionally use a pooling layer such as summing which aggregates node feature vectors into a graph representation to enable graph-level tasks such as graph classification/regression. Due to the superior graph representation learning ability, GNNs have achieved state-of-the-art performance on semi-supervised node classification <ref type="bibr" target="#b19">(Kipf &amp; Welling, 2016)</ref>, network embedding <ref type="bibr" target="#b12">(Hamilton et al., 2017)</ref>, graph classification , and link prediction , etc.</p><p>GNNs for matrix completion The matrix completion problem has been studied using GNNs. <ref type="bibr" target="#b29">Monti et al. (2017)</ref> develop a multi-graph CNN (MGCNN) model to extract user and item latent features from their respective nearest-neighbor networks.  propose graph convolutional matrix completion (GC-MC) which directly applies a GNN to the user-item bipartite graph to extract user and item latent features using a GNN. The SpectralCF model of <ref type="bibr" target="#b44">(Zheng et al., 2018)</ref> uses a spectral-GNN on the bipartite graph to learn node embeddings. Although using GNNs for matrix completion, all these models are still transductive -MGCNN and SpectralCF require graph Laplacians which do not generalize to new graphs, while GC-MC uses one-hot encoding of node IDs as initial node features, thus cannot generalize to unseen users/items. A recent inductive graphbased recommender system, PinSage <ref type="bibr" target="#b39">(Ying et al., 2018a)</ref>, uses node content as initial node features (instead of the one-hot encoding in GC-MC), and is successfully used in recommending related pins in Pinterest. Although being inductive, PinSage relies heavily on the rich visual and text content associated with the pins which is not often accessible in other recommendation tasks. In comparison, our IGMC model is inductive and does not rely on any content. All previous approaches use node-level GNNs to learn embeddings for nodes, while our IGMC uses a graph-level GNN to learn representations for subgraphs. We will discuss this crucial difference in more details in Section 4.</p><p>Another related previous work is <ref type="bibr" target="#b13">(Hartford et al., 2018)</ref>, which defines exchangeable matrix layers to perform permutation-equivariant operations on matrices to achieve inductive matrix completion without using content. In particular, the operation updates each matrix entry by a weighted sum of itself, entries of its row, entries of its column, and all other entries of the matrix, where parameters for each of the four components are shared across all entries. It can also be regarded as a GNN with the exceptions that 1) the message passing is performed on edges (final edge features are pooled into node features), and 2) all edges (including those not connected to the center edge) pass messages</p><formula xml:id="formula_0">Algorithm 1 ENCLOSING SUBGRAPH EXTRACTION 1: input: h, target user-item pair (u, v), the bipartite graph G 2: output: the h-hop enclosing subgraph G h u,v for (u, v) 3: U = U fringe = {u}, V = V fringe = {v} 4: for i = 1, 2, . . . , h do 5: U fringe = {u i : u i ∼ V fringe } \ U 6: V fringe = {v i : v i ∼ U fringe } \ V 7: U fringe = U fringe , V fringe = V fringe 8: U = U ∪ U fringe , V = V ∪ V fringe 9: Let G h u,v be the vertex-induced subgraph from G using vertices U, V 10: Remove edge (u, v) from G h u,v . 11: end for 12: return G h u,v</formula><p>Note: {ui : ui ∼ V fringe } is the set of nodes that are adjacent to at least one node in V fringe with any edge type.</p><p>to the center edge in each round. One limitation of <ref type="bibr" target="#b13">(Hartford et al., 2018)</ref> is that it takes the entire rating matrix as input, which might raise concerns for large matrices. In comparison, our IGMC takes only local subgraphs as input which avoids the issue and enables predicting individual ratings.</p><p>Link prediction based on graph patterns Learning supervised heuristics (graph patterns) has been studied for link prediction in simple graphs. <ref type="bibr" target="#b41">Zhang &amp; Chen (2017)</ref> propose Weisfeiler-Lehman Neural Machine (WLNM), which learns graph structure features using a fully-connected neural network on the subgraphs' adjacency matrices. Later, they improve this work by replacing the fullyconnected neural network with a GNN and achieves state-of-the-art link prediction results . Our work generalizes this line of research from predicting link existence in simple graphs to predicting values of links in bipartite graphs (i.e., matrix completion). In <ref type="bibr" target="#b5">(Chen et al., 2005;</ref><ref type="bibr" target="#b46">Zhou et al., 2007)</ref>, traditional link prediction heuristics are adapted to bipartite graphs which show promising performance for recommender systems. Our work differs in that we do not use any predefined heuristics, but learn general graph structure features using a GNN. Another similar work to ours is <ref type="bibr" target="#b22">(Li &amp; Chen, 2013)</ref>, where graph kernels are used to learn graph structure features. However, graph kernels require quadratic time and space complexity to compute and store the kernel matrices thus are unsuitable for modern recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INDUCTIVE GRAPH-BASED MATRIX COMPLETION (IGMC)</head><p>We now present our Inductive Graph-based Matrix Completion (IGMC) framework. We use G to denote the undirected bipartite graph constructed from the given rating matrix R. In G, a node is either a user (denoted by u, corresponding to a row in R) or an item (denoted by v, corresponding to a column in R). Edges can exist between user and item, but cannot exist between two users or two items. Each edge (u, v) has a value r = R u,v , corresponding to the rating that u gives to v. We use R to denote the set of all possible ratings (e.g., R = {1, 2, 3, 4, 5} in MovieLens), and use N r (u) to denote the set of u's neighbors that connect to u with edge type r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ENCLOSING SUBGRAPH EXTRACTION</head><p>The first part of IGMC is enclosing subgraph extraction. For each observed rating R u,v , we extract an h-hop enclosing subgraph around (u, v) from G. Algorithm 1 describes the BFS procedure for extracting h-hop enclosing subgraphs. We will feed these enclosing subgraphs to a GNN and regress on their ratings. Then, for each testing (u, v) pair, we again extract its h-hop enclosing subgraph from G, and use the trained GNN model to predict its rating. Note that after extracting a training enclosing subgraph for (u, v), we should remove the edge (u, v) because it is the target to predict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NODE LABELING</head><p>The second part of IGMC is node labeling. Before we feed an enclosing subgraph to the GNN, we first apply a node labeling to it, which gives an integer label to every node in the subgraph. The purpose is to use different labels to mark nodes' different roles in a subgraph. Ideally, our node labeling should be able to: 1) distinguish the target user and target item between which the target rating is located, and 2) differentiate user-type nodes from item-type nodes. Otherwise, the GNN cannot tell between which user and item to predict the rating, and might lose node-type information. To satisfy these conditions, we propose a node labeling as follows: We first give label 0 and 1 to the target user and target item, respectively. Then, we determine other nodes' labels according to at which hop they are included in the subgraph in Algorithm 1. If a user-type node is included at the i th hop, we will give it a label 2i. If an item-type node is included at the i th hop, we will give it 2i + 1. Such a node labeling can sufficiently discriminate: 1) target nodes from "context" nodes, 2) users from items (users always have even labels), and 3) nodes of different distances to the target rating.</p><p>Note that this is not the only possible way of node labeling, but we empirically verified its excellent performance. The one-hot encoding of these node labels will be treated as the initial node features x 0 of the subgraph when fed to the GNN. Note that our node labels are determined completely inside each enclosing subgraph, thus are independent of the global bipartite graph. Given a new enclosing subgraph, we can as well predict its rating even if all of its nodes are from a different bipartite graph, because IGMC purely relies on graph patterns within local enclosing subgraphs without leveraging any global information specific to the bipartite graph. Our node labeling is also different from using the global node IDs as in GC-MC . Using one-hot encoding of global IDs is essentially transforming the first message passing layer's parameters into latent node embedding associated with each particular ID (equivalent to an embedding lookup table). Such a model cannot generalize to nodes whose IDs are out of range, thus is transductive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GRAPH NEURAL NETWORK ARCHITECTURE</head><p>The third part of IGMC is to train a graph neural network (GNN) model predicting ratings from the enclosing subgraphs. In previous node-based approaches such as GC-MC, a node-level GNN is applied to the entire bipartite graph to extract node embeddings. Then, the node embeddings of u and v are input to an inner-product or bilinear operator to reconstruct the rating on <ref type="bibr">(u, v)</ref>. In contrast, IGMC applies a graph-level GNN to the enclosing subgraph around (u, v) and maps the subgraph to the rating. There are thus two components in our GNN: 1) message passing layers that extract a feature vector for each node in the subgraph, and 2) a pooling layer to summarize a subgraph representation from node features.</p><p>To learn the rich graph patterns introduced by the different edge types, we adopt the relational graph convolutional operator (R-GCN) <ref type="bibr" target="#b35">(Schlichtkrull et al., 2018)</ref> as our GNN's message passing layers, which has the following form:</p><formula xml:id="formula_1">x l+1 i = W l 0 x l i + r∈R j∈Nr(i) 1 |N r (i)| W l r x l j ,<label>(1)</label></formula><p>where x l i denotes node i's feature vector at layer l, W l 0 and {W l r |r ∈ R} are learnable parameter matrices. Since neighbors j connected to i with different edge types r are processed by different parameter matrices W l r , we are able to learn a large amount of enriched graph patterns inside the edge types, such as the average rating the target user gives to items, the average rating the target item receives, and by which paths the two target nodes are connected, etc. We stack L message passing layers with tanh activations between two layers. Following <ref type="bibr" target="#b37">Xu et al., 2018)</ref>, node i's feature vectors from different layers are concatenated as its final representation h i :</p><formula xml:id="formula_2">h i = concat(x 1 i , x 2 i , . . . , x L i ).<label>(2)</label></formula><p>Next, we pool the node representations into a graph-level feature vector. There are many choices such as summing, averaging, SortPooling , DiffPooling <ref type="bibr" target="#b40">(Ying et al., 2018b)</ref>, etc.</p><p>In this work, however, we use a different pooling layer which concatenates the final representations of only the target user and item as the graph representation:</p><formula xml:id="formula_3">g = concat(h u , h v ),<label>(3)</label></formula><p>where we use h u and h v to denote the final representations of the target user and target item, respectively. Our particular choice is due to the extra importance that these two target nodes carry compared to other context nodes. Although being very simple, we empirically verified its better performance than summing and other advanced pooling layers for our matrix completion tasks.</p><p>After getting the final graph representation, we use an MLP to output the predicted rating:</p><formula xml:id="formula_4">r = w σ(Wg),<label>(4)</label></formula><p>where W and w are parameters of the MLP which map the graph representation g to a scalar ratinĝ r, and σ is an activation function (we take ReLU in this paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MODEL TRAINING</head><p>Loss function We minimize the mean squared error (MSE) between the predictions and the ground truth ratings:</p><formula xml:id="formula_5">L = 1 |{(u, v)|Ω u,v = 1}| (u,v):Ωu,v=1 (R u,v −R u,v ) 2 ,<label>(5)</label></formula><p>where we use R u,v andR u,v to denote the true rating and predicted rating of (u, v), repsectively, and Ω is a 0/1 mask matrix indicating the observed entries of the rating matrix R.</p><p>Adjacent rating regularization The R-GCN layer (1) used in our GNN has different parameters W r for different rating types. One drawback here is that it fails to take the magnitude of ratings into consideration. For instance, a rating of 4 and a rating of 5 in MovieLens both indicate that the user likes the movie, while a rating of 1 indicates that the user does not like the movie. Ideally, we expect our model to be aware of the fact that a rating of 4 is more similar to 5 than 1 is. In R-GCN, however, ratings 1, 4 and 5 are only treated as three independent edge types -the magnitude and order information of the ratings is completely lost. To fix that, we propose an adjacent rating regularization (ARR) technique, which encourages ratings adjacent to each other to have similar parameter matrices. Assume the ratings in R exhibit an ordering r 1 , r 2 , . . . , r |R| which indicates increasingly higher preference that users have for items. Then, the ARR regularizer is:</p><formula xml:id="formula_6">L ARR = i=1,2,...,|R|−1 W ri+1 − W ri 2 F ,<label>(6)</label></formula><p>where · F denotes the Frobenius norm of a matrix. The above regularizer restrains the parameter matrices of adjacent ratings from having too much differences, which not only takes into consideration of the ratings' order, but also helps the optimization of those infrequent ratings by transferring knowledge from their adjacent ratings. The final loss function is given by:</p><formula xml:id="formula_7">L final = L + λL ARR ,<label>(7)</label></formula><p>where λ trades-off the importance of the MSE loss and the ARR regularizer. There are many other ways to model rating magnitude and order, which are left for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GRAPH-LEVEL GNN VS. NODE-LEVEL GNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) (a)</head><p>Compared to previous graph matrix completion approaches such as PinSage and GC-MC, one important difference of IGMC is that it uses a graph-level GNN to map the enclosing subgraph around the target user and item to their rating (left <ref type="figure">figure (a)</ref>), instead of using a node-level GNN on the bipartite graph G to learn target user's and item's embeddings and use the node embeddings to predict the rating (left <ref type="figure">figure (b)</ref>). One drawback of the latter node-based approach is that the learned node embeddings are essentially encoding the two rooted subtrees around the two nodes independently, which fails to model the interactions and correspondences between the nodes of the two trees. For example, from the two subtrees of the left figure (b) we do not really know whether the two target nodes are just isolated from each other like in (b) or actually densely connected like in (a); these two cases look identical to a node-based approach. In comparison, a graph-level GNN can discriminate the two cases through a sufficient number of message passing rounds. Since the learning is confined to the subgraph, stacking multiple graph convolution layers will learn more and more refined local structural features which can adequately discriminate up to all subgraphs that the Weisfeiler-Lehman algorithm can discriminate <ref type="bibr" target="#b37">(Xu et al., 2018)</ref>. However, for node-based approaches, since there is no subgraph boundary, stacking multiple graph convolutions will only extend the convolution range to unrelated distant nodes and oversmooth the node embeddings <ref type="bibr" target="#b21">(Li et al., 2018)</ref>. This is reflected in that previous node-based approaches mainly use only one or two message passing layers <ref type="bibr" target="#b39">Ying et al., 2018a)</ref>.</p><p>Nevertheless, using a graph-level GNN on every target rating's enclosing subgraph has higher complexity than using a node-level GNN on the entire bipartite graph. Suppose the bipartite graph has |E| edges. Then performing one round of message passing using a node-level GNN has O(|E|) complexity. Assume the maximum number of edges in all enclosing subgraphs is K. Performing one round of message passing for all enclosing subgraphs then has O(K|E|) complexity. In practice, we can use subsampling to restrict K to a small number to reduce IGMC's complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conduct experiments on five common matrix completion datasets: Flixster <ref type="bibr" target="#b15">(Jamali &amp; Ester, 2010)</ref>, Douban <ref type="bibr" target="#b26">(Ma et al., 2011)</ref>, YahooMusic <ref type="bibr" target="#b8">(Dror et al., 2011)</ref>, MovieLens-100K and MovieLens-1M <ref type="bibr" target="#b27">(Miller et al., 2003)</ref>. For ML-100K, we train and evaluate on the canonical u1.base/u1.test train/test split. For ML-1M, we randomly split it into 90% and 10% train/test sets. For Flixster, Douban and YahooMusic we use the preprocessed subsets and splits provided by <ref type="bibr" target="#b29">(Monti et al., 2017)</ref>. Dataset statistics are summarized in <ref type="table" target="#tab_0">Table 1</ref>. We implemented IGMC using pytorch geometric <ref type="bibr" target="#b11">(Fey &amp; Lenssen, 2019</ref>). We tuned model hyperparameters based on cross validation results on ML-100K, and used them across all datasets. The final architecture uses 4 R-GCN layers with 32, 32, 32, 32 hidden dimensions. Basis decomposition with 4 bases is used to reduce the number of parameters in W r <ref type="bibr" target="#b35">(Schlichtkrull et al., 2018)</ref>. The final MLP has 128 hidden units and a dropout rate of 0.5. We use 1-hop enclosing subgraphs for all datasets, and find them sufficiently good. We find using 2 or more hops can slightly increase the performance but take much longer training time. For each enclosing subgraph, we randomly drop out its adjacency matrix entries with a probability of 0.2 during the training. We set the λ in (7) to 0.001. We train our model using the Adam optimizer (Kingma &amp; Ba, 2014) with a batch size of 50 and an initial learning rate of 0.001, and multiply the learning rate by 0.1 every 20 epochs for ML-1M, and every 50 epochs for all other datasets. Our code is publicly available at https://github.com/muhanzhang/IGMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FLIXSTER, DOUBAN AND YAHOOMUSIC</head><p>For these three datasets, we compare our IGMC with GRALS <ref type="bibr" target="#b31">(Rao et al., 2015)</ref>, sRGCNN <ref type="bibr" target="#b29">(Monti et al., 2017)</ref>, GC-MC , F-EAE <ref type="bibr" target="#b13">(Hartford et al., 2018)</ref>, and PinSage <ref type="bibr" target="#b39">(Ying et al., 2018a)</ref>. Among them, GRALS is a graph regularized matrix completion algorithm. GC-MC and sRGCNN are transductive node-level-GNN-based matrix completion methods. F-EAE uses exchangeable matrix layers to perform inductive matrix completion without using content. PinSage is an inductive node-level-GNN-based model using content, which is originally used to predict related pins and is adapted to predicting ratings here. We further implemented an inductive GC-MC model (IGC-MC) which replaces the one-hot encoding of node IDs with the content features to make it  inductive. The content in these datasets are presented in the form of user and item graphs. We summarize whether each algorithm is inductive and whether it uses content in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>We train our model for 40 epochs, and save the model parameters every 10 epochs. The final predictions are given by averaging the predictions from epochs 10, 20, 30 and 40. We repeat the experiment five times and report the average RMSEs. The baseline results are taken from <ref type="bibr" target="#b13">(Hartford et al., 2018)</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows the results. Our model achieves the smallest RMSEs on all three datasets without using any content, significantly outperforming all the compared baselines, regardless of whether they are transductive or inductive. Further, except F-EAE, all the baselines have used content information to assist the matrix completion. This further highlights IGMC's great performance advantages without relying on content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ML-100K AND ML-1M</head><p>We further conduct experiments on MovieLens datasets. Side information is present for both users (age, gender, occupation, etc.) and movies (genres). For ML-100K, we compare against matrix completion (MC) (Candès &amp; Recht, 2009), inductive matrix completion (IMC) <ref type="bibr" target="#b14">(Jain &amp; Dhillon, 2013)</ref>, geometric matrix completion (GMC) <ref type="bibr" target="#b16">(Kalofolias et al., 2014)</ref>, as well as GRALS, sRGCNN, GC-MC, F-EAE and PinSage. We train IGMC for 80 epochs and report the ensemble performance of epochs 50, 60, 70 and 80. For ML-1M, besides the baselines GC-MC, F-EAE and PinSage, we further include state-of-the-art algorithms including PMF <ref type="bibr" target="#b28">(Mnih &amp; Salakhutdinov, 2008)</ref>, I-RBM <ref type="bibr" target="#b32">(Salakhutdinov et al., 2007)</ref>, NNMF <ref type="bibr" target="#b10">(Dziugaite &amp; Roy, 2015)</ref>, I-AutoRec <ref type="bibr" target="#b36">(Sedhain et al., 2015)</ref> and CF-NADE <ref type="bibr" target="#b45">(Zheng et al., 2016)</ref>. We train IGMC for 40 epochs and report the ensemble performance of epochs 25, 30, 35 and 40. The experiments are repeated five times and the average results are reported in <ref type="table" target="#tab_2">Table 3</ref> (standard deviations are less than 0.001). As we can see, IGMC achieves the best performance on ML-100K, in parallel with GC-MC despite that IGMC is an inductive model, while GC-MC is transductive and additionally uses content information. For ML-1M, IGMC cannot catch up with state-of-the-art transductive models such as CF-NADE and GC-MC, but outperforms other inductive models. We will analyze this dataset further in Section 5.3. Test RMSE To gain insight into when inductive graph-based matrix completion is more suitable than transductive methods, we compare IGMC with GC-MC on ML-1M under different sparsity levels of the rating matrix. We sequentially increase the sparsity level by randomly keeping only 0.2, 0.1, 0.05, 0.01, and 0.001 of the original training ratings. Then, we train both models on the sparsified rating matrices, and evaluate on the original test set. <ref type="figure" target="#fig_2">Figure 2</ref> shows the results. As we can see, although IGMC falls behind GC-MC initially with full ratings, it starts to perform better after the sparsity ratio is less than 20%. The advantage becomes even greater under extremely sparse cases. This seems to indicate that IGMC is a better choice than transductive methods when there is not a large amount of training data, which is particularly suitable for the initial rating collection phase of a recommender system. It also suggests that transductive matrix completion relies more on the dense user-item interactions than inductive graph-based matrix completion does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GC-MC IGMC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">TRANSFER LEARNING</head><p>A great advantage of an inductive model is its potential for transferring to other tasks. We conduct a transfer learning experiment by applying the IGMC model trained on ML-100K to Flixster, Douban and YahooMusic. Among the three datasets, only Douban has exactly the same rating types as <ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5)</ref>. Thus for Flixster and YahooMusic, we bin their edge types into groups 1 to 5 before feeding into the ML-100K model, and multiply the YahooMusic predictions by 20 to account for the different scales. Despite all the compromises, the transferred IGMC model achieves excellent performance <ref type="table" target="#tab_3">(Table 4</ref>). We also show the transfer learning results of other two inductive models, IGC-MC and F-EAE. Note that an inductive model using content features (such as PinSage) is not transferrable, due to the different feature spaces between MovieLens and the target datasets. Thus for IGC-MC, we replace its content features with node degrees. As we can see, IGMC outperforms the other two models by large margins in terms of transfer learning ability. Furthermore, the transferred IGMC even outperforms a wide range of baselines trained especially on each dataset <ref type="table" target="#tab_1">(Table 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">ABLATION STUDIES</head><p>To understand the individual contributions of some components in IGMC, we conduct several ablation studies. In particular, we are interested in: 1) whether the proposed pooling layer in Equation (3) helps; 2) whether the proposed adjacent rating regularization (ARR) helps; and 3) whether incorporating content can further improve IGMC's performance. We present the results in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">VISUALIZATION</head><p>Finally, we visualize 10 testing enclosing subgraphs with the highest and lowest predicted ratings for Flixster, Douban, YahooMusic, and ML-100K, respectively, in <ref type="figure" target="#fig_4">Figure 3</ref>. As we can see, there are substantially different patterns between high-score and low-score subgraphs, which is why IGMC can predict ratings merely from these subgraphs. For example, high-score subgraphs typically show both high user average rating and high item average rating, while low-score subgraphs often have mixed ratings from non-target users and have low user average rating.   We visualize edge ratings using the color map shown in the right. Higher ratings are redder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have proposed Inductive Graph-based Matrix Completion (IGMC). Instead of learning transductive latent features, IGMC learns local graph patterns related to ratings inductively based on graph neural networks. Compared to previous inductive matrix completion methods, IGMC does not rely on content (side information) of users/items. We show that IGMC has highly competitive performance compared to state-of-the-art baselines. In addition, IGMC is transferrable to new tasks without any retraining, a property much desired in those recommendation tasks having few training data. We hope IGMC can provide a new idea to matrix completion and recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ABLATION STUDIES</head><p>In this section, we conduct ablation experiments to study the individual contributions of some components in IGMC. In particular, we are interested in: 1) whether the proposed pooling layer in Equation <ref type="formula" target="#formula_3">(3)</ref> helps; 2) whether the proposed adjacent rating regularization (ARR) in Equation <ref type="formula" target="#formula_6">(6)</ref> helps; and 3) whether incorporating content can further improve IGMC's performance. Therefore, we compare the original IGMC with: 1) IGMC with SumPooling replacing the proposed pooling layer, 2) IGMC without ARR by setting λ in (7) to 0, and 3) IGMC with content by concatenating target user and item's content feature vectors to the graph representation g before feeding into the MLP (4), which is similar to . The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. From <ref type="table" target="#tab_5">Table 5</ref>, we have the following observations. Firstly, using the proposed pooling layer shows a huge improvement over a standard SumPooling. This might be because SumPooling assigns equal importance to all nodes in a subgraph, which fails to distinguish the target user and item from the context nodes. This indicates that a pooling layer able to highlight the target user and item is important for IGMC.</p><p>Secondly, we can see that disabling ARR results in a 0.003 performance drop on ML-100K, while seeming to have no influence on the other three datasets. One possible explanation is that ARR is more useful for modeling large and dense enclosing subgraphs, since ARR enhances the modeling power by modeling the extra relationships between edge types in terms of their magnitude differences. Another possible reason is that we only tuned ARR's λ on ML-100K and used λ = 0.001 uniformly on all datasets. This is partly verified by that when increasing λ to 0.1 for YahooMusic, we can further decrease IGMC's RMSE to 18.98±0.140. We did not fully verify this, and leave better ways of modeling rating magnitude for future study.</p><p>Thirdly, we observe that incorporating content does not improve IGMC's performance on ML-100K, and often hurts the performance on the other datasets. For Flixster, Douban and YahooMusic, this phenomenon can be explained by that the content of these three datasets are user/item's respective graphs, which has little to no gains to a model that has already exploited graph structure information between users and items very well. In addition, the content feature vectors are presented in 3000-dimensional adjacency vectors, which might pose new problems due to their size and sparsity. For ML-100K, the lost of some performance when adding content actually contradicts with our initial experiments. In our initial experiments when the RMSE on ML-100K without content was around 0.910, we observed that adding content was indeed helpful and reduced the RMSE to</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>ML-1M results under different sparsity ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Testing enclosing subgraphs for Flixster, Douban, YahooMusic, and ML-100K. Top 5 and bottom 5 are subgraphs with the highest and lowest predicted ratings, respectively. In each subgraph, red nodes in the left are users, blue nodes in the right are items; the bottom red and blue nodes are the target user and item; the predicted rating and true rating (in parenthesis) are shown underneath.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of each dataset.</figDesc><table><row><cell>Dataset</cell><cell>Users Items</cell><cell cols="2">Ratings Density</cell><cell>Rating types</cell></row><row><cell>Flixster</cell><cell>3,000 3,000</cell><cell>26,173</cell><cell cols="2">0.0029 0.5, 1, 1.5, ..., 5</cell></row><row><cell>Douban</cell><cell>3,000 3,000</cell><cell>136,891</cell><cell>0.0152</cell><cell>1, 2, 3, 4, 5</cell></row><row><cell cols="2">YahooMusic 3,000 3,000</cell><cell>5,335</cell><cell>0.0006</cell><cell>1, 2, 3, ..., 100</cell></row><row><cell>ML-100K</cell><cell>943 1,682</cell><cell>100,000</cell><cell>0.0630</cell><cell>1, 2, 3, 4, 5</cell></row><row><cell>ML-1M</cell><cell cols="2">6,040 3,706 1,000,209</cell><cell>0.0447</cell><cell>1, 2, 3, 4, 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>RMSE test results on Flixster, Douban and YahooMusic.</figDesc><table><row><cell>Model</cell><cell cols="3">Inductive Content Flixster</cell><cell>Douban</cell><cell>YahooMusic</cell></row><row><cell>GRALS</cell><cell>no</cell><cell>yes</cell><cell>1.245</cell><cell>0.833</cell><cell>38.0</cell></row><row><cell>sRGCNN</cell><cell>no</cell><cell>yes</cell><cell>0.926</cell><cell>0.801</cell><cell>22.4</cell></row><row><cell>GC-MC</cell><cell>no</cell><cell>yes</cell><cell>0.917</cell><cell>0.734</cell><cell>20.5</cell></row><row><cell>IGC-MC</cell><cell>yes</cell><cell>yes</cell><cell cols="3">0.999±0.062 0.990±0.082 21.3±0.989</cell></row><row><cell>F-EAE</cell><cell>yes</cell><cell>no</cell><cell>0.908</cell><cell>0.738</cell><cell>20.0</cell></row><row><cell>PinSage</cell><cell>yes</cell><cell>yes</cell><cell cols="3">0.954±0.005 0.739±0.002 22.9±0.629</cell></row><row><cell>IGMC (ours)</cell><cell>yes</cell><cell>no</cell><cell cols="3">0.872±0.001 0.721±0.001 19.1±0.138</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>RMSE test results on MovieLens-100K (left) and MovieLens-1M (right).</figDesc><table><row><cell>Model</cell><cell cols="3">Inductive Content ML-100K</cell><cell>Model</cell><cell cols="3">Inductive Content ML-1M</cell></row><row><cell>MC</cell><cell>no</cell><cell>no</cell><cell>0.973</cell><cell>PMF</cell><cell>no</cell><cell>no</cell><cell>0.883</cell></row><row><cell>IMC</cell><cell>no</cell><cell>yes</cell><cell>1.653</cell><cell>I-RBM</cell><cell>no</cell><cell>no</cell><cell>0.854</cell></row><row><cell>GMC</cell><cell>no</cell><cell>yes</cell><cell>0.996</cell><cell>NNMF</cell><cell>no</cell><cell>no</cell><cell>0.843</cell></row><row><cell>GRALS</cell><cell>no</cell><cell>yes</cell><cell>0.945</cell><cell>I-AutoRec</cell><cell>no</cell><cell>no</cell><cell>0.831</cell></row><row><cell>sRGCNN</cell><cell>no</cell><cell>yes</cell><cell>0.929</cell><cell>CF-NADE</cell><cell>no</cell><cell>no</cell><cell>0.829</cell></row><row><cell>GC-MC</cell><cell>no</cell><cell>yes</cell><cell>0.905</cell><cell>GC-MC</cell><cell>no</cell><cell>no</cell><cell>0.832</cell></row><row><cell>IGC-MC</cell><cell>yes</cell><cell>yes</cell><cell>1.142</cell><cell>IGC-MC</cell><cell>yes</cell><cell>yes</cell><cell>1.259</cell></row><row><cell>F-EAE</cell><cell>yes</cell><cell>no</cell><cell>0.920</cell><cell>F-EAE</cell><cell>yes</cell><cell>no</cell><cell>0.860</cell></row><row><cell>PinSage</cell><cell>yes</cell><cell>yes</cell><cell>0.951</cell><cell>PinSage</cell><cell>yes</cell><cell>yes</cell><cell>0.906</cell></row><row><cell>IGMC</cell><cell>yes</cell><cell>no</cell><cell>0.905</cell><cell>IGMC</cell><cell>yes</cell><cell>no</cell><cell>0.857</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>RMSE of transferring the model trained on ML-100K to Flixster, Douban and YahooMusic.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="5">Inductive Content Flixster Douban YahooMusic</cell></row><row><cell></cell><cell>IGC-MC</cell><cell></cell><cell>yes</cell><cell>no</cell><cell>1.290</cell><cell>1.144</cell><cell>25.7</cell></row><row><cell></cell><cell>F-EAE</cell><cell></cell><cell>yes</cell><cell>no</cell><cell>0.987</cell><cell>0.766</cell><cell>23.3</cell></row><row><cell></cell><cell cols="2">IGMC (ours)</cell><cell>yes</cell><cell>no</cell><cell>0.906</cell><cell>0.759</cell><cell>20.1</cell></row><row><cell cols="5">5.3 SPARSE RATING MATRIX ANALYSIS</cell><cell></cell></row><row><cell>1.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>0.2</cell><cell>0.1 Sparsity 0.05</cell><cell>0.01</cell><cell>0.001</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>RMSE test results of ablation experiments.</figDesc><table><row><cell>Model</cell><cell>Flixster</cell><cell>Douban</cell><cell cols="2">YahooMusic ML-100K</cell></row><row><cell>IGMC (original)</cell><cell cols="3">0.872±0.001 0.721±0.001 19.1±0.138</cell><cell>0.905±0.001</cell></row><row><cell cols="4">IGMC (SumPooling) 0.879±0.002 0.729±0.003 22.9±1.102</cell><cell>0.933±0.001</cell></row><row><cell>IGMC (no ARR)</cell><cell cols="3">0.872±0.001 0.721±0.001 19.2±0.140</cell><cell>0.908±0.001</cell></row><row><cell>IGMC (content)</cell><cell cols="3">0.886±0.002 0.726±0.001 19.1±0.069</cell><cell>0.906±0.001</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work is supported in part by the National Science Foundation under award numbers III-1526012 and SCH-1622678, and by the National Institute of Health under award number 1R21HS024581.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gediminas</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="734" to="749" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recommender systems survey. Knowledge-based systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Bobadilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Gutiérrez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="109" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exact matrix completion via convex optimization. Foundations of Computational mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Recht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="717" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Link prediction approach to collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsinchun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL&apos;05)</title>
		<meeting>the 5th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL&apos;05)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="141" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive survey of neighborhood-based recommendation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender systems handbook</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="107" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The yahoo! music dataset and kdd-cup&apos;11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on KDD Cup</title>
		<meeting>the 2011 International Conference on KDD Cup</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel M</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06443</idno>
		<title level="m">Neural network matrix factorization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep models of interactions across sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1909" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0626</idno>
		<title level="m">Provable inductive matrix completion</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A matrix factorization technique with trust propagation for recommendation in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Jamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM conference on Recommender systems</title>
		<meeting>the fourth ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.1717</idno>
		<title level="m">Matrix completion on graphs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new status index derived from sociometric analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recommendation as link prediction in bipartite graphs: A graph kernelbased machine learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsinchun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="880" to="890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Content-based recommender systems: State of the art and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Lops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Marco De Gemmis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semeraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender systems handbook</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="73" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
		<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Movielens unplugged: experiences with an occasionally connected recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istvan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shyong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on Intelligent user interfaces</title>
		<meeting>the 8th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="263" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3700" to="3710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Collaborative filtering with graph information: Consistency and scalable methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2107" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Restricted boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Collaborative filtering recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Frankowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilad</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The adaptive web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="291" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Speedup matrix completion with side information: Application to multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2301" to="2309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman neural machine for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="575" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spectral collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems</title>
		<meeting>the 12th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09477</idno>
		<title level="m">A neural autoregressive approach to collaborative filtering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bipartite network projection and personal recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matúš</forename><surname>Medo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Cheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46115</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">However, after we introduced ARR and redid the hyperparameter tuning, the RMSE of ML-100K without content became 0.905, and adding content no longer helped. We hypothesize that the benefits of content reduce with the better modeling of graph structure features</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">) found, directly concatenating content with initial node features (the one-hot encoding vectors) as the input to GNN often led to worse performance due to information flow bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">On the other hand, as Berg et</title>
		<editor>Zhang &amp; Chen</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In our experiments we only concatenate the target user and item&apos;s content vectors with the final graph representation output by the GNN, similar to. We leave exploring better ways to combine content and graph structures for future work</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
