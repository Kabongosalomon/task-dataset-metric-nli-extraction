<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhou</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixian</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our pioneering effort for emotion recognition in conversation (ERC) with pre-trained language models. Unlike regular documents, conversational utterances appear alternately from different parties and are usually organized as hierarchical structures in previous work. Such structures are not conducive to the application of pre-trained language models such as XLNet. To address this issue, we propose an all-in-one XLNet model, namely DialogXL, with enhanced memory to store longer historical context and dialog-aware self-attention to deal with the multi-party structures. Specifically, we first modify the recurrence mechanism of XLNet from segment-level to utterance-level in order to better model the conversational data. Second, we introduce dialog-aware self-attention in replacement of the vanilla self-attention in XLNet to capture useful intra-and interspeaker dependencies. Extensive experiments are conducted on four ERC benchmarks with mainstream models presented for comparison. The experimental results show that the proposed model outperforms the baselines on all the datasets. Several other experiments such as ablation study and error analysis are also conducted and the results confirm the role of the critical modules of DialogXL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Emotion recognition in conversation (ERC) is an emerging task in natural language processing (NLP) that aims to identify the emotion of each utterance in a conversation. It can be regarded as an extension of traditional emotion detection from text, or an arising problem in dialogue systems that helps generate emotion-aware dialogues <ref type="bibr" target="#b27">(Zhou et al. 2017)</ref>. Empirical evidence shows that the conversational context of an utterance plays an indispensable role in this task . Moreover, the emotion also tends to stay unchanged within a short context of the conversation. It is thus very critical to effectively model the alternate utterances by different parties.</p><p>To solve this problem, many recent works focus on deep neural networks with hierarchical structures to model the conversational data <ref type="bibr" target="#b6">Ghosal et al. 2019;</ref><ref type="bibr" target="#b13">Jiao et al. 2019;</ref><ref type="bibr" target="#b26">Zhong, Wang, and Miao 2019)</ref>. In these works, each utterance is firstly encoded separately into an utterance representation, which is then modeled sequentially and hierarchically. Although the structures seem to comply with the organization of utterances, they ignore the direct dependencies between words in different utterances. In addition, they are not conducive to the application of pretrained language models such as BERT <ref type="bibr" target="#b5">(Devlin et al. 2018)</ref> and XLNet , which have achieved superior performance in many dialogue system tasks other than ERC <ref type="bibr" target="#b18">(Madotto, Wu, and Fung 2018;</ref><ref type="bibr" target="#b0">Bao et al. 2020;</ref><ref type="bibr" target="#b12">Henderson et al. 2019)</ref>.</p><p>There are two main challenges to directly apply these pre-trained language models to ERC. First, conversations in ERC are usually multi-party and there can be intra-and inter-speaker dependencies <ref type="bibr" target="#b6">(Ghosal et al. 2019)</ref>. Existing pre-trained language models are not readily feasible to encode these dependencies. Second, almost all language models are constrained by the input length. When the input sequence exceeds the limit, it has to be truncated, which may lead to loss of information in distant historical utterances <ref type="bibr" target="#b6">Ghosal et al. 2019)</ref>.</p><p>To cope with the above challenges, we introduce an allin-one XLNet model, namely DialogXL, for emotion recognition in multi-turn multi-party conversation. DialogXL intends to apply a strong pre-trained language model to ERC without constructing a complicated, hierarchical model in processing the conversational data. Specifically, it first replaces XLNet's segment recurrence by a more flexible and memory-saving utterance recurrence to utilize historical utterances. Utterance recurrence stores the hidden states of historical utterances in a memory bank and reuses them while identifying a query utterance. Next, the selfattention in XLNet's Transformer layers is substituted for dialog-aware self-attention, which consists of four different types of attention, namely local self-attention, global selfattention, speaker self-attention, and listener self-attention. Dialog-aware self-attention allows DialogXL to model the inter-and intra-speaker dependencies under different reception fields in the historical context. We conduct extensive experiments on four ERC benchmarks and the results show that the proposed model, DialogXL, outperforms all the baselines on the datasets. Furthermore, several studies are conducted to verify the modules of DialogXL, and an error analysis is used to delve into the reasons behind the errors.</p><p>To conclude, our contributions are as follows:</p><p>• DialogXL is the first effort of pre-trained language models designed for emotion recognition in conversation (ERC).</p><p>• We propose a memory-saving utterance recurrence to replace XLNet's segment recurrence. The new approach allows DialogXL to cache up to 1000 historical words of a conversation, which is more powerful than the vanilla XLNet model.</p><p>• Unlike the original self-attention that merely computes attention weights between words, our dialog-aware selfattention computes them by different reception fields and party roles, allowing us to capture useful intra-and interspeaker dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Emotion Recognition in Conversation</head><p>Emotion recognition in conversation (ERC) has emerged as an important problem in recent years and has attracted numerous interests from the NLP community. The availability of large conversational datasets <ref type="bibr" target="#b1">(Busso et al. 2008;</ref><ref type="bibr" target="#b22">Schuller et al. 2012;</ref><ref type="bibr" target="#b15">Li et al. 2017;</ref><ref type="bibr" target="#b2">Chen et al. 2018;</ref> account partly for this phenomenon, and the increasing interests in dialogue systems may also explain it. Recent works on ERC generally resort to deep learning models. For example, CMN ) and ICON ) both utilize gated recurrent unit (GRU) and memory networks. <ref type="bibr" target="#b19">Majumder et al. (2019)</ref> propose a recurrent-based model to model the party state, global state and emotional dynamics. <ref type="bibr" target="#b13">Jiao et al. (2019)</ref> propose a hierarchical GRU structure that trains utterance-level and conversation-level encoders jointly. <ref type="bibr" target="#b6">Ghosal et al. (2019)</ref> propose a graph neural network based model to encode speaker dependencies and temporal information. <ref type="bibr" target="#b26">Zhong, Wang, and Miao (2019)</ref> incorporate external knowledge bases to support the identification.  introduce transfer learning from utterance generation to ERC.</p><p>The modalities of data used in the above works are not the same. Specifically, <ref type="bibr" target="#b19">Majumder et al. 2019)</ref> utilize textual, audio and video modalities, while the latest research <ref type="bibr" target="#b13">(Jiao et al. 2019;</ref><ref type="bibr" target="#b6">Ghosal et al. 2019;</ref><ref type="bibr" target="#b26">Zhong, Wang, and Miao 2019;</ref> tends to use only the textual modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained Language Models</head><p>The effectiveness of large pre-trained language models <ref type="bibr" target="#b5">(Devlin et al. 2018;</ref><ref type="bibr" target="#b24">Yang et al. 2019;</ref><ref type="bibr" target="#b16">Liu et al. 2019;</ref><ref type="bibr" target="#b3">Conneau et al. 2020)</ref> has been well exhibited in many NLP tasks such as machine reading comprehension, text classification, machine translation. Among the language models, BERT <ref type="bibr" target="#b5">(Devlin et al. 2018</ref>) utilizes bi-directional Transformer encoders as well as pre-training schemes of masked language modeling and next sentence prediction. XLNet  is another powerful pre-trained language model, which excels at processing long documents with the segment recurrence mechanism. In addition, it combines the strengths of both auto-encoding and auto-regressive language modeling. There have been some recent works that apply pre-trained language models to dialog-related tasks <ref type="bibr" target="#b0">(Bao et al. 2020;</ref><ref type="bibr" target="#b7">Ham et al. 2020;</ref><ref type="bibr" target="#b12">Henderson et al. 2019)</ref>, but they have yet to be applied to emotion recognition in conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>There are two challenges to overcome in order to apply pretrained language models to emotion recognition in conversation (ERC). The first challenge is how to encode a long historical context with hundreds of words. The second is how to model the intra-and inter-speaker dependencies of different parties. Instead of building a hierarchical network as previous work, we propose DialogXL 1 to address these two challenges on the basis of XLNet with two improvements.</p><p>The overview architecture of DialogXL is shown in <ref type="figure">Figure</ref> 1. It consists of an embedding layer, 12 Transformer layers, and a feed-forward neural network. The model identifies the emotion for each utterance in turn when a conversation comes in. Compared with XLNet, DialogXL has a more effective memory bank equipped during the training and testing phases, storing hidden states of historical utterances for future reuses. The memory bank is updated by a new utterance recurrence mechanism. And the hidden states at each Transformer layer are derived by dialog-aware selfattention. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Definition</head><p>In ERC, a conversation is defined as a list of utterances {u 1 , u 2 , ..., u N }, where N is the number of utterances. Each utterance u i consists of n i tokens, namely u i = {w i1 , w i2 , ..., w ini }. A discrete value y i ∈ S is used to denote the emotion label of u i , where S is the set of emotion labels. The speaker is denoted by a function p(·). For example, p(u i ) ∈ P denotes the speaker of u i and P is the collection of all speaker roles in an ERC dataset. The objective of this task is to output the emotion label y t for a given query utterance u t based on its historical context {u 1 , u 2 , ..., u t−1 } and the corresponding speaker information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Input</head><p>At each time step t, the query utterance u t is prepended with the special token "[CLS]":</p><formula xml:id="formula_0">x t = {[CLS], w t1 , w t2 , ..., w tnt }.</formula><p>(1)</p><p>The utterance is then passed to the embedding layer. In Di-alogXL, this layer consists of only word embedding. The output of the embedding layer is treated as input hidden states to the first Transformer layer:</p><formula xml:id="formula_1">h 0 t = Embedding(x t )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utterance Recurrence</head><p>XLNet <ref type="formula">(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory</head><p>Hidden states  named segment recurrence, which caches previous hidden states in a memory bank and revisits them in future computations. However, this mechanism is ineffective when directly applied to conversational emotion recognition for two reasons. First, the "segment" in XLNet refers to a fixed-length sequence rather than a linguistic unit such as a sentence. The conversation in ERC is defined in terms of utterances, which are typically full sentences or paragraphs. Therefore, it is essential to keep the utterances complete rather than segmented into pieces. Second, segment recurrence constrains segments in the same training batch to have the same length, which results in too many paddings stored in memory. By contrast, the proposed utterance recurrence stores the historical context in memory without paddings, allowing the memory to store a longer historical context. The memory, denoted by m, works like a stack. Every time a new set of hidden states are generated for a query utterance, they are concatenated with the current memory. To prevent from introducing noises into the memory, only the hidden states of the utterance tokens are stored, with the hidden states of the "[CLS]" and padding positions ignored. Formally, for the t-th utterance, at each Transformer layer l the new memory m l is updated as:</p><formula xml:id="formula_2">m l = m l h l t,1:1+nt<label>(3)</label></formula><p>where denotes the concatenation operation. This update strategy is useful especially during the batching operation. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, updating memory with only the hidden states of utterance tokens makes the memory more compact, for the noises introduced by padding are mostly eliminated and more space is freed to cache a longer context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialog-Aware Self-Attention</head><p>Utterances occur alternately by different parties in a conversation, and the vanilla self-attention in XLNet cannot be directly applied to the multi-party setting. To this end, we replace the self-attention by dialog-aware self-attention, which enables our model to encode conversation contexts in a multi-turn multi-party setting. The new self-attention consists of four types of self-attention: global self-attention and local self-attention for different sizes of receptive fields, and speaker self-attention and listener self-attention for intra-and inter-speaker dependencies. We implement the dialog-aware self-attention by skillfully changing the masking strategies of self-attention, without the need to add any extra embeddings or parameters, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Dialog-aware self-attention is multi-headed. For each attention head of the l-th Transformer layer, the attention output o l t is computed as follows: Global Self-Attention Global self-attention takes all the historical context and the query utterance as the reception field. It is the same as the vanilla self-attention, in which the query utterance pays attention to the whole context. This setting allows our model to attend to previously distant utterances which may also be useful . Thus, no masking is made for global self-attention:</p><formula xml:id="formula_3">h l−1 Transformer Block −1 −2 Transformer Block − 1 −3 −1 −2 −3 Transformer Block −1 −2 Transformer Block − 1 −3 −1 −2 −3 Transformer Block −1 −2 Transformer Block − 1 −3 −1 −2 −3 Transformer Block −1 −2 Transformer Block − 1 −3 −1 −2 −3 (a) (b) (c) (d)</formula><formula xml:id="formula_4">s global ij = 0<label>(10)</label></formula><p>Local Self-Attention Local self-attention only has a reception field of ω latest historical utterances, where ω is a hyperparameter. The motivation for this attention is that intuitively speaker's emotion is mostly influenced by the recent utterances. In local self-attention, we mask the attentions between the query utterance and the historical utterances outside the reception field:</p><formula xml:id="formula_5">s local ij = +∞, j / ∈ Idx({u t−ω , u t−ω+1 , ..., u t−1 , u t }) 0, Otherwise<label>(11)</label></formula><p>where Idx(U) is a function that maps the utterance tokens in U to the corresponding positions in the key matrix k l t . Speaker Self-Attention Speaker self-attention considers only the historical context spoken by the present speaker. It intends to model the intra-speaker dependency <ref type="bibr" target="#b6">(Ghosal et al. 2019)</ref> by identifying emotional clues in the speaker's historical utterances. In speaker self-attention, we mask the attentions between the query utterance and the utterances spoken by other speakers:</p><formula xml:id="formula_6">s speaker ij = +∞, j ∈ Idx({u | p(u) = p(u t )}) 0, Otherwise<label>(12)</label></formula><p>Listener Self-Attention Listener self-attention considers only the historical utterances spoken by other speakers. It intends to model the inter-speaker dependency <ref type="bibr" target="#b6">(Ghosal et al. 2019)</ref>, meaning that the present speaker's emotion may be influence by other speakers' words. In listener self-attention, we mask the attentions between the query utterance and the utterances made by the present speaker:</p><formula xml:id="formula_7">s listener ij = +∞, j ∈ Idx({u | p(u) = p(u t )}) 0, Otherwise<label>(13)</label></formula><p>The outputs of the four types of self-attention are concatenated and passed through a normalization layer followed by a feed-forward network to generate the output for this Transformer layer:</p><formula xml:id="formula_8">o l t = K k=1 f k (m l−1 , h l−1 t , s c k )<label>(14)</label></formula><formula xml:id="formula_9">h l t = FeedForward(LayerNorm( o l t ))<label>(15)</label></formula><p>where K is the number of self-attention heads, and c k ∈ {global, local, speaker, listener} is the corresponding type of dialog-aware attention for the k-th attention head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Training</head><p>We take the hidden state of "[CLS]" at the last layer as the final encoding of the query utterance and the historical context, and pass it through a feed-forward neural network to get the predicted emotion:</p><formula xml:id="formula_10">h t = h L t,0<label>(16)</label></formula><formula xml:id="formula_11">z t = ReLU(W h h t + b h )<label>(17)</label></formula><formula xml:id="formula_12">P t = softmax(W z z t + b z )<label>(18)</label></formula><formula xml:id="formula_13">y t = argmax k∈S (P t [k])<label>(19)</label></formula><p>For the training of our model, we use the standard crossentropy loss as the loss function:</p><formula xml:id="formula_14">L(θ) = − M i=1 N t=1 P t [y i,t ]<label>(20)</label></formula><p>where M is the number of conversations in the training set, and θ is the collection of trainable parameters in DialogXL. <ref type="table">Train  Val  Test  Train  Val  Test  IEMOCAP  120  31  5810  1623  MELD  1038  114  280  9989 1109 2610  DailyDialog 11118 1000 1000 87170 8069 7740  EmoryNLP  713  99  85  9934 1344 1328   Table 1</ref>: The statistics of four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Conversations # Uterrances</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>In this section, we present the experimental settings such as implementation details, datasets, metrics, and baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We initialize the proposed DialogXL by pre-trained XLNet-Base  and employ AdamW optimizer <ref type="bibr" target="#b17">(Loshchilov and Hutter 2018)</ref> during training. Hyperparameter tuning for each dataset is conducted with hold-out validation on the validation set. The tunable hyperparameters include learning rate, number of heads for the four types of attentions in dialog-aware self-attention 3 , the max length of memory 4 , and the dropout rate. The results of BERT, XLNet, and DialogXL reported in our experiments are all based on the average score of 5 random runs on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We evaluate DialogXL on four multi-turn multi-party ERC datasets. The statistics of them are shown in <ref type="table">Table 1</ref>. IEMOCAP <ref type="bibr" target="#b1">(Busso et al. 2008</ref>): A multimodal conversational dataset for emotion recognition, with two parties included for each conversation. The emotion labels include neutral, happiness, sadness, anger, frustrated, and excited. Since this dataset has no validation set, we follow <ref type="bibr" target="#b26">(Zhong, Wang, and Miao 2019)</ref> to use the last 20 dialogues in the training set for validation.</p><p>MELD : A multimodal dataset for emotion recognition collected from the TV show Friends.</p><p>The emotion labels include neutral, happiness, surprise, sadness, anger, disgust, and fear.</p><p>DailyDialog <ref type="bibr" target="#b15">(Li et al. 2017</ref>): Human-written daily communications, with emotion labels including neutral, happiness, surprise, sadness, anger, disgust, and fear. Since it has no speaker information, we consider the utterance turns as the speaker turns by default.</p><p>EmoryNLP <ref type="bibr" target="#b25">(Zahiri and Choi 2017)</ref>: TV show scripts collected from Friends, but varies from MELD in the choice of scenes and emotion labels. The emotion labels of this dataset include neutral, sad, mad, scared, powerful, peaceful, and joyful. Following recent works <ref type="bibr" target="#b6">(Ghosal et al. 2019;</ref><ref type="bibr" target="#b26">Zhong, Wang, and Miao 2019;</ref>, we utilize only the textual data of the above datasets for our experiments.</p><p>The evaluation metrics are chosen as micro-F1 for DailyDialog 5 and weighted-F1 for the other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head><p>We compare DialogXL with the following baselines: Previous methods: CMN , Dia-logueRNN , HiGRU <ref type="bibr" target="#b13">(Jiao et al. 2019)</ref>, DialogueGCN <ref type="bibr" target="#b6">(Ghosal et al. 2019)</ref>, TL-ERC , and KET <ref type="bibr" target="#b26">(Zhong, Wang, and Miao 2019)</ref>. BERT <ref type="bibr" target="#b5">(Devlin et al. 2018)</ref>: The BERT baseline for ERC, initialized with the pre-trained parameters of BERT-base. We concatenate historical utterances and the query utterance in order and then feed them into BERT for classification. The hyperparameters are tuned the same as DialogXL.</p><p>XLNet : The XLNet baseline with the original segment recurrence and vanilla self-attention, initialized with the pre-trained parameters of XLNet-base. The hyperparameters are tuned the same as DialogXL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Results</head><p>The overall results of our DialogXL and the baselines are reported in <ref type="table">Table 2</ref>. We can clearly note that DialogXL reaches a new state of the art on all of the four datasets. Besides, we can make another two observations as follows, which help to understand the ERC task and the pros and cons of DialogXL.</p><p>First, in general, there are considerable improvements for the pre-trained language models over the others on MELD, DailyDialog, and EmoryNLP. However, the improvements of DialogXL over BERT and XLNet are not significant on these datasets. After delving into the datasets, we found that the dialogues in these datasets are relatively short (mostly 5 to 9 utterances). So the current language models, BERT and XLNet, can already encode the entire historical context and the query utterance in most cases. On these short dialogues, however, the advantages of DialogXL are not shown up completely.</p><p>Second, while inferior performance of BERT and XLNet is observed to the other baselines on IEMOCAP, the improvements of DialogXL over BERT and XLNet are significant. After examining the dataset, we realized the dialogues in IEMOCAP are much longer (around 70 utterances per dialog) than the other datasets. In this case, BERT and XL-Net cannot encode too much historical context effectively, while such baselines as DialogueRNN and DialogueGCN can reach distant utterances and also encode other key features such as speaker information. Moreover, our DialogXL can both encode the historical context effectively by the utterance recurrence and capture the speaker information by the dialog-aware self-attention, allowing it to achieve superior performance to all the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of the Enhanced Memory</head><p>One of the contributions of DialogXL lies in the enhanced memory with utterance recurrence. Here, we study how the  <ref type="table">Table 2</ref>: Overall performance on the four datasets. The scores marked by "*" is based on our re-implementation, because of the differences in evaluation metrics and data statistics between the corresponding work and ours. utterance recurrence and the maximum memory length contribute to the final results. We change the maximum memory length from 100 to 1000 with an interval of 100 and plot the test scores on IEMOCAP, which has sufficient utterances in each conversation. The memory waste rate of segment recurrence in XLNet is also plotted in terms of the percentage of paddings in memory. Since the proposed utterance recurrence has 0 memory waste in theory, its memory waste rate is not plotted. Three models are studied for this experiment: XLNet with the original segment recurrence, XLNet with utterance recurrence, and DialogXL. The results are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. We can note that segment recurrence always leads to a memory waste rate of over 60% for each different memory length. The rate drops with the memory length increases, along with the growth of the three models. When the memory length exceeds 700, their performance generally stops improving any more, which indicates that increasing the maximum memory length only contributes to the test results within a certain range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>In this ablation study, we analyze the impact of dialog-aware self-attention by removing each type of dialog-aware selfattention from DialogXL. The results on two representative datasets, IEMOCAP and MELD, are presented in <ref type="table">Table 3</ref>.  <ref type="table">Table 3</ref>: Results of ablation study on IEMOCAP and MELD.</p><p>We can observe that the performance of DialogXL drops on both IEMOCAP and MELD when any type of the selfattention is removed, suggesting that all these self-attentions contribute to the improvement of DialogXL. Nevertheless, their contributions can be distinguished. When speaker selfattention or listener self-attention is removed, considerable drops are observed. But when they are both removed, the drops are more obvious. This implies the importance of the inter-/intra-speaker dependency <ref type="bibr" target="#b6">(Ghosal et al. 2019)</ref>.</p><p>Moreover, when local self-attention is removed, the F1 score drops the most on IEMOCAP, which contains long utterances (around 70) for each conversation. This indicates that the historical context near a query utterance is more important for this dataset. The drop on MELD is not as obvious as on IEMOCAP, because MELD has much shorter conversations (5 to 9 utterances per conversation). Finally, the removal of global self-attention leads to the least performance degradation. The reason could be twofold. First, global utterances are not as important as local utterances. Second, the speaker self-attention and listener self-attention already capture some useful information from distant utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speaker Role Embedding</head><p>Our speaker self-attention and listener self-attention model the speaker dependencies <ref type="bibr" target="#b6">(Ghosal et al. 2019)</ref> by directly letting the model know which part of the utterances should be attended to. Another way to let a pre-trained language model understand the speaker dependencies in dialog is speaker role embedding <ref type="bibr" target="#b0">(Bao et al. 2020;</ref><ref type="bibr" target="#b7">Ham et al. 2020)</ref>, which maps each participant to a trainable embedding vector. Here, we make a simple comparison between the two approaches of embedding different parties on IEMOCAP and DailyDialog. To this end, we replace the speaker selfattention and listener self-attention of DialogXL with the speaker role embeddings, and refer to the resulting model as DialogXL-emb. The results of comparison are shown in <ref type="table">Table 4</ref>. We can observe that our explicit speaker&amp;listencer self-attention is more effective than the speaker role embedding approach. As a result, the proposed attention mechanism can be potentially applied to other dialog tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Study</head><p>Although our DialogXL has a novel framework and achieves a new state of the art, we still want to figure out its possible shortcomings to motivate the future research. Therefore, we carry out an error study on IEMOCAP. In short, we found that DialogXL's powerful capability of directly capturing word-level features in the historical context can be a    <ref type="table">Table 4</ref>: Results of comparison between direct speaker role embedding and our speaker&amp;listener self-attention approach on the IEMOCAP and DailyDialog datasets.</p><p>double-edge sword. As illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>, the word-level attention mechanism based on semantic relevance can help make a good prediction (Case #1), but it may also lead to a mistake by focusing too much on the semantic relevance between the query utterance and historical utterances (Case #2). As a result, it seems to be necessary to combine with other mechanisms rather than merely relying on the popular attention to carry out the emotion recognition in dialogues. Besides, we also observe from our bad cases that some of them are mentioned in previous works, such as emotional shifts (i.e., the emotion labels of two consecutive utterances from a same speaker are different) <ref type="bibr" target="#b19">Majumder et al. 2019)</ref>. Roughly, our model commits mistakes for 45% of these cases, which calls for further investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed an all-in-one XLNet model, namely DialogXL, for emotion recognition in conversation (ERC). To model the multi-turn multi-party conversational data, DialogXL contributes two improvements on the basis of XLNet and Transformer-XL. First, an enhanced memory was introduced to replace XLNet's vanilla memory to store historical contexts more effectively. Second, a dialogaware self-attention mechanism was proposed to deal with the multi-turn multi-party data structures. Extensive experiments were conducted on four ERC benchmarks and the results show that the proposed model outperforms all the baselines on the datasets. The effectiveness of the two improvements is also confirmed by extensive analyses. Furthermore, we have the following three findings. First, the original segment recurrence mechanism stores more than 60% paddings in memory, making it ineffective to encode the historical contexts for ERC. Second, the traditional speaker role embedding strategy is not as effective as our speaker&amp;listener self-attention, which could also be applied to other dialog tasks. Finally, an error analysis reveals that merely relying on the attention mechanism may mislead the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of our DialogXL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the memory update strategies by utterance recurrence and segment recurrence. The batch size is 4, with each row corresponding to a conversation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Demonstration of dialog-aware self-attention: (a) global self-attention, (b) local self-attention, (c) speaker selfattention, and (d) listener self-attention. The utterance to be identified is u t , while the hidden states of u t−3 , u t−2 , and u t−1 are cached in memory. The speaker identities are: p(u t ) = p(u t−2 ) and p(u t−1 ) = p(u t−3 ). The window size of local self-attention is 2. The upper part of each subgraph is the attention mask for u t and other utterances, with masked attention weights colored grey. The lower part of each subgraph is the attention flows between two consecutive Transformer layers, where solid blue lines represent self-attention within u t and dashed orange lines represent attention between u t and the memory m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The results of vanilla XLNet, XLNet with utterance recurrence, and our DialogXL by different maximum memory lengths on IEMOCAP. The memory waste rate of segment recurrence in XLNet is provided for reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Results of error analysis, where two query utterances are provided, along with the visualization of attention weights between the current utterance at the [CLS] token and the most attended historical utterance (selected according to the highest average attention weight among all the attention heads of the last layer). The darker colors mean larger attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b24">Yang et al. 2019)</ref> and Transformer-XL address the limitation of input size by a mechanism</figDesc><table><row><cell>h 2</cell><cell>m 1 ′</cell><cell>Layer</cell><cell></cell><cell>Layer</cell><cell></cell></row><row><cell>Dialog-aware Self-attention</cell><cell>Utterance Recurrence</cell><cell>Layer 2 …</cell><cell>…</cell><cell>Layer 2 …</cell><cell>…</cell></row><row><cell></cell><cell></cell><cell>Layer 1</cell><cell></cell><cell>Layer 1</cell><cell></cell></row><row><cell>h 1</cell><cell>m 1</cell><cell cols="2">Embedding Layer</cell><cell>Embedding Layer</cell><cell></cell></row><row><cell cols="2">A DialogXL Transformer layer</cell><cell>[CLS]</cell><cell>… …</cell><cell>…</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Utterance #1</cell><cell>Utterance #2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Yeah. It's just, I never thought it would actually happen, you know? I mean-He's such a fighter and he's always so positive about everything.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Case #1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Utterance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Text</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="11">Emotion Prediction Most attend</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">sad</cell><cell></cell><cell></cell><cell cols="2">sad</cell><cell></cell><cell>5</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>The</cell><cell cols="2">end ,</cell><cell>he</cell><cell cols="2">' s</cell><cell>been</cell><cell>sick</cell><cell>for</cell><cell>awhile</cell><cell>.</cell><cell>He</cell><cell cols="2">had cancer</cell><cell cols="2">.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Case #2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Utterance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Text</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="11">Emotion Prediction Most attend</cell></row><row><cell>14</cell><cell cols="17">He just did everything. He was literally, literally the most talented guy I have ever met in my entire life.</cell><cell></cell><cell cols="2">sad</cell><cell></cell><cell cols="3">happy</cell><cell></cell><cell cols="2">10</cell><cell></cell></row><row><cell>He</cell><cell>wanted</cell><cell>to</cell><cell>travel</cell><cell>,</cell><cell>you</cell><cell>know</cell><cell>.</cell><cell>And</cell><cell>he</cell><cell>'</cell><cell>s</cell><cell>so</cell><cell>talented</cell><cell>.</cell><cell>Probably</cell><cell>the</cell><cell>most</cell><cell>talented</cell><cell>person</cell><cell>I</cell><cell>have</cell><cell>ever</cell><cell>met</cell><cell>in</cell><cell>my</cell><cell>entire</cell><cell cols="2">life</cell><cell>.</cell></row></table><note>7</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The implementation is available at https://github.com/shen-wzh3/DialogXL. 2 XLNet's permutation language modeling and two-stream selfattention, which are designed for language modeling tasks, are not included in DialogXL.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = m l−1 ||h l−1 t (4) q l t , k l t , v l t = h l−1 t W l q , h l−1 t W l k , h l−1 t W l v (5) a l t = RelPosAttn(q l t , k l t ) (6) a l t = a l t − s (7) o l t = softmax( a l t )v l t (8) where W l q , W l k ,and W l v are trainable parameters for each attention head, and RelPosAttn(·) are the relative position attention adopted from Transformer-XL and XLNet.The attention mask s in Equation(7)is a matrix with the same shape as the attention weights a l t . The value of s ij is set to +∞ only when the attention between the i-th vector in q l t and j-th vector in k l t is masked, and set to 0 otherwise. For the sake of convenience, we denote Equation(4)to Equation (8) by a function f (·):o l t = f (m l−1 , h l−1 t , s)(9)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The sum of the four types of attention heads is always 12. 4 When implementing utterance recurrence in practice, the length of memory is set not to exceed a threshold due to the limit of computational resource. Once the size of memory exceeds the threshold, the earliest hidden states will be dropped.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The category of neutral with significantly more samples than others are not taken into account as before.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The paper was supported by the Fundamental Research Funds for the Central Universities (No.19lgpy220) and the Program for Guangdong Introducing Innovative and Entrepreneurial Teams (No.2017ZT07X355). This work was also supported by MindSpore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020: 58th annual meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">IEMOCAP: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">335</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">EmotionLines: An Emotion Corpus of Multi-Party Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Ting-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th International Conference on Language Resources and Evaluation, LREC 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1597" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Cross-lingual Representation Learning at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020: 58th annual meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019 : The 57th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="154" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Endto-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2020: 58th annual meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Icon: Interactive conversational memory network for multimodal emotion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Emotion Recognition in Conversations with Transfer Learning from Generative Conversation Modeling. arXiv: Computation and Language</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vulić</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03688</idno>
		<title level="m">ConveRT: Efficient and accurate conversational representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hi-GRU: Hierarchical Gated Recurrent Units for Utterance-Level Emotion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019</title>
		<meeting>the 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="986" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fixing Weight Decay Regularization in Adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08217</idno>
		<title level="m">Mem2seq: Effectively incorporating knowledge bases into endto-end task-oriented dialog systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dialoguernn: An attentive rnn for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6818" to="6825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019 : The 57th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emotion recognition in conversation: Research challenges, datasets, and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="100943" to="100953" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">AVEC 2012: the continuous audio/visual emotion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Multimodal interaction</title>
		<meeting>the 14th ACM international conference on Multimodal interaction</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Zahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="44" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Emotional chatting machine: Emotional conversation generation with internal and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01074</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
