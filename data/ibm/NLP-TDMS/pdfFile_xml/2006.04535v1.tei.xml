<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving k-Means Clustering Performance with Disentangled Internal Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abien</forename><forename type="middle">Fred</forename><surname>Agarap</surname></persName>
							<email>abienagarap@dlsu.edu.ph</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Studies De</orgName>
								<orgName type="institution">La Salle University Manila</orgName>
								<address>
									<country key="PH">Philippines</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnulfo</forename><forename type="middle">P</forename><surname>Azcarraga</surname></persName>
							<email>arnulfo.azcarraga@dlsu.edu.ph</email>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Studies De</orgName>
								<orgName type="institution">La Salle University</orgName>
								<address>
									<settlement>Manila</settlement>
									<country key="PH">Philippines</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving k-Means Clustering Performance with Disentangled Internal Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-clustering</term>
					<term>disentanglement</term>
					<term>encoding</term>
					<term>internal representations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep clustering algorithms combine representation learning and clustering by jointly optimizing a clustering loss and a non-clustering loss. In such methods, a deep neural network is used for representation learning together with a clustering network. Instead of following this framework to improve clustering performance, we propose a simpler approach of optimizing the entanglement of the learned latent code representation of an autoencoder. We define entanglement as how close pairs of points from the same class or structure are, relative to pairs of points from different classes or structures. To measure the entanglement of data points, we use the soft nearest neighbor loss, and expand it by introducing an annealing temperature factor. Using our proposed approach, the test clustering accuracy was 96.2% on the MNIST dataset, 85.6% on the Fashion-MNIST dataset, and 79.2% on the EMNIST Balanced dataset, outperforming our baseline models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION AND RELATED WORKS</head><p>Clustering is an unsupervised learning task that groups a set of objects in a way that the objects in a group share more similarities among them than those from other groups. It is a widely-studied task as its applications include but are not limited to its use in data analysis and visualization, anomaly detection, sequence analysis, and natural language processing. Like other machine learning methods, clustering algorithms heavily rely on the choice of feature representation. For this reason, the design of preprocessing pipelines and feature transformations (a.k.a. feature engineering) consumes a considerable amount of time. In turn, the task of feature engineering plays a crucial role in the success of machine learning methods. However, due to its labor-intensive nature, it hinders faster development, autonomous learning, and reusability across tasks.</p><p>We take k-means clustering as an example, which typically uses the Euclidean distance among points in a given feature space (e.g. for images, it could be the raw pixels or gradientorientation histograms); for difficult image datasets like CI-FAR10 <ref type="bibr" target="#b0">[1]</ref>, clustering with Euclidean distance on raw pixels may be ineffective.</p><p>Hence, the move towards automatically learning the best representation for a given data has gained mainstream attention since the success of deep learning for computer vision <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, where the exceptional gains on benchmark tasks have resulted from automatically learning better feature representation. The task of automatically learning feature representation is known as representation learning.</p><p>More explicitly, representation learning is the task of learning the most salient features of a given data, i.e. features that imply the underlying structure of the data. It is implicitly done in a supervised deep neural network by using its hidden layers to learn and to provide representations for its last layer, thereby rendering a task such as classification or regression easier. For instance, data points that are not linearly separable in the raw feature space may become linearly separable at the last hidden layer through the composition of feature representations in the hidden layers. So, by automatically learning the representations instead of feature engineering, we are unencumbered of the task to learn the best possible feature representation for better performance in downstream tasks such as classification, clustering, and regression. To further take advantage of representation learning, it may be explicitly designed to forge representations in favor of a downstream task such as clustering. In the following subsections, we briefly discuss related works that use the aforementioned strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Embedded Clustering (DEC)</head><p>Xie et al. <ref type="bibr">(2016)</ref>  <ref type="bibr" target="#b4">[5]</ref> introduced the Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using a deep neural network. DEC learns a mapping f θ : X → Z where X is the original feature space, while Z is the lower-dimensional feature space. Then, it iteratively optimizes the Kullback-Leibler divergence to minimize the within-cluster distance of each cluster in Z. They had a clustering accuracy of 84.30% on the MNIST dataset <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Variational Deep Embedding (VaDE)</head><p>Another approach to strongly influence learned representations to favor clustering is the Variational Deep Embedding (VaDE) by <ref type="bibr" target="#b6">Jiang et al. (2016)</ref>  <ref type="bibr" target="#b6">[7]</ref>. It is an unsupervised generative clustering approach that employs the framework of Variational Autoencoder <ref type="bibr" target="#b7">[8]</ref>, by combining a Gaussian Mixture Model (GMM) and a deep neural network (DNN). Specifically, a cluster is chosen by the GMM from which the latent representation z is sampled, and then the DNN decodes z to an observable data x. Their approach had a clustering accuracy of 94.46% on the MNIST dataset <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ClusterGAN</head><p>Similar to DEC <ref type="bibr" target="#b4">[5]</ref> and VaDE <ref type="bibr" target="#b6">[7]</ref>, ClusterGAN <ref type="bibr" target="#b8">[9]</ref> also uses learned representation for clustering, but their approach built on the Generative Adversarial Network (GAN) <ref type="bibr" target="#b9">[10]</ref> framework. That is, they incorporated a clustering-specific loss term to the minimax objective. They had a clustering accuracy of 95% on the MNIST dataset <ref type="bibr" target="#b5">[6]</ref> and 63% on the Fashion-MNIST dataset <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. N2D: (not too) deep clustering</head><p>DEC <ref type="bibr" target="#b4">[5]</ref>, VaDE <ref type="bibr" target="#b6">[7]</ref>, and ClusterGAN <ref type="bibr" target="#b8">[9]</ref> are all deep clustering algorithms, i.e. methods that combine a deep neural network as a data encoder f θ : X → Z, and a clustering network, that jointly learns a clustering loss and a nonclustering loss (e.g. minimax objective for GAN). In contrast, <ref type="bibr" target="#b11">McConville et al. (2019)</ref>  <ref type="bibr" target="#b11">[12]</ref> proposed N2D (Not too Deep) clustering, wherein they performed manifold learning on the latent code representation from an autoencoder, and used the learned manifold for clustering. They found that using UMAP in their framework achieves the best clustering-friendly manifold of the latent code representation. Using this approach, they had a clustering accuracy of 94.8% on the MNIST dataset <ref type="bibr" target="#b5">[6]</ref> and 67.2% on the Fashion-MNIST dataset <ref type="bibr" target="#b10">[11]</ref>.</p><p>Similar to N2D <ref type="bibr" target="#b11">[12]</ref>, we propose a relatively simpler approach compared to deep clustering algorithms. But we also use an autoencoder to learn the latent code representation of a data, then use the said representation for clustering. We draw our difference on how to learn a more clustering-friendly latent code representation.</p><p>Instead of learning such a representation by using a manifold learning technique such as Isomap <ref type="bibr" target="#b12">[13]</ref>, t-SNE <ref type="bibr" target="#b13">[14]</ref>, or UMAP <ref type="bibr" target="#b14">[15]</ref>, we regularize the autoencoder reconstruction loss with the soft nearest neighbor loss <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. The said loss function measures the lack of separation of class (or structural, for unsupervised tasks) manifolds in representation space-in other words, the entanglement of data points from different classes or structures.</p><p>We focus on minimizing the entanglement of class manifolds in a latent code representation to derive a more clustering-friendly representation.</p><p>Our contributions are as follows:</p><p>1) We expand the soft nearest neighbor loss by introducing an annealing temperature factor, and we use it to learn a latent code representation that is primed for clustering (Section II). 2) We present comparatively strong clustering results in terms of clustering accuracy, normalized mutual information, and adjusted Rand index (Section III).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3)</head><p>A simple yet effective way of clustering on (disentangled) latent code representation (Section III).</p><p>II. LEARNING DISENTANGLED REPRESENTATIONS We consider the problem of clustering a set of N points {x i ∈ X} N i=1 into k clusters, each represented by a centroid µ j∈1,...,k . Instead of directly clustering the original features X, we transform the data with a non-linear mapping Z = enc(X), where Z is the latent code representation. But to learn a more clustering-friendly representation, we propose to learn to disentangle them, i.e. isolate class-or structure-similar data points, which implicitly maximizes the inter-cluster variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Autoencoder</head><p>An autoencoder is a neural network that aims to find the function mapping the features x to itself through the use of an encoder function h = enc(x) that learns the latent code representation of the features, and a decoder function that reconstructs the features from the latent code representation r = dec(h). To learn the reconstruction task, it minimizes a loss function L(x, dec(enc(x))), where L is a function penalizing the decoder output dec(enc(x)) for being dissimilar from x. Typically, this reconstruction loss is the Mean Squared</p><formula xml:id="formula_0">Error (MSE) 1 n n i=1 dec(enc(x i )) − x i 2 2 .</formula><p>Then, similar to other neural networks, it is usually trained with a gradientbased method aided with backpropagation of errors.</p><p>The reconstruction task of an autoencoder has a by-product of learning good internal representations, and so, we take advantage of this for clustering, particularly, the latent code representation z = enc(x) -thus drawing our similarity with N2D <ref type="bibr" target="#b11">[12]</ref>. For our experiments, we used the binary cross entropy (Eq. 1) as the reconstruction loss.</p><formula xml:id="formula_1">rec (x, r) = 1 n n i=1 −x i log(r i ) + (1 − x i ) log(1 − r i ) (1)</formula><p>We used this in lieu of MSE since our features X were normalized to values [0, 1] ∈ R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Soft Nearest Neighbors Loss</head><p>In our context, we define entanglement as how close pairs of representations from the same class or structure are, relative to pairs of representations from different classes or structures. <ref type="bibr" target="#b16">Frosst et al. (2019)</ref>  <ref type="bibr" target="#b16">[17]</ref> used the same term in the same context. A low entanglement implies that representations from the same class or structure are closer than they are to representations from different classes or structures. To measure the entanglement of representations, <ref type="bibr" target="#b16">Frosst et al. (2019)</ref>  <ref type="bibr" target="#b16">[17]</ref> expanded the non-linear neighborhood component analysis (NCA) <ref type="bibr" target="#b15">[16]</ref> objective by introducing the temperature factor T , and called this modified objective the soft nearest neighbor loss.</p><p>They defined the soft nearest neighbor loss as the non-linear NCA at temperature T , for a batch of b samples (x, y),</p><formula xml:id="formula_2">sn (x, y, T ) = − 1 b i∈1...b log          j∈1...b j =i yi=yj e − x i −x j 2 T k∈1...b k =i e − x i −x k 2 T         <label>(2)</label></formula><p>where x may be the raw input features or the learned representations in the hidden layers of a neural network. We may describe soft nearest neighbor loss as the negative log probability of sampling a neighboring point j from the same class as i in a batch b, similar to the probabilistic sampling by <ref type="bibr" target="#b17">Goldberger et al. (2005)</ref>  <ref type="bibr" target="#b17">[18]</ref>. A low value of soft nearest neighbor loss is tantamount to a low entanglement (high disentanglement). In our experiments, we used cosine similarity instead of Euclidean distance. 1) Temperature: Frosst et al. (2019) <ref type="bibr" target="#b16">[17]</ref> described the temperature factor T as a way to control the relative importance given to the distances between pairs of points, i.e. at low temperatures, the loss is dominated by small distances while the actual distances between widely separated representations become less relevant. Conversely, at high temperatures, the distances between widely separated points dominate the loss.</p><p>2) Annealing Temperature: We build on this idea of the temperature influencing the importance of distances between pairs of points, and extend the soft nearest neighbor loss by introducing an annealing temperature (Eq. 3) instead of a fixed one,</p><formula xml:id="formula_3">T = 1 (η + i) γ<label>(3)</label></formula><p>where i is the current training epoch, and we set η = 1 and γ = 0.55 for our experiments, similar to Neelakantan et al.</p><p>(2015) <ref type="bibr" target="#b18">[19]</ref>. We show a disentangled latent code representation learned with the soft nearest neighbor loss, both with fixed temperature and with annealing temperature, in <ref type="figure">Figure 1</ref>. In both cases, as we minimize the soft nearest neighbor loss, the latent code representation becomes more clustering-friendly. However, with annealing temperature, we gain a lower entanglement at an earlier training epoch, thus the disentanglement of representations starts earlier than with a fixed temperature. <ref type="figure">Fig. 1</ref>. Comparing the soft nearest neighbor loss with annealing temperature and with fixed temperature. We sampled and randomly labelled 300 data points from a Gaussian distribution, and ran gradient descent on them with soft nearest neighbor loss. The figure at the left shows the initial condition of the labelled points. We can see the separation of clusters in the latent code from epoch 20 to epoch 50, rendering the classes more isolated. We present disentangled representations on benchmark datasets later in the paper. This figure is best viewed in color.</p><p>We also used the lowest soft nearest neighbor loss among the hidden layers of our autoencoder:</p><formula xml:id="formula_4">sn = arg min sn (x, y, T )<label>(4)</label></formula><p>Using the arg min configuration, we achieved a more stable computation of the soft nearest neighbor loss during training. Now that we have defined our contribution, we lay down the objective function used by our autoencoder to learn the disentangled representations for clustering. We define a composite loss (Eq. 5) that consists of the autoencoder reconstruction loss rec , and the soft nearest neighbor loss sn , with an α parameter which directly influences the disentanglement computation -we set α = 100. Note that our goal is not necessarily to learn a good reconstruction of the input data, but to learn a good disentangled representation that we can use to improve clustering performance.</p><formula xml:id="formula_5">L(f, x, y) = rec (x, r) + α · i∈k sn f i (x), y<label>(5)</label></formula><p>Another contribution of this work is the simplicity of our proposed method. So, unlike DEC <ref type="bibr" target="#b4">[5]</ref>, VaDE <ref type="bibr" target="#b6">[7]</ref>, and ClusterGAN <ref type="bibr" target="#b8">[9]</ref> which uses an auxiliary clustering network, we use a simple k-Means clustering <ref type="bibr" target="#b19">[20]</ref> on the disentangled latent code representations.</p><p>Thus far, we have discussed our proposed method of disentangling learned representations to improve clustering performance. However, autoencoding and clustering are both unsupervised learning tasks, while we are proposing to use the soft nearest neighbor loss, a loss function that uses labels to illuminate the class similarity structure of internal representations learned by a neural network. With this in mind, we formulated two different soft nearest neighbor loss functions: (1) supervised, and <ref type="formula" target="#formula_2">(2)</ref> unsupervised. In the unsupervised setting, we simply perform the same probabilistic sampling of a neighboring point j to i, but we do not constrain them to come from the same class.</p><p>To simulate the lack of labelled data, we used a small labelled subset of the benchmark datasets for the supervised configuration of the soft nearest neighbor loss. The different soft nearest neighbor loss configurations we used in our experiments are listed in <ref type="table" target="#tab_0">Table I</ref>. We use <ref type="table" target="#tab_0">Table I as a lookup table of shorthand to save  space on Tables III, IV</ref>, and V. Model configurations labelled as SNNL 1-4 use a fixed temperature, while those that are labelled as SNNL 5-8 use our annealing temperature (Eq. 3). Moreover, model configurations with even numbers use the unsupervised soft nearest neighbor loss, while the ones with odd numbers use the supervised version.</p><p>Finally, we summarize the details of our proposed method as follows, 1) Train an autoencoder with the composite loss (Eq. 5) using a gradient-based method with backpropagation. 2) Use the disentangled latent code representation z = enc(x) of the autoencoder for k-Means clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CLUSTERING ON DISENTANGLED REPRESENTATIONS</head><p>To demonstrate the effectiveness of our approach, we conducted experiments on benchmark datasets, and lay down the clustering performance of the related models on the same benchmark datasets we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Autoencoder Model</head><p>We used a fully connected network with d − 500 − 500 −  <ref type="bibr" target="#b15">[16]</ref>. The c-latent code layer andd-reconstruction layer used logistic function while the remaining hidden layers used ReLU function <ref type="bibr" target="#b21">[22]</ref>. All hidden layers were initialized with He initialization <ref type="bibr" target="#b20">[21]</ref>. We set c = 70 across all datasets and models for a more fair comparison. Finally, we trained our model using Adam <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 1 × 10 −3 for 50 epochs on all our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. k-Means Clustering</head><p>We ran k-Means clustering, with centroids initialized using k-means++ <ref type="bibr" target="#b23">[24]</ref>, on the disentangled latent code representation from our autoencoder for nine times. The first run started with 10 iterations, with each run incremented by 10, i.e. the first clustering ran for 10 iterations, while the ninth ran for 90 iterations. We recorded the clustering performance on the ninth run for each model on each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>For all the models, we used the number of ground-truth categories in each dataset as the number of clusters. We used six different metrics to evaluate the clustering performance of our baseline and experimental models. For the first three metrics, the values lie in the interval [0, 1], where values closer to 1 correspond to a better clustering performance. The values for the fourth metric lie in the interval [−1, 1], where values near 1 are better while values near -1 are worse. The last two metrics are unbounded, but only the first of which implies a better clustering performance when its values are higher, while the last metric requires a lower value for better performance.</p><p>1) Clustering Accuracy: In clustering, accuracy (ACC) is defined as the best match between the ground-truth labels and the predicted clusters <ref type="bibr" target="#b24">[25]</ref>. Using the ground-truth labels as pseudo-cluster labels is known as cluster assumption in the semi-supervised learning literature <ref type="bibr" target="#b25">[26]</ref>.</p><formula xml:id="formula_6">ACC = max m n i=1 1{l i = m (c i )} n ,<label>(6)</label></formula><p>where l i is the ground-truth label, c i is the cluster prediction, and m ranges over all possible one-to-one mappings between clusters and labels.</p><p>2) Normalized Mutual Information: The Normalized Mutual Information (NMI) is the normalization of mutual information (MI) score to have its value within [0, 1] ∈ R, where 0 denotes no mutual information while 1 denotes perfect correlation. It is formally defined as follows,</p><formula xml:id="formula_7">N M I = 2I(y, c) [H(y) + H(c)]<label>(7)</label></formula><p>where y is the ground-truth label, c is the cluster prediction, H is the entropy, and I is the mutual information between the ground-truth labels and the cluster predictions.</p><p>3) Adjusted Rand Index: The Adjusted Rand Index (ARI) <ref type="bibr" target="#b26">[27]</ref> is the Rand Index (RI) adjusted for chance. RI is the similarity between two clusterings by considering all pairs of points and counting pairs assigned to the same or different clusters in the predicted and true clusterings (see Eq. 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RI = T P + T N T P + F P</head><formula xml:id="formula_8">+ F N + T N<label>(8)</label></formula><p>where T P is the true positive, T N is the true negative, F P is the false positive, and F N is the false negative. ARI is then computed from RI by using Eq. 9,</p><formula xml:id="formula_9">ARI = RI − E[RI] max(RI) − E[RI]<label>(9)</label></formula><p>ARI values lie within [0, 1] ∈ R, where 0 denotes random labelling independently of the number of clusters, while 1 denotes the clusterings are identical up to a permutation. 4) Silhouette Score: The Silhouette Score (SIL) <ref type="bibr" target="#b27">[28]</ref> measures the similarity of examples to their own cluster compared to other clusters, and it is computed by using Eq. 10,</p><formula xml:id="formula_10">SIL = b(i) − a(i) max [a(i), b(i)]<label>(10)</label></formula><p>where a(i) is the distance between point i and all other points in its cluster, and can be computed by using Eq. 11,</p><formula xml:id="formula_11">a(i) = 1 |C i | − 1 j∈Ci,i =j d(i, j)<label>(11)</label></formula><p>where C i is the predicted cluster for point i, and d(i, j) is the distance between points i and j. Then, b(i) is the distance between point i and all other points in the next nearest cluster, and can be computed by using Eq. 12,</p><formula xml:id="formula_12">b(i) = min k =i 1 |C k | j∈C k d(i, j)<label>(12)</label></formula><p>Any distance metric may be used, but we used the Euclidean distance metric in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>Calinski-Harabasz Score: The Calinski-Harabasz score <ref type="bibr" target="#b28">[29]</ref> (CHS) is defined as the ratio between within-cluster dispersion and between-cluster dispersion, and it is computed by using Eq. 13. A higher CHS implies better cluster separation.</p><formula xml:id="formula_13">CHS = T r(B k ) T r(W k ) × N − k k − 1<label>(13)</label></formula><p>where B k is the between-cluster dispersion matrix given by Eq. 14,</p><formula xml:id="formula_14">B k = q n q (c q − c)(c q − c) T<label>(14)</label></formula><p>and W k is the within-cluster dispersion given by Eq. 15,</p><formula xml:id="formula_15">W k = k q=1 x∈Cq (x − c q )(x − c q ) T<label>(15)</label></formula><p>where N is the number of points in a data, C q is the set of points in cluster q, c q is the center of cluster q, c is the center of C, and n q is the number of points in cluster q. 6) Davies-Bouldin Index: The Davies-Bouldin Index (DBI) is defined as the ratio of within-cluster distances to betweencluster distances <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. A lower DBI implies better cluster separation. It is computed by using Eq. 16,</p><formula xml:id="formula_16">DBI = 1 k k i=1 max i =j R j<label>(16)</label></formula><p>where R is the average similarity among clusters given by Eq. 17.</p><formula xml:id="formula_17">R ij = s i + s j d ij<label>(17)</label></formula><p>where s i is the cluster diameter which is the average distance between each point in cluster i and the cluster centroid, and d ij is the distance between centroids i and j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Datasets Description</head><p>We evaluate and compare our baseline and experimental methods on three image datasets. We list the dataset statistics in <ref type="table" target="#tab_0">Table II</ref>. On the other hand, we used the full training sets for k-Means clustering on the original feature representation, and for the unsupervised learning models (i.e. baseline autoencoder, SNNL-{2, 4, 6, 8}), and we evaluated them on the full test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Clustering Performance</head><p>We evaluate the performance of our experimental method in its different configurations. For our baseline models, we used k-Means clustering on (1) the original feature representation, encoded using principal components analysis (PCA) to 70 dimensions in order to avoid the "curse of dimensionality" <ref type="bibr" target="#b32">[33]</ref>, and (2) on the latent code representation from an autoencoder trained with reconstruction loss (Eq. 1) only. Then, we retrieved the reported clustering performance of DEC <ref type="bibr" target="#b4">[5]</ref>, VaDE <ref type="bibr" target="#b6">[7]</ref>, ClusterGAN <ref type="bibr" target="#b8">[9]</ref>, and N2D <ref type="bibr" target="#b11">[12]</ref> from literature as additional baseline results. We report the average clustering performance across four runs of each model, as well as their best clustering performance on each of the benchmark dataset.</p><p>We present empirical evidence that consistently shows our method significantly outperforms all our baseline models on each of the benchmark dataset we used. Specifically, our method configurations SNNL-5 and SNNL-7.</p><p>1) MNIST: We had the highest average clustering accuracy of 95.5% on the MNIST dataset using SNNL-7 configuration, and the highest best clustering accuracy of 96.2% using SNNL-5 configuration. Meanwhile, for each of our related models, only their best clustering accuracy was reported. Their results are as follows: DEC had 84.3%, VaDE had 94.5%, N2D had 94.8%, and ClusterGAN had 95%. If we follow the related works to report the best performance, we outperformed our closest baseline, ClusterGAN, with 1.2% in clustering accuracy. For the full results of clustering performance on the MNIST dataset, we refer the reader to <ref type="table" target="#tab_0">Table III.</ref> 2) Fashion-MNIST: We had the highest clustering accuracy of 84.4% (average) and 85.6% (best) on the Fashion-MNIST dataset using SNNL-5. Meanwhile, our related models had the following results: ClusterGAN had 63% and N2D had 67.2%. However, we should note here that N2D was trained on the  For the full results of clustering performance on the Fashion-MNIST dataset, we refer the reader to <ref type="table" target="#tab_0">Table IV.</ref> 3) EMNIST Balanced: We had the highest clustering accuracy of 78.5% (average) and 79.2% (best) on the EM-NIST Balanced dataset using SNNL-5 configuration, while our baseline autoencoder had 35.6% (average) and 36.1% (best). Neither the deep clustering algorithms nor N2D used the EMNIST Balanced dataset (or any other EMNIST subsets). For the full results of clustering performance on the EMNIST Balanced dataset, we refer the reader to <ref type="table" target="#tab_4">Table V.</ref> The full results in <ref type="table" target="#tab_0">Table III</ref>, IV, and V not only show that our experimental methods outperformed our baseline methods, but also show that the supervised configurations (SNNL-{3, 5, 7}) of the soft nearest neighbor loss performed better than the unsupervised ones (SNNL-{2, 4, 6, 8}), with the exception of SNNL-1 on the EMNIST Balanced dataset. This emphasizes the requisite of labels for the soft nearest neighbor loss to illuminate the neighborhood structure of a dataset, thus learning a more clustering-friendly feature representation.</p><formula xml:id="formula_18">0.843 - - - - - - - - - - VaDE [7]* - 0.945 - - - - - - - - - - N2D [12]* - 0.948 - 0.882 - - - - - - - - ClusterGAN [9]* - 0.95 - 0.89 - 0.89 - - - - - - SNNL-</formula><p>Furthermore, SNNL-{3, 5, 7} also outperformed our baseline methods in terms of NMI and ARI. Intuitively, having high NMI and ARI scores for clustering implies that there is a high number of similar data points in each cluster. In other words, similar data points have been correctly assigned to their clusters, which in our case was based on the pseudo-cluster labels we used. We can also see that SNNL-{1, 3, 5, 7} had the best scores in terms of SIL, CHS, and DBI. Having a high SIL score implies that the points in a cluster are more similar among themselves than they are to the points from a different cluster. Then, both CHS and DBI measure cluster separation, i.e. CHS defines better separated clusters through variance ratios, while DBI defines better separated clusters through distances among clusters.</p><p>Our results support that our method and its variants were able to learn a more clustering-friendly feature representation by having better defined clusters and by having correctly clustered data points, which we visually inspect in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visualizing Disentangled Latent Representation</head><p>We show the resulting disentangled latent representation for the test set of MNIST, Fashion-MNIST, and EMNIST Balanced datasets in <ref type="figure">Figure 2</ref>  we only visualized a randomly chosen 10 clusters for easier and cleaner visualization. We can see in the aforementioned figure that the latent code representation for each dataset indeed became more clustering-friendly by having well-defined clusters (indicated by the cluster dispersion) and correct cluster assignments (indicated by the cluster colors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Clustering on Fewer Labelled Examples</head><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we show that even with fewer labelled training examples, the clustering accuracy on disentangled latent code representation is still better than on the original feature representation and on the latent code representation from a baseline autoencoder. We trained an autoencoder with SNNL-1 and SNNL-5 configurations on randomly picked 1,000 examples, 3,000 examples, and 6,000 examples from the MNIST and Fashion-MNIST datasets, and treated them as labelled data. Then we evaluated on the full test sets. In contrast, we used the full MNIST and Fashion-MNIST sets for both the original feature representation and the latent code representation from our baseline autoencoder.</p><p>On MNIST, our best model configuration was SNNL-5 that had a clustering accuracy of 90.85%, 92.95%, and 94.78% when trained on 1,000 examples, 3,000 examples, and 6,000  The results in <ref type="figure" target="#fig_2">Figure 3</ref> are average clustering accuracy across four runs of each model on the small labelled subsets of MNIST and Fashion-MNIST datasets. To support these results, we can observe the same trend of clustering performance in terms of NMI in <ref type="figure" target="#fig_3">Figure 4</ref>. With this, we draw a parallel with the non-linear NCA <ref type="bibr" target="#b15">[16]</ref> performance, where they employed unsupervised pre-training for kNN classification, and found an even better test error rate when they fine-tuned on a small fraction of labelled MNIST dataset -a test error rate of 1%.</p><p>We argue that this robustness of clustering performance, despite smaller labelled subsets were used for SNNL-trained autoencoders, is due to the soft nearest neighbor loss enabling the learning of a latent code representation that takes into account the distances among pairs of points from the same class or structure relative to points from different classes or structures. In other words, it brings out the neighborhood structure of a dataset.</p><p>Finally, we were able to further improve the clustering performance on disentangled latent code representation with our annealing temperature factor. The intuition as to how this annealing factor improves the soft nearest neighbor loss builds on the results in <ref type="figure">Figure 1</ref>, and may be described as follows: as the training progresses, the data points of similar class or structure become closer. Hence, there are fewer widely separated points of the same class or structure as training progresses. Consequently, this renders the use of high temperatures less relevant over time, since the class-or structure-similar points are already becoming entangled -which in turn, makes the latent code representation disentangled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>Compared to deep clustering methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, we employed a simpler approach to cluster the latent code representation from an autoencoder. We used a composite loss of the autoencoder reconstruction loss and the soft nearest neighbor loss to learn a more clustering-friendly latent code representation from an autoencoder, thereby improving the k-Means clustering performance on our datasets. We expanded the soft nearest neighbor loss by introducing an annealing temperature factor, which led to an even better disentanglement and k-Means clustering performance. We posit that our annealing mechanism helps by adapting the temperature needed throughout a training. Using our approach, we had a clustering accuracy of 95.5% (96.2% on best run), 84.4% (85.6% on best run), and 78.5% (79.2% on best run) on the MNIST, Fashion-MNIST, and EMNIST Balanced datasets respectively, outperforming all our baseline models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>2000 − c − 2000 − 500 − 500 −d units, where d andd (s.t. d =d) refer to the dimensionality of the features, and c refers to the dimensionality of the latent code representation -similar to the one used by Salakhutdinov and Hinton (2007)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. These latent code representations were obtained after training an autoencoder with SNNL-7 for 50 epochs on a subset of 10,000 training examples each for MNIST and Fashion-MNIST datasets, and on a subset of 20,000 training examples of EMNIST Balanced dataset. However, since the EMNIST Balanced dataset has 47 clusters,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Test clustering accuracy on the MNIST and Fashion-MNIST test sets when small subsets of labelled data are used for training. Both the original representation and the baseline autoencoder do not take advantage of the labelled dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Normalized mutual information (NMI) on the MNIST and Fashion-MNIST test sets when small subsets of labelled data are used for training. Both the original representation and the baseline autoencoder do not take advantage of the labelled dataset. examples respectively. Using the same model configuration on Fashion-MNIST, we had a clustering accuracy of 79.63%, 82.35%, and 83.78% when trained on 1,000 examples, 3,000 examples, and 6,000 examples respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CONFIGURATIONS</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell>OF AUTOENCODER (AE) TRAINED WITH SOFT</cell></row><row><cell></cell><cell>NEAREST NEIGHBOR LOSS (SNNL)</cell></row><row><cell cols="2">Shorthand Full SNNL-trained Autoencoder Configuration</cell></row><row><cell>SNNL-1</cell><cell>Supervised SNNL-trained AE w/ fixed T</cell></row><row><cell>SNNL-2</cell><cell>Unsupervised SNNL-trained AE w/ fixed T</cell></row><row><cell>SNNL-3</cell><cell>Supervised SNNL-trained AE w/ arg min and fixed T</cell></row><row><cell>SNNL-4</cell><cell>Unsupervised SNNL-trained AE w/ arg min and fixed T</cell></row><row><cell>SNNL-5</cell><cell>Supervised SNNL-trained AE w/ annealing T</cell></row><row><cell>SNNL-6</cell><cell>Unsupervised SNNL-trained AE w/ annealing T</cell></row><row><cell>SNNL-7</cell><cell>Supervised SNNL-trained AE w/ arg min and annealing T</cell></row><row><cell>SNNL-8</cell><cell>Unsupervised SNNL-trained AE w/ arg min and annealing T</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DATASETS</head><label>II</label><figDesc>The EMNIST Balanced is a subset of the EMNIST handwritten characters classification dataset<ref type="bibr" target="#b31">[32]</ref>, that has a 47 balanced classes of handwritten digits and letters. It has 112,800 training examples and 18,800 test examples -also all in grayscale, and of size 28 by 28 pixels. We reshaped each image to a 784-dimensional vector. For the supervised configuration of the soft nearest neighbor loss, we simulate the lack of labelled data by randomly picking 10,000 labelled training examples each for the MNIST and the Fashion-MNIST datasets, and randomly picking 20,000 labelled training examples for the EMNIST balanced dataset, since it has to be clustered in 47 groups. Despite using a small subset of labelled training examples, we still used the full test sets for the clustering task.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>pixels. We reshaped each image to a 784-dimensional</cell></row><row><cell></cell><cell></cell><cell></cell><cell>vector.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3) EMNIST Balanced:</cell></row><row><cell></cell><cell></cell><cell>STATISTICS.</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3"># Samples Input Dimension # Clusters</cell></row><row><cell>MNIST</cell><cell>70,000</cell><cell>784</cell><cell>10</cell></row><row><cell>Fashion-MNIST</cell><cell>70,000</cell><cell>784</cell><cell>10</cell></row><row><cell>EMNIST Balanced</cell><cell>131,600</cell><cell>784</cell><cell>47</cell></row><row><cell cols="4">1) MNIST: The MNIST handwritten digit classification</cell></row><row><cell cols="4">dataset [6] consists of 60,000 training examples and</cell></row><row><cell cols="4">10,000 test examples -all in grayscale, and of size 28 by</cell></row><row><cell cols="4">28 pixels. We reshaped each image to a 784-dimensional</cell></row><row><cell>vector.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">2) Fashion-MNIST: The Fashion-MNIST [11] is a more</cell></row><row><cell cols="4">challenging alternative to the MNIST dataset, which</cell></row><row><cell cols="4">consists of 60,000 training examples and 10,000 test</cell></row><row><cell cols="4">examples -also all in grayscale, and of size 28 by 28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CLUSTERING</head><label>III</label><figDesc>PERFORMANCE ON THE MNIST DATASET.</figDesc><table><row><cell>Method</cell><cell>ACC Average</cell><cell>Best</cell><cell>NMI Average</cell><cell>Best</cell><cell>ARI Average</cell><cell>Best</cell><cell>SIL Average</cell><cell>Best</cell><cell>Average</cell><cell>CHS</cell><cell>Best</cell><cell>DBI Average</cell><cell>Best</cell></row><row><cell>Original</cell><cell>0.525</cell><cell>0.547</cell><cell>0.497</cell><cell>0.499</cell><cell>0.367</cell><cell>0.367</cell><cell>0.078</cell><cell>0.078</cell><cell>469.231</cell><cell></cell><cell>471.494</cell><cell>2.605</cell><cell>2.592</cell></row><row><cell>SNNL-2</cell><cell>0.549</cell><cell>0.552</cell><cell>0.512</cell><cell>0.52</cell><cell>0.387</cell><cell>0.393</cell><cell>0.086</cell><cell>0.089</cell><cell>443.535</cell><cell></cell><cell>459.947</cell><cell>2.631</cell><cell>2.619</cell></row><row><cell>SNNL-4</cell><cell>0.562</cell><cell>0.569</cell><cell>0.516</cell><cell>0.521</cell><cell>0.4</cell><cell>0.409</cell><cell>0.086</cell><cell>0.089</cell><cell>442.462</cell><cell></cell><cell>458.644</cell><cell>2.640</cell><cell>2.583</cell></row><row><cell>Baseline AE</cell><cell>0.56</cell><cell>0.576</cell><cell>0.518</cell><cell>0.528</cell><cell>0.399</cell><cell>0.419</cell><cell>0.086</cell><cell>0.088</cell><cell>445.146</cell><cell></cell><cell>457.095</cell><cell>2.638</cell><cell>2.613</cell></row><row><cell>SNNL-6</cell><cell>0.562</cell><cell>0.58</cell><cell>0.512</cell><cell>0.53</cell><cell>0.4</cell><cell>0.429</cell><cell>0.084</cell><cell>0.088</cell><cell>437.957</cell><cell></cell><cell>449.735</cell><cell>2.647</cell><cell>2.616</cell></row><row><cell>SNNL-8</cell><cell>0.575</cell><cell>0.589</cell><cell>0.528</cell><cell>0.543</cell><cell>0.413</cell><cell>0.433</cell><cell>0.085</cell><cell>0.088</cell><cell>437.458</cell><cell></cell><cell>447.399</cell><cell>2.657</cell><cell>2.636</cell></row><row><cell>DEC [5]*</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V CLUSTERING</head><label>V</label><figDesc>PERFORMANCE ON THE EMNIST-BALANCED DATASET.Fig. 2. Three-dimensional visualization comparing the original representation and the disentangled latent representation of the three datasets. To achieve this visualization, the representations were encoded using t-SNE with perplexity = 50 and learning rate = 10, optimized for 5,000 iterations, with the same random seed set for all computations. However, for clustering, we used higher dimensionality to achieve better clustering performance. This figure is best viewed in color.</figDesc><table><row><cell>Method</cell><cell>ACC Average</cell><cell>Best</cell><cell>NMI Average</cell><cell>Best</cell><cell>ARI Average</cell><cell>Best</cell><cell>SIL Average</cell><cell>Best</cell><cell>Average</cell><cell>CHS</cell><cell>Best</cell><cell>DBI Average</cell><cell>Best</cell></row><row><cell>Original</cell><cell>0.321</cell><cell>0.33</cell><cell>0.418</cell><cell>0.42</cell><cell>0.173</cell><cell>0.176</cell><cell>0.051</cell><cell>0.052</cell><cell>237.078</cell><cell></cell><cell>237.413</cell><cell>2.659</cell><cell>2.636</cell></row><row><cell>SNNL-1</cell><cell>0.319</cell><cell>0.349</cell><cell>0.662</cell><cell>0.674</cell><cell>0.335</cell><cell>0.356</cell><cell>0.897</cell><cell>0.902</cell><cell cols="3">71353.661 78749.196</cell><cell>1.079</cell><cell>1.069</cell></row><row><cell>SNNL-6</cell><cell>0.348</cell><cell>0.352</cell><cell>0.441</cell><cell>0.444</cell><cell>0.193</cell><cell>0.196</cell><cell>0.041</cell><cell>0.041</cell><cell>222.108</cell><cell></cell><cell>225.193</cell><cell>2.712</cell><cell>2.695</cell></row><row><cell>SNNL-2</cell><cell>0.343</cell><cell>0.353</cell><cell>0.439</cell><cell>0.442</cell><cell>0.191</cell><cell>0.197</cell><cell>0.037</cell><cell>0.04</cell><cell>223.75</cell><cell></cell><cell>224.457</cell><cell>2.698</cell><cell>2.658</cell></row><row><cell>SNNL-4</cell><cell>0.344</cell><cell>0.353</cell><cell>0.438</cell><cell>0.445</cell><cell>0.190</cell><cell>0.196</cell><cell>0.038</cell><cell>0.041</cell><cell>225.839</cell><cell></cell><cell>227.568</cell><cell>2.674</cell><cell>2.641</cell></row><row><cell>SNNL-8</cell><cell>0.35</cell><cell>0.356</cell><cell>0.442</cell><cell>0.444</cell><cell>0.195</cell><cell>0.197</cell><cell>0.04</cell><cell>0.042</cell><cell>223.449</cell><cell></cell><cell>225.474</cell><cell>2.692</cell><cell>2.669</cell></row><row><cell>Baseline AE</cell><cell>0.356</cell><cell>0.361</cell><cell>0.446</cell><cell>0.449</cell><cell>0.198</cell><cell>0.201</cell><cell>0.042</cell><cell>0.044</cell><cell>224.537</cell><cell></cell><cell>228.909</cell><cell>2.709</cell><cell>2.687</cell></row><row><cell>SNNL-3</cell><cell>0.396</cell><cell>0.44</cell><cell>0.667</cell><cell>0.699</cell><cell>0.391</cell><cell>0.438</cell><cell>0.6</cell><cell>0.737</cell><cell cols="3">11138.224 16324.421</cell><cell>1.378</cell><cell>1.345</cell></row><row><cell>SNNL-7</cell><cell>0.701</cell><cell>0.743</cell><cell>0.753</cell><cell>0.775</cell><cell>0.529</cell><cell>0.634</cell><cell>0.580</cell><cell>0.761</cell><cell>4832.462</cell><cell></cell><cell>7098.482</cell><cell>0.852</cell><cell>0.647</cell></row><row><cell>SNNL-5</cell><cell>0.785</cell><cell>0.792</cell><cell>0.776</cell><cell>0.783</cell><cell>0.641</cell><cell>0.655</cell><cell>0.677</cell><cell>0.687</cell><cell>4697.866</cell><cell></cell><cell>5025.238</cell><cell>0.646</cell><cell>0.607</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuxi</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05148</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clustergan: Latent space clustering in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipto</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">N2d: (not too) deep clustering via clustering the local manifold of an autoencoded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcconville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05968</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Vin De Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Analyzing and improving representations with the soft nearest neighbor loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01889</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Neighbourhood components analysis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adding gradient noise improves learning for very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06807</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the eighteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image clustering using local discriminant models and global integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2761" to="2773" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semisupervised learning (chapelle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<editor>o. et al.</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phipps</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A dendrite method for cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadeusz</forename><surname>Calinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerzy</forename><surname>Harabasz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-theory and Methods</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A cluster separation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">W</forename><surname>Bouldin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="224" to="227" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On clustering validation techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Halkidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Batistakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of intelligent information systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="107" to="145" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">EMNIST: Extending MNIST to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adaptive control processes: a guided tour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Princeton university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
