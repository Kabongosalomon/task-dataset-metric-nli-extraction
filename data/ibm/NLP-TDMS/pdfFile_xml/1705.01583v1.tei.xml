<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017">2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Saarland University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo-Hammad</forename><surname>Shafiei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Saarland University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universidad</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rey</forename><forename type="middle">Juan</forename><surname>Carlos</surname></persName>
						</author>
						<title level="a" type="main">VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera</title>
					</analytic>
					<monogr>
						<title level="m">ACM TOG</title>
						<imprint>
							<date type="published" when="2017">2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. We recover the full global 3D skeleton pose in real-time from a single RGB camera, even wireless capture is possible by streaming from a smartphone (left). It enables applications such as controlling a game character, embodied VR, sport motion analysis and reconstruction of community video (right). Community videos (CC BY) courtesy of Real Madrid C.F. [2016]  and RUSFENCING-TV [2017].</p><p>We present the first real-time method to capture the full global 3D skeletal pose of a human in a stable, temporally consistent manner using a single RGB camera. Our method combines a new convolutional neural network (CNN) based pose regressor with kinematic skeleton fitting. Our novel fullyconvolutional pose formulation regresses 2D and 3D joint positions jointly in real time and does not require tightly cropped input frames. A real-time kinematic skeleton fitting method uses the CNN output to yield temporally stable 3D global pose reconstructions on the basis of a coherent kinematic skeleton. This makes our approach the first monocular RGB method usable in real-time applications such as 3D character control-thus far, the only monocular methods for such applications employed specialized RGB-D cameras. Our method's accuracy is quantitatively on par with the best offline 3D monocular RGB pose estimation methods. Our results are qualitatively comparable to, and sometimes better than, results from monocular RGB-D approaches, such as the Kinect. However, we show that our approach is more broadly applicable than RGB-D solutions, i.e., it works for outdoor scenes, community videos, and low quality commodity RGB cameras.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. We recover the full global 3D skeleton pose in real-time from a single RGB camera, even wireless capture is possible by streaming from a smartphone <ref type="bibr">(left)</ref>. It enables applications such as controlling a game character, embodied VR, sport motion analysis and reconstruction of community video (right). Community videos (CC BY) courtesy of <ref type="bibr" target="#b71">Real Madrid C.F. [2016]</ref> and RUSFENCING-TV <ref type="bibr">[2017]</ref>.</p><p>We present the first real-time method to capture the full global 3D skeletal pose of a human in a stable, temporally consistent manner using a single RGB camera. Our method combines a new convolutional neural network (CNN) based pose regressor with kinematic skeleton fitting. Our novel fullyconvolutional pose formulation regresses 2D and 3D joint positions jointly in real time and does not require tightly cropped input frames. A real-time kinematic skeleton fitting method uses the CNN output to yield temporally stable 3D global pose reconstructions on the basis of a coherent kinematic skeleton. This makes our approach the first monocular RGB method usable in real-time applications such as 3D character control-thus far, the only monocular methods for such applications employed specialized RGB-D cameras. Our method's accuracy is quantitatively on par with the best offline 3D monocular RGB pose estimation methods. Our results are qualitatively comparable to, and sometimes better than, results from monocular RGB-D approaches, such as the Kinect. However, we show that our approach is more broadly applicable than RGB-D solutions, i.e., it works for outdoor scenes, community videos, and low quality commodity RGB cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Optical skeletal motion capture of humans is widely used in applications such as character animation for movies and games, sports and biomechanics, and medicine. To overcome the usability constraints imposed by commercial systems requiring marker suits <ref type="bibr" target="#b56">[Menache 2000</ref>], researchers developed marker-less motion capture methods that estimate motion in more general scenes using multi-view This work is was funded by the ERC Starting Grant project CapReal (335545). Dan Casas was supported by a Marie Curie Individual Fellow, grant agreement 707326.</p><p>video <ref type="bibr" target="#b60">[Moeslund et al. 2006]</ref>, with recent solutions being real-time <ref type="bibr" target="#b90">[Stoll et al. 2011]</ref>. The swell in popularity of applications such as realtime motion-driven 3D game character control, self-immersion in 3D virtual and augmented reality, and human-computer interaction, has led to new real-time full-body motion estimation techniques using only a single, easy to install, depth camera, such as the Microsoft Kinect <ref type="bibr" target="#b57">[Microsoft Corporation 2010</ref>. RGB-D cameras provide valuable depth data which greatly simplifies monocular pose reconstruction. However, RGB-D cameras often fail in general outdoor scenes (due to sunlight interference), are bulkier, have higher power consumption, have lower resolution and limited range, and are not as widely and cheaply available as color cameras.</p><p>Skeletal pose estimation from a single color camera is a much more challenging and severely underconstrained problem. Monocular RGB body pose estimation in 2D has been widely researched, but estimates only the 2D skeletal pose <ref type="bibr" target="#b15">[Bourdev and Malik 2009;</ref><ref type="bibr" target="#b27">Felzenszwalb et al. 2010;</ref><ref type="bibr" target="#b28">Felzenszwalb and Huttenlocher 2005;</ref><ref type="bibr" target="#b29">Ferrari et al. 2009;</ref><ref type="bibr" target="#b68">Pishchulin et al. 2013;</ref><ref type="bibr" target="#b52">Wei et al. 2016</ref>]. Learning-based discriminative methods, in particular deep learning methods <ref type="bibr" target="#b51">Lifshitz et al. 2016;</ref><ref type="bibr" target="#b63">Newell et al. 2016;</ref><ref type="bibr" target="#b96">Tompson et al. 2014</ref>], represent the current state of the art in 2D pose estimation, with some of these methods demonstrating real-time performance <ref type="bibr" target="#b18">[Cao et al. 2016;</ref><ref type="bibr" target="#b52">Wei et al. 2016]</ref>. Monocular RGB estimation of the 3D skeletal pose is a much harder challenge tackled by relatively fewer methods <ref type="bibr" target="#b14">[Bogo et al. 2016;</ref><ref type="bibr">Tekin et al. 2016b,c;</ref><ref type="bibr" target="#b112">Zhou et al. , 2015b</ref>. Unfortunately, these methods are typically offline, and they often reconstruct 3D joint positions individually per image, which are temporally unstable, and do not enforce constant bone lengths. Most approaches also capture local 3D pose relative to a bounding box, and not the full global 3D pose. This makes them unsuitable for applications such as real-time 3D character control.</p><p>In this paper, we present the first method that captures temporally consistent global 3D human pose-in terms of joint angles of a single, stable kinematic skeleton-in real-time (30 Hz) from a single RGB video in a general environment. Our approach builds upon the top performing single RGB 3D pose estimation methods using convolutional neural networks (CNNs) <ref type="bibr" target="#b55">[Mehta et al. 2016;</ref><ref type="bibr" target="#b67">Pavlakos et al. 2016]</ref>. High accuracy requires training comparably deep networks which are harder to run in real-time, partly due to additional preprocessing steps such as bounding box extraction. <ref type="bibr" target="#b55">Mehta et al. [2016]</ref> use a 100-layer architecture to predict 2D and 3D joint positions simultaneously, but is unsuitable for real-time execution. To improve runtime, we use a shallower 50-layer network. However, for best quality at real-time frame rates, we do not merely use a shallower variant, but extend it to a novel fully-convolutional formulation. This enables higher accuracy 2D and 3D pose regression, in particular of end effectors (hands, feet), in real-time. In contrast to existing solutions our approach allows operation on non-cropped images, and where run-time is a concern, it can be used to bootstrap a simple bounding box tracker. We also combine the CNN-based joint position regression with an efficient optimization step to fit a 3D skeleton to these reconstructions in a temporally stable way, yielding the global pose and joint angles of the skeleton. In summary, we contribute by proposing the first real-time method to capture global 3D kinematic skeleton pose from single RGB video. To strike a good compromise between computational complexity and accuracy, our method combines:</p><p>• A new real-time, fully-convolutional 3D body pose formulation using CNNs that yields 2D and 3D joint positions simultaneously and forgoes the need to perform expensive bounding box computations. • Model-based kinematic skeleton fitting against the 2D/3D pose predictions to produce temporally stable joint angles of a metric global 3D skeleton, in real time.</p><p>Our real-time method achieves state-of-the-art accuracy comparable to the best offline RGB pose estimation methods on standard 3D human body pose benchmarks, particularly for end effector positions (Section 5.2). Our results are qualitatively comparable to, and sometimes better than, state-of-the-art single RGB-D methods <ref type="bibr" target="#b33">[Girshick et al. 2011]</ref>, even commercial ones <ref type="bibr">[Microsoft Corporation 2015]</ref>. We experimentally show that this makes ours the first single-RGB method usable for similar real-time 3D applications-so far only feasible with RGB-D input-such as game character control or immersive first person virtual reality (VR). We further show that our method succeeds in settings where existing RGB-D methods would not, such as outdoor scenes, community videos, and even with low quality video streams from ubiquitous mobile phone cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our goal is stable 3D skeletal motion capture from (1) a single camera (2) in real-time. We focus the discussion of related work on approaches from the large body of marker-less motion capture research that contributed to attaining either of these properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-view:</head><p>With multi-view setups markerless motion-captue solutions attain high accuracy. Tracking of a manually initialized actor model from frame to frame with a generative image formation model is common. See <ref type="bibr" target="#b60">[Moeslund et al. 2006</ref>] for a complete overview. Most methods target high quality with offline computation <ref type="bibr" target="#b17">[Bregler and Malik 1998;</ref><ref type="bibr" target="#b36">Howe et al. 1999;</ref><ref type="bibr" target="#b53">Loper and Black 2014;</ref><ref type="bibr" target="#b84">Sidenbladh et al. 2000;</ref><ref type="bibr" target="#b89">Starck and Hilton 2003</ref>]. Real-time performance can be attained by representing the actor with Gaussians <ref type="bibr" target="#b77">[Rhodin et al. 2015;</ref><ref type="bibr" target="#b90">Stoll et al. 2011;</ref><ref type="bibr" target="#b104">Wren et al. 1997</ref>] and other approximations <ref type="bibr" target="#b54">[Ma and Wu 2014]</ref>, in addition to formulations allowing model-toimage fitting. However, these tracking-based approaches often lose track in local minima of the non-convex fitting functions they optimize and require separate initialization, e.g. using <ref type="bibr" target="#b14">[Bogo et al. 2016;</ref><ref type="bibr" target="#b75">Rhodin et al. 2016b;</ref><ref type="bibr" target="#b88">Sminchisescu and Triggs 2001]</ref>. Robustness could be increased with a combination of generative and discriminative estimation <ref type="bibr" target="#b26">[Elhayek et al. 2016]</ref>, even from a single input view <ref type="bibr" target="#b81">[Rosales and Sclaroff 2006;</ref><ref type="bibr" target="#b86">Sminchisescu et al. 2006]</ref>, and egocentric perspective <ref type="bibr" target="#b74">[Rhodin et al. 2016a</ref>]. We utilize generative tracking components to ensure temporal stability, but avoid model projection through a full image formation model to speed up estimation. Instead, we combine discriminative pose estimation with kinematic fitting to succeed in our underconstrained setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monocular Depth-based:</head><p>The additional depth channel provided by RGB-D sensors has led to robust real-time pose estimation solutions <ref type="bibr" target="#b7">[Baak et al. 2011;</ref><ref type="bibr" target="#b31">Ganapathi et al. 2012;</ref><ref type="bibr" target="#b54">Ma and Wu 2014;</ref><ref type="bibr" target="#b83">Shotton et al. 2013;</ref><ref type="bibr" target="#b103">Wei et al. 2012;</ref><ref type="bibr" target="#b106">Ye and Yang 2014]</ref> and the availability of low-cost devices has enabled a range of new applications. Even real-time tracking of general deforming objects <ref type="bibr" target="#b114">[Zollhöfer et al. 2014</ref>] and template-free reconstruction <ref type="bibr" target="#b24">[Dou et al. 2016;</ref><ref type="bibr" target="#b38">Innmann et al. 2016;</ref><ref type="bibr" target="#b62">Newcombe et al. 2015;</ref> has been demonstrated. RGB-D information overcomes forwardbackwards ambiguities in monocular pose estimation. Our goal is a video solution that overcomes depth ambiguities without relying on a specialized active sensor.</p><p>Monocular RGB: Monocular generative motion capture has only been shown for short clips and when paired with strong motion priors <ref type="bibr" target="#b98">[Urtasun et al. 2006]</ref> or in combination with discriminative re-initialization <ref type="bibr" target="#b81">[Rosales and Sclaroff 2006;</ref><ref type="bibr" target="#b86">Sminchisescu et al. 2006</ref>], since generative reconstruction is fundamentally underconstrained. Using photo-realistic template models for model fitting enables more robust monocular tracking of simple motions, but requires more expensive offline computation <ref type="bibr" target="#b22">[de La Gorce et al. 2008</ref>]. Samplingbased methods avoid local minima <ref type="bibr" target="#b8">[Balan et al. 2005;</ref><ref type="bibr" target="#b13">Bo and Sminchisescu 2010;</ref><ref type="bibr" target="#b23">Deutscher and Reid 2005;</ref><ref type="bibr" target="#b30">Gall et al. 2010</ref>]. However, real-time variants can not guarantee global convergence due to a limited number of samples, such as particle swarm optimization techniques <ref type="bibr" target="#b64">[Oikonomidis et al. 2011</ref>]. Structure-from-motion techniques exploit motion cues in a batch of frames <ref type="bibr" target="#b32">[Garg et al. 2013]</ref>, and have also been applied to human motion estimation <ref type="bibr" target="#b34">[Gotardo and Martinez 2011;</ref><ref type="bibr" target="#b47">Lee et al. 2013;</ref><ref type="bibr" target="#b66">Park and Sheikh 2011;</ref><ref type="bibr" target="#b113">Zhu et al. 2011</ref>]. However, batch optimization does not apply to our real-time setting, where frames are streamed sequentially. For some applications manual annotation and correction of frames is suitable, for instance to enable movie actor reshaping <ref type="bibr" target="#b43">[Jain et al. 2010]</ref> and garment replacement in video <ref type="bibr" target="#b79">[Rogge et al. 2014</ref>]. In combination <ref type="figure">Fig. 2</ref>. Overview. Given a full-size image I t at frame t , the person-centered crop B t is efficiently extracted by bounding box tracking, using the previous frame's keypoints K t −1 . From the crop, the CNN jointly predicts 2D heatmaps H j, t and our novel 3D location-maps X j, t , Y j, t and Z j, t for all joints j. The 2D keypoints K t are retrieved from H j, t and, after filtering, are used to read off 3D pose P L t from X j, t , Y j, t and Z j, t . These per-frame estimates are combined to stable global pose P G t by skeleton fitting. Information from frame t − 1 is marked in gray-dashed.</p><p>with physical constraints, highly accurate reconstructions are possible from monocular video <ref type="bibr" target="#b102">[Wei and Chai 2010]</ref>. <ref type="bibr" target="#b99">Vondrak et al. [2012]</ref> succeed without manual annotation by simulating bipedcontrollers, but require batch-optimization. While these methods can yield high-quality reconstructions, interaction and expensive optimization preclude live applications. Discriminative 2D human pose estimation is often an intermediate step to monocular 3D pose estimation. Pictorial structure approaches infer body part locations by message passing over a huge set of pose-states <ref type="bibr" target="#b0">[Agarwal and Triggs 2006;</ref><ref type="bibr" target="#b5">Andriluka et al. 2009;</ref><ref type="bibr" target="#b15">Bourdev and Malik 2009;</ref><ref type="bibr" target="#b28">Felzenszwalb and Huttenlocher 2005;</ref><ref type="bibr" target="#b29">Ferrari et al. 2009;</ref><ref type="bibr" target="#b45">Johnson and Everingham 2010]</ref> and have been extended to 3D pose estimation <ref type="bibr" target="#b3">[Amin et al. 2013;</ref><ref type="bibr" target="#b9">Balan et al. 2007;</ref><ref type="bibr" target="#b10">Belagiannis et al. 2014;</ref>. Recent approaches outperform these methods in computation time and accuracy by leveraging large image databases with 2D joint location annotation, which enables high accuracy prediction with deep CNNs <ref type="bibr" target="#b11">[Belagiannis and Zisserman 2016;</ref><ref type="bibr" target="#b37">Hu et al. 2016;</ref><ref type="bibr" target="#b52">Wei et al. 2016</ref>], on multiple GPUs, even at real-time rates <ref type="bibr" target="#b18">[Cao et al. 2016]</ref>. Given 2D joint locations, lifting them to 3D pose is challenging. Existing approaches use bone length and depth ordering constraints <ref type="bibr" target="#b61">[Mori and Malik 2006;</ref><ref type="bibr" target="#b92">Taylor 2000]</ref>, sparsity assumptions <ref type="bibr" target="#b100">[Wang et al. 2014;</ref><ref type="bibr">Zhou et al. 2015,a]</ref>, joint limits <ref type="bibr" target="#b2">[Akhter and Black 2015]</ref>, inter-penetration constraints <ref type="bibr" target="#b14">[Bogo et al. 2016</ref>], temporal dependencies <ref type="bibr" target="#b75">[Rhodin et al. 2016b]</ref>, and regression <ref type="bibr" target="#b105">[Yasin et al. 2016</ref>]. Treating 3D pose as a hidden variable in 2D estimation is an alternative <ref type="bibr" target="#b16">[Brau and Jiang 2016]</ref>. However, the sparse set of 2D locations loses image evidence, e.g. on forward-backwards orientation of limbs, which leads to erroneous estimates in ambiguous cases. To overcome these ambiguities, discriminative methods have been proposed that learn implicit depth features for 3D pose directly from more expressive image representations. Rosales and Sclaroff regress 3D pose from silhouette images with the specialized mappings architecture [2000], Agarwal and Triggs with linear regression <ref type="bibr">[2006]</ref>, and Elgammal and Lee through a joint embedding of images and 3D pose <ref type="bibr">[2004]</ref>. Sminchisescu further utilized temporal consistency to propagate pose probabilities with a Bayesian mixture of experts Markov model <ref type="bibr">[2007]</ref>. Relying on the recent advances in machine learning techniques and compute capabilities, approaches for direct 3D pose regression from the input image have been proposed, using structured learning of latent pose <ref type="bibr" target="#b49">[Li et al. 2015a;</ref><ref type="bibr" target="#b93">Tekin et al. 2016a</ref>], joint prediction of 2D and 3D pose <ref type="bibr" target="#b48">[Li and Chan 2014;</ref><ref type="bibr" target="#b94">Tekin et al. 2016b;</ref><ref type="bibr" target="#b105">Yasin et al. 2016]</ref>, transfer of features from 2D datasets <ref type="bibr" target="#b55">[Mehta et al. 2016]</ref>, novel pose space formulations <ref type="bibr" target="#b67">[Pavlakos et al. 2016]</ref> and classification over example poses <ref type="bibr" target="#b70">[Pons-Moll et al. 2014;</ref><ref type="bibr" target="#b78">Rogez and Schmid 2016]</ref>. Relative per-bone predictions <ref type="bibr" target="#b48">[Li and Chan 2014]</ref>, kinematic skeleton models , or root centered joint positions <ref type="bibr" target="#b41">[Ionescu et al. 2014a</ref>] are used as the eventual output space. Such direct 3D pose regression methods capture depth relations well, but 3D estimates usually do not accurately match the true 2D location when re-projected to the image, because estimations are done in cropped images that lose camera perspective effects, using a canonical height, and minimize 3D loss instead of projection to 2D. Furthermore, they only deliver joint positions, are temporally unstable, and none has shown realtime performance. We propose a method to combine 2D and 3D estimates in real-time along with temporal tracking. It is inspired by the method of Tekin et al. <ref type="bibr">[2016c]</ref>, where batches of frames are processed offline after motion compensation, and is related to the recently proposed per-frame combination of 2D and 3D pose <ref type="bibr" target="#b94">[Tekin et al. 2016b]</ref>.</p><p>Notably, only few methods target real-time monocular reconstruction. Exceptions are the regression of 3D pose from Haar features by <ref type="bibr" target="#b12">Bissacco et al. [2007]</ref> and detection of a set of discrete poses from edge direction histograms in the vicinity of the previous frame pose <ref type="bibr" target="#b91">[Taycher et al. 2006</ref>]. Both only obtain temporally unstable, coarse pose, not directly usable in our applications. Chai and Hodgins obtain sufficient quality to drive virtual avatars in real-time, but require visual markers <ref type="bibr" target="#b20">[Chai and Hodgins 2005]</ref>. The use of CNNs in real time has been explored for variants of the object detection problem, for instance bounding box detection and pedestrian detection methods have leveraged application specific architectures <ref type="bibr" target="#b6">[Angelova et al. 2015;</ref><ref type="bibr" target="#b52">Liu et al. 2016;</ref><ref type="bibr" target="#b72">Redmon et al. 2015</ref>] and preprocessing steps <ref type="bibr" target="#b73">[Ren et al. 2015]</ref>.</p><p>In a similar vein, we propose a 3D pose estimation approach that leverages a novel fully-convolutional CNN formulation to predict 2D and 3D pose jointly. In combination with inexpensive preprocessing and an optimization based skeletal fitting method, it enables high accuracy pose estimation, while running at more than 30 Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW</head><p>Our system is capable of obtaining a temporally consistent, full 3D skeletal pose of a human from a monocular RGB camera. Estimating 3D pose from a single RGB camera is a challenging, underconstrained problem with inherent ambiguities. <ref type="figure">Figure 2</ref> provides an overview of our method to tackle this challenging problem. It consists of two primary components. The first is a convolutional neural network (CNN) to regress 2D and 3D joint positions under the ill-posed monocular capture conditions. It is trained on annotated 3D human pose datasets <ref type="bibr" target="#b42">[Ionescu et al. 2014b;</ref><ref type="bibr" target="#b55">Mehta et al. 2016]</ref>, additionally leveraging annotated 2D human pose datasets <ref type="bibr" target="#b45">Johnson and Everingham 2010]</ref> for improved in-the-wild performance. The second component combines the regressed joint positions with a kinematic skeleton fitting method to produce a temporally stable, camera-relative, full 3D skeletal pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Pose Regression:</head><p>The core of our method is a CNN that predicts both 2D, and root (pelvis) relative 3D joint positions in real-time. The new proposed fully-convolutional pose formulation leads to results on par with the state-of-the-art offline methods in 3D joint position accuracy (see Section 5.2 for details). Being fully-convolutional, it can operate in the absence of tight crops around the subject. The CNN is capable of predicting joint positions for a diverse class of activities regardless of the scene settings, providing a strong basis for further pose refinement to produce temporally consistent full-3D pose parameters Kinematic Skeleton Fitting: The 2D and the 3D predictions from the CNN, together with the temporal history of the sequence, can be leveraged to obtain temporally consistent full-3D skeletal pose, with the skeletal root (pelvis) localized in camera space. Our approach uses an optimization function that: (1) combines the predicted 2D and 3D joint positions to fit a kinematic skeleton in a least squares sense, (2) ensures temporally smooth tracking over time. We further improve the stability of the tracked pose by applying filtering steps at different stages.</p><p>Skeleton Initialization (Optional): The system is set up with a default skeleton which works well out of the box for most humans. For more accurate estimates, the relative body proportions of the underlying skeleton can be adapted to that of the subject, by averaging CNN predictions for a few frames at the beginning. Since monocular reconstruction is ambiguous without a scale reference, the CNN predicts height normalized 3D joint positions. Users only need to provide their height (distance from head to toe) once, so that we can track the 3D pose in true metric space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REAL-TIME MONOCULAR 3D POSE ESTIMATION</head><p>In this section, we describe in detail the different components of our method to estimate a temporally consistent 3D skeletal motion from monocular RGB input sequences. As input, we assume a continuous stream of monocular RGB images {..., I t −1 , I t }. For frame t in the input stream, the final output of our approach is P G t which is the full global 3D skeletal pose of the person being tracked. Because this output is already temporally consistent and in global 3D space, it can be readily used in applications such as character control. We use the following notation for the output in the intermediate components of our method. The CNN pose regressor jointly estimates the 2D joint positions K t and root-relative 3D joint positions P L t (Section 4.1). The 3D skeleton fitting component combines the 2D and 3D joint position predictions to estimate a smooth, temporally consistent pose P G t (θ, d), which is parameterized by the global position d in camera space, and joint angles θ of the kinematic skeleton S. J indicates the number of joints. We drop the frame-number subscript t in certain sections to aid readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CNN Pose Regression</head><p>The goal of CNN pose regression is to obtain joint positions, both, in 2D image space and 3D. For 2D pose estimation with neural nets, the change of formulation from direct regression of x, y body-joint coordinates <ref type="bibr" target="#b97">[Toshev and Szegedy 2014</ref>] to a heatmap based bodyjoint detection formulation <ref type="bibr" target="#b96">[Tompson et al. 2014</ref>] has been the key driver behind the recent developments in 2D pose estimation. The heatmap based formulation naturally ties image evidence to pose estimation by predicting a confidence heatmap H j,t over the image plane for each joint j ∈ {1..J }.</p><p>Existing approaches to 3D pose estimation lack such an imageto-prediction association, often directly regressing the root-relative joint locations <ref type="bibr" target="#b41">[Ionescu et al. 2014a</ref>], leading to predicted poses whose extent of articulation doesn't reflect that of the person in the image. See <ref type="figure">Figure 9</ref>. Treating pose as a vector of joint locations also causes a natural gravitation towards networks with fully-connected formulations <ref type="bibr" target="#b55">[Mehta et al. 2016;</ref><ref type="bibr" target="#b78">Rogez and Schmid 2016;</ref><ref type="bibr" target="#b93">Tekin et al. 2016a;</ref><ref type="bibr" target="#b107">Yu et al. 2016]</ref>, restricting the inputs to tight crops at a fixed resolution, a limitation that needs to be overcome. These methods assume the availability of tight bounding boxes, which necessitates supplementation with separate bounding box estimators for actual usage, which further adds to the run-time of these methods. The fully-convolutional formulation of Pavlakos et al. <ref type="bibr" target="#b67">[Pavlakos et al. 2016]</ref> seeks to alleviate some of these issues, but is limited by the expensive per-joint volumetric formulation, which still relies on cropped input and does not scale well to larger image sizes.</p><p>We overcome these limitations through our new formulation, by extending the 2D heatmap formulation to 3D using three additional location-maps X j , Y j , Z j per joint j, capturing the root-relative locations x j , y j and z j respectively. To have the 3D pose prediction linked more strongly to the 2D appearance in the image, the x j , y j and z j values are read off from their respective location-maps at the position of the maximum of the corresponding joint's 2D heatmap H j , and stored in P L = {x, y, z}, where x ∈ R 1×J is a vector that stores the coordinate x location of each joint maximum. The pose formulation is visualized in <ref type="figure">Figure 3</ref>. Networks using this fully-convolutional formulation are not constrained in input image size, and can work without tight crops. Additionally, the network provides 2D and 3D joint location estimates without additional overhead, which we exploit in subsequent steps for real-time estimation. Section 5.2 shows the improvements afforded by this formulation.</p><p>Loss Term: To enforce the fact that we are only interested in x j , y j and z j from their respective maps at joint j's 2D location, the joint location-map loss is weighted stronger around the joint's 2D <ref type="figure">Fig. 3</ref>. Schema of the fully-convolutional formulation for predicting root relative joint locations. For each joint j, the 3D coordinates are predicted from their respective location-maps X j , Y j , Z j at the position of the maximum in the corresponding 2D heatmap H j . The structure observed here in the location-maps emerges due to the spatial loss formulation. See Section 4.1.</p><p>location. We use the L2 loss. For x j is the loss formulation is</p><formula xml:id="formula_0">Loss(x j ) = ∥H GT j ⊙ (X j − X GT j )∥ 2 ,<label>(1)</label></formula><p>where GT indicates ground truth and ⊙ is the Hadamard product. The location maps are weighted with the respective ground truth 2D heatmap H GT j , which in turn have confidence equal to a Gaussian with a small support localized at joint j's 2D location. Note that no structure is imposed on the location-maps. The structure that emerges in the predicted location-maps is indicative of the correlation of x j and y j with root relative location of joint j in the image plane. See <ref type="figure">Figure 3</ref>.</p><p>Network Details: We use the proposed formulation to adapt the ResNet50 network architecture of <ref type="bibr" target="#b35">He et al. [2016]</ref>. We replace the layers of ResNet50 from res5a onwards with the architecture depicted in <ref type="figure">Figure 5</ref>, producing the heatmaps and location-maps for all joints j ∈ {1..J }. After training, the Batch Normalization <ref type="bibr" target="#b40">[Ioffe and Szegedy 2015]</ref> layers are merged with the weights of their preceding convolution layers to improve the speed of the forward pass.</p><p>Intermediate Supervision: We predict the 2D heatmaps and 3D location-maps from the features at res4d and res5a, tapering down the weights of intermediate losses with increasing iteration count. Additionally, similar to the root-relative location-maps X j , Y j and Z j , we predict kinematic parent-relative location-maps ∆X j , ∆Y j and ∆Z j from the features at res5b and compute bone length-maps as:</p><formula xml:id="formula_1">BL j = ∆X j ⊙ ∆X j + ∆Y j ⊙ ∆Y j + ∆Z j ⊙ ∆Z j .<label>(2)</label></formula><p>These intermediate predictions are subsequently concatenated with the intermediate features, to give the network an explicit notion of bone lengths to guide the predictions. See <ref type="figure">Figure 5</ref>. Experiments showed that the deeper variants of ResNet offer only small gains for a substantial increase (1.5×) in computation time, prompting us to choose ResNet50 to enable real-time, yet highly accurate joint location estimation with the proposed formulation.</p><p>Training: The network is pretrained for 2D pose estimation on MPII  and LSP <ref type="bibr">Everingham 2010, 2011]</ref> to allow superior in-the-wild performance, as proposed by <ref type="bibr" target="#b55">Mehta et al. [Mehta et al. 2016]</ref>. For 3D pose, we use MPI-INF-3DHP <ref type="bibr" target="#b55">[Mehta et al. 2016]</ref> and Human3.6m <ref type="bibr" target="#b42">[Ionescu et al. 2014b</ref>]. We take training sequences for all subjects except S9 and S11 from Human3.6m. We sample frames as per <ref type="bibr" target="#b41">[Ionescu et al. 2014a</ref>]. For MPI-INF-3DHP, we consider all 8 training subjects. We choose sequences from all 5 chest-high cameras, 2 head-high cameras (angled down) and 1 knee-high camera (angled up) to learn some degree of invariance to the camera viewpoint. The sampled frames have at least one joint move by &gt; 200mm between them. We use various combinations of background, occluder (chair), upper-body clothing and lower-body clothing augmentation for 70% of the selected frames. We train with person centered crops, and use image scale augmentation at 2 scales (0.7×, 1.0×), resulting in 75k training samples for Human3.6m and 100k training samples for MPI-INF-3DHP. <ref type="figure" target="#fig_0">Figure  4</ref> shows a few representative frames of training data. In addition to the 17 joints typically considered, we use foot tip positions. The ground truth joint positions are with respect to a height normalized skeleton (knee-neck height 92 cm). We make use of the Caffe [2014] framework for training, and use the Adadelta [Zeiler 2012] solver with learning rate tapered down with increasing iterations.</p><p>Bounding Box Tracker: Existing offline solutions process each frame in a separate person-localization and bounding box (BB) cropping step <ref type="bibr" target="#b55">[Mehta et al. 2016;</ref><ref type="bibr" target="#b95">Tekin et al. 2016c]</ref> or assume bounding boxes are available <ref type="bibr" target="#b48">[Li and Chan 2014;</ref><ref type="bibr" target="#b50">Li et al. 2015b;</ref><ref type="bibr" target="#b67">Pavlakos et al. 2016;</ref><ref type="bibr" target="#b93">Tekin et al. 2016a;</ref>. Although our fullyconvolutional formulation allows the CNN to work without requiring cropping, the run-time of the CNN is highly dependent on the input image size. Additionally, the CNN is trained for subject sizes in the range of 250-340 px in the frame, requiring averaging of predictions at multiple image scales per frame (scale space search) if processing the full frame at each time step. Guaranteeing real-time rates necessitates restricting the size of the input to the network and tracking the scale of the person in the image to avoid searching the scale space in each frame. We do this in an integrated way. The 2D pose predictions from the CNN at each frame are used to determine the BB for the next frame through a slightly larger box around the predictions. The smallest rectangle containing the keypoints K is computed and augmented with a buffer area 0.2× the height vertically and 0.4× the width horizontally. To stabilize the estimates, <ref type="figure">Fig. 5</ref>. Network Structure. The structure above is preceded by ResNet50/100 till level 4. We use kinematic parent relative 3D joint location predictions ∆X, ∆Y, ∆Z as well as bone length maps BL constructed from these as auxiliary tasks.The network predicts 2D location heatmaps H and root relative 3D joint locations X, Y, Z. Refer to Section 4.1.</p><p>the BB is shifted horizontally to the centroid of the 2D predictions, and its corners are filtered with a weighted average with the previous frame's BB using a momentum of 0.75. To normalize scale, the BB crop is resized to 368x368 px. The BB tracker starts with (slow) multi-scale predictions on the full image for the first few frames, and hones in on the person in the image making use of the BB-agnostic predictions from the fully convolutional network. The BB tracking is easy to implement and without runtime overhead, since the proposed fully-convolutional network outputs 2D and 3D pose jointly and operates on arbitrary input sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Kinematic Skeleton Fitting</head><p>Applying per-frame pose estimation techniques on a video does not exploit and ensure temporal consistency of motion, and small pose inaccuracies lead to temporal jitter, an unacceptable artifact for most graphics applications. We combine the 2D and 3D joint positions in a joint optimization framework, along with temporal filtering and smoothing, to obtain an accurate, temporally stable and robust result. First, the 2D predictions K t are temporally filtered <ref type="bibr" target="#b19">[Casiez et al. 2012</ref>] and used to obtain the 3D coordinates of each joint from the location-map predictions, giving us P L t . To ensure skeletal stability, the bone lengths inherent to P L t are replaced by the bone lengths of the underlying skeleton in a simple retargeting step that preserves the bone directions of P L t . The resulting 2D and 3D predictions are combined by minimizing the objective energy</p><formula xml:id="formula_2">E total (θ, d) = E IK (θ, d) + E proj (θ, d) + E smooth (θ, d) + E depth (θ, d),<label>(3)</label></formula><p>for skeletal joint angles θ and the root joint's location in camera space d. The 3D inverse kinematics term E IK determines the overall pose by similarity to the 3D CNN output P L t . The projection term E proj determines global position d and corrects the 3D pose by re-projection onto the detected 2D keypoits K t . Both terms are implemented with the L2 loss, E proj = ∥Π(P G Parameters: The energy terms E IK , E proj , E smooth and E depth are weighted with ω IK = 1, ω proj = 44, ω smooth = 0.07 and ω depth = 0.11, respectively. The parameters of the 1 Euro Filter <ref type="bibr" target="#b19">[Casiez et al. 2012]</ref> are empirically set to f c min = 1.7, β = 0.3 for filtering K t , to f c min = 0.8, β = 0.4 for P L t , and to f c min = 20, β = 0.4 for filtering P G t . Our implementation uses the Levenberg-Marquardt algorithm from the Ceres library <ref type="bibr" target="#b1">[Agarwal et al. 2017</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We show live applications of our system at 30 Hz. The reconstruction quality is high and we demonstrate the usefulness of our method 3D character control, embodied virtual reality, and pose tracking from low quality smartphone camera streams. See Section 5.3 and <ref type="figure">Figure 1</ref>. Results are best observed in motion in the supplemental video. The importance of the steps towards enabling these applications with a video solution are thoroughly evaluated in more than 10 sequences. Results are comparable in quality to depth-camera based solutions like the Kinect [Microsoft Corporation 2013] and significantly outperform existing monocular video-based solutions. As the qualitative baseline we choose the state-of-the-art 2D to 3D lifting approach of  and the 3D regression approach of <ref type="bibr" target="#b55">Mehta et al. [2016]</ref>, which estimate joint-positions offline. The accuracy improvements are further quantitatively validated on the established H3.6M dataset <ref type="bibr" target="#b42">[Ionescu et al. 2014b</ref>] and the MPI-INF-3DHP dataset <ref type="bibr" target="#b55">[Mehta et al. 2016]</ref>. The robustness to diverse persons, clothing and scenes is demonstrated on several real-time examples and community videos. Please see our project webpage for more results and details 1 .</p><p>Computations are performed on a 6-core Xeon CPU, 3.8 GHz and a single Titan X (Pascal architecture) GPU. The CNN computation takes ≈18 ms, the skeleton fitting ≈7-10 ms, and preprocessing and filtering 5 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with Active Depth Sensors (Kinect)</head><p>We synchronously recorded video from an RGB camera and a colocated Kinect sensor in a living room scenario. <ref type="figure">Figure 6</ref> shows representative frames. Although the depth sensor provides additional information, our reconstructions from just RGB are of a similar quality. The Kinect results are of comparable stability to ours, but yield erroneous reconstructions when limbs are close to scene objects, such as when sitting down. Our RGB solution, however, succeeds in this case, although is slightly less reliable in depth estimation. A challenging case for both methods is the tight crossing of legs. Please see the supplemental video for a visual comparison.</p><p>The video solution succeeds also in situations with direct sunlight <ref type="figure" target="#fig_1">(Figure 7)</ref>, where IR-based depth cameras are inoperable. Moreover, RGB cameras can simply be equipped with large field-of-view (FOV) lenses and, despite strong distortions, successfully track humans <ref type="bibr" target="#b74">[Rhodin et al. 2016a</ref>]. On the other hand, existing active sensors are limited to relatively small FOVs, which severely limits the tracking volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Video Solutions</head><p>Qualitative Evaluation: We qualitatively compare against the 3D pose regression method of <ref type="bibr" target="#b55">Mehta et al. [2016]</ref> and  on Sequence 6 (outdoor) of MPI-INF-3DHP test set. Our results are comparable to the quality of these offline methods (see <ref type="figure" target="#fig_3">Figure 10</ref>). However, the per frame estimates of these offline methods exhibits jitter over time, a drawback of most existing solutions. Our full pose results are temporally stable and are computed at real-time frame rate of 30 Hz.</p><p>The kinematic skeleton fitting estimates global translation d. <ref type="figure" target="#fig_2">Figure 8</ref> demonstrates that estimates are drift-free, the feet position matches with the same reference point after performing a circular walk. The smoothness constraint in depth direction limits sliding of the character away from the character, as pictured in the supplemental video sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Evaluation:</head><p>We compare our method with the stateof-the-art approach of <ref type="bibr" target="#b55">Mehta et al. [2016]</ref> on the MPI-INF-3DHP dataset, using the more robust Percentage of Correct Keypoints metric (3D PCK @150mm) on the 14 joints spanned by head, neck, shoulders, elbow, wrist, hips, knees and ankles. We train both, our model, as well as that of Mehta et al. on the same data (Human3.6m + MPI-INF-3DHP), as detailed in Section 4.1, to be compatible in terms of the camera viewpoints selected, and use ResNet100 as the base architecture for a fair comparison. <ref type="table" target="#tab_0">Table 1</ref> shows the results of the raw 3D predictions from our network on ground-truth bounding box cropped frames. We see that the results are comparable to that of Mehta et al. The slight increase in accuracy on going to a 1 http://gvv.mpi-inf.mpg.de/projects/VNect/ <ref type="figure">Fig. 6</ref>. Side-by-side pose comparison with our method (top) and Kinect (bottom). Overall estimated poses are of similar quality (first two frames). Both the Kinect (third and fourth frames) and our approach (fourth and fifth frames) occasionally predict erroneous poses.  50-layer network is possibly due to the better gradient estimates coming from larger mini-batches that can be fit into memory while training, on account of the smaller size of the network. Evidence that our method ties the estimated 3D positions strongly to image appearance than previous methods can also be gleaned from the fact that our approach performs significantly better for activity classes such as Standing/Walking, Sports and Miscellaneous without significant self-occlusions. We do lose some performance on activity classes with significant self-occlusion such as Sitting/Lying on the floor. We additionally report the Mean Per Joint Position Error (MPJPE) numbers in mm. Note that MPJPE is not a robust measure, and is heavily influenced by large outliers, and hence the worse performance on the MPJPE measure (124.7mm vs 117.6mm) despite the better 3D PCK results (76.6% vs 75.7%).</p><p>We further investigate the nature of errors of our method. We first look at the joint-wise breakup of accuracy of our fully-convolutional  <ref type="figure">9</ref>. A visual look at the direct 3D predictions resulting from our fullyconvolutional formulation vs Mehta et al. Our formulation allows the predictions to be more strongly tied to image evidence, leading to overall better pose quality, particular for the end effectors. The red arrows point to mispredictions.   <ref type="figure" target="#fig_4">Figure 11</ref> shows that the accuracy of ankles for our formulation is significantly better, while the accuracy of the head is markedly worse.</p><p>In <ref type="figure">Figure 9</ref>, we visually compare the two methods, further demonstrating the strong tie-in to image appearance that our formulation affords, and the downsides of the strong tie-in. We also show that our method is prone to occasional large mispredictions when the body joint 2D location detector misfires. It is these large outliers that obfuscate the reported MPJPE numbers. <ref type="figure" target="#fig_5">Figure 12</ref>, which plots the fraction of mispredicted joints vs. the error threshold on MPI-INF-3DHP test set shows that our method has a higher fraction of perjoint mispredictions beyond 300mm. It explains the higher MPJPE numbers compared to Mehta et al. despite equivalent PCK performance. The various filtering stages employed in the full pipeline ameliorate these large mispredictions.</p><p>For Human3.6m, we follow the protocol as in earlier work <ref type="bibr" target="#b67">[Pavlakos et al. 2016;</ref><ref type="bibr">Tekin et al. 2016b,c]</ref>, and evaluate on all actions and cameras for subject number 9 and 11, and report Mean Per Joint Position Error (mm) for root relative 3D joint positions from our network. See <ref type="table">Table 3</ref>. Note that despite the occasional large outliers affecting the MPJPE measure, our predictions are still better than most of the existing methods.  The accuracy attained from single view methods is still below that of real-time multi-view methods, which can achieve a mean accuracy of the order of 10mm <ref type="bibr" target="#b90">[Stoll et al. 2011</ref>].</p><p>Generalization to Different Persons and Scenes: We tested our method on a variety of actors, it succeeds for different body shapes, gender and skin tone. See supplemental video. To further validate the robustness we applied the methods to community videos from YouTube, see <ref type="figure">Figure 1</ref>. It generalizes well to the different backgrounds and camera types.</p><p>Model Components: To demonstrate that our fully-convolutional pose formulation is less sensitive to inexact cropping than networks using a fully-connected formulation, we emulate a noisy BB estimator by jittering the ground-truth bounding box corners of MPI-INF-3DHP test set uniformly at random in the range of +/-40 px. This also captures scenarios where one or more end effectors are not in the frame, so a loss in accuracy is expected for all methods. <ref type="table" target="#tab_1">Table 2</ref> shows that the fully-connected formulation of Mehta et al. suffers a worse hit in accuracy than our approach, going down by 7.9 PCK, while our comparable network goes down by only 5.5 PCK.</p><p>We show the effect of the various components of our full pipeline on the TS1 sequence of MPI-INF-3DHP test set in <ref type="figure">Figure 13</ref>. Without the E IK component of E total the tracking accuracy goes down to a <ref type="figure">Fig. 13</ref>. Fraction of joints correctly predicted on the TS1 sequence of MPI-INF-3DHP test set, as determined by the distance between the predicted joint location and the ground truth joint location being below the error threshold. The dotted line marks the 150mm threshold for which the 3D PCK numbers are reported. We see that only using the 2D predictions as constraints for skeleton fitting (blue) performs significantly worse than using both 2D and 3D predictions as constraints (red). Though adding 1 Euro filtering (purple) visually improves the results, the slightly higher error here is due to the sluggish recovery from tracking failures. The 3D predictions from the CNN (green) are also shown.</p><p>PCK of 46.1% compared to a PCK of 81.7% when E IK is used. The raw CNN 3D predictions in conjunction with the BB tracker result in a PCK of 80.3%. Using E IK in E total produces consistently better results for all thresholds lower than 150 mm. This shows the improvements brought about by our skeleton fitting term. Additionally, as shown in the supplementary video, using 1 Euro filtering produces qualitatively better results, but the overall PCK decreases slightly (79.7%) due to slower recovery from tracking failures. The influence of the smoothness and filtering steps on the temporal consistency are further analyzed in the supplemental video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Applications</head><p>Our approach is suitable for various interactive applications since it is real-time, temporally stable, fully automatic, and exports data directly in a format amenable to 3D character control.</p><p>Character Control: Real-time motion capture solutions provide a natural interface for game characters and virtual avatars, which go beyond classical mouse and gamepad control. We applied our method on motions common in activities like tennis, dance, and juggling, see <ref type="figure" target="#fig_0">Figures 1 and 14</ref>. The swing of the arm and leg motion is nicely captured and could, for instance, be used in a casual sports and dancing game, but also for motion analysis of professional athletes to optimize their motion patterns. We also show successful results in non front-facing motions such as turning and writing on a wall, as well as squatting.</p><p>Virtual Reality: The recent availability of cheap head-mounted displays has sparked a range of new applications. Many products use handheld devices to track the user's hand position for interaction. Our solution enables them from a single consumer color camera. Beyond interaction, our marker-less full-body solution enables embodied virtual reality, see <ref type="figure">Figure 1</ref>. A rich immersive feeling is created by posing a virtual avatar of the user exactly to their own real pose. With our solution the real and virtual pose are aligned such that users perceive the virtual body as their own. <ref type="table">Table 3</ref>. Results of our raw CNN predictions on Human3.6m, evaluated on the ground truth bounding box crops for all frames of Subject 9 and 11. Our CNNs use only Human3.6m as the 3D training set, and are pretrained for 2D pose prediction. The error measure used is Mean Per Joint Position Error (MPJPE) in millimeters. Note again that the error measure used is not robust, and subject to obfuscation from occasional large mispredictions, such as those exhibited by our raw CNN predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sit</head><p>Take  Ubiquitous Motion Capture with Smartphones: Real-time monocular 3D pose estimation lends itself to application on low quality smartphone video streams. By streaming the video to a machine with sufficient capabilities for our algorithm, one can turn any smartphone into a lightweight, fully-automatic, handheld motion capture sensor, see <ref type="figure">Figure 15</ref> and the accompanying video. Since smartphones are widespread, it enables the aforementioned applications for casual users without requiring additional sensing devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS</head><p>Depth estimation from a monocular image is severely ill posed, slight inaccuracies in the estimation can lead to largely different depth estimates, which manifests also in our results in slight temporal jitter. We claim improved stability and temporal consistency compared to existing monocular RGB 3D pose estimation methods. This uncertainty could be further reduced with domain specific knowledge, e.g., foot-contact constraints when the floor location is known, and head-pose stabilization with the position of headmounted-displays in VR applications, which is readily obtained with IMU-sensors. A downside of our CNN prediction formulation is that mispredictions of 2D joint locations result in implausible 3D poses. This is ameliorated in the tracker through skeleton retargeting and pose filtering. This could be addressed directly in the CNN through imposition of stronger interdependencies between predictions. Additionally, the performance on poses with significant amounts of self occlusion remains a challenge.</p><p>Further, very fast motions can exceed the convergence radius of our IK optimization, but the integration of per frame 2D and 3D pose yields quick recovery from erroneous poses. Initial experiments with 256 × 256 px input to the CNN show that much higher frame rates are possible with no loss in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>The availability of sufficient annotated 3D pose training data remains an issue. Even the most recent annotated real 3D pose data sets, or combined real/synthetic data sets <ref type="bibr" target="#b21">[Chen et al. 2016;</ref><ref type="bibr" target="#b42">Ionescu et al. 2014b;</ref><ref type="bibr" target="#b55">Mehta et al. 2016</ref>] are a subset of real world human pose, shape, appearance and background distributions. Recent top performing methods explicitly address this data sparsity by training <ref type="figure">Fig. 15</ref>. Handheld recording with a readily available smartphone camera (left) and our estimated pose (right), streamed to and processed by a GPU enabled PC. similarly deep networks, but with architectural changes enabling improved intermediate training supervision <ref type="bibr" target="#b55">[Mehta et al. 2016]</ref>.</p><p>Our implementation only supports a single person, although the proposed fully-convolutional formulation could be scaled to multiple persons. Such an extension is currently precluded due to the lack of multi-person datasets, required to train multi-person 3D pose regressors. One possible approach is to adapt the multi-person 2D pose methods of  and <ref type="bibr" target="#b18">Cao et al. [2016]</ref>.</p><p>We also analyze the impact of 2D joint position mispredictions on the 3D joint position predictions from our fully-convolutional formulation. We decouple the 3D predictions from the 2D predictions by looking up the 3D joint positions from their location-maps using the ground truth 2D joint positions. See <ref type="table">Table 4</ref>. We see a 3D PCK improvement of 2.8, which is congruent with the notion of a stronger tie-in of the predicted joint positions with the image plane, which causes the 3D joint predictions to be erroneous when 2D joint detection misfires. The upside of this is that the 3D predictions can be improved through improvements to 2D joint position prediction. Alternatively, optimization formulations that directly operate on the heatmaps and the location-maps could be constructed. Our fully-convolutional formulation can also benefit from iterative refinement, akin to heatmap-based 2D pose estimation approaches <ref type="bibr" target="#b37">[Hu et al. 2016;</ref><ref type="bibr" target="#b63">Newell et al. 2016</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We have presented the first method that estimates the 3D kinematic pose of a human, including global position, in a stable, temporally consistent manner from a single RGB video stream at 30 Hz. Our approach combines a fully-convolutional CNN that regresses 2D and 3D joint positions and a kinematic skeleton fitting method, producing a real-time temporally stable 3D reconstruction of the motion. In contrast to most existing approaches, our network can operate without strict bounding boxes, and facilitates inexpensive bounding box tracking. We have shown results in a variety of challenging real-time scenarios, including live streaming from a smartphone <ref type="table">Table 4</ref>. Results on MPI-INF-3DHP test set with the 3D joint position lookup in the location-maps done using the ground truth 2D locations rather the predicted 2D locations. We see that the location maps have captured better 3D pose information, which can perhaps be extracted through optimization methods operating directly on heatmaps and location-maps. The evaluation uses 2 scales (0.7, 1.0). camera, as well as in community videos. A number of applications have been presented, such as embodied VR and interactive character control for computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stand</head><p>Qualitative and quantitative evaluations demonstrate that our approach compares to offline state-of-the-art monocular RGB methods and approaches the quality of real-time RGB-D methods. Hence, we believe our method is a significant step forward to democratizing 3D human pose estimation, lifting both the need for special cameras such as the IR-based depth cameras, as well as the long and heavy processing times.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Representative training frames from Human3.6m and MPI-INF-3DHP 3D pose datasets. Also shown are the background, clothing and occluder augmentations done on MPI-INF-3DHP training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 7 .</head><label>7</label><figDesc>Our approach succeeds in strong illumination and sunlight (center right and right), while the IR-based depth estimates of the Microsoft Kinect are erroneous (left) and depth-based tracking fails (center left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 8 .</head><label>8</label><figDesc>The estimated 3D pose is drift-free. The motion of the person starts and ends at the marked point (orange), both in the real world and in our reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 10 .</head><label>10</label><figDesc>Side-by-side comparison of our full method (left), against the offline joint-position estimation methods of Mehta et al. [2016] (middle) and Zhou et al. [2015] (right). Our real-time results are of a comparable quality to these offline methods. 2D joint positions for Zhou et al. are generated using Convolutional Pose Machines [2016].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 .</head><label>11</label><figDesc>Joint-wise breakdown of the accuracy of Mehta et al. and Our ResNet100 based CNN predictions on MPI-INF-3DHP test set. ResNet100 CNN predictions vs Mehta et al. 's formulation with fullyconnected layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 12 .</head><label>12</label><figDesc>Fraction of joints incorrectly predicted on MPI-INF-3DHP test set, as determined by the distance between the predicted joint location and the ground truth joint location being greater than the error threshold. The dotted line marks the threshold for which the 3D PCK numbers are reported. At bottom right we see that our method has larger occasional mispredictions, which result in higher MPJPE numbers despite otherwise similar performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 14 .</head><label>14</label><figDesc>Application to entertainment. The real-time 3D pose estimation method provides a natural motion interface, e.g. for sport games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of our network against state of the art on MPI-INF-3DHP test set, using ground-truth bounding boxes. We report the Percentage of Correct Keypoints measure in 3D, and the Area Under the Curve for the same, as proposed by MPI-INF-3DHP. We additionally report the Mean Per Joint Position Error in mm. Higher PCK and AUC is better, and lower MPJPE is better.</figDesc><table><row><cell></cell><cell></cell><cell>Stand/</cell><cell></cell><cell cols="3">Sit On Crouch/ On the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell>Scales</cell><cell cols="3">Walk Exercise Chair</cell><cell>Reach</cell><cell cols="3">Floor Sports Misc.</cell><cell>Total</cell><cell></cell></row><row><cell></cell><cell></cell><cell>PCK</cell><cell>PCK</cell><cell>PCK</cell><cell>PCK</cell><cell>PCK</cell><cell>PCK</cell><cell cols="3">PCK PCK AUC MPJPE(mm)</cell></row><row><cell>Ours</cell><cell>0.7, 1.0</cell><cell>87.6</cell><cell>76.4</cell><cell>71.4</cell><cell>71.6</cell><cell>47.8</cell><cell>82.5</cell><cell>78.9</cell><cell>75.0 39.5</cell><cell>127.8</cell></row><row><cell>(ResNet 100)</cell><cell>1.0</cell><cell>86.4</cell><cell>72.3</cell><cell>68.0</cell><cell>65.4</cell><cell>40.7</cell><cell>80.5</cell><cell>76.3</cell><cell>71.4 36.9</cell><cell>142.8</cell></row><row><cell>Ours</cell><cell>0.7, 1.0</cell><cell>87.7</cell><cell>77.4</cell><cell>74.7</cell><cell>72.9</cell><cell>51.3</cell><cell>83.3</cell><cell>80.1</cell><cell>76.6 40.4</cell><cell>124.7</cell></row><row><cell>(ResNet 50)</cell><cell>1.0</cell><cell>86.7</cell><cell>73.9</cell><cell>69.8</cell><cell>66.1</cell><cell>44.7</cell><cell>82.0</cell><cell>79.4</cell><cell>73.3 37.8</cell><cell>138.7</cell></row><row><cell>[Mehta et al. 2016]</cell><cell>0.7, 1.0</cell><cell>86.6</cell><cell>75.3</cell><cell>74.8</cell><cell>73.7</cell><cell>52.2</cell><cell>82.1</cell><cell>77.5</cell><cell>75.7 39.3</cell><cell>117.6</cell></row><row><cell>(ResNet 100)</cell><cell>1.0</cell><cell>86.3</cell><cell>72.4</cell><cell>71.5</cell><cell>67.6</cell><cell>49.2</cell><cell>81.0</cell><cell>76.2</cell><cell>73.2 37.8</cell><cell>126.6</cell></row></table><note>Fig.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on MPI-INF-3DHP test set with the bounding box corners randomly jittered between +/-40px to emulate noise from a BB estimator. Our fully-convolutional formulation is more robust than a comparative fully-connected formulation. The evaluation is at a single scale (1.0).</figDesc><table><row><cell></cell><cell>Stand/</cell><cell cols="2">Sit On Crouch/ On the</cell></row><row><cell>Network</cell><cell cols="3">Walk Exerc. Chair Reach Floor Sport Misc.</cell><cell>Total</cell></row><row><cell></cell><cell cols="3">PCK PCK PCK PCK PCK PCK PCK PCK AUC</cell></row><row><cell cols="3">Ours (ResNet 100) 86.0 71.0 65.0</cell><cell>61.1</cell><cell>37.4 78.9 75.5 69.5 35.8</cell></row><row><cell cols="3">Ours (ResNet 50) 84.9 69.4 65.1</cell><cell>61.9</cell><cell>40.8 78.6 77.6 70.1 35.7</cell></row><row><cell cols="3">[Mehta et al. 2016] 81.2 64.2 67.1</cell><cell>62.1</cell><cell>43.5 76.0 71.1 67.8 34.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ) − K t ∥ 2 and E IK = ∥(P G t − d) − P L t ∥ 2 ,(4)where Π is the projection function from 3D to the image plane, and P G t = P G t (θ, d). We assume the pinhole projection model. If the camera calibration is unknown a vertical field of view of 54 degrees is assumed. Temporal stability is enforced with smoothness prior E smooth = ∥ P G t ∥ 2 , penalizing the acceleration P G t . To counteract the strong depth uncertainty in monocular reconstruction, we penalize large variations in depth additionally withE depth = ∥[ P G t ] z ∥ 2 where [ P G t ] z is the z component of 3D velocity P G t .Finally, the 3D pose is also filtered with the 1 Euro filter<ref type="bibr" target="#b19">[Casiez et al. 2012</ref>].</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering 3D human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keir</forename><surname>Mierle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename></persName>
		</author>
		<ptr target="http://ceres-solver.org" />
	</analytic>
	<monogr>
		<title level="j">Ceres Solver</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiview Pictorial Structures for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-Time Pedestrian Detection With Deep Network Cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Abhijit Ogale, and Dave Ferguson</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Data-Driven Approach for Real-Time Full Body Pose Reconstruction from a Depth Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Baak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meinard</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A quantitative evaluation of video-based 3D person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Alexandru O Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detailed human shape and pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Alexandru O Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">E</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haussecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
	<note>Bernt Schiele, Nassir Navab, and Slobodan Ilic</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02914</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Recurrent Human Pose Estimation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast human pose estimation using appearance and motion via multi-dimensional boosting regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Twin gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="28" to="52" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic Estimation of 3D Human Pose and 1:12 • D. Mehta et. al. Shape from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation via Deep Learning from 2D Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Brau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tracking people with twists and exponential maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08050</idno>
		<title level="m">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">1âĆň filter: a simple speed-based low-pass filter for noisy input in interactive systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géry</forename><surname>Casiez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2527" to="2530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance animation from lowdimensional control signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="686" to="696" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Synthesizing Training Images for Boosting Human 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Model-based hand tracking with texture, shading and self-occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Gorce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Articulated body motion capture by stochastic search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Deutscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="185" to="205" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fusion4d: Real-time performance capture of challenging scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Degtyarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">Ryan</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Orts</forename><surname>Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">114</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inferring 3D body pose from silhouettes using activity manifold learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan-Su</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<meeting>the 2004 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">681</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MARCOnI -ConvNet-based MARker-less Motion Capture in Outdoor and Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Edilson De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose search: retrieving people using their pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimization and Filtering for Human Motion Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="75" to="92" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time human pose tracking from range data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Plagemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="738" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dense variational reconstruction of non-rigid surfaces from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1272" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient regression of general-activity human poses from depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="415" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Computing smooth time trajectories for camera and deformable shape in structure from motion with occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">U</forename><surname>Paulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleix M</forename><surname>Gotardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2051" to="2065" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bayesian Reconstruction of 3D Human Motion from Single-Camera Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Nicholas R Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Leventon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="820" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">VolumeDeform: Real-time Volumetric Non-rigid Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Innmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Iterated secondorder label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1661" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MovieReshape: Tracking and Reshaping of Humans in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Thormählen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1145/1866158.1866174</idno>
		<ptr target="https://doi.org/10.1145/1866158.1866174" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
		<meeting>the 22nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12</idno>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning Effective Human Pose Estimation from Inaccurate Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Procrustean normal distribution for non-rigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungchan</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhwai</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1280" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Human Pose Estimation using Deep Consensus Voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ita</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">OpenDR: An approximate differentiable renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Real-time and robust hand tracking with a single depth camera. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1133" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09813v2</idno>
		<title level="m">Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Understanding motion capture for computer animation and video games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Menache</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Morgan kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Kinect for Xbox 360</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft Corporation</surname></persName>
		</author>
		<ptr target="http://www.xbox.com/en-US/xbox-360/accessories/kinect." />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Microsoft Corporation</title>
		<ptr target="http://www.xbox.com/en-US/xbox-one/accessories/kinect." />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Kinect for Xbox One</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<ptr target="https://developer.microsoft.com/en-us/windows/kinect." />
		<title level="m">Microsoft Corporation. 2015. Kinect SDK</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A Survey of Advances in Vision-based Human Motion Capture and Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Krãĳger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="90" to="126" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Recovering 3d human body configurations using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">DynamicFusion: Reconstruction and Tracking of Non-Rigid Scenes in Real-Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Efficient modelbased 3D tracking of hand articulations using Kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iason</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonis A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BmVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Holoportation: Virtual 3D Teleportation in Real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Degtyarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Symposium on User Interface Software and Technology</title>
		<meeting>the 29th Annual Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="741" to="754" />
		</imprint>
	</monogr>
	<note>Mingsong Dou, and others</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">3D reconstruction of a smooth articulated trajectory from a monocular image sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07828</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3487" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2337" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Cristiano Ronaldo and Coentrao continue their recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Real</forename><surname>Madrid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=xqiPuX_buOo." />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">General automatic human shape and motion capture using volumetric contour cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="509" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A Versatile Scene Model With Differentiable Visibility Applied to Generative Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02046</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Garment replacement in monocular video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenz</forename><surname>Rogge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Klose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Specialized mappings and the estimation of human body pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rómer</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Motion</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Combining generative and discriminative models in a framework for articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rómer</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusfencing-Tv</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=0gOcMsWUkCU" />
		<title level="m">The Most Beautiful Strike / Saber Woman</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mat</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Stochastic tracking of 3D human figures using 2D image motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="702" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Looselimbed people: Estimating 3D human pose and motion using non-parametric belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning joint top-down and bottom-up processes for 3D visual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1743" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">BM 3 E: Discriminative Density Propagation for Visual Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2030" to="2044" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Covariance scaled sampling for monocular 3D body tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">447</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Model-based multiple view reconstruction of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Fast articulated motion tracking using a sums of Gaussians body model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Conditional random people: Tracking humans with crfs and grid filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Taycher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Demirdjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Camillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="677" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05708</idno>
		<title level="m">Fusing 2D Uncertainty and 3D Cues for Monocular Body Pose Estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Direct Prediction of 3D Body Poses from Motion Compensated Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Temporal motion models for monocular and multiview 3d human body tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="177" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Video-based 3D motion capture through biped control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Vondrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odest</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions On Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2361" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Convolutional Pose Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Videomocap: modeling physically realistic human motion from monocular video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2010" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Accurate realtime full-body motion capture using a single depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">188</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Pfinder: real-time tracking of the human body</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Christopher Richard Wren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Paul</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="780" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A Dual-Source Approach for 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Real-time simultaneous pose and shape estimation for articulated objects using a single depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2345" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Marker-less 3D Human Motion Capture with Monocular Image Sequence and Height-Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feilinand</forename><surname>Yonghao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Yilin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">3D shape estimation from 2D landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4447" to="4455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Deep Kinematic Pose Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Worktp on Geometry Meets Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04309</idno>
		<title level="m">Spyridon Leonardos, and Kostas Daniilidis. 2015a. Sparse Representation for 3D Shape Estimation: A Convex Relaxation Approach</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Kosta Derpanis, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">3D motion reconstruction for realworld camera motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Real-time Non-rigid Reconstruction using an RGB-D Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Loop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
