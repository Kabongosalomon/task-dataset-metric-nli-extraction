<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantically Adaptive Image-to-image Translation for Domain Adaptation of Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Musto</surname></persName>
							<email>luigi.musto@studenti.unipr.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Parma Parma</orgName>
								<address>
									<region>IT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Zinelli</surname></persName>
							<email>andrea.zinelli1@studenti.unipr.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Parma Parma</orgName>
								<address>
									<region>IT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantically Adaptive Image-to-image Translation for Domain Adaptation of Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>MUSTO, ZINELLI: SEMANTICALLY ADAPTIVE IMAGE-TO-IMAGE TRANSLATION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain shift is a very challenging problem for semantic segmentation. Any model can be easily trained on synthetic data, where images and labels are artificially generated, but it will perform poorly when deployed on real environments. In this paper, we address the problem of domain adaptation for semantic segmentation of street scenes. Many state-of-the-art approaches focus on translating the source image while imposing that the result should be semantically consistent with the input. However, we advocate that the image semantics can also be exploited to guide the translation algorithm. To this end, we rethink the generative model to enforce this assumption and strengthen the connection between pixel-level and feature-level domain alignment. We conduct extensive experiments by training common semantic segmentation models with our method and show that the results we obtain on the synthetic-to-real benchmarks surpass the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks for the semantic segmentation of street scenes require to be trained on large and heterogeneous datasets to achieve good accuracy and generalize well. Nevertheless, they still might fail in unseen scenarios and environments (e.g. because of adverse weather). Collecting and manually annotating datasets which can cover all these scenarios requires a huge effort, since the cost of per-pixel labeling is too high.</p><p>Simulators, instead, allow to generate unlimited labeled data with low effort. Driving simulators, for example, only require to setup the needed scene and to drive in it to collect the required data. Despite the advances and the photorealism of modern computer graphics, simulators still fail at generating images visually similar to the real ones, which is why models trained naively on such kind of data perform poorly when deployed in the real world.</p><p>This setting falls in the more general problem of Domain Adaptation: we have access to two domains, source and target, and we want to exploit the source domain to maximize the accuracy in the target domain, for a given task. When we do not have access to the target labels, but only source ones, we call this Unsupervised Domain Adaptation (UDA). In our case we can formalize the source and target domains to be a synthetic and real dataset. c 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.  <ref type="figure">Figure 1</ref>: Core idea of our image-to-image translation system. We use the segmentation network M to get the semantic map M(X S ) from the source image X S . The semantic map acts as guidance for the translation model F S→T , which translates X S to the target domain. The translated image X S→T is then fed to M again and we get M(X S→T ). Finally we impose the cross-domain semantic consistency by using the Symmetric Cross-Entropy Loss L SCE .</p><p>The most recent solutions to this problem adopt a two-steps approach. The first step is to perform image-to-image translation, where a generative model (e.g. CycleGAN <ref type="bibr" target="#b50">[51]</ref>) or a stylization method <ref type="bibr" target="#b7">[8]</ref> is employed to translate the source images to the target domain. The second step involves training the segmentation network on the translated images, where various methods can be employed to align the features extracted in the two domains.</p><p>We focus on improving the first step, making the translation model aware of the task that has to be performed on the resulting images. Different loss functions have been introduced to impose that the task network gives the same result on the two domains <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref>. Here, instead, we rethink the generator architecture itself and design it to condition the image translation according to the predicted classes. This not only enhances the capabilities of the generator, but also strengthens the connection between translation and segmentation, since the generated features are connected to the corresponding class by the network itself.</p><p>Similar to the related work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref> we test our method by adapting both the GTA5 <ref type="bibr" target="#b35">[36]</ref> and SYNTHIA <ref type="bibr" target="#b36">[37]</ref> synthetic datasets to Cityscapes <ref type="bibr" target="#b5">[6]</ref> and show that our results surpass the current state-of-the-art for the commonly used segmentation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work can be split into two cooperating parts: UDA for semantic segmentation and image-to-image translation. Here we separately review the most relevant approaches to these tasks, highlighting our contributions.</p><p>Unsupervised domain adaptation We aim at using synthetic data to perform semantic segmentation on real images, where no labels are available. This can be framed as a problem of UDA, where the main idea is to align the source and target distributions at either feature level, pixel level, or both. This has been applied to image classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> by minimizing the Maximum Mean Discrepancy <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref>, measuring the correlation distance <ref type="bibr" target="#b40">[41]</ref> or with adversarial learning <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel-level Alignment</head><p>Feature-level Alignment <ref type="figure">Figure 2</ref>: Overview of our method. M is the segmentation network, which is shared for all the steps. F is the translation network. D seg is the segmentation discriminator. The losses are detailed in Section 3. We omitted the reconstruction and consistency pipelines, together with the target image discriminator. The pixel-level alignment for T → S is symmetric to S → T . The dashed arrow between M(X T ) and Y SSL T is used to indicate that the pseudo-label generation is performed offline, before feature-level alignment.</p><p>Semantic segmentation is a much more complex task and the first solution for domain adaptation has been proposed in <ref type="bibr" target="#b14">[15]</ref> with global distribution alignment at feature level. Curriculum learning was used in <ref type="bibr" target="#b49">[50]</ref>, where the authors proposed first to learn the global distribution of the image and the local distribution of superpixels, and then train the segmentation network according to these properties. Global feature alignment was also adopted in <ref type="bibr" target="#b41">[42]</ref>, where different discriminators are employed for features at different levels. Other works introduced class-wise adversarial learning <ref type="bibr" target="#b3">[4]</ref> and pseudo-labels <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b51">52]</ref>. CLAN <ref type="bibr" target="#b30">[31]</ref> improves feature level alignment by reweighting the adversarial loss with a discrepancy map based on categories. Feature level alignment, however, is not enough to adapt to different domains, which is why the most recent approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49]</ref> introduced also pixellevel alignment. CyCADA <ref type="bibr" target="#b15">[16]</ref> trains the segmentation network on images translated with CycleGAN <ref type="bibr" target="#b50">[51]</ref> and a semantic consistency loss. DCAN <ref type="bibr" target="#b48">[49]</ref> adopts a custom image-toimage translation network and performs feature alignment both in the translation and in the segmentation step. CrDoCo <ref type="bibr" target="#b4">[5]</ref> uses a cross-domain consistency loss to improve the translation. Similarly BDL <ref type="bibr" target="#b23">[24]</ref> links translation and segmentation with a perceptual loss, where the training is iterated to gradually improve both tasks.</p><p>Our work builds on top of these ideas, but we rethink the generator architecture to condition the image-to-image translation with the semantic guidance of the segmentation network.</p><p>Image-to-image translation In order to translate synthetic images into real looking ones without using paired couples, the most common approach is to use Generative Adversarial Networks <ref type="bibr" target="#b11">[12]</ref>. By learning how to trick the discriminator, the generator network becomes able to generate images aligned with the target distribution.</p><p>Nevertheless, only with the introduction of the cycle consistency <ref type="bibr" target="#b50">[51]</ref> the generated images look realistic. UNIT <ref type="bibr" target="#b25">[26]</ref> develops a more complex assumption: by combining VAEs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35]</ref> with CoGANs <ref type="bibr" target="#b24">[25]</ref>, they enforce that two domains share a common latent space which can be used to move from one domain to the other and back. This approach evolves in MUNIT <ref type="bibr" target="#b17">[18]</ref>, where the shared latent space is formalized as the content and combined with the target style to generate multimodal realistic outputs.</p><p>GTA5 sample GTA5 pred (D) GTA5 → CS (D) GTA5 pred (F) GTA5 → CS (F) <ref type="figure">Figure 3</ref>: Translation from GTA5 <ref type="bibr" target="#b35">[36]</ref> to Cityscapes <ref type="bibr" target="#b5">[6]</ref>. We take a sample from GTA5, get the predicted segmentation using M, and generate X S→T . We present the results obtained with both DeepLabV2 <ref type="bibr" target="#b1">[2]</ref> and FCN8s <ref type="bibr" target="#b27">[28]</ref> used as semantic guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalization layers</head><p>The key insight for image-to-image translation is in the ability to disentangle style and content. In fact, in order to move from one domain to the other, one has to be able to change the style while preserving the image content.</p><p>It has been noted <ref type="bibr" target="#b16">[17]</ref> that the most effective way to swap styles is by using normalization layers. Batch Normalization <ref type="bibr" target="#b18">[19]</ref> has been used in <ref type="bibr" target="#b44">[45]</ref>, but <ref type="bibr" target="#b46">[47]</ref> found that replacing it with Instance Normalization (IN) <ref type="bibr" target="#b45">[46]</ref> leads to significant improvements. IN works in the feature space in the same way Contrast Normalization works in the pixel space, which makes it much more effective. A more general approach has been introduced by Adaptive IN (AdaIN), which computes the affine transformation from a style input. UNIT <ref type="bibr" target="#b25">[26]</ref> uses IN to swap the source and target style. Instead of performing a global translation with IN, we exploit the task network to translate each region of the image according to its semantic meaning. To this end, we choose to denormalize the generator activations with the SPADE layer <ref type="bibr" target="#b33">[34]</ref>, giving a result that naturally cooperates with the learning of the semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our objective is to train a deep neural network M to perform semantic segmentation on a target (real) dataset T . We assume we only have the target images X T without the target labels Y T . In order to do this, we use synthetic data from a source (synthetic) dataset S, where we have both the images X S and the labels Y S . This problem setting is UDA for semantic segmentation, which means that we want to reduce the domain shift caused by the difference in visual appearance of the two domains.</p><p>As depicted in <ref type="figure">Figure 2</ref>, we follow the recent work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24]</ref> and take advantage of both pixel-level and feature-level alignment to reduce the domain shift. We can see them as two separate subtasks, but we will also show how they actually need to cooperate to improve each other in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test sample</head><p>Ground truth GTA5 → CS w/ D GTA5 → CS w/ F <ref type="figure">Figure 4</ref>: Semantic Segmentation adapted from GTA5 <ref type="bibr" target="#b35">[36]</ref> to Cityscapes <ref type="bibr" target="#b5">[6]</ref>. We take a sample X T from the Cityscapes validation set and show the segmentation predictions M(X T ) of both the adapted DeepLabV2 <ref type="bibr" target="#b1">[2]</ref> and FCN8s <ref type="bibr" target="#b27">[28]</ref> networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pixel-level alignment</head><p>For pixel-level alignment, we make use of an image-to-image translation network F S→T , which learns through adversarial training to visually align X S to X T by generating X S→T = F S→T (X S ). Some visual examples of the results of this pipeline are depicted in <ref type="figure">Figure 6</ref>. Inspired by <ref type="bibr" target="#b25">[26]</ref> [18], we assume that S and T share a common latent space Z and design two coupled GANs to train the desired system. The training objective for the image translation model is comprised of several loss functions computed as the sum of two components, one per domain. The final objective is then:</p><formula xml:id="formula_0">L = λ recon L recon + λ GAN L GAN + λ CC I L CC I + λ CC H L CC H + λ SCE L SCE<label>(1)</label></formula><p>Image reconstruction We have an encoder for each domain, E S and E T , coupled with a corresponding generator for each domain, G S and G T , to form two Autoencoders. The encoders extract the latent code z ∼ Z, which is fed to the generators along with the semantic features predicted by M. Therefore, a translated image is indicated as</p><formula xml:id="formula_1">x A→B = G B (E A (x A ), M(x A )</formula><p>) and the image reconstruction loss is:</p><formula xml:id="formula_2">L S recon = E x S ∼X S [||x S→S − x S ||] L T recon = E x T ∼X T [||x T →T − x T ||]<label>(2)</label></formula><p>Adversarial loss By combining E S with G T and vice versa, we get the actual translation models F S→T and F T →S , which are trained in an adversarial fashion to trick the corresponding discriminators D T and D S :</p><formula xml:id="formula_3">L S GAN = 1 2 E x S ∼X S [(D S (x S )) 2 ] + 1 2 E x T ∼X T [(D S (x T →S ) − 1) 2 ] L T GAN = 1 2 E x T ∼X T [(D T (x T )) 2 ] + 1 2 E x S ∼X S [(D T (x S→T ) − 1) 2 ]<label>(3)</label></formula><p>Cycle consistency By combining F S→T with F T →S and viceversa we can now apply the cycle consistency loss to images and latent spaces:</p><formula xml:id="formula_4">L S CC I = E x S ∼X S [||x S T − x S ||] L T CC I = E x T ∼X T [||x T S − x T ||] L S CC H = E z S ∼Z [||z S→T − z S ||] L T CC H = E z T ∼Z [||z T →S − z T ||]<label>(4)</label></formula><p>where z S→T refers to the latent space extracted by E T from x S→T and viceversa.</p><p>Symmetric cross-entropy Finally, we impose that the segmentation predicted for the translated image has to be consistent with the one predicted for the original one through a symmetric cross-entropy loss, which is made of two contributions. For the S → T case, the first contribution assumes that M(x S→T ) is the ground truth label and tries to align M(x S ) with it. The second contribution assumes that M(x S ) is the ground truth label and tries to align M(x S→T ) with it. The T → S case is symmetrical to the first one.</p><formula xml:id="formula_5">L S SCE = − E x S ∼X S [M(x S→T ) log M(x S ) ] − E x S ∼X S [M(x S ) log M(x S→T ) ] L T SCE = − E x T ∼X T [M(x T →S ) log M(x T )] − E x T ∼X T [M(x T ) log M(x T →S )]<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantically adaptive generator</head><p>Recent generator architectures <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27</ref>] make use of AdaIN to remove the source style and inject the target one. However, we observe that the global denormalization performed by AdaIN might be suboptimal for the image translation task. This is why we redesigned our generator to adaptively denormalize each pixel based on its semantics. We use M to extract a segmentation map m ∈ R B×C×H×W from the input image, where C is the number of classes. When feeding it to the generator, we choose to represent this semantic guidance as the unnormalized output of M. In the supplementary material we detail the reasons behind this choice and the other possibilities.</p><p>Given an input activation x ∈ R B×C ×H ×W , m is resized to H ×W and fed to the SPADE layer, which outputs γ, β ∈ R B×C ×H ×W . We then normalize x by using Instance Normalization and use γ and β to denormalize it:</p><formula xml:id="formula_6">y b,c,h,w = γ b,c,h,w x b,c,h,w − µ b,c σ b,c + β b,c,h,w<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>Pixel-level alignment has given a great boost to the research in UDA problems, but the gap with the performance achievable with full supervision is still huge. We believe that the image translation methods still need a lot of improvements and this is why we focused on redesigning the generator to include a semantic conditioning. Our claim is that adaptively denormalizing each pixel based on its class allows the translation model to produce results which are better for domain adaptation, since each region gets injected with features that are more consistent with its semantic. This connection strengthens the bridge with feature-level alignment (see <ref type="figure">Figure 1</ref>), which before our work was induced only by consistency losses.  <ref type="table">Table 1</ref>: Results of adapting GTA5 <ref type="bibr" target="#b35">[36]</ref> to Cityscapes <ref type="bibr" target="#b5">[6]</ref>. D stands for DeepLabV2 <ref type="bibr" target="#b1">[2]</ref> with ResNet101 <ref type="bibr" target="#b12">[13]</ref>, while F stands for FCN8s <ref type="bibr" target="#b27">[28]</ref> with VGG16 <ref type="bibr" target="#b39">[40]</ref> as backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feature-level alignment</head><p>For feature-level alignment, we train M on X T and X S→T by combining supervision on X S→T , self-supervision on X T and adversarial learning. The loss, in this case, is given by</p><formula xml:id="formula_7">L = λ seg L seg + λ SSL L SSL + λ adv L adv<label>(7)</label></formula><p>We set λ seg = 1, λ SSL = 1, λ adv = 10 −3 for DeepLabV2, λ adv = 10 −4 for FCN8s and use the same optimization hyperparameters of <ref type="bibr" target="#b23">[24]</ref> to train both networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation loss</head><p>The main supervision for the segmentation task is given by training the network on (X S→T ,Y S ), where X S→T are images translated from the synthetic to the real domain. This is formulated as the common cross-entropy loss:</p><formula xml:id="formula_8">L seg = −E x∼X S→T ,y∼Y S K ∑ k=1 1 [k=y] log(M(x) k )<label>(8)</label></formula><p>Self-supervised segmentation Following <ref type="bibr" target="#b23">[24]</ref>, we also adopt self-supervision to improve the adaptation model. To this end, we compute M(X T ) and use as labels the high confidence predictions, creating Y SSL T :</p><formula xml:id="formula_9">Y SSL T =    arg max 1≤k≤K M(X T ) k , if M(X T ) k ≥ th SSL −1, otherwise<label>(9)</label></formula><p>where K is the number of classes, −1 is the index ignored and th SSL is the confidence threshold, which we use to filter the uncertain predictions. In our experiments we set th SSL = 0.9. This makes us able to compute a cross-entropy loss also on the target dataset:  <ref type="table">Table 2</ref>: Results of adapting SYNTHIA <ref type="bibr" target="#b36">[37]</ref> to Cityscapes <ref type="bibr" target="#b36">[37]</ref>. D stands for DeepLabV2 <ref type="bibr" target="#b1">[2]</ref> with ResNet101 <ref type="bibr" target="#b12">[13]</ref>, while F stands for FCN8s <ref type="bibr" target="#b27">[28]</ref> with VGG16 <ref type="bibr" target="#b39">[40]</ref> as backbone network.</p><formula xml:id="formula_10">L SSL = −E x∼X T ,y∼Y SSL T K ∑ k=1 1 [k=y] log(M(x) k )<label>(10)</label></formula><p>Adversarial loss Supervision on pixel-level aligned images and self-supervision on target images are not enough to learn a full model. This is why we also make use of adversarial training by feeding the semantic maps to a discriminator D seg , which has to distinguish the maps predicted by M for S and T , giving:</p><formula xml:id="formula_11">L adv = E x T ∼X T [log(D seg (M(x T )))] + E x S→T ∼X S→T [log(1 − D seg (M(x S→T )))]<label>(11)</label></formula><p>This loss enforces an output space alignment <ref type="bibr" target="#b41">[42]</ref>, which means that M has to learn how to predict semantic maps with distributions that are aligned regardless of the input domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present our experimental results for the synthetic to real adaptation using two dataset settings: GTA5 <ref type="bibr" target="#b35">[36]</ref> to Cityscapes <ref type="bibr" target="#b5">[6]</ref> and SYNTHIA <ref type="bibr" target="#b36">[37]</ref> to Cityscapes. We evaluate the mean intersection-over-union (IoU) on the Cityscapes validation set and show how our method outperforms the current state-of-the-art by adopting the same segmentation models. Finally, we conduct an ablation study to highlight the value of our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation network</head><p>We choose to adapt two segmentation networks: DeepLabV2 <ref type="bibr" target="#b1">[2]</ref> with ResNet101 <ref type="bibr" target="#b12">[13]</ref> and FCN8s <ref type="bibr" target="#b27">[28]</ref> with VGG16 <ref type="bibr" target="#b39">[40]</ref>. Both networks are trained on images downsampled to 1024x512 with batch size 1.</p><p>We initialize the segmentation networks from <ref type="bibr" target="#b23">[24]</ref> to speed up the training process. In order to show the independence from this initialization, we also conduct one experiment where we train DeepLabV2 from scratch for the GTA→Cityscapes task, and we find this to be in line with the results that we get by initializing it with <ref type="bibr" target="#b23">[24]</ref>.</p><p>Translation network For the translation part, we describe the architecture of the encoders, generators and discriminators.</p><p>The encoder is made by few downsampling blocks, followed by residual blocks for further processing of the latent code and they all use IN <ref type="bibr" target="#b45">[46]</ref>. Symmetrically, the generators take in the latent code and process it with residual blocks, where IN and SPADE are combined to normalize the feature maps. These are followed by upsampling blocks with Layer Normalization <ref type="bibr" target="#b0">[1]</ref>. We found LN to better preserve the style in the generated activations.</p><p>In each domain we have discriminators for multiple scales <ref type="bibr" target="#b47">[48]</ref>, each being a Patch Discriminator <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. The GAN <ref type="bibr" target="#b11">[12]</ref> objective we choose is the one proposed in LSGAN <ref type="bibr" target="#b31">[32]</ref>. We apply Spectral Normalization <ref type="bibr" target="#b32">[33]</ref> to all the models described here.</p><p>When training the translation model we resize the input images to 1024x512 and take 512x512 random crops out of them. We use Adam <ref type="bibr" target="#b21">[22]</ref> as optimizer with β 1 = 0.9 and β 2 = 0.99. We apply TTUR <ref type="bibr" target="#b13">[14]</ref> and set the initial learning rate to be 10 −4 . The learning rate is scheduled to decay to 0 after 1000000 iterations with a 'poly' scheduling where the power is 0.9. The batch size is 1 for all the experiments. The loss weights are set to λ recon = 10, λ GAN = 1, λ CC I = 10, λ CC H = 1, λ SCE = 10.</p><p>Bidirectional learning Pixel-level and feature-level alignment are not performed in an end-to-end fashion. Besides being highly expensive in terms of memory requirements, we found this approach to be very unstable and it did not lead to good results.</p><p>We adopt a policy similar to <ref type="bibr" target="#b23">[24]</ref> and iteratively recreate X S→T when M stops improving on the target dataset. Before each training of the segmentation network, we also generate new pseudo-labels Y SSL T . We found this procedure to significantly improve the final mIoU compared to a single iteration of pixel-level and feature-level alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with State of the Art</head><p>GTA5 to Cityscapes For the GTA5 <ref type="bibr" target="#b35">[36]</ref> to Cityscapes <ref type="bibr" target="#b5">[6]</ref> task, we evaluate on all the 19 classes used in the Cityscapes benchmark since the datasets are fully compatible. Some visual results for this setting are presented in <ref type="figure">Figure 4</ref>. In this case, the upper bounds in terms of mIoU are 65.1 for DeepLabV2 <ref type="bibr" target="#b1">[2]</ref> and 60.3 for FCN8s <ref type="bibr" target="#b27">[28]</ref>, which are the results achievable by training with the target labels. In <ref type="table">Table 1</ref> we compare our results with the related work. In terms of mIoU, we get respectively +1.9% and +4.2% over the state-of-theart with the two networks.</p><p>SYNTHIA to Cityscapes SYNTHIA <ref type="bibr" target="#b36">[37]</ref> has been adopted in the past by the other works for its overlapping with 16 of the Cityscapes classes. For the SYNTHIA to Cityscapes task we compare our results with the state-of-the-art in <ref type="table">Table 2</ref> and present some visual results in the supplementary material. For a fair comparison, the results of the DeepLabV2 architecture are limited to the 13 classes adopted by the other works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42]</ref>. The upper bounds in terms of mIoU are 71.7 for DeepLabV2 and 59.5 for FCN8s. In the case of DeepLabV2 we surpass the current state-of-the-art in mIoU by +3.7%. For FCN8s, instead, we get +2.5% on the mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study</head><p>In order to weight our contribution, we perform an ablation study of the proposed method (see <ref type="table" target="#tab_3">Table 3</ref>). For each experiment, we report 3 values: the mIoU; the gain wrt the lower bound, which is a naive training on the source dataset; the remaining gap wrt the upper bound, which is the result for training with target labels (called oracle prediction).   <ref type="table">Table 4</ref>: Image quality evaluation. We report the Inception Score (IS) <ref type="bibr" target="#b37">[38]</ref> and the Fréchet Inception Distance (FID) <ref type="bibr" target="#b13">[14]</ref> of the images generated in each setting of our experiments.</p><p>The experiments are conducted with DeepLabV2 <ref type="bibr" target="#b1">[2]</ref> for the GTA5 <ref type="bibr" target="#b35">[36]</ref> to Cityscapes <ref type="bibr" target="#b5">[6]</ref> task, for which the lower bound is 33.6 and the upper bound is 65.1.</p><p>We first show the baseline results that we get by using the generator with no Symmetric Cross-Entropy L SCE and no semantic guidance. In this setting, the residual blocks of the generator use IN <ref type="bibr" target="#b45">[46]</ref> layers and the image-to-image translation is completely unrelated to the semantic segmentation. Secondly, we add the semantic guidance with the SPADE <ref type="bibr" target="#b33">[34]</ref> layer. This setting can still benefit from the semantic guidance in the translation, but loses the ability to enforce the cross-domain consistency for the segmentation task. Then we swap back the SPADE layer with IN and enable L SCE . This setting resembles the one used in <ref type="bibr" target="#b4">[5]</ref>, where the architecture of CycleGAN <ref type="bibr" target="#b50">[51]</ref> is replaced by ours. Finally, we show that the best results are achieved by the combination of the two elements, which completely bridges the translation and segmentation tasks and is the final setting of our work.</p><p>We can see that when we remove SPADE or L SCE the mIoU drops, suggesting that they both have an important contribution to get the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generated image quality</head><p>We also report the quality of the images generated by our image-to-image translation model. In <ref type="table">Table 4</ref> we report the Inception Score (IS) <ref type="bibr" target="#b37">[38]</ref> of the images X S→T and the Fréchet Inception Distance (FID) <ref type="bibr" target="#b13">[14]</ref> with the Cityscapes training set. Although the IS of the produced images is low in every setting, the FID results indicate that the semantic guidance induced by DeepLabV2 is the one that best visually aligns the synthetic domain to Cityscapes. The images translated from SYNTHIA, however, have a much greater distance from Cityscapes than the ones translated from GTA5, regardless of the network used as semantic guidance. We note that this is possibly due to the bigger initial gap in visual appearance between the two domains, since the FID between the original SYNTHIA and Cityscapes is 156.92, while the FID between the original GTA5 and Cityscapes is only 62.42.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Representation of the semantic input</head><p>The image synthesis network of SPADE takes as input a one-hot encoding of the ground truth semantic segmentation. Here, instead, we use the unnormalized output of M for every translation that we perform. This is a consequence of the cycle consistency constraints.</p><p>As explained in the main article, we have to perform both the S T and T S cycles, which is why we have to train both G S and G T by feeding them semantic maps aligned with the input images. In UDA problems, we do not have access to Y T , which is why we use M(X T ) for the T S cycle. However, we note that the refined output classes predicted by M are far from the ground truth and cannot give an accurate conditioning, especially in the target domain when the segmentation is still in the initial training phases. Because of this, we choose to use as semantic guidance the unnormalized output of M. This representation has the advantage of carrying the confidence of the prediction, which could potentially be used by the SPADE layers to avoid denormalizing a region with the incorrect class (e.g. on the borders of objects, where the segmentation tends to fail more easily).</p><p>In the S T cycle, we could use Y S as semantic guidance, but this would lead to inconsistent input distributions for the SPADE layers, which is why we adopt M(X S ) as semantic guidance in this case too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed architecture C Fake segmentation</head><p>The effect of using the SPADE layers in the image-to-image translation model can be seen better when there is a mismatch between the source image and the semantic guidance. To show this effect, we feed the SPADE layers with a segmentation map extracted from an image that is different from the one being translated. In <ref type="figure">Figure 5</ref>, we can see how the denormalization wrongly creates some features in the region of the image they do not belong to (i.e. green on the road).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source for segmentation Semantic segmentation</head><p>Source for translation Translated image <ref type="figure">Figure 5</ref>: Fake segmentation for image-to-image translation. We take two different samples X 1 S (a) and X 2 S (c) from GTA5. We then use M to get the predicted segmentation M(X 1 S ) (b) and use it as semantic guidance for the translation of X 1 S to get X S→T = F S→T (X 1 S , M(X 2 S )). The result (d) emphasizes the effect of the semantic guidance in our image-to-image translation method.  <ref type="figure">Figure 6</ref>: Additional translations from GTA5 to Cityscapes. We take a sample X S from GTA5, get the predicted segmentation using M, and generate X S→T . We present the results obtained with both DeepLabV2 and FCN8s used as semantic guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYNTHIA sample</head><p>SYNTHIA pred (D)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYNTHIA→CS (D)</head><p>SYNTHIA pred (F) SYNTHIA→CS (F) <ref type="figure">Figure 7</ref>: Translations from SYNTHIA to Cityscapes. We take a sample X S from SYN-THIA, get the predicted segmentation using M, and generate X S→T . We present the results obtained with both DeepLabV2 and FCN8s used as semantic guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CS sample</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>No adaptation (GTA) GTA5→CS SYNTHIA→CS <ref type="figure">Figure 8</ref>: Additional segmentation results. We take a sample X T from the Cityscapes validation set and get the predicted segmentation using M. Here we show the different results obtainable with M being DeepLabV2. First we show the results obtained with M trained with no adaptation on GTA5, then the results obtained by adapting GTA5 and SYNTHIA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>35.6 80.1 19.8 17.5 38.0 39.9 41.5 82.7 27.9 73.6 64.9 19 65.0 12.0 28.6 4.5 31.1 42.0 42.7 AdaptSegNet [42] D 86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4 DCAN [49] D 85.0 30.8 81.3 25.8 21.2 22.2 25.4 26.6 83.4 36.7 76.2 58.9 24.9 80.7 29.5 42.9 2.5 26.9 11.6 41.7 CLAN [31] D 87.0 27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2 BDL [24] D 91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5 Ours D 91.2 43.3 85.2 38.6 25.9 34.7 41.3 41.0 85.5 46.0 86.5 61.7 33.8 85.5 34.4 48.7 0.0 36.1 37.8 50.4 Curriculum [50] F 74.9 22.0 71.7 6.0 11.9 8.4 16.3 11.1 75.7 13.3 66.5 38.0 9.3 55.2 18.8 18.9 0.0 16.8 16.6 28.9 CBST [52] F 66.7 26.8 73.7 14.8 9.5 28.3 25.9 10.1 75.5 15.7 51.6 47.2 6.2 71.20.6 29.0 28.2 84.5 40.9 82.3 52.4 24.4 81.2 21.8 44.8 31.5 26.5 33.7 46.5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GTA5 → Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Arch.</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veget</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>mbike</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell>Cycada [16]</cell><cell cols="16">D 86.7 9 3.7</cell><cell>2.2</cell><cell cols="4">5.4 18.9 32.4 30.9</cell></row><row><cell>Cycada [16]</cell><cell cols="18">F 85.2 37.2 76.5 21.8 15.0 23.8 22.9 21.5 80.5 31.3 60.7 50.5 9.0 76.9 17.1 28.2 4.5</cell><cell>9.8</cell><cell>0.0</cell><cell>35.4</cell></row><row><cell>DCAN [49]</cell><cell cols="20">F 82.3 26.7 77.4 23.7 20.5 20.4 30.3 15.9 80.9 25.4 69.5 52.6 11.1 79.6 24.9 21.2 1.3 17.0 6.7</cell><cell>36.2</cell></row><row><cell>LSD [39]</cell><cell cols="21">F 88.0 30.5 78.6 25.2 23.5 16.7 23.5 11.6 78.7 27.2 71.9 51.3 19.5 80.4 19.8 18.3 0.9 20.8 18.4 37.1</cell></row><row><cell>CLAN [31]</cell><cell cols="20">F 88.0 30.6 79.2 23.4 20.5 26.1 23.0 14.8 81.6 34.5 72.0 45.8 7.9 80.5 26.6 29.9 0.0 10.7 0.0</cell><cell>36.6</cell></row><row><cell>CrDoCo [5]</cell><cell cols="21">F 89.1 33.2 80.1 26.9 25.0 18.3 23.4 12.8 77.0 29.1 72.4 55.1 20.2 79.9 22.3 19.5 1.0 20.1 18.7 38.1</cell></row><row><cell>BDL [24]</cell><cell cols="21">F 89.2 40.9 81.2 29.1 19.2 14.2 29.0 19.6 83.7 35.9 80.7 54.7 23.3 82.7 25.8 28.0 2.3 25.7 19.9 41.3</cell></row><row><cell>Ours</cell><cell cols="6">F 91.1 46.4 82.9 33.2 27.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3 51.4 Ours D 87.7 49.7 81.6 ---19.3 18.5 81.1 83.7 58.7 31.8 73.3 47.9 37.1 45.7 55.1 FCNsITW [15] F 11.5 19.6 30.8 4.4 0.0 20.3 0.1 11.7 42.3 68.7 51.2 3.8 54.34.0 78.3 0.3 0.6 26.7 15.9 29.5 81.0 81.1 55.5 21.9 77.2 23.5 11.8 47.5 41.5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">SYNTHIA → Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Arch.</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veget</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>mbike</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell cols="5">AdaptSegNet [42] D 79.2 37.2 78.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="11">9.9 10.5 78.2 80.5 53.5 19.6 67.0 29.5 21.6 31.3 45.9</cell></row><row><cell>CLAN [31]</cell><cell cols="4">D 81.3 37.0 80.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="11">16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7 47.8</cell></row><row><cell>BDL [24]</cell><cell cols="4">D 86.0 46.7 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="8">14.1 0 3.2</cell><cell>0.2</cell><cell>0.6</cell><cell>20.2</cell></row><row><cell>Curriculum [50]</cell><cell cols="8">F 65.2 26.1 74.9 0.1 0.5 10.7 3.5</cell><cell cols="10">3.0 76.1 70.6 47.1 8.2 43.2 20.7 0.7 13.1 29.0</cell></row><row><cell>CBST [52]</cell><cell cols="15">F 69.6 28.7 69.5 12.1 0.1 25.4 11.9 13.6 82.0 81.9 49.1 14.5 66.0 6.6</cell><cell cols="3">3.7 32.4 35.4</cell></row><row><cell>DCAN [49]</cell><cell cols="18">F 79.9 30.4 70.8 1.6 0.6 22.3 6.7 23.0 76.9 73.9 41.9 16.7 61.7 11.5 10.3 38.6 35.4</cell></row><row><cell>CLAN [31]</cell><cell cols="4">F 80.4 30.7 74.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.4</cell><cell cols="8">8.0 77.1 79.0 46.5 8.9 73.8 18.2 2.2</cell><cell>9.9</cell><cell>39.3</cell></row><row><cell>CrDoCo [5]</cell><cell cols="18">F 84.9 32.8 80.1 4.3 0.4 29.4 14.2 21.0 79.2 78.3 50.2 15.9 69.8 23.4 11.0 15.6 38.2</cell></row><row><cell>BDL [24]</cell><cell cols="18">F 72.0 30.3 74.5 0.1 0.3 24.6 10.2 25.2 80.5 80.0 54.7 23.2 72.7 24.0 7.5 44.9 39.0</cell></row><row><cell>Ours</cell><cell cols="2">F 79.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study. We report the mIoU, the gain wrt the lower bound (i.e. training naively on source), the gap wrt the upper bound (i.e. training on target).</figDesc><table><row><cell>Setting</cell><cell>Network</cell><cell>IS</cell><cell>FID</cell></row><row><cell>GTA5 → Cityscapes</cell><cell cols="3">DeepLabV2 4.9 27.9</cell></row><row><cell>GTA5 → Cityscapes</cell><cell>FCN8s</cell><cell cols="2">4.8 40.3</cell></row><row><cell cols="4">SYNTHIA → Cityscapes DeepLabV2 5.0 100.8</cell></row><row><cell>SYNTHIA → Cityscapes</cell><cell>FCN8s</cell><cell cols="2">4.9 113.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head> <ref type="table">Kernel  size  Stride  Input  channels   Output  channels   Output  upsampling  Residual  Activation  function  Normalization  Spectral  normalization  7  1  3  64  --ReLU  IN  4  2  64  128  --ReLU  IN  4  2  128  256  --ReLU  IN  3  1</ref> </div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reweighted adversarial adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Wassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7976" to="7985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Cheng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Crdoco: Pixellevel domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>P Kingma Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Domain stylization: A strong, simple baseline for synthetic to real image domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zedlewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09384</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Daml: Domain adaptation metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2980" to="2989" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixellevel adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1QRgziT-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3752" to="3761" />
		</imprint>
	</monogr>
	<note>Ser Nam Lim, and Rama Chellappa</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6924" to="6932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><forename type="middle">Gokhan</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="518" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2020" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
