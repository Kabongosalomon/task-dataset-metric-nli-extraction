<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Class-Balanced Loss Based on Effective Number of Samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Alphabet Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Class-Balanced Loss Based on Effective Number of Samples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of longtailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula (1−β n )/(1−β), where n is the number of samples and β ∈ [0, 1) is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets. * The work was performed while Yin Cui and Yang Song worked at Google (a subsidiary of Alphabet Inc.).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent success of deep Convolutional Neural Networks (CNNs) for visual recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b15">16]</ref> owes much to the availability of large-scale, real-world annotated datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b39">40]</ref>. In contrast with commonly used visual recognition datasets (e.g., CIFAR <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39]</ref>, Ima-geNet ILSVRC 2012 <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref> and CUB-200 Birds <ref type="bibr" target="#b41">[42]</ref>) that exhibit roughly uniform distributions of class labels, realworld datasets have skewed <ref type="bibr" target="#b20">[21]</ref> distributions, with a longtail: a few dominant classes claim most of the examples, while most of the other classes are represented by relatively few examples. CNNs trained on such data perform poorly for weakly represented classes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4]</ref>.  <ref type="figure">Figure 1</ref>. Two classes, one from the head and one from the tail of a long-tailed dataset (iNaturalist 2017 <ref type="bibr" target="#b39">[40]</ref> in this example), have drastically different number of samples. Models trained on these samples are biased toward dominant classes (black solid line). Reweighing the loss by inverse class frequency usually yields poor performance (red dashed line) on real-world data with high class imbalance. We propose a theoretical framework to quantify the effective number of samples by taking data overlap into consideration. A class-balanced term is designed to re-weight the loss by inverse effective number of samples. We show in experiments that the performance of a model can be improved when trained with the proposed class-balanced loss (blue dashed line).</p><p>A number of recent studies have aimed to alleviate the challenge of long-tailed training data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b43">44]</ref>. In general, there are two strategies: re-sampling and cost-sensitive re-weighting. In re-sampling, the number of examples is directly adjusted by over-sampling (adding repetitive data) for the minor class or under-sampling (removing data) for the major class, or both. In cost-sensitive re-weighting, we influence the loss function by assigning relatively higher costs to examples from minor classes. In the context of deep feature representation learning using CNNs, re-sampling may either introduce large amounts of duplicated samples, which slows down the training and makes the model susceptible to overfitting when oversampling, or discard valuable examples that are important for feature learning when under-sampling. Due to these disadvantages of applying re-sampling for CNN training, the present work focuses on re-weighting approaches, namely, how to design a better class-balanced loss.</p><p>Typically, a class-balanced loss assigns sample weights inversely proportionally to the class frequency. This simple heuristic method has been widely adopted <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>. However, recent work on training from large-scale, real-world, long-tailed datasets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28]</ref> reveals poor performance when using this strategy. Instead, they use a "smoothed" version of weights that are empirically set to be inversely proportional to the square root of class frequency. These observations suggest an interesting question: how can we design a better class-balanced loss that is applicable to a diverse array of datasets?</p><p>We aim to answer this question from the perspective of sample size. As illustrated in <ref type="figure">Figure 1</ref>, we consider training a model to discriminate between a major class and a minor class from a long-tailed dataset. Due to highly imbalanced data, directly training the model or re-weighting the loss by inverse number of samples cannot yield satisfactory performance. Intuitively, the more data, the better. However, since there is information overlap among data, as the number of samples increases, the marginal benefit a model can extract from the data diminishes. In light of this, we propose a novel theoretical framework to characterize data overlap and calculate the effective number of samples in a modeland loss-agnostic manner. A class-balanced re-weighting term that is inversely proportional to the effective number of samples is added to the loss function. Extensive experimental results indicate that this class-balanced term provides a significant boost to the performance of commonly used loss functions for training CNNs on long-tailed datasets.</p><p>Our key contributions can be summarized as follows: (1) We provide a theoretical framework to study the effective number of samples and show how to design a class-balanced term to deal with long-tailed training data. <ref type="bibr" target="#b1">(2)</ref> We show that significant performance improvements can be achieved by adding the proposed class-balanced term to existing commonly used loss functions including softmax cross-entropy, sigmoid cross-entropy and focal loss. In addition, we show our class-balanced loss can be used as a generic loss for visual recognition by outperforming commonly-used softmax cross-entropy loss on ILSVRC 2012. We believe our study on quantifying the effective number of samples and classbalanced loss can offer useful guidelines for researchers working in domains with long-tailed class distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most of previous efforts on long-tailed imbalanced data can be divided into two regimes: re-sampling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b49">50]</ref> (including over-sampling and under-sampling) and costsensitive learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Re-Sampling. Over-sampling adds repeated samples from minor classes, which could cause the model to overfit. To solve this, novel samples can be either interpolated from neighboring samples <ref type="bibr" target="#b4">[5]</ref> or synthesized <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50]</ref> for minor classes. However, the model is still error-prone due to noise in the novel samples. It was argued that even if oversampling incurs risks from removing important samples, under-sampling is still preferred over over-sampling <ref type="bibr" target="#b8">[9]</ref>.</p><p>Cost-Sensitive Learning. Cost-Sensitive Learning can be traced back to a classical method in statistics called importance sampling <ref type="bibr" target="#b19">[20]</ref>, where weights are assigned to samples in order to match a given data distribution. Elkan et al. <ref type="bibr" target="#b9">[10]</ref> studied how to assign weights to adjust the decision boundary to match a given target in the case of binary classification. For imbalanced datasets, weighting by inverse class frequency <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref> or a smoothed version of inverse square root of class frequency <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28]</ref> are often adopted. As a generalization of smoothed weighting with a theoretically grounded framework, we focus on (a) how to quantify the effective number of samples and (b) using it to re-weight the loss. Another line of important work aims to study sample difficulty in terms of loss and assign higher weights to hard examples <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref>. Samples from minor classes tend to have higher losses than those from major classes as the features learned in minor classes are usually poorer. However, there is no direct connection between sample difficulty and the number of samples. A side effect of assigning higher weights to hard examples is the focus on harmful samples (e.g., noisy data or mislabeled data) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>. In our work, we do not make any assumptions on the sample difficulty and data distribution. By improving the focal loss <ref type="bibr" target="#b25">[26]</ref> using our class-balanced term in experiments, we show that our method is complementary to re-weighting based on sample difficulty.</p><p>It is noteworthy to mention that previous work has also explored other ways of dealing with data imbalance, including transferring the knowledge learned from major classes to minor classes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b43">44]</ref> and designing a better training objective via metric learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Covering and Effective Sample Size. Our theoretical framework is inspired by the random covering problem <ref type="bibr" target="#b17">[18]</ref>, where the goal is to cover a large set by a sequence of i.i.d. random small sets. We simplify the problem in Section 3 by making reasonable assumptions. Note that the effective number of samples proposed in this paper is different from the concept of effective sample size in statistics. The effective sample size is used to calculate variance when samples are correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Effective Number of Samples</head><p>We formulate the data sampling process as a simplified version of random covering. The key idea is to associate each sample with a small neighboring region instead of a single point. We present our theoretical framework and the formulation of calculating effective number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Sampling as Random Covering</head><p>Given a class, denote the set of all possible data in the feature space of this class as S. We assume the volume of S is N and N ≥ 1. Denote each data as a subset of S that has the unit volume of 1 and may overlap with other data. Consider the data sampling process as a random covering problem where each data (subset) is randomly sampled from S to cover the entire set of S. The more data is being sampled, the better the coverage of S is. The expected total volume of sampled data increases as the number of data increases and is bounded by N . Therefore, we define:</p><p>Definition 1 (Effective Number). The effective number of samples is the expected volume of samples.</p><p>The calculation of the expected volume of samples is a very difficult problem that depends on the shape of the sample and the dimensionality of the feature space <ref type="bibr" target="#b17">[18]</ref>. To make the problem tamable, we simplify the problem by not considering the situation of partial overlapping. That is, we assume a newly sampled data can only interact with previously sampled data in two ways: either entirely inside the set of previously sampled data with the probability of p or entirely outside with the probability of 1−p, as illustrated in <ref type="figure">Figure 2</ref>. As the number of sampled data points increases, the probability p will be higher.</p><p>Before we dive into the mathematical formulations, we discuss the connection between our definition of effective number of samples and real-world visual data. Our idea is to capture the diminishing marginal benefits by using more data points of a class. Due to intrinsic similarities among real-world data, as the number of samples grows, it is highly possible that a newly added sample is a near-duplicate of existing samples. In addition, CNNs are trained with heavy data augmentations, where simple transformations such as random cropping, re-scaling and horizontal flipping will be applied to the input data. In this case, all augmented examples are also considered as same with the original example. Presumably, the stronger the data augmentation is, the smaller the N will be. The small neighboring region of a sample is a way to capture all near-duplicates and instances that can be obtained by data augmentation. For a class, N can be viewed as the number of unique prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mathematical Formulation</head><p>Denote the effective number (expected volume) of samples as E n , where n ∈ Z &gt;0 is the number of samples.  <ref type="figure">Figure 2</ref>. Giving the set of all possible data with volume N and the set of previously sampled data, a new sample with volume 1 has the probability of p being overlapped with previous data and the probability of 1 − p not being overlapped.</p><formula xml:id="formula_0">Proposition 1 (Effective Number). E n = (1−β n )/(1−β), where β = (N − 1)/N .</formula><p>Proof. We prove the proposition by induction. It is obvious that E 1 = 1 because there is no overlapping. So E 1 = (1−β 1 )/(1−β) = 1 holds. Now let's consider a general case where we have previously sampled n−1 examples and are about to sample the n th example. Now the expected volume of previously sampled data is E n−1 and the newly sampled data point has the probability of p = E n−1 /N to be overlapped with previous samples. Therefore, the expected volume after sampling n th example is:</p><formula xml:id="formula_1">E n = pE n−1 +(1−p)(E n−1 +1) = 1+ N − 1 N E n−1 . (1) Assume E n−1 = (1 − β n−1 )/(1 − β) holds, then E n = 1+β 1 − β n−1 1 − β = 1 − β + β − β n 1 − β = 1 − β n 1 − β .<label>(2)</label></formula><p>The above proposition shows that the effective number of samples is an exponential function of n. The hyperparameter β ∈ [0, 1) controls how fast E n grows as n increases.</p><p>Another explanation of the effective number E n is:</p><formula xml:id="formula_2">E n = (1 − β n )/(1 − β) = n j=1 β j−1 .<label>(3)</label></formula><p>This means that the j th sample contributes β j−1 to the effective number. The total volume N for all possible data in the class can then be calculated as:</p><formula xml:id="formula_3">N = lim n→∞ n j=1 β j−1 = 1/(1 − β).<label>(4)</label></formula><p>This is consistent with our definition of β in the proposition.</p><formula xml:id="formula_4">Implication 1 (Asymptotic Properties). E n = 1 if β = 0 (N = 1). E n → n as β → 1 (N → ∞).</formula><p>Proof. If β = 0, then E n = (1 − 0 n )/(1 − 0) = 1. In the case of β → 1, denote f (β) = 1 − β n and g(β) = 1 − β.</p><p>Since lim β→1 f (β) = lim β→1 g(β) = 0, g (β) = −1 = 0 and lim β→1 f (β)/g (β) = lim β→1 (−nβ n−1 )/(−1) = n exists, using L'Hôpital's rule, we have</p><formula xml:id="formula_5">lim β→1 E n = lim β→1 f (β) g(β) = lim β→1 f (β) g (β) = n.<label>(5)</label></formula><p>The asymptotic property of E n shows that when N is large, the effective number of samples is same as the number of samples n. In this scenario, we think the number of unique prototypes N is large, thus there is no data overlap and every sample is unique. On the other extreme, if N = 1, this means that we believe there exist a single prototype so that all the data in this class can be represented by this prototype via data augmentation, transformations, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Class-Balanced Loss</head><p>The Class-Balanced Loss is designed to address the problem of training from imbalanced data by introducing a weighting factor that is inversely proportional to the effective number of samples. The class-balanced loss term can be applied to a wide range of deep networks and loss functions.</p><p>For an input sample x with label y ∈ {1, 2, . . . , C} 1 , where C is the total number of classes, suppose the model's estimated class probabilities are p = [p 1 , p 2 , . . . , p C ] , where p i ∈ [0, 1] ∀ i, we denote the loss as L(p, y). Suppose the number of samples for class i is n i , based on Equation 2, the proposed effective number of samples for class</p><formula xml:id="formula_6">i is E ni = (1 − β ni i )/(1 − β i ), where β i = (N i − 1)/N i .</formula><p>Without further information of data for each class, it is difficult to empirically find a set of good hyperparameters N i for all classes. Therefore, in practice, we assume N i is only dataset-dependent and set N i = N , β i = β = (N − 1)/N for all classes in a dataset.</p><p>To balance the loss, we introduce a weighting factor α i that is inversely proportional to the effective number of samples for class i: α i ∝ 1/E ni . To make the total loss roughly in the same scale when applying α i , we normalize α i so that C i=1 α i = C. For simplicity, we abuse the notation of 1/E ni to denote the normalized weighting factor in the rest of our paper.</p><p>Formally speaking, given a sample from class i that contains n i samples in total, we propose to add a weighting factor (1 − β)/(1 − β ni ) to the loss function, with hyperparameter β ∈ [0, 1). The class-balanced (CB) loss can be written as:</p><formula xml:id="formula_7">CB(p, y) = 1 E ny L(p, y) = 1 − β 1 − β ny L(p, y),<label>(6)</label></formula><p>where n y is the number of samples in the ground-truth class y. We visualize class-balanced loss in <ref type="figure" target="#fig_2">Figure 3</ref> as a function of n y for different β. Note that β = 0 corresponds to no re-weighting and β → 1 corresponds to re-weighing by inverse class frequency. The proposed novel concept of effective number of samples enables us to use a hyperparameter β to smoothly adjust the class-balanced term between no re-weighting and re-weighing by inverse class frequency. The proposed class-balanced term is model-agnostic and loss-agnostic in the sense that it's independent to the choice of loss function L and predicted class probabilities p. To demonstrate the proposed class-balanced loss is generic, we show how to apply class-balanced term to three commonly used loss functions: softmax cross-entropy loss, sigmoid cross-entropy loss and focal loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Class-Balanced Softmax Cross-Entropy Loss</head><p>Suppose the predicted output from the model for all classes are z = [z 1 , z 2 , . . . , z C ] , where C is the total number of classes. The softmax function regards each class as mutual exclusive and calculate the probability distribution over all classes as p i = exp(z i )/ C j=1 exp(z j ), ∀ i ∈ {1, 2, . . . , C}. Given a sample with class label y, the softmax cross-entropy (CE) loss for this sample is written as:</p><formula xml:id="formula_8">CE softmax (z, y) = − log exp(z y ) C j=1 exp(z j ) .<label>(7)</label></formula><p>Suppose class y has n y training samples, the class-balanced (CB) softmax cross-entropy loss is:</p><formula xml:id="formula_9">CB softmax (z, y) = − 1 − β 1 − β ny log exp(z y ) C j=1 exp(z j )</formula><p>. (8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Class-Balanced Sigmoid Cross-Entropy Loss</head><p>Different from softmax, class-probabilities calculated by sigmoid function assume each class is independent and not mutually exclusive. When using sigmoid function, we regard multi-class visual recognition as multiple binary classification tasks, where each output node of the network is performing a one-vs-all classification to predict the probability of the target class over the rest of classes. Compared with softmax, sigmoid presumably has two advantages for real-world datasets: (1) Sigmoid doesn't assume the mutual exclusiveness among classes, which aligns well with realworld data, where a few classes might be very similar to each other, especially in the case of large number of finegrained classes. (2) Since each class is considered independent and has its own predictor, sigmoid unifies singlelabel classification with multi-label prediction. This is a nice property to have since real-world data often has more than one semantic label.</p><p>Using same notations as softmax cross-entropy, for simplicity, we define z t i as:</p><formula xml:id="formula_10">z t i = z i , if i = y. −z i , otherwise.<label>(9)</label></formula><p>Then the sigmoid cross-entropy (CE) loss can be written as:</p><formula xml:id="formula_11">CE sigmoid (z, y) = − C i=1 log sigmoid(z t i ) = − C i=1 log 1 1 + exp(−z t i )</formula><p>.</p><p>The class-balanced (CB) sigmoid cross-entropy loss is:</p><formula xml:id="formula_13">CB sigmoid (z, y) = − 1 − β 1 − β ny C i=1 log 1 1 + exp(−z t i )</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Class-Balanced Focal Loss</head><p>The recently proposed focal loss (FL) <ref type="bibr" target="#b25">[26]</ref> adds a modulating factor to the sigmoid cross-entropy loss to reduce the relative loss for well-classified samples and focus on difficult samples. Denote p t i = sigmoid(z t i ) = 1/(1 + exp(−z t i )), the focal loss can be written as:  The class-balanced (CB) focal loss is:</p><formula xml:id="formula_15">FL(z, y) = − C i=1 (1 − p t i ) γ log(p t i ).<label>(12)</label></formula><formula xml:id="formula_16">CB focal (z, y) = − 1 − β 1 − β ny C i=1 (1 − p t i ) γ log(p t i ). (13)</formula><p>The original focal loss has an α-balanced variant. The class-balanced focal loss is same as α-balanced focal loss when α t = (1−β)/(1−β ny ). Therefore, the class-balanced term can be viewed as an explicit way to set α t in focal loss based on the effective number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The proposed class-balanced losses are evaluated on artificially created long-tailed CIFAR <ref type="bibr" target="#b23">[24]</ref> datasets with controllable degrees of data imbalance and real-world longtailed datasets iNaturalist 2017 <ref type="bibr" target="#b39">[40]</ref> and 2018 <ref type="bibr" target="#b0">[1]</ref>. To demonstrate our loss is generic for visual recognition, we also present experiments on ImageNet data (ILSVRC 2012 <ref type="bibr" target="#b32">[33]</ref>). We use deep residual networks (ResNet) <ref type="bibr" target="#b15">[16]</ref> with various depths and train all networks from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Long-Tailed CIFAR. To analyze the proposed classbalanced loss, long-tailed versions of CIFAR <ref type="bibr" target="#b23">[24]</ref> are created by reducing the number of training samples per class according to an exponential function n = n i µ i , where i Dataset Name</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long-Tailed CIFAR-10</head><p>Long- <ref type="table">Tailed CIFAR-100  Imbalance  200  100  50  20  10  1  200  100  50  20</ref>   <ref type="table">Table 2</ref>. Classification error rate of ResNet-32 trained with different loss functions on long-tailed CIFAR-10 and CIFAR-100. We show best results of class-balanced loss with best hyperparameters (SM represents Softmax and SGM represents Sigmoid) chosen via crossvalidation. Class-balanced loss is able to achieve significant performance gains. * denotes the case when each class has same number of samples, class-balanced term is always 1 therefore it reduces to the original loss function.</p><p>is the class index (0-indexed), n i is the original number of training images and µ ∈ (0, 1). The test set remains unchanged. We define the imbalance factor of a dataset as the number of training samples in the largest class divided by the smallest. <ref type="figure" target="#fig_3">Figure 4</ref> shows number of training images per class on long-tailed CIFAR-100 with imbalance factors ranging from 10 to 200. We conduct experiments on longtailed CIFAR-10 and CIFAR-100. iNaturalist. The recently introduced iNaturalist species classification and detection dataset <ref type="bibr" target="#b39">[40]</ref> is a real-world long-tailed dataset containing 579,184 training images from 5,089 classes in its 2017 version and 437,513 training images from 8,142 classes in its 2018 version <ref type="bibr" target="#b0">[1]</ref>. We use the official training and validation splits in our experiments.</p><p>ImageNet. We use the ILSVRC 2012 <ref type="bibr" target="#b32">[33]</ref> split containing 1,281,167 training and 50,000 validation images. <ref type="table">Table 1</ref> summarizes all datasets used in our experiments along with their imbalance factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation</head><p>Training with sigmoid-based losses. Conventional training scheme of deep networks initializes the last linear classification layer with bias b = 0. As pointed out by Lin et al. <ref type="bibr" target="#b25">[26]</ref>, this could cause instability of training when using sigmoid function to get class probabilities. This is because using b = 0 with sigmoid function in the last layer induces huge loss at the beginning of the training as the output probability for each class is close to 0.5. Therefore, for training with sigmoid cross-entropy loss and focal loss, we assume the class prior is π = 1/C for each class, where C is the number of classes, and initialize the bias of the last layer as b = − log ((1 − π) /π). In addition, we remove the L 2 regularization (weight decay) for the bias b of the last layer.</p><p>We used Tensorflow <ref type="bibr" target="#b1">[2]</ref> to implement and train all the models by stochastic gradient descent with momentum. We trained residual networks with 32 layers (ResNet-32) to conduct all experiments on CIFAR. Similar to Zagoruyko et al. <ref type="bibr" target="#b45">[46]</ref>, we noticed a disturbing effect in training ResNets on CIFAR that both loss and validation error gradually went up after the learning rate drop, especially in the case of high data imbalance. We found that setting learning rate decay to 0.01 instead of 0.1 solved the problem. Models on CIFAR were trained with batch size of 128 on a single NVIDIA Titan X GPU for 200 epochs. The initial learning rate was set to 0.1, which was then decayed by 0.01 at 160 epochs and again at 180 epochs. We also used linear warm-up of learning rate <ref type="bibr" target="#b12">[13]</ref> in the first 5 epochs. On iNaturalist and ILSVRC 2012 data, we followed the same training strategy used by Goyal et al. <ref type="bibr" target="#b12">[13]</ref> and trained residual networks with batch size of 1024 on a single cloud TPU. Since the scale of focal loss is smaller than softmax and sigmoid cross-entropy loss, when training with focal loss, we used 2× and 4× larger learning rate on ILSVRC 2012 and iNaturalist respectively. Code, data and pre-trained models are available at: https://github.com/richardaecn/ class-balanced-loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Visual Recognition on Long-Tailed CIFAR</head><p>We conduct extensive studies on long-tailed CIFAR datasets with various imbalance factors. <ref type="table">Table 2</ref> shows the performance of ResNet-32 in terms of classification error rate on the test set. We present results of using softmax cross-entropy loss, sigmoid cross-entroy loss, focal loss with different γ, and the proposed class-balanced loss with best hyperparameters chosen via cross-validation. The search space of hyperparameters is {softmax, sigmoid, focal} for loss type, β ∈ {0.9, 0.99, 0.999, 0.9999} (Section 4), and γ ∈ {0.5, 1.0, 2.0} for focal loss <ref type="bibr" target="#b25">[26]</ref>.</p><p>From results in <ref type="table">Table 2</ref>, we have the following observations: (1) With properly selected hyperparameters, classbalanced loss is able to significantly improve the performance of commonly used loss functions on long-tailed datasets. (2) Softmax cross-entropy is overwelmingly used as the loss function for visual recognition tasks. How-  ever, following the training strategy in Section 5.2, sigmoid cross-entropy and focal loss are able to outperform softmax cross-entropy in most cases. <ref type="formula" target="#formula_2">(3)</ref> The best β is 0.9999 on CIFAR-10 unanimously. But on CIFAR-100, datasets with different imbalance factors tend to have different and smaller optimal β.</p><p>To understand the role of β and class-balanced loss better, we use the long-tailed dataset with imbalance factor of 50 as an example to show the error rate of the model when trained with and without the class-balanced term in <ref type="figure" target="#fig_4">Figure 5</ref>. Interestingly, for CIFAR-10, class-balanced term always improves the performance of the original loss and more performance gain can be obtained with larger β. However, on CIFAR-100, only small values of β improve the performance, whereas larger values degrade the performance. <ref type="figure" target="#fig_5">Figure 6</ref> illustrates the effective number of samples under different β. On CIFAR-10, when re-weighting based on β = 0.9999, the effective number of samples is close to the number of samples. This means the best re-weighting strategy on CIFAR-10 is similar with re-weighting by in-verse class frequency. On CIFAR-100, the poor performance of using larger β suggests that re-weighting by inverse class frequency is not a wise choice. Instead, we need to use a smaller β that has smoother weights across classes. This is reasonable because β = (N − 1)/N , so larger β means larger N . As discussed in Section 3, N can be interpreted as the number of unique prototypes. A fine-grained dataset should have a smaller N compared with a coarsegrained one. For example, the number of unique prototypes of a specific bird species should be smaller than the number of unique prototypes of a generic bird class. Since classes in CIFAR-100 are more fine-grained than CIFAR-10, CIFAR-100 should have smaller N compared with CIFAR-10. This explains our observations on the effect of β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Visual Recognition on Large-Scale Datasets</head><p>To demonstrate the proposed class-balanced loss can be used on large-scale real-world datasets, we present results of training ResNets with different depths on iNaturalist 2017, iNaturalist 2018 and ILSVRC 2012.   <ref type="table">Table 3</ref> summarizes the top-1 and top-5 error rate on the validation set of all datasets. We use the class-balanced focal loss since it has more flexibility and find β = 0.999 and γ = 0.5 yield reasonably good performance on all datasets. From results we can see that we are able to outperform commonly used softmax cross-entropy loss on ILSVRC 2012, and by large margins on iNaturalist. Notably, ResNet-50 is able to achieve comparable performance with ResNet-152 on iNaturalist and ResNet-101 on ILSVRC 2012 when using class-balanced focal loss to replace softmax crossentropy loss. Training curves on ILSVRC 2012 and iNaturalist 2018 are shown in <ref type="figure" target="#fig_6">Figure 7</ref>. Class-balanced focal loss starts to show its advantage after 60 epochs of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Discussion</head><p>In this work, we have presented a theoretically sounded framework to address the problem of long-tailed distribution of training data. The key idea is to take data overlap into consideration to help quantify the effective number of samples. Following this framework, we further propose a class-balanced loss to re-weight loss inversely with the effective number of samples per class. Extensive studies on artificially induced long-tailed CIFAR datasets have been conducted to understand and analyze the proposed loss. The benefit of the class-balanced loss has been verified by experiments on both CIFAR and large-scale datasets including iNaturalist and ImageNet.</p><p>Our proposed framework provides a non-parametric means of quantifying data overlap, since we don't make any assumptions about the data distribution. This makes our loss generally applicable to a wide range of existing models and loss functions. Intuitively, a better estimation of the effective number of samples could be obtained if we know the data distribution. In the future, we plan to extend our framework by incorporating reasonable assumptions on the data distribution or designing learning-based, adaptive methods.</p><p>We present more comprehensive experimental results in this appendix.</p><p>Visual Recognition on Long-Tailed CIFAR. On longtailed CIFAR datasets with imbalance factors of 200, 100, 50, 20 and 10, we trained ResNet-32 models <ref type="bibr" target="#b15">[16]</ref> using softmax loss (SM), sigmoid loss (SGM) and focal loss with both the original loss and class-balanced variants with β = 0.9, 0.99, 0.999 and 0.9999. For focal loss, we used γ = 0.5, 1.0 and 2.0. In addition to long-tailed CIFAR-10 and CIFAR-100 datasets mentioned in Section 5.1 of the main paper, we also conduct experiments on CIFAR-20 dataset, which has same images as CIFAR-100 dataset but annotated with 20 coarse-grained class-labels <ref type="bibr" target="#b23">[24]</ref>.</p><p>Classification error rates on long-tailed CIFAR-10, CIFAR-20 and CIFAR-100 datasets are shown in <ref type="figure">Figure 8</ref>, <ref type="figure">Figure 9</ref> and <ref type="figure">Figure 10</ref> respectively. Each row in the figure corresponds to the model trained with a specific loss function, in the form of {loss name} {γ} {β}, and each column corresponds to a long-tailed dataset with specific imbalance factor. We color-code each column to visualize results, with lighter colors represent lower error rates and darker colors for higher error rates. Note that results in each column of <ref type="table">Table 2</ref> in the main paper are classification error rates using the original losses and the best-performed class-balanced loss that is same as the lowest error rates in the corresponding column of <ref type="figure">Figure 8</ref> and <ref type="figure">Figure 10</ref> (marked by underline). From these results, we can see that in general, higher β yields better performance on CIFAR-10. However, on CIFAR-20 and CIFAR-100, lower β is needed to achieve good performance, suggesting that we cannot directly reweight the loss by inverse class-frequency, but to re-weight based on the effective number of samples. These results support the analysis in Section 5.3 of our main paper.  <ref type="figure">Figure 9</ref>. Classification error rates of ResNet-32 models trained with different loss functions on long-tailed CIFAR-20 (CIFAR-100 with 20 coarse-grained class-labels).  <ref type="figure">Figure 10</ref>. Classification error rates of ResNet-32 models trained with different loss functions on long-tailed CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>200</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>o v e r la p p e d( 1p )    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of the proposed class-balanced term(1 − β)/(1 − β ny ),where ny is the number of samples in the groundtruth class. Both axes are in log scale. For a long-tailed dataset where major classes have significantly more samples than minor classes, setting β properly re-balances the relative loss across classes and reduces the drastic imbalance of re-weighing by inverse class frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Number of training samples per class in artificially created long-tailed CIFAR-100 datasets with different imbalance factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Classification error rate when trained with and without the class-balanced term. On CIFAR-10, class-balanced loss yields consistent improvement across different β and the larger the β is, the larger the improvement is. On CIFAR-100, β = 0.99 or β = 0.999 improves the original loss, whereas a larger β hurts the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>CIFAR-10 (Imbalance Factor = 50) number of samples effective number of samples (0.9999) effective number of samples (0.999) effective number of samples (0.99) effective number of samples (0.9) CIFAR-100 (Imbalance Factor = 50) number of samples effective number of samples (0.9999) effective number of samples (0.999) effective number of samples (0.99) effective number of samples (0.9) Effective number of samples with different β on long-tailed CIFAR-10 and CIFAR-100 with the imbalance of 50. This is a semi-log plot with vertical axis in log scale. When β → 1, effective number of samples is same as number of samples. When β is small, effective number of samples are similar across all classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Training curves of ResNet-50 on ILSVRC 2012 (left) and iNaturalist 2018 (right). Class-balanced focal loss with β = 0.999 and γ = 0.5 outperforms softmax cross-entropy after 60 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>63.77 60.40 54.68 47.41 42.01 28.39 *</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>1</cell></row><row><cell>Softmax</cell><cell>34.32</cell><cell>29.64</cell><cell>25.19</cell><cell>17.77</cell><cell>13.61</cell><cell cols="6">6.61 65.16 61.68 56.15 48.86 44.29</cell><cell>29.07</cell></row><row><cell>Sigmoid</cell><cell>34.51</cell><cell>29.55</cell><cell>23.84</cell><cell>16.40</cell><cell>12.97</cell><cell cols="6">6.36 64.39 61.22 55.85 48.57 44.73</cell><cell>28.39</cell></row><row><cell>Focal (γ = 0.5)</cell><cell>36.00</cell><cell>29.77</cell><cell>23.28</cell><cell>17.11</cell><cell>13.19</cell><cell cols="6">6.75 65.00 61.31 55.88 48.90 44.30</cell><cell>28.55</cell></row><row><cell>Focal (γ = 1.0)</cell><cell>34.71</cell><cell>29.62</cell><cell>23.29</cell><cell>17.24</cell><cell>13.34</cell><cell cols="6">6.60 64.38 61.59 55.68 48.05 44.22</cell><cell>28.85</cell></row><row><cell>Focal (γ = 2.0)</cell><cell>35.12</cell><cell>30.41</cell><cell>23.48</cell><cell>16.77</cell><cell>13.68</cell><cell cols="6">6.61 65.25 61.61 56.30 48.98 45.00</cell><cell>28.52</cell></row><row><cell cols="12">Class-Balanced 12.51 6.36  Loss Type 31.11 25.43 20.73 15.64 SM Focal Focal SM SGM SGM Focal Focal SGM Focal Focal</cell><cell>SGM</cell></row><row><cell>β</cell><cell cols="5">0.9999 0.9999 0.9999 0.9999 0.9999</cell><cell>-</cell><cell>0.9</cell><cell>0.9</cell><cell>0.99</cell><cell cols="2">0.99 0.999</cell><cell>-</cell></row><row><cell>γ</cell><cell>-</cell><cell>1.0</cell><cell>2.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.0</cell><cell>1.0</cell><cell>-</cell><cell>0.5</cell><cell>0.5</cell><cell>-</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Classification error rates of ResNet-32 models trained with different loss functions on long-tailed CIFAR-10.</figDesc><table><row><cell>SM</cell><cell>57.90</cell><cell>51.60</cell><cell>45.95</cell><cell>38.20</cell><cell>33.77</cell><cell>max</cell></row><row><cell>SM_0.9</cell><cell>57.71</cell><cell>51.36</cell><cell>47.12</cell><cell>37.77</cell><cell>33.89</cell><cell></cell></row><row><cell>SM_0.99</cell><cell>56.30</cell><cell>48.80</cell><cell>44.98</cell><cell>37.73</cell><cell>33.68</cell><cell></cell></row><row><cell>SM_0.999</cell><cell>63.28</cell><cell>53.67</cell><cell>45.83</cell><cell>36.12</cell><cell>32.28</cell><cell></cell></row><row><cell>SM_0.9999</cell><cell>68.80</cell><cell>60.51</cell><cell>49.13</cell><cell>36.84</cell><cell>32.23</cell><cell></cell></row><row><cell>SGM</cell><cell>58.54</cell><cell>51.98</cell><cell>47.86</cell><cell>37.53</cell><cell>34.16</cell><cell></cell></row><row><cell>SGM_0.9</cell><cell>58.87</cell><cell>53.79</cell><cell>47.57</cell><cell>37.42</cell><cell>32.82</cell><cell></cell></row><row><cell>SGM_0.99</cell><cell>54.82</cell><cell>49.99</cell><cell>45.23</cell><cell>36.96</cell><cell>33.75</cell><cell></cell></row><row><cell>SGM_0.999</cell><cell>61.48</cell><cell>48.52</cell><cell>46.14</cell><cell>35.53</cell><cell>31.66</cell><cell></cell></row><row><cell>SGM_0.9999</cell><cell>62.79</cell><cell>55.82</cell><cell>50.35</cell><cell>36.73</cell><cell>31.94</cell><cell></cell></row><row><cell>Focal_0.5</cell><cell>59.05</cell><cell>52.86</cell><cell>48.34</cell><cell>37.87</cell><cell>33.72</cell><cell></cell></row><row><cell>Focal_0.5_0.9</cell><cell>58.69</cell><cell>50.95</cell><cell>47.60</cell><cell>36.89</cell><cell>33.97</cell><cell></cell></row><row><cell>Focal_0.5_0.99</cell><cell>54.99</cell><cell>49.76</cell><cell>45.79</cell><cell>37.27</cell><cell>33.35</cell><cell></cell></row><row><cell>Focal_0.5_0.999</cell><cell>59.13</cell><cell>49.04</cell><cell>45.55</cell><cell>35.12</cell><cell>31.69</cell><cell></cell></row><row><cell>Focal_0.5_0.9999</cell><cell>65.97</cell><cell>53.61</cell><cell>48.06</cell><cell>36.19</cell><cell>31.50</cell><cell></cell></row><row><cell>Focal_1.0</cell><cell>58.82</cell><cell>52.64</cell><cell>47.52</cell><cell>39.59</cell><cell>33.86</cell><cell></cell></row><row><cell>Focal_1.0_0.9</cell><cell>58.49</cell><cell>52.94</cell><cell>47.68</cell><cell>38.15</cell><cell>33.88</cell><cell></cell></row><row><cell>Focal_1.0_0.99</cell><cell>56.74</cell><cell>49.75</cell><cell>44.75</cell><cell>37.98</cell><cell>33.01</cell><cell></cell></row><row><cell>Focal_1.0_0.999</cell><cell>58.53</cell><cell>53.23</cell><cell>45.49</cell><cell>35.37</cell><cell>31.54</cell><cell></cell></row><row><cell>Focal_1.0_0.9999</cell><cell>61.73</cell><cell>54.18</cell><cell>47.85</cell><cell>36.53</cell><cell>31.66</cell><cell></cell></row><row><cell>Focal_2.0</cell><cell>57.75</cell><cell>52.81</cell><cell>50.16</cell><cell>38.05</cell><cell>33.95</cell><cell></cell></row><row><cell>Focal_2.0_0.9</cell><cell>58.98</cell><cell>52.19</cell><cell>47.88</cell><cell>37.72</cell><cell>33.35</cell><cell></cell></row><row><cell>Focal_2.0_0.99</cell><cell>55.43</cell><cell>50.08</cell><cell>45.72</cell><cell>38.49</cell><cell>33.35</cell><cell></cell></row><row><cell>Focal_2.0_0.999</cell><cell>55.37</cell><cell>54.99</cell><cell>47.33</cell><cell>36.86</cell><cell>31.66</cell><cell></cell></row><row><cell>Focal_2.0_0.9999</cell><cell>62.62</cell><cell>60.39</cell><cell>46.38</cell><cell>36.50</cell><cell>30.90</cell><cell>min</cell></row><row><cell cols="2">SM SM_0.9 SM_0.99 SM_0.999 SM_0.9999 SGM SGM_0.9 SGM_0.99 SGM_0.999 SGM_0.9999 Focal_0.5 Focal_0.5_0.9 Focal_0.5_0.99 Focal_0.5_0.999 Focal_0.5_0.9999 Focal_1.0 Focal_1.0_0.9 Focal_1.0_0.99 Focal_1.0_0.999 Focal_1.0_0.9999 Focal_2.0 Focal_2.0_0.9 Focal_2.0_0.99 Focal_2.0_0.999 Focal_2.0_0.9999 Imbalance Factor Figure 8. 200 34.32 35.00 34.74 31.31 31.11 34.51 35.24 34.69 32.24 31.93 36.00 35.37 34.65 31.93 31.97 34.71 34.97 34.90 31.85 32.06 35.12 35.08 34.65 33.34 33.77 100 29.64 30.07 28.53 29.24 27.63 29.55 29.89 29.53 28.59 27.19 29.77 28.23 28.23 28.49 26.11 29.62 28.93 29.61 28.63 25.43 30.41 29.70 28.43 27.61 28.08 50 25.19 24.54 23.50 22.72 21.95 23.84 23.71 23.18 22.23 21.04 23.28 23.36 22.88 21.87 20.76 23.29 23.30 22.63 22.01 20.78 23.48 23.50 24.28 22.21 20.73 20 17.77 17.60 16.90 17.24 15.64 16.40 16.75 16.55 15.87 16.02 17.11 16.73 16.90 16.68 16.18 17.24 16.54 17.30 16.22 16.53 16.77 17.36 17.16 16.76 15.79 10 13.61 14.12 13.23 13.30 13.46 12.97 13.26 13.24 12.90 12.51 13.19 13.19 13.35 12.66 12.93 13.34 13.41 12.52 12.90 13.08 13.68 13.35 13.07 12.54 12.55 min max Imbalance Factor</cell><cell>100</cell><cell>50</cell><cell>20</cell><cell>10</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For simplicity, we derive the loss function by assuming there is only one ground-truth label for a sample.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was supported in part by a Google Focused Research Award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: More Experimental Results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://github.com/visipedia/inat_comp.5" />
		<title level="m">The iNaturalist 2018 Competition Dataset</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sharing representations for long tail computer vision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class rectification hard mining for imbalanced deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<editor>IJ-CAI</editor>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and system sciences</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00941</idno>
		<title level="m">Deep active learning over the long tail</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adasyn: Adaptive synthetic sampling approach for imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random coverings in several dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Janson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mathematica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The class imbalance problem: A systematic study. Intelligent data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Methods of reducing sample size in monte carlo computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Operations Research Society of America</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The advanced theory of statistics. The advanced theory of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kendall</surname></persName>
		</author>
		<idno>1946. 1</idno>
		<imprint/>
	</monogr>
	<note>2nd Ed</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV, 2015. 1, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep imbalanced attribute classification using visual attention aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A comparative study of cost-sensitive boosting algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The devil is in the tails: Fine-grained classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01450</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset. California Institute of Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Feature transfer learning for deep face recognition with long-tail data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09014</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A scalable exemplar-based subspace clustering algorithm for classimbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Training cost-sensitive neural networks with methods addressing the class imbalance problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via classbalanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
