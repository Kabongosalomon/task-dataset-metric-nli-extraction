<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Danielczuk</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Matl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Mahler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
						</author>
						<title level="a" type="main">Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive handlabeled datasets are available. As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any handlabeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE. arXiv:1809.05825v2 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Category-agnostic instance segmentation, or the ability to mask the pixels belonging to each individual object in a scene regardless of the object's class, has potential to enhance robotic perception pipelines for applications such as instancespecific grasping, where a target object must be identified and grasped among potentially unknown distractor objects in a cluttered environment. For example, recent approaches to grasp planning on unknown objects have used deep learning to generate robot grasping policies from massive datasets of images, grasps, and reward labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. While these methods are effective at generalizing across a wide variety of objects, they search for high-quality grasp affordances across an entire scene and do not distinguish between the objects they are grasping. Nonetheless, these methods may be extended to plan grasps for a particular target object by constraining grasp planning to an object mask produced by category-agnostic instance segmentation.  Object segmentation without prior models of the objects is difficult due to sensor noise and occlusions. Computer vision techniques for generating category-agnostic object proposals <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> often oversegment and require secondary pruning steps to find a set of valid independent objects. A variety of recent methods have demonstrated the ability to accurately segment RGB images into pre-defined semantic classes such as humans, bicycles, and cars by training deep neural networks on massive, hand-labeled datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. These techniques require time-consuming human labeling to generate training data <ref type="bibr" target="#b6">[7]</ref>, and existing datasets consist of RGB images of natural scenes that are very different from the types of cluttered scenes commonly encountered in warehouses or fulfillment centers. Adding new object classes or generating data for new types of environments requires additional manual labeling. Thus, in robotics, pixelwise object segmentation is often avoided <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> or used for a small number of object classes, where semantic segmentation networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> or predefined features <ref type="bibr" target="#b11">[12]</ref> can be used.</p><p>To address these issues, we present a method and dataset for training Mask R-CNN <ref type="bibr" target="#b5">[6]</ref>, a popular instance segmentation network, to perform category-agnostic instance segmentation on real depth images without training on hand-labeled data -and, in fact, without training on real data at all. We build on recent research which suggests that networks trained on synthetic depth images can transfer well from simulation to reality in some domains <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15]</ref> and that depth cues can enhance instance segmentation in simulated images <ref type="bibr" target="#b16">[16]</ref>. To learn an instance segmentation network that transfers from simulation to reality, we propose to train on a large synthetic dataset of depth images with domain randomization <ref type="bibr" target="#b17">[17]</ref> over a diverse set of 3D objects, camera poses, and camera intrinsic parameters. This paper contributes: 1) A method for rapidly generating a synthetic dataset of depth images and segmentation masks using domain randomization for robust transfer from simulation to reality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) The Warehouse Instance Segmentation Dataset for</head><p>Object Manipulation (WISDOM), a hybrid sim/real dataset designed for training and evaluating categoryagnostic instance segmentation methods in the context of robotic bin picking.</p><formula xml:id="formula_0">3) Synthetic Depth Mask R-CNN (SD Mask R-CNN), a</formula><p>Mask R-CNN adaptation designed to perform deep category-agnostic object instance segmentation on depth images, trained on WISDOM-Sim. 4) Experiments evaluating the sim-to-real generalization abilities of SD Mask R-CNN and performance benchmarks comparing it against a set of baseline instance segmentation methods. In an experimental evaluation on WISDOM-Real's highresolution dataset, SD Mask R-CNN achieves significantly higher average precision and recall than baseline learning methods fine-tuned on WISDOM-Real training images, and also generalizes to a low-resolution sensor. We employ SD Mask R-CNN as part of an instance-specific grasping pipeline on an ABB YuMi bimanual industrial robot and find that it can increase success rate by 20% over standard point cloud segmentation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>This work builds on prior research in region proposal generation, neural architectures for image segmentation, and use of synthetic data for learning models in computer vision and robotics. The approach presented here is motivated and informed by robotic grasping, manipulation, and bin-picking tasks.</p><p>a) Box and Region Proposals: Early work in computer vision focused on using bottom-up cues for generating box and region proposals in images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>. Such techniques typically detect contours in images to obtain a hierarchical segmentation. Regions from such a hierarchical segmentation are combined together and used with low-level objectness cues to produce a list of regions that cover the objects present in the image. The focus of these techniques is on getting high recall, and the soup of output region proposals is used with a classifier to detect or segment objects of interest <ref type="bibr" target="#b21">[21]</ref>.</p><p>More recently, given advances in learning image representations, researchers have used feature learning techniques (specifically CNN based models) to tackle this problem of producing bounding box proposals <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref> and region proposals <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref>. Unlike bottom-up segmentation methods, these techniques use data-driven methods to learn highlevel semantic markers for proposing and classifying object segments. Some of these learning-based region proposal techniques <ref type="bibr" target="#b25">[25]</ref> have built upon advances in models for image segmentation and use fine-grained information from early layers in CNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">26]</ref> to produce high quality regions.</p><p>While most work in computer vision has used RGB images to study these problems, researchers have also studied similar problems with depth data. Once again there are bottomup techniques <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32]</ref> that use low-level geometry-based cues to come up with region proposals, as well as more recent top-down learning-based techniques to produce proposals <ref type="bibr" target="#b33">[33]</ref> in the form of image segments or 3D bounding boxes that contain objects in the scene. Shao et al. <ref type="bibr" target="#b16">[16]</ref> combined color and depth modalities, featurizing objects and clustering the features to produce object instance segmentation masks on simulated RGB-D images.</p><p>A parallel stream of work has tackled the problem of classspecific segmentation. Some of these works ignore object instances and study semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35]</ref>, while others try to distinguish between instances <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">36]</ref>. Similar research has also been done in context of input from depth sensors <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref>. b) Synthetic Data for Training Models: Our research is related to a number of recent efforts for rapidly acquiring large training datasets containing image and ground truth masks with limited or no human labeling. The most natural way is to augment training with synthetic color and depth images collected in simulation. This idea has been explored extensively for training semantic segmentation networks for autonomous driving <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b15">15]</ref> and for estimating human and object pose <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref>. Another approach is to use self-supervision to increase training dataset size by first hand-aligning 3D models to images with easy-to-use interfaces <ref type="bibr" target="#b43">[43]</ref> or algorithmically matching a set of 3D CAD models to initial RGB-D images <ref type="bibr" target="#b8">[9]</ref>, and then projecting each 3D model into a larger set of images from camera viewpoints with known 6-DOF poses. In comparison, we generate synthetic training datasets for category-agnostic object segmentation in a robot bin picking domain. c) Robotics Applications: Segmentation methods have been applied extensively to grasping target objects, most notably in the Amazon Robotics Challenge (ARC). Many classical grasping pipelines consisted of an alignment phase, in which 3D CAD models or scans are matched to RGB-D point clouds, and an indexing phase, in which precomputed grasps are executed given the estimated object pose <ref type="bibr" target="#b44">[44]</ref>. In the 2015 ARC, the winning team followed a similar strategy, using a histogram backprojection method to segment objects from shelves and point cloud heuristics for grasp planning <ref type="bibr" target="#b45">[45]</ref>. In 2016, many teams used deep learning to segment objects for the alignment phase, training semantic segmentation networks with separate classes for each object instance on hand-labeled <ref type="bibr" target="#b46">[46]</ref> or self-supervised datasets <ref type="bibr" target="#b8">[9]</ref>. Team ACRV, the winners of the 2017 ARC, fine-tuned RefineNet to segment and classify 40 unique known objects in a bin, with a system to quickly learn new items with a semi-automated procedure <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b47">47]</ref>. In contrast, our method uses deep learning for category-agnostic segmentation, which can can be used to segment a wide variety of objects not seen in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM STATEMENT</head><p>We consider the problem of depth-based category-agnostic instance segmentation, or finding subsets of pixels corresponding to unique unknown objects in a single depth image.</p><p>To formalize category-agnostic instance segmentation, we use the following definitions: Every state x corresponds to a set of visible foreground object masks :</p><formula xml:id="formula_1">1) States: Let x = {O 1 , . . . , O m , B 1 , . . . , B n ,</formula><formula xml:id="formula_2">M = {(M i : M i = ∅) ∀i ∈ {1, . . . , m}}.</formula><p>The goal of category-agnostic object instance segmentation is to find M given a depth image y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SYNTHETIC DATASET GENERATION METHOD</head><p>To efficiently learn category-agnostic instance segmentation, we generate a synthetic training dataset of N paired depth images and ground truth object masks:</p><formula xml:id="formula_3">D = {(y k , M k )} N k=1</formula><p>. The proposed dataset generation method samples training examples using two distributions: a taskspecific state distribution, p(x), that randomizes over a diverse set of object geometries, object poses, and camera parameters; and an observation distribution, p(y|x), that models sensor operation and noise.</p><p>To sample a single datapoint, we first sample a state x k ∼ p(x) using a dataset of 3D CAD models, dynamic simulation, and domain randomization <ref type="bibr" target="#b17">[17]</ref> over the object states, camera intrinsic parameters, and camera pose for robust transfer from simulation to reality. Next, we sample a synthetic depth image y k ∼ p(y k | x k ) using rendering. Finally, we compute the visible object masks M j determining the set of pixels in the depth image with a corresponding 3D point on the surface of object O j . Specifically, we render a depth image of each object in isolation and add a pixel to the mask if it is within a threshold from the corresponding full-state depth image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. WISDOM DATASET</head><p>To test the effectiveness of this method, we generate the Warehouse Instance Segmentation Dataset for Object Manipulation (WISDOM), a hybrid sim/real dataset designed to train and test category-agnostic instance segmentation networks in a robotic bin-picking environment. WISDOM includes WISDOM-Sim, a large synthetic dataset of depth images generated using the simulation pipeline, and WISDOM-Real, a set of hand-labeled real RGB-D images for evaluating performance in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. WISDOM-Sim</head><p>For WISDOM-Sim, we consider an environment for robotic bin picking consisting of a table and a bin full of objects imaged with an overhead depth camera. In general, p(x) can be represented as a product over distributions on:</p><p>1) Foreground and background object counts (m and n):</p><p>We draw m from a Poisson distribution with mean λ = 7.5, truncated to a maximum of 10. We set n = 2 since we use two fixed background objects: a table and a bin. 2) Background object states ({B j } n 1 ): We set the geometry and pose of the background objects to fixed values.</p><formula xml:id="formula_4">3) Foreground object states ({O j } m 1 ):</formula><p>We sample the m foreground objects uniformly from a dataset of 1,664 3D triangular mesh models from Thingiverse, including objects augmented with artificial cardboard backing to mimic common packages. Object poses are sampled by selecting a random pose above the bin from a uniform distribution, dropping each object into the bin one-by-one in pybullet dynamic simulation, and simulating until all objects come to rest <ref type="bibr" target="#b48">[48]</ref>. 4) Camera state (C): We sample camera poses uniformly at random from a bounded set of spherical coordinates above the bin. We sample intrinsic parameters uniformly at random from intervals centered on the parameters of a Photoneo PhoXi S industrial depth camera. Because the high-resolution depth sensor we use has very little white noise, we fix p(y|x) to simply perform perspective depth rendering using an OpenGL z-buffer.</p><p>We used these distributions to sample a dataset of 50,000 synthetic depth images containing 320,000 individual ground-truth segmasks. Generating 50k datapoints took approximately 26 hours on a desktop with an Intel i7-6700 3.4 GHz CPU. The synthetic images are broken into training and validation sets with an 80/20 split, where the split is both on images as well as objects (i.e. no objects appear in both the training and validation sets). The training set has 40,000 images of 1,280 unique objects, while the validation set contains 10,000 images of 320 unique objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. WISDOM-Real</head><p>To evaluate the real-world performance of categoryagnostic instance segmentation methods and their ability to generalize to novel objects across different types of depth sensors, we collected a hand-labeled dataset of real RGB-D images. WISDOM-Real contains a total of 800 hand-labeled RGB-D images of cluttered bins, with 400 from both a highresolution Photoneo PhoXi industrial sensor (1032x772 with 0.05 mm depth precision) and a low-resolution Primesense Carmine (640x480 with 1 mm depth precision). Missing  depth values were filled in using fast inpainting <ref type="bibr" target="#b49">[49]</ref> with an averaging kernel.</p><p>The objects in these bins were sampled from a diverse set of 50 novel objects with highly-varied geometry, all of which are commonly found around the home (see <ref type="figure" target="#fig_3">Figure 3</ref>), and have no corresponding CAD model in WISDOM-Sim. This set of 50 objects was split randomly into even training and test sets. The training set is reserved for learning methods that require real training data, while the test set is used to test generalization to novel objects. We generated 100 bins containing objects from the training set and an additional 300 bins containing objects from the test set. For each bin, a truncated Poisson distribution (λ = 5) was used to determine the number of objects to be placed in the bin, and the objects were sampled uniformly at random from the appropriate subset. The sampled objects were shaken together in a plastic box to randomize their poses and dumped into a large black bin with identical geometry to the bin used in WISDOM-Sim, and once the objects settled, each camera took an RGB-D image from above. Sample bins are shown in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>After dataset collection, all images were hand-labeled to identify unique object masks using the same tools used to label the COCO dataset <ref type="bibr" target="#b6">[7]</ref>. We estimate that labeling the 800 real images took over 35 hours of effort due to time spent collecting on images, labeling object masks, and data cleaning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SYNTHETIC DEPTH MASK R-CNN</head><p>To adapt Mask R-CNN to perform category-agnostic instance segmentation on depth images, we made several modifications:</p><p>1) We treat depth images as grayscale images and triplicate the depth values across three channels to match the input size of the original network. 2) We reduce the number of classes to two. Each proposed instance mask is classified as background or as a foreground object. Of these, only foreground masks are visualized.  which uses a ResNet 101 and FPN backbone <ref type="bibr" target="#b50">[50]</ref>. This implementation closely follows the original Mask R-CNN paper in <ref type="bibr" target="#b5">[6]</ref>. We made the modifications listed above and trained the network on WISDOM-Sim with an 80-20 train-val split for 60 epochs with a learning rate of 0.01, momentum of 0.9, and weight decay of 0.0001 on a Titan X GPU. On our setup, training took approximately 24 hours and a single forward pass took 105 ms (average of 600 trials). We call the final trained network a Synthetic Depth Mask R-CNN (SD Mask R-CNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTS</head><p>We compare performance of SD-Mask-R-CNN with several baseline methods for category-agnostic instance segmentation on RGB-D images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baselines</head><p>We use four baselines: two Point Cloud Library methods and two color-based Mask R-CNNs pre-trained on COCO and fine-tuned on WISDOM-Real images. For fine-tuning, the image shape and dataset-specific parameters such as mean pixel were set based on the dataset being trained on (e.g., either color or depth images).</p><p>1) Point Cloud Library Baselines: The Point Cloud Library, an open-source library for processing 3D data, provides several methods for segmenting point clouds <ref type="bibr" target="#b28">[28]</ref>. We used two of these methods: Euclidean clustering and regiongrowing segmentation. Euclidean clustering adds points to clusters based on the Euclidean distance between neighboring points. If a point is within a sphere of a set radius from its neighbor, then it is added to the cluster <ref type="bibr" target="#b27">[27]</ref>. Region-growing segmentation operates in a similar way to Euclidean clustering, but instead of considering Euclidean distance between neighboring points, it discriminates clusters based on the difference of angle between normal vectors and curvature <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. We tuned the parameters of each method on the first ten images of the high-res and low-res WISDOM-Real training sets.</p><p>2) Fine-Tuned Mask R-CNN Baselines: As deep learning baselines, we used two variants of Mask R-CNN, one trained on color images and one trained on depth images triplicated across the color channels. Both these variants were pre-trained on RGB images from the COCO dataset and then fine-tuned using the 100 color or depth images from the WISDOM-Real high-res training set. All images were rescaled and padded to be 512 by 512 pixels, and the depth images were treated as grayscale images. Both implementations were fine-tuned on the 100 images for 10 epochs with a learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmarks</head><p>We compare the category-agnostic instance segmentation performance of all methods using the widely-used COCO instance segmentation benchmarks <ref type="bibr" target="#b6">[7]</ref>. Of the metrics in the benchmark, we report average precision (AP) over ten IoU thresholds over a range from 0.50 to 0.95 with a step size of 0.05, and we report average recall (AR) given a maximum of 100 detections. Averaging over several IoU thresholds rewards better localization from detectors, so we report this score as our main benchmark as opposed to simply the average precision for an IoU threshold of 0.50. All scores are for the segmentation mask IoU calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance</head><p>We ran each of the methods on three test datasets: 2000 images from the WISDOM-Sim validation set and 300 real test images each from the Primesense and Phoxi cameras. All real test images were rescaled and padded to be 512 by 512 pixels. The results are shown in <ref type="table" target="#tab_1">Table I</ref>, and full precision-recall curves for each dataset can be found in the supplemental file. The SD Mask R-CNN network shows significant improvement over both the PCL baselines and the fine-tuned Mask R-CNN baselines, and is also robust to sensor noise.</p><p>An example of each method's performance on each of the real datasets can be seen in <ref type="figure">Figure 5</ref>. The visualizations suggest that the PCL baselines tend to undersegment the scene and cluster nearby objects as a single object. The finetuned Mask R-CNN implementations separate objects more effectively, but the color implementation may incorrectly predict multiple object segments on different colored pieces of the same object. In contrast, the SD Mask R-CNN network can group parts of objects that may be slightly discontinuous in depth space, and is agnostic to color. It is able to segment the scenes with high accuracy despite significant occlusion and variation in shape. Table I also shows SD Mask R-CNN can perform similarly on low-res Primesense images, suggesting that the network can generalize to other camera intrinsics and poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Robotics Application: Instance-Specific Grasping</head><p>To demonstrate the usefulness of SD Mask R-CNN in a robotics task, we ran experiments utilizing category-agnostic instance segmentation as the first phase of an instancespecific grasping pipeline. In this task, the goal is to identify and grasp a particular target object from a bin filled with other distractor objects.</p><p>We randomly selected a subset of ten objects from WISDOM-Real's test set and trained an instance classification network on ten RGB images of each object from a <ref type="figure">Fig. 5</ref>: Images from both sensors with ground truth and object masks generated by each method. For the baseline methods, the better performing method was chosen for each scenario. While the baseline methods tend to undersegment (PCL) or oversegment (Fine-Tuned Mask R-CNN), SD Mask R-CNN segments the objects correctly. variety of poses by fine-tuning a VGG-16 <ref type="bibr" target="#b51">[51]</ref> classifier. During each iteration, one object at random was chosen as the target and all of the objects were shaken, dumped into the black bin, and imaged with the Photoneo PhoXi depth sensor. We then segmented the depth image using either SD Mask R-CNN or one of the baseline methods, colorized the mask using the corresponding RGB sensor image, and then labeled each mask with the pre-trained classifier. The mask with the highest predicted probability of being the target was fed to a Dex-Net 3.0 <ref type="bibr" target="#b14">[14]</ref> policy for planning suction cup grasps, and the planned grasp was executed by an ABB YuMi equipped with a suction gripper. Each iteration was considered a success if the target object was successfully grasped, lifted, and removed from the bin.</p><p>We measured performance on 50 iterations of this task each for three instance segmentation methods. SD Mask R-CNN achieved a success rate of 74%, significantly higher than the PCL Euclidean clustering baseline (56%). Furthermore, SD Mask R-CNN had performance on par with a Mask R-CNN that was fine-tuned on 100 real color images (78%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. DISCUSSION AND FUTURE WORK</head><p>We presented WISDOM, a dataset of images and object segmentation masks for the warehouse object manipulation environment, images that are currently unavailable in other major segmentation datasets. Training SD Mask R-CNN, an adaptation of Mask R-CNN, on synthetic depth images from WISDOM-Sim enables transfer to real images without expensive hand-labeling, suggesting that depth alone can encode segmentation cues. SD Mask R-CNN outperforms PCL segmentation methods and Mask R-CNN fine-tuned on real color and depth images for the object instance segmentation task, and can be used as part of a successful instance-specific grasping pipeline. <ref type="figure">Figure 6</ref> shows preliminary results of the effects of training image dataset size and training object dataset size on the performance of SD Mask R-CNN. We trained SD Mask R-CNN on random subsets of the training dataset with sizes {4k, 8k, 20k, 40k} and on four 40k image datasets containing 100, 400, 800, and 1600 unique objects. Both AP and AR increase with image dataset size and number of <ref type="figure">Fig. 6</ref>: Effect of increasing synthetic training dataset size (left) and increasing the number of unique objects (right) on the performance of SD Mask R-CNN on the WISDOM-Real high-res test set. These results suggest that more data could continue to increase performance. unique objects used in training, although the image dataset size appears to have a stronger correlation with performance. These results suggest that performance might continue to improve with orders of magnitude more training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. ACKNOWLEDGMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. WISDOM Dataset Statistics</head><p>The real dataset has 3849 total instances with an average of 4.8 object instances per image, fewer than the 7.7 instances per image in the Common Objects in Context (COCO) dataset and the 6.5 instances per image in WISDOM-Sim, but more instances per image than both ImageNet and PASCAL VOC (3.0 and 2.3, respectively). Additionally, it has many more instances that are close to, overlapping with, or occluding other instances, thus making it a more representative dataset for tasks such as bin picking in cluttered environments. Since it is designed for manipulation tasks, most of the objects are much smaller in area than in the COCO dataset, which aims to more evenly distribute instance areas. In the WISDOM dataset, instances take up only 2.28% of total image area in the simulated images, 1.60% of the total image area on average for the high-res images, and 0.94% of the total image area for the low-res images. <ref type="figure">Figure 7</ref> compares the distributions of these metrics to the COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Precision-Recall Evaluation</head><p>We performed additional experiments to analyze the precision-recall performance of SD Mask R-CNN along with the baseline methods for category-agnostic instance segmentation on RGB-D images: RGB object proposals <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, cluster-based geometric segmentation methods from PCL <ref type="bibr" target="#b28">[28]</ref>, and Mask R-CNN fine-tuned for instance segmentation from the WISDOM-Real dataset. We also include a variant of SD Mask R-CNN fine-tuned on real depth images from WISDOM-Real. We evaluate performance using the widely-used COCO instance segmentation benchmarks <ref type="bibr" target="#b6">[7]</ref>.</p><p>The RGB object proposal baselines were based on two algorithms: Geodesic Object Proposals (GOP) <ref type="bibr" target="#b3">[4]</ref> and Multiscale Combinatorial Grouping (MCG) <ref type="bibr" target="#b2">[3]</ref>. GOP identifies critical level sets in signed geodesic distance transforms of the original color images and generates object proposal masks based on these <ref type="bibr" target="#b3">[4]</ref>. MCG employs combinatorial grouping of multi-scale segmentation masks and ranks object proposals in the image. For each of these methods, we take the top 100 detections. We then remove masks where less than half of the area of the mask overlaps with the foreground segmask of the image and apply non-max suppression with a threshold of 0.5 Intersection-over-Union (IoU). <ref type="figure">Figure 8</ref> shows precision-recall curves on three test datasets: 2000 images from the WISDOM-Sim validation set and 300 test images each from the Primesense and Phoxi cameras. Our learning-based method produces a ranked list of regions, that can be operated at different operating points. Not only does SD Mask-RCNN achieve higher precision at the same operating point than PCL, it is able to achieve a much higher overall recall without any compromise in precision at the cost of only a handful of extra regions.</p><p>We also evaluate the performance of SD Mask R-CNN and several baseline on the WISDOM-Sim test set.</p><p>To evaluate the effectiveness of SD Mask R-CNN in a robotics task, we performed experiments in which we used  segmentation as the first phase of an instance-specific grasping pipeline. In the experiment, an ABB YuMi robot was presented a pile of ten known objects in a bin and instructed to grasp one specific target object from the bin using a suction gripper. An attempt was considered successful if the robot lifted the target object out of the bin and successfully transported the object to a receptacle. One approach to this problem is to collect real images of the items piled in the bin, labeling object masks in each image, and using that data to train or fine-tune a deep neural network for object classification and segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. However, that data collection process is time consuming and must be re-performed for new object sets, and training and fine-tuning a Mask R-CNN can take some time. Instead, our experimental pipeline uses a class-agnostic instance segmentation method followed by a standard CNN classifier, which is easier to generate training data for and faster to train.</p><p>To train the classifier, we collected ten RGB images of each target object in isolation. Each image was masked and cropped automatically using depth data, and then each crop was augmented by randomly masking the crop with overlaid planes to simulate occlusions. From the initial set of 100 images, we produced a dataset of 1,000 images with an 80-20 train-validation split. We then used this dataset to fine-tune the last four layers of a VGG-16 network <ref type="bibr" target="#b51">[51]</ref> pre-trained on Imagenet. Fine-tuning the network for 20 epochs took less than two minutes on a Titan X GPU, and the only human intervention required was capturing the initial object images.</p><p>Given a pre-trained classifier, the procedure used to execute instance-specific suction grasps was composed of three phases. First, an RGB-D image was taken of the bin with the PhoXi and a class-agnostic instance segmentation method is used to detect object masks. Then, the classifier was used to choose the mask that is most likely to belong to the target object. Finally, a suction grasp was planned and executed by constraining grasps planned by Dex-Net 3.0 to the target object mask <ref type="bibr" target="#b14">[14]</ref>.</p><p>We benchmarked SD Mask R-CNN on this pipeline against two segmentation methods. First, we compared against the PCL Euclidean Clustering method to evaluate baseline performance. Second, we compared with Mask R-CNN fine-tuned on the WISDOM-Real training dataset to evaluate whether SD Mask R-CNN is competitive with methods trained on real data.</p><p>Each segmentation method was tested in 50 independent <ref type="figure">Fig. 7</ref>: Distributions of the instances per image and instance size for the WISDOM dataset, with comparisons to the COCO dataset. Average number of instances are listed in parentheses next to each dataset. The number of instances and relative object size make this dataset more applicable to manipulation tasks. <ref type="figure">Fig. 8</ref>: Average Jaccard Index, Average Recall, and Precision-Recall (at IoU = 0.5) curves for each method and the high-res (top row) and low-res (bottom row) dataset, using segmentation metrics. The fine-tuned SD Mask R-CNN implementation outperforms all baselines on both sensors in the WISDOM-real dataset. The precision-recall curves suggest that the dataset contains some hard instances that are unable to be recalled by any method. These instances are likely heavily occluded objects whose masks get merged with the adjacent mask or flat objects that cannot be distinguished from the bottom of the bin.  trials. Each trial involved shaking the ten objects in a box, pouring them into the bin, and allowing the system to select a target object uniformly at random to attempt to locate and grasp. The results of these instance-specific grasping experiments are shown in <ref type="table" target="#tab_1">Table III</ref>. SD Mask R-CNN outperforms the PCL baseline and achieves performance on par with Mask R-CNN fine-tuned on real data, despite the fact that SD Mask R-CNN was training on only synthetic data. This suggests that high-quality instance segmentation can be achieved without expensive data collection from humans or self-supervision. This could significantly reduce the effort needed to take advantage of object segmentation for new robotic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Color image (left) and depth image segmented by SD Mask RCNN (right) for a heap of objects. Despite clutter, occlusions, and complex geometries, SD Mask RCNN is able to correctly mask each of the objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>C} be a ground truth state which contains (A) a set of m foreground objects in the environment, (B) a set of n background objects (e.g. bins, tables), and (C) a depth camera. Here, each object state O i or B j is defined by the object's geometry and 6-DOF pose, while the camera state C is defined by its intrinsics matrix K and its 6-DOF pose (R, t) ∈ SE(3).2) Observations: Let y ∈ R H×W + be a depth image observation of the state x generated from C with height H and width W . Let the pixel space U = [0, H − 1] × [0, W −1] be the set of all real-valued pixel coordinates in the depth image. 3) Object Mask: Let M i ⊆ U be a mask for foreground object O i , or the set of pixels in y that were generated by the surface of O i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Dataset generation procedure for the WISDOM synthetic dataset. A subset of 3D CAD models from a training dataset of 1,600 objects are dropped into a virtual bin using dynamic simulation with pybullet. A virtual camera captures both a synthetic depth image of the scene and object segmasks based on the pixelwise projection of each unique 3D object. This process is repeated to generate 50,000 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Objects included in the WISDOM-Real dataset. 25 objects were used for fine-tuning, while a separate set of 25 were held out for evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Example bins from the WISDOM-Real test set. The number of objects in each bin was chosen from a Poisson distribution with mean 5, with a minimum of two objects per bin. The highly-varied geometry and occlusions make these bins challenging to segment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 )</head><label>3</label><figDesc>We modify the network input to zero-pad the 512x384 pixel images in WISDOM-Sim to 512x512 images and set the region proposal network anchor scales and ratios to correspond to the 512x512 image size.4) For efficiency, we swapped out the ResNet 101 backbone with a smaller ResNet 35 backbone. 5) We set the mean pixel value to be the average pixel value of the simulated dataset. Training was based on Matterport's open-source Keras and TensorFlow implementation of Mask R-CNN from GitHub, Mask R-CNN (Depth) 0.370 0.616 0.331 0.546 FT Mask R-CNN (Color) 0.384 0.608 0.385 0.613 SD Mask R-CNN 0.516 0.647 0.356 0.465</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>This work was supported in part by a Google Cloud Focused Research Award for the Mechanical Search Project jointly to UC Berkeley's AU-TOLAB and the Stanford Vision &amp; Learning Lab, in affiliation with the Berkeley AI Research (BAIR) Lab, Berkeley Deep Drive (BDD), the Real-Time Intelligent Secure Execution (RISE) Lab, and the CITRIS "People and Robots" (CPAR) Initiative. The Authors were also supported by the Department of Defense (DoD) through the National Defense Science &amp; Engineering Graduate Fellowship (NDSEG) Program, the SAIL-Toyota Research initiative, the U.S. National Science Foundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot Collaborative Systems, Scalable Collaborative Human-Robot Learning (SCHooL) Project, the NSF National Robotics Initiative Award 1734633, and in part by donations from Siemens, Google, Amazon Robotics, Toyota Research Institute, Autodesk, ABB, Knapp, Loccioni, Honda, Intel, Comcast, Cisco, Hewlett-Packard and by equipment grants from PhotoNeo, and NVidia. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Sponsors. We thank our colleagues who provided helpful feedback, code, and suggestions, in particular Michael Laskey, Vishal Satish, Daniel Seita, and Ajay Tanwani.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Department of Electrical Engineering and Computer Science 2 Department of Industrial Engineering and Operations Research 1−2 The AUTOLAB at UC Berkeley; Berkeley, CA 94720, USA {mdanielczuk, mmatl, sgupta, andrewyli, andrew lee, jmahler, goldberg}@berkeley.edu</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Average precision and average recall (as defined by COCO benchmarks) on each dataset for each of the methods considered. SD Mask R-CNN is the highest performing method, even against Mask R-CNN pretrained on the COCO dataset and fine-tuned on real color and depth images from WISDOM-Real.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Average precision and average recall (as defined by COCO benchmarks) on the WISDOM-Sim dataset for the PCL baselines SD Mask R-CNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Results of semantic segmentation experiments, where success is defined as grasping and lifting the correct object. (Success Rate) Number of successful grasps of the correct object over 50 trials. (Prec. @ 0.5) Success rate when the classifier was &gt; 50% certain that the selected segment was the target object. (# Corr. Targets) Number of times the robot targeted the correct object out of 50 trials.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niyaz</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskey</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doan</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ojea</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Robotics: Science and Systems</title>
		<meeting>Robotics: Science and Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using geometry to detect grasp poses in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Research</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="307" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="725" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno>arxiv: 170306870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn. arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijaynarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int. Conf. Robotics and Automation (ICRA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1386" to="1383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cartman: The lowcost cartesian manipulator that won the amazon robotics challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mctaggart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kelly-Boxall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wade-Mccue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Erskine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grinover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gurman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hunn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06283</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nimbro picking: Versatile part handling for warehouse automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Periyasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int. Conf. Robotics and Automation (ICRA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3032" to="3039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic multi-class segmentation for the amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eppner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Höfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee/Rsj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Int</surname></persName>
		</author>
		<title level="m">Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rosaen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int. Conf. Robotics and Automation (ICRA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="746" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dex-net 3.0: Computing robust robot suction grasp targets in point clouds using a new analytic model and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gealy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int. Conf. Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Clusternet: Instance segmentation in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08894</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Category independent object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="575" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepbox: Learning objectness with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2479" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1990" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and finegrained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic 3d object maps for everyday manipulation in human living environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KI-Künstliche Intelligenz</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="345" to="348" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d is here: Point cloud library (pcl)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int. Conf. Robotics and Automation (ICRA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Octree-based region growing for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-V</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Truong-Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Laefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertolotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="88" to="100" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">International archives of photogrammetry, remote sensing and spatial information sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heuvel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vosselmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="248" to="253" />
		</imprint>
	</monogr>
	<note>Segmentation of point clouds using smoothness constraint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A review on deep learning techniques applied to semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06857</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Depth-aware object instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="325" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-instance object segmentation with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3470" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2686" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Labelfusion: A pipeline for generating ground truth labels for real rgbd data of cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Manuelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int. Conf. Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards reliable grasping and manipulation in household environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciocarlie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Ucan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="241" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Lessons from the amazon picking challenge: Four aspects of building robotic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eppner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Höfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sieverling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rgb-d object detection and semantic segmentation for autonomous manipulation in clutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Periyasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="page">0278364917713117</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Semantic segmentation from limited training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Erskine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grinover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gurman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hunn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07665</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning deep policies for robot bin picking by simulating robust grasping sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast digital image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B M R</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Appeared in the Proceedings of the International Conference on Visualization, Imaging and Image Processing</title>
		<meeting><address><addrLine>Marbella, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="106" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Mask r-cnn for object detection and instance segmentation on keras and tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdulla</surname></persName>
		</author>
		<ptr target="https://github.com/matterport/MaskRCNN" />
		<imprint>
			<biblScope unit="page" from="2017" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
