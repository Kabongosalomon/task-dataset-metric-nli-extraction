<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequential Random Network for Fine-grained Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorong</forename><surname>Li</surname></persName>
							<email>lichaorong88@163.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Yibin university</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chongqing University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Electronic Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Electronic Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengqing</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yibin university</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anping</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yibin university</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Huang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Chengdu University of Information Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequential Random Network for Fine-grained Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Convolutional Neural Network (DCNN) and</head><p>Transformer have achieved remarkable successes in image recognition. However, their performance in fine-grained image recognition is still difficult to meet the requirements of actual needs. This paper proposes a Sequence Random Network (SRN) to enhance the performance of DCNN. The output of DCNN is one-dimensional features. This onedimensional feature abstractly represents image information, but it does not express well the detailed information of image. To address this issue, we use the proposed SRN, which composed of BiLSTM and several Tanh-Dropout blocks (called BiLSTM-TDN), to further process DCNN one-dimensional features for highlighting the detail information of image. After the feature transform by BiLSTM-TDN, the recognition performance has been greatly improved. We conducted the experiments on six fine-grained image datasets. Except for FGVC-Aircraft, the accuracy of the proposed methods on the other datasets exceeded 99%. Experimental results show that BiLSTM-TDN is far superior to the existing state-of-the-art methods. In addition to DCNN, BiLSTM-TDN can also be extended to other models, such as Transformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-grained image classification aims to classify specific object categories, such as different airplanes, cars, birds, dogs into more detailed subcategories. Compared with the traditional image classification problem, finegrained image classification has tiny distinctions between classes. It is also affected by the occlusion, scale, posture, etc., in the image resulting in large differences in the appearance of objects. Therefore, fine-grained image classification is more difficult and more challenging. For finegrained image classification, we can use general image classification methods, such as deep networks (ResNet <ref type="bibr" target="#b60">[61]</ref> and EfficientNet <ref type="bibr" target="#b50">[51]</ref>) and specific fine-grained image classification methods.</p><p>General image classification network models General image classification network models General image classification network models: It has been eight years since the emergence of the deep convolutional network AlexNet, after which various network models have been proposed and have achieved brilliant results in artificial intelligence field. Practice shows that stacking more convolutional layers in a hierarchical manner can provide deep networks with the ability to learn complex representations at different levels. In fact, CNN has made great progress in terms of depth (number of layers of the network) or width (network branches); the number of layers of the network has increased from the initial more than ten layers to the current more than 1000 layers. For example, networks such as WideResNet <ref type="bibr" target="#b46">[47]</ref>, Xception <ref type="bibr" target="#b8">[9]</ref> improve their performance by simultaneously increasing network depth and width. At present, there are many types of networks used for image classification, but their effectiveness in identifying fine-grained images has much room for improvement.</p><p>Fine-grained image classification methods Fine-grained image classification methods Fine-grained image classification methods: According to the strength of supervision information used in the train, fine-grained classification models can be divided into two categories: Strong supervision methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b56">57]</ref> and Weak supervision methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56]</ref> .</p><p>In addition to the category label of the image, the strongly supervised models need extra information to obtain better classification accuracy during model training, such as the object bounding box, part annotation and other additional manual annotation information. Part-based R-CNN <ref type="bibr" target="#b58">[59]</ref> uses R-CNN algorithm to detect object-level (such as birds) and its local areas (head, body, etc.). S. Branson et al. <ref type="bibr" target="#b4">[5]</ref> first computes an estimate of the object's pose which is used to compute local image features, and in turn, used for classification; the features are computed by applying deep convolutional models to image patches that are located and normalized by the pose.</p><p>Although the classification model based on strong su-pervision information has achieved satisfactory classification accuracy, the acquisition of label information is very expensive, which limits the practical application of this type of algorithm to a certain extent. Therefore, an obvious trend in current fine-grained image classification is that the only image-level annotation information is used during model training, and no additional or Part Annotation information is used to achieve classification accuracy comparable to strongly supervised classification models. This is the fine-grained classification model based on weak supervised information. The weakly supervised fine-grained classification model is similar to the strong-supervised classification model, and it also needs to use global and local information. The difference is that weakly supervised fine-grained classification model hopes to achieve better local information without the help of annotation information. Of course, in terms of classification accuracy, the current best weaksupervised classification model still has a gap with the best strong-supervised classification model. The Neural Activation Constellations <ref type="bibr" target="#b47">[48]</ref> is to use the convolutional network features to generate key points and then use the key points to extract local area information. Chowdhury et al. <ref type="bibr" target="#b34">[35]</ref> designed an end-to-end network model, called Bilinear CNN(B-CNN), and achieved the best classification accuracy on CUB200-2011 dataset. B-CNN uses two CNNs (CNN A and CNN B) to extract the feature of image, and the outputs of two CNNs at each location are combined employing the matrix outer product and average pooled to obtain the bilinear feature representation.</p><p>In recent years, many studies have integrated the attention model into the depth model to identify fine-grained images, and the substantial performance improvement is achieved. The advantage of Attention model is that it can focus on the details parts of image to extract the global features. OPAM <ref type="bibr" target="#b44">[45]</ref> is an attention-based model including an object-level attention model which is used to localize object for learning object features, and a part-level attention model which is used to select the discriminative parts for exploiting the subtle and local features. Liu et al. <ref type="bibr" target="#b35">[36]</ref> propose BARM to actualize the bidirectional reinforcement for fine-grained image recognition. BARM consists of one attention agent for discriminate part regions proposing and one recognition agent for feature extraction and recognition.</p><p>In weakly supervised classification, Transformer <ref type="bibr" target="#b24">[25]</ref> has shown superior performance compared to DCNN in most applications. Transformer model uses stacked Self-Attention/Multi-head-Attention, point-wise, and fully connected layers to be introduced into the classification task. Ruyi et al. proposed ACNet <ref type="bibr" target="#b20">[21]</ref> for fine-grained image classification, which uses the Self-Attention transformer module to enforce the network to capture discriminative detail features of images. Dosovitskiy et al. <ref type="bibr" target="#b0">[1]</ref> proposed Vision Transformer (ViT) which uses the Multi-Head At-  No matter it is strong-supervised learning or weaksupervised learning, the biggest problem currently faced by them is that it is difficult to accurately focus on the most distinguishing areas of images because the areas are so irregular that the recognition accuracy is limited. For example, in <ref type="figure" target="#fig_0">Fig.1</ref>, if the birds are identified from their approximate motion states, the models are likely to make wrong judgments: the birds in the column direction belong to the same category (in fact, the birds in same row belong to the same category). Strongly supervised models and weakly supervised models both attempt to extract salient features by applying Convolutional Neural Networks (CNN) or matrix transformation (Transformer) on two-dimensional im-ages. Different from these methods, we analyze the onedimensional features of a Deep CNN (DCNN) to obtain significant new features that can distinguish fine-grained images (see <ref type="figure" target="#fig_1">Fig.2</ref>). In our method, we proposed Sequential Random Network (SRN) for the one-dimensional features transformation which is very effective, and the recognition performance on the six public fine-grained datasets exceeds the State-Of-The-Art (SOTA) methods. The main contributions of this article are in the following two aspects:</p><p>• We proposed Sequential Random Network (SRN) which consists of two basic modules: BiLSTM and Tanh-Dropout (TD) block. BiLSTM uses the correlation information in the forward and backward directions of the sequence to predict the output. It can make the features produced from the same category become more similar through the correlation of the sequence, and the features produced from different categories become more different. The function of TD blocks is similar to the feature selection function. They can eliminate some unimportant features, and perform feedback learning by randomly discarding some feature elements, making the features distinguishable.</p><p>• We proposed feature transformation approach for finegrained image classification by using the proposed SRN based on the DCNN one-dimensional (1D) features. The transformation regards 1D features as sequences and considers the correlation between these feature sequences. Compared with the strongly supervised model based on two-dimensional images, this method does not need to manually label the regions of image; compared with the weakly supervised deep model, it also does not require extra technique such as Attention mechanism to focus on spatial geometric position information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature processing</head><p>Feature processing can be categorized into three types: Feature extraction, Feature transform, and Feature selection.</p><p>Feature extraction: Given an image I ∈ R (N,M ) , the features Y ∈ R (N ,M ) are obtained by using the extractor Φ: Y = Φ(I). The most typical case, Y ∈ R (1,M ) is the one-dimensional feature vector. Traditional methods such as SIFT <ref type="bibr" target="#b59">[60]</ref> and HOG <ref type="bibr" target="#b26">[27]</ref>, or more advanced DCNN and Transformer can be used as the feature extractors of image. Usually, the features extracted by feature extractors may not be able to meet the application requirements. Therefore, further analysis for the features yield from image is very necessary.</p><formula xml:id="formula_0">Feature transform: A set of new features Y ∈ R (N ,M )</formula><p>is obtained from a set of existing features X ∈ R (N,M ) through mapping Γ, denoted by Y = Γ(X). Features can be one-dimensional or two-dimensional. For one-dimensional features, Principal Component Analysis (PCA) <ref type="bibr" target="#b57">[58]</ref>, Linear Discriminant Analysis(LDA) <ref type="bibr" target="#b36">[37]</ref> and Fully connected network <ref type="bibr" target="#b31">[32]</ref>, are commonly used methods. For twodimensional features, 2D-PCA <ref type="bibr" target="#b33">[34]</ref>, 2D-LDA <ref type="bibr" target="#b32">[33]</ref> and 2D convolution <ref type="bibr" target="#b5">[6]</ref> are commonly used methods.</p><p>Feature selection: Choosing a subset from all the features by using a mapping f (·), denoted by Y = f (X), where Y ⊆ X. Usually, the values of the feature elements have not modified, but the number of feature elements has decreased. Decision trees <ref type="bibr" target="#b18">[19]</ref> and random forests <ref type="bibr" target="#b39">[40]</ref> belong to feature selection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Activation layer and Dropout layer</head><p>The main function of the activation function is to perform non-linear transformation of data and solve the problem of insufficient expression and classification capabilities of linear models. Another important function is normalization, mapping the input data to a certain range. It's advantage of is that it can limit the expansion of data and prevent the risk of overflow caused by excessive data. There are several activation functions for DCNN, such as Sigmoid, ReLU and Tanh which is a double tangent activation function, expressed as follows:</p><formula xml:id="formula_1">T anh(x) = e x − e −x e x + e −x<label>(1)</label></formula><p>The Dropout <ref type="bibr" target="#b48">[49]</ref> is commonly used in DCNN. During training, a subset of the weight set W in the layer is randomly discarded with dropout probability p (that is the retaining probability is 1 − p), which is expressed as follows:</p><formula xml:id="formula_2">y = W | (1−p) * x<label>(2)</label></formula><p>Where x represents the input of this layer; * is elementwise multiplication; y is the output of this layer. When performing classification, the output is multiplied by retaining probability 1 − p, which is expressed as follows:</p><formula xml:id="formula_3">y = W * (1 − p)x<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">BiLSTM</head><p>In recent years, LSTM network <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b45">46]</ref> which is a type of Recurrent Neural Network (RNN), have emerged in image processing domain. LSTM has a chain form of repeated neural network modules designed to solve the long-term dependency problem. In a standard RNN, this repeated structural module is a very simple, such as a Tanh layer, while the repeating module of LSTM is more complex and much more effective. The basic cell of LSTM is shown in <ref type="figure" target="#fig_2">Fig.3</ref>. The output is a combination of several aspects including the current input x t , the previous input h t−1 (hidden units), and the influence of the cell state C t−1 . Overall, LSTM consists of three gates. Three gates (forget gate, input gates and output gate) combine these inputs and control the output. The forget gate is expressed as follows:</p><formula xml:id="formula_4">f t = σ(W f · [h t−1 , x t ] + b f )<label>(4)</label></formula><p>There are two important parts in the input gates which will update the cell state, denoted by:</p><formula xml:id="formula_5">i t = σ(W i · [h t−1 , x t ] + b f ), C = tanh(W C · [h t−1 , x t ] + b C )<label>(5)</label></formula><p>The new cell state C t is expressed as</p><formula xml:id="formula_6">C t = f t * C t−1 + i t * C t<label>(6)</label></formula><p>The final output h t (or y t ) is the result of multiplying the output o t of the output gate and the current cell state C t</p><formula xml:id="formula_7">o t = σ(W o [h t−1 , x t ] + b o ), h t = o t * tanh(C t )<label>(7)</label></formula><p>Bi-directional LSTM (BiLSTM) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b45">46]</ref> is the improved version of LSTM. It is a combination of forward LSTM and backward LSTM; <ref type="figure" target="#fig_3">Fig.4</ref> shows the schematic diagram of BiLSBM. The forward LSTM serves the forward calculation, and the input at time t is the sequence data x t multiplied by weight U at time t and the output A t−1 multiplied by weight W at time t − 1. The backward LSTM serves the reverse calculation, the input A t at time t is the sequence data X t multiplied by Weight U at time t and the output A t multiplied by weight W at time t + 1. The final output value at time t depends on the sum of the two directions.</p><formula xml:id="formula_8">A t = f (W A t−1 + U X t )<label>(8)</label></formula><p>A </p><formula xml:id="formula_9">t = f (W A t+1 + U X t )<label>(9)</label></formula><formula xml:id="formula_10">y t = g(V A t + V A t )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature transform with SRN</head><p>The process of our proposed one-dimensional feature selection approach SRN is shown in <ref type="figure" target="#fig_4">Fig.5</ref>. The method uses a pre-trained network, such as ResNet, as the backbone, and then trains the pre-trained network model on a specific image dataset by using transfer learning to obtain the transfer learned network model. By using the transfer learned network model, we can obtain the two new one-dimensional DCNN datasets: train set(1D feature) and test set(1D feature). Finally, feature transform network SRN is trained on train set(1D feature) and evaluate the performance on the test set(1D feature). In this work, the SRN consists of two parts BiLSTM and the combination of Tanh layer and Dropout layer (Therefore, it is also called BiLSTM-TDN), as shown in <ref type="figure" target="#fig_5">Fig.6</ref>. The first part is the feature transformation component, which is a BiLSTM layer. We regard the one-dimensional DCNN features as a sequence. The feature sequence represents the image sequence, and there are many similarities between image sequences, such as similar backgrounds and similar object shapes. Therefore, there is a strong correlation between the feature sequence. This correlation can be employed with BiLSTM to obtain the transformed features which have the more discriminative capability. The second part is a feature selection component composed of Tanh layer and Dropout layer (Tanh-Dropout block, called TD block). TD block first uses Tanh to perform nonlinear mapping on the output of BiLSTM layer and then uses Dropout layer to discard some features randomly. Several TD blocks can be used in BiLSTM-TDN. The flow of a se- quence x being processed is as follows:</p><formula xml:id="formula_11">x → y 1 = Dropout(x) → y 2 = BiLST M (y 1 ) → y 3 = T D(y 2 ) → · · · y n = T D(y n−1 )<label>(11)</label></formula><p>The two parts of the BiLSTM-TDN are mainly to further improve the discriminability of DCNN as much as possible.</p><p>The following experiments show that if only BiLSTM layer is used without the feature selection part TD blocks, the model's performance will be greatly reduced (See the experiment and discussion part for details). Dropouts of TD blocks are used to improve generalizability and reduce overfitting. At the front of BiLSTM-TDN is the Sequence layer, which is used to organize the serialized data, followed by a Dropout layer which is used to randomly discard some features before the DCNN features are fed to the BiLSTM layer.</p><p>When the classic PCA and LDA calculate the feature transformation matrix, the correlation between the samples is also considered, but all training samples are required to participate in the calculation at the same time; if the number of samples is large, it is obvious that the computational overhead and memory overhead are very large. Unlike PCA and LDA, when BiLSTM-TDN calculates the correlation between samples, the data sequences are entered one by one, which is similar to a fully connected network. Compared to fully connected network, BiLSTM-TDN can use the correlation of several pieces of data in two directions (forward and backward directions) to jointly update the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The experiments were conducted on six fine-trained image datasets. FGVC-Aircraft <ref type="bibr" target="#b38">[39]</ref> dataset contains 10200 In the experiments, we used three pre-trained networks, including ResNet18, ResNet101 and EfficientNet-b0, which were trained on ImageNet. The experimental hardware environment mainly composed of Intel i9 CPU, 64G memory and 2080ti GPU. All images are three-channel color images, and they are all scaled to the size of 224×224. The number of BiLSTM hidden state units is set to 600; the dropout probability of each layer is set as <ref type="table" target="#tab_0">Table 1</ref>. Because the training set of flowers has very few samples to avoid overfitting during training, we appropriately increased the dropout probability p.</p><p>The experimental results of the three pre-trained networks on six datasets are shown in Table2. We see that the performance of the three network models on fine-grained images is very unsatisfactory. The recognition accuracies of most network models are between 60%-80%. The best recognition result is 87.78% (ResNet101) on Oxford-IIIT Pets. Although ResNet101 shows the best performance, it is difficult to compete with other SOTA methods. After feature transformation by BiLSTM-TDN, the performance of these network models has been amazingly improved. Except for FGVC-Aircraft, the recognition accuracies on the other five databases have exceeded 98%; the recognition accuracy of ResNet101 has risen steadily to over 99%, and the best accuracy is 99.93% on Food-101. Overall, ResNet101 has the best performance; ResNet18 and EfficientNet-b0 have similar performance. It is necessary to know that ResNet18 has only 71 layers in total, while EfficientNet-b0 has 290 layers. Therefore, after feature transformation through BiLSTM-TDN, actually, the performance of ResNet18 is better than the performance of EfficientNet-b0.</p><p>BiLSTM-TDN can also integrate the features of multiple network models. <ref type="table">Table 3</ref> shows the integrated performance of these network models. It can be seen that compared with a single network model, the effect of integration is also obvious, and it is also very robust. On FGVC-Aircraft, the combination of the three models (ResNet18+ResNet101+Effic) has increased to 97.77% of recognition accuracy; on Food-101, it has increased to 99.97%; the performance of the model pairwise combination has also been improved to varying degrees.</p><p>In order to illustrate the superiority of our proposed sequential random network, we the comparisons with the state-of-the-art (SOTA) methods on 6 datasets. For the sake of fairness, we do not choose the different models which yield the best record on the datasets, but choose a fixed model ResNet101 and the combined model ResNet18+ResNet101+Effic (the combination of the three models) for the comparison.</p><p>On FGVC-Aircraft, the best recognition accuracy we can find is 94.7% (most SOTA levels are between 90%-93%), and the recognition accuracy of BiSTM-TDN (ResNet101) is 96.99%, The recognition accuracy of combined features reached 97.77%, which significantly exceeded the existing record. On Stanford-Cars, the current highest recognition accuracy is 96.20. In contrast, the recognition accuracies of our BiLSTM-TDN are 99.03% and 99.19% respectively, increasing about 3%. On the most commonly used CUB200-2011, our method has demonstrated unparalleled advancement. The two methods have achieved recognition accuracies of 99.06% and 99.24% respectively, while the best recognition accuracy of the SOTA methods is 91.8%. The identification of Oxford-IIIT Pets is much easier. There are several methods that have achieved 97% accuracy. Our method also achieved a very high accuracy; both of our methods achieved 99.91%, which is also the latest record on this dataset.</p><p>Earlier, the recognition accuracy of Food-101 was around 95%. The CAP <ref type="bibr" target="#b1">[2]</ref> that published in 2021 increased the accuracy to 98.6%, which is the best record we found. Our methods are not to be outdone, the recognition accuracy of the two methods reached 99.93% and 99.97% re- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DCNN</head><p>Backbone Accuracy API-Net <ref type="bibr" target="#b43">[44]</ref> DenseNet161 90.30 MC-Loss <ref type="bibr" target="#b6">[7]</ref> B-CNN 92.90 DF-GMM <ref type="bibr" target="#b53">[54]</ref> ResNet50 93.80 DAT <ref type="bibr" target="#b21">[22]</ref> InceptionV3 94.10 DCL <ref type="bibr" target="#b7">[8]</ref> ResNet50 93.00 MMAL-Net <ref type="bibr" target="#b11">[12]</ref> ResNet50 94.70 94.70 94.70 ACNet <ref type="bibr" target="#b20">[21]</ref> ResNet50 91.5 R50D,2020 <ref type="bibr" target="#b23">[24]</ref> ResNet50 92.4 NAT <ref type="bibr" target="#b37">[38]</ref> -91.68 CIN <ref type="bibr" target="#b13">[14]</ref> ResNet101 92.8 DB <ref type="bibr" target="#b49">[50]</ref> ResNet spectively. Regarding the Flower-102, because there were too few training samples, we augmented the number of samples by scaling and shifting the images for BiLSTM-TDN (ResNet101), called BiLSTM-TDN(AU)(ResNet101). The current SOTA methods have achieved good results on Flowers, and the recognition accuracy has been pushed to the level of 99.74% by the Transformer-based method ViT-L, which is higher than that of BiLSTM -TDN (ResNet101). However, the accuracy of ResNet18+ResNet101+Effic and the BiLSTM -TDN (AU) (ResNet101) also exceed the accuracy of ViT-L, reaching 99.78%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablaton and Discuss</head><p>The above experiments manifest that the performance improvement of our proposed method BiLSTM-TDN in recognition accuracy is very obvious. Except for FGVC- Aircraft, the experimental results of our method on the other five datasets are all higher than 99%, which overwhelmingly surpasses the existing SOTA methods. We should also be aware that the performance of BiLSTM-TDN is affected by several factors. The two most important factors are the number of hidden layer units of BiLSTM and the dropout probability of Dropout layers.</p><p>With other parameters fixed, we tested the influence of the number of hidden units of BiLSTM on the recognition accuracy. The number of hidden layer units used in the test ranges from 400 to 1200 for the experiments on FGVC-Aircraft and Stanford-Cars (see <ref type="figure" target="#fig_6">Fig.7</ref>). It can be seen that there is no obvious rule to follow the impact of the num- ber of hidden units on performance, but we can roughly see that ResNet101 achieves better results when its hidden unit is set to 60; when ResNet18's is set to 800 and EfficientNet-b0's is set to 1000, the recognition effect is better. The performance of our network is affected by ran- domness, which comes from the initial parameters, gradient descent or Dropout algorithms, etc. The Dropout maybe is one of the biggest sources of randomness in BiLSMT-TDN. In order to test the randomness of our proposed BiLSTM-TDN and its impact on performance, we fixed the network parameters, then trained and test several times to verify its robustness. The dropout probabilities p in the first Dropout layer and the four TD blocks are set to 0.6, 0.6, 0.6, 0.5, 0.5, respectively. We have conducted 5 experiments on Flowers-102, and the recognition accuracies are: 99.78%, 99.83%, 99.71%, 99.78% and 99.79%; the average is 99.78%; the fluctuation range is about -0.05% to +0.05%. We have tested the performance of individual BiLSTM and BiLSTM combined with different numbers of TD blocks. It can be seen from <ref type="figure" target="#fig_7">Fig.8</ref> that the performance of BiLSTM-TDN gradually increases when the number of TD blocks in BiL-STM increased, and the performance is pretty good when four TD blocks are used. If five TD blocks are used, the number of training iterations for the convergence rapid increases and the performance of model does not increase but decreases. In order to further verify the superiority of our proposed sequential random network BiLSTM-TDN in feature transformation, we extracted the output vector of the last Dropout layer from four images in Flower-102 dataset. We intercepted the first 100-dimensional features from the output vector, converted them into matrices(see <ref type="figure" target="#fig_8">Fig.9</ref>). Obviously, the features of two images from the same category are very similar, while the features from two different categories are quite different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>A simple and effective sequential random network, called BiLSTM-TDN, is proposed in this work. BiLSTM-TDN consists of two parts, BiLSTM unit and quite a few consecutive TD blocks. The experimental results on six publicly available datasets show that our method has an overwhelming advantage. The structure of BiLSTM-TDN may not be optimal; therefore, the performance of BiLSTM-TDN should have room for improvement. In future work, we will continue to study the structure and performance of BiLSTM-TDN, such as the relationship between the number of 1D features and the number of BiL-STM hidden units, as well as the number of TD blocks and the dropout probability of Dropout layer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Two different types of birds. The birds in same row belong to the same category. The red irregular areas are the parts that distinguishes them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The proposed method for fine-grained image classification. We use Sequential Random Network (SRN) to transform the 1D features of DCNN into a feature vector which can express the different area of two categories. tention to extract the local features of image. Kim et al.<ref type="bibr" target="#b25">[26]</ref> introduced a learnable module, Volumetric Transformer Network (VTN) that predicts channel-wise warping fields, to reconfigure intermediate CNN features spatially and channel-wisely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>LSTM cell</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>BiLSTM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The flowchart of the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>The proposed SRN, called BiLSTM-TDN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Performance is affected by the number of hidden units in BiLSTM on FGVC-Aircraft(a) and Stanford-Cars (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Ablaton tests on Flowers-102. BiLSTM means that there is only BiLSTM in the model; DP1+TD(i) means that the model contains BiLSTM, the first Dropout layer and i TD blocks, where i = 0, · · · , 5 indicates the number of TD blocks. combination indicates the ResNet18+ResNet101+Effic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>The features extracted from the last Dropout layer. The two rows are shown the features of two different flower categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dropout probability setting in BiLSTM-TDN.</figDesc><table><row><cell>Dropout-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>The recognition results of our proposed method Sequential Random Network (BiLSTM-TDN) on six data sets. BiLSTM-TDN experimental results of the combination of multiple model features. Effic Effic Effic refers to the EfficientNet-b0.</figDesc><table><row><cell>Dataset</cell><cell cols="6">Transfer learning ResNet18 ResNet101 EfficientNet-b0 ResNet18 ResNet101 EfficientNet-b0 BiLSTM-TDN</cell></row><row><cell>FGVC-Aircraft</cell><cell>59.34 59.34 59.34</cell><cell>52.77</cell><cell>51.54</cell><cell>96.27</cell><cell>96.99 96.99 96.99</cell><cell>96.54</cell></row><row><cell>Stanford-Cars</cell><cell>63.98</cell><cell>64.66</cell><cell>65.50 65.50 65.50</cell><cell>98.63</cell><cell>99.03 99.03 99.03</cell><cell>98.84</cell></row><row><cell>CUB200-2011</cell><cell>62.72</cell><cell>68.63 68.63 68.63</cell><cell>66.70</cell><cell>98.56</cell><cell>99.06 99.06 99.06</cell><cell>98.53</cell></row><row><cell>Oxford-IIIT Pets</cell><cell>83.18</cell><cell>87.78 87.78 87.78</cell><cell>84.6</cell><cell>99.83</cell><cell>99.91 99.91 99.91</cell><cell>99.83</cell></row><row><cell>Food-101</cell><cell>71.13</cell><cell>77.75 77.75 77.75</cell><cell>75.58</cell><cell>99.85</cell><cell>99.93 99.93 99.93</cell><cell>99.80</cell></row><row><cell>Flower-102</cell><cell>85.1</cell><cell>85.93 85.93 85.93</cell><cell>83.89</cell><cell>99.70</cell><cell>99.71 99.71 99.71</cell><cell>98.76</cell></row><row><cell>Dataset</cell><cell cols="6">ResNet18+Effic ResNet101+Effic ResNet18+ResNet101 ResNet18+ResNet101+Effic</cell></row><row><cell>FGVC-Aircraft</cell><cell>97.65</cell><cell>97.08</cell><cell></cell><cell>96.78</cell><cell>97.77 97.77 97.77</cell><cell></cell></row><row><cell>Stanford-Cars</cell><cell>99.20</cell><cell>99.19</cell><cell></cell><cell>99.21 99.21 99.21</cell><cell>99.19</cell><cell></cell></row><row><cell>CUB200-2011</cell><cell>98.77</cell><cell>99.15</cell><cell></cell><cell>99.22</cell><cell>99.24 99.24 99.24</cell><cell></cell></row><row><cell>Oxford-IIIT Pets</cell><cell>99.91</cell><cell>99.94 99.94 99.94</cell><cell></cell><cell>99.94 99.94 99.94</cell><cell>99.91</cell><cell></cell></row><row><cell>Food-101</cell><cell>99.94</cell><cell>99.94</cell><cell></cell><cell>99.94</cell><cell>99.97 99.97 99.97</cell><cell></cell></row><row><cell>Flower-102</cell><cell>99.79 99.79 99.79</cell><cell>99.73</cell><cell></cell><cell>99.77</cell><cell>99.78</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Recent results on FGVC-Aircraft</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Recent results on CUB200-2011</figDesc><table><row><cell>DCNN</cell><cell>Backbone</cell><cell>Accuracy</cell></row><row><cell>API-Net[44]</cell><cell>DenseNet161</cell><cell>90.00</cell></row><row><cell>MC-Loss[7]</cell><cell>B-CNN</cell><cell>86.40</cell></row><row><cell>DF-GMM[54]</cell><cell>ResNet50</cell><cell>88.80</cell></row><row><cell>PC-Softmax[41]</cell><cell>-</cell><cell>89.73</cell></row><row><cell>MGE-CNN[31]</cell><cell>Resnet101</cell><cell>89.40</cell></row><row><cell>DCL[8]</cell><cell>ResNet50</cell><cell>87.80</cell></row><row><cell>MMAL-Net[12]</cell><cell>ResNet50</cell><cell>89.60</cell></row><row><cell>ACNet[21]</cell><cell>ResNet50</cell><cell>88.1</cell></row><row><cell>CIN[14]</cell><cell>ResNet101</cell><cell>88.1</cell></row><row><cell>DB[50]</cell><cell>ResNet50</cell><cell>88.6</cell></row><row><cell>FDL[10]</cell><cell>ResNet50</cell><cell>88.35</cell></row><row><cell>CAP[2]</cell><cell>Xceptio</cell><cell>91.8 91.8 91.8</cell></row><row><cell>BiSTM-TDN</cell><cell>ResNet101</cell><cell>99.06 99.06 99.06</cell></row><row><cell cols="3">BiSTM-TDN Res18+Res101+Effic 99.24 99.24 99.24</cell></row><row><cell cols="3">Table 7. Recent results on Oxford-IIIT Pets</cell></row><row><cell>DCNN</cell><cell>Backbone</cell><cell>Accuracy</cell></row><row><cell>ViT-H[1]</cell><cell>Transformer</cell><cell>97.56 97.56 97.56</cell></row><row><cell>BiT-L[28]</cell><cell>ResNet</cell><cell>96.62</cell></row><row><cell>DAT[22]</cell><cell>AmoebaNet-B</cell><cell>97.1</cell></row><row><cell>R50D[24]</cell><cell>ResNet50</cell><cell>94.3</cell></row><row><cell>NAT[38]</cell><cell>-</cell><cell>96.11</cell></row><row><cell>Dom-Ad[23]</cell><cell>InceptionV3</cell><cell>97.1</cell></row><row><cell>CAP[2]</cell><cell>NASNet-M</cell><cell>97.3</cell></row><row><cell>BiSTM-TDN</cell><cell>ResNet101</cell><cell>99.91 99.91 99.91</cell></row><row><cell cols="3">BiSTM-TDN Res18+Res101+Effic 99.91 99.91 99.91</cell></row><row><cell cols="3">Table 8. Recent results on Food-101</cell></row><row><cell>DCNN</cell><cell>Backbone</cell><cell>Accuracy</cell></row><row><cell>NAT[38]</cell><cell>-</cell><cell>91.11</cell></row><row><cell>SOP[29]</cell><cell>-</cell><cell>88.4</cell></row><row><cell>DAT[22]</cell><cell>AmoebaNet-B</cell><cell>95.3</cell></row><row><cell>R50D[24]</cell><cell>ResNet50</cell><cell>92.47</cell></row><row><cell>Dom-Ad[23]</cell><cell>AmoebaNet-B</cell><cell>95.3</cell></row><row><cell>CAP[2]</cell><cell>-</cell><cell>98.6 98.6 98.6</cell></row><row><cell>BiSTM-TDN</cell><cell>ResNet101</cell><cell>99.93 99.93 99.93</cell></row><row><cell cols="3">BiSTM-TDN Res18+Res101+Effic 99.97 99.97 99.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .</head><label>9</label><figDesc>Recent results on Flowers</figDesc><table><row><cell>DCNN</cell><cell>Backbone</cell><cell>Accuracy</cell></row><row><cell>ViT-L[1]</cell><cell>Transformer</cell><cell>99.74 99.74 99.74</cell></row><row><cell>BiT-L[28]</cell><cell>ResNet</cell><cell>99.63</cell></row><row><cell>DAT[22]</cell><cell>AmoebaNet-B</cell><cell>97.7</cell></row><row><cell>MC-Loss[7]</cell><cell>B-CNN</cell><cell>97.7</cell></row><row><cell>R50D[24]</cell><cell>ResNet50</cell><cell>98.9</cell></row><row><cell>NAT[38]</cell><cell>-</cell><cell>99.63</cell></row><row><cell>SOP[29]</cell><cell>-</cell><cell>97.62</cell></row><row><cell>DeiT[20]</cell><cell>-</cell><cell>98.9</cell></row><row><cell>CAP[2]</cell><cell>Xceptio</cell><cell>97.7</cell></row><row><cell>BiSTM-TDN</cell><cell>ResNet101</cell><cell>99.71 99.71 99.71</cell></row><row><cell>BiSTM-TDN (AU)</cell><cell>ResNet101</cell><cell>99.78 99.78 99.78</cell></row><row><cell cols="3">BiSTM-TDN Res18+Res101+Effic 99.78 99.78 99.78</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
	</analytic>
	<monogr>
		<title level="m">The Ninth International Conference on Learning Representations (ICLR</title>
		<editor>ander Kolesnikov Dirk Weissenborn Xiaohua Zhai Thomas Unterthiner Mostafa Dehghani Matthias Minderer Georg Heigold Sylvain Gelly Jakob Uszkoreit Neil Houlsby A.Dosovitskiy, Lucas Beyer</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context-aware attentional pooling (cap) for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Hewage Asish Bera Ardhendu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Behera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wharton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Script identification in natural scene image and video frames using an attention based convolutional-lstm network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishik</forename><surname>Konwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umapada</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="172" to="184" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Arxiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Surfconv: Bridging 3d and 2d convolution for rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>R Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The devil is in the channels: Mutual-channel loss for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4683" to="4695" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franc¸ois</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02357v3</idno>
		<title level="m">Xception: Deep learning with depthwise separable convolutions</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun Zha Lingfeng Ma Lingyun Yu Yongdong Zhang Chuanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multibranch and multi-scale attention learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guisheng Zhai Yizhao Liu Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09150v3</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Channel interaction networks for finegrained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smj Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hmd</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03347v</idno>
		<title level="m">Spinalnet: Deep neural network with gradual input</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning hierarchical decision trees for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename><forename type="middle">Chi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="937" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Training dataefficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs Douze Francisco Massa Alexandre Sablayrolles Hervé Jégou</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877v2</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention convolutional binary neural tree for finegrained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Domain adaptive transfer learning with specialist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vijay Vasudevan Simon Kornblith Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiyi</forename><surname>Le Ruoming Pang Jiquan Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07056v1</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<idno type="arXiv">arXiv:2009.13239v1</idno>
		<title level="m">Scalable transfer learning with expert models</title>
		<editor>Basil Mustafa Cedric Renggli André Susano Pinto Sylvain Gelly Daniel Keysers Joan Puigcerver, Carlos Riquelme and Neil Houlsby</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Compounding the performance improvements of assembled techniques in a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae Kwan Lee Hyemin Lee Geonmo Gu Jungkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeryun</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiho</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06268v2</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169v1</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Volumetric transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bfo meets hog: Feature extraction based on histograms of oriented p.d.f. gradients for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumi</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="491" to="507" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Power normalizations in fine-grained image, few-shot image and graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13975v1</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8332" to="8340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully connected network-based intra prediction for image coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="3236" to="3247" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">2d-lda: A statistical linear discriminant analysis for image matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baozong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="527" to="532" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">L1-norm-based 2dpca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems Man and Cybernetics Part B</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1170" to="1175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bilinear cnns for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Tsung Yu Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bidirectional attention-recognition model for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhineng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1785" to="1795" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Boosting image classification with ldabased feature combination for digital photograph management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingxing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="887" to="901" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sreekumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<title level="m">Neural architecture transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151v1</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Incremental learning of random forests for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ristin</forename><surname>Marko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Rethinking recurrent neural networks and other improvements for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardete</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Phong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15161v1</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao Peiqin Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3130" to="13137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object-part attention model for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangteng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Understanding lstm-a tutorial into long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Staudemeyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09586v1</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos Komodakis Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146v4</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fine-grained recognition: Accounting for subtle differences between similar classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946v5</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Category-specific semantic coherency learning for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 28th ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Graph-propagation based correlation learning for weakly supervised fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12289" to="12296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Weakly supervised fine-grained image classification via guassian mixture model oriented discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9746" to="9755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep attention-based spatially recursive networks for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1791" to="1802" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Augmenting strong supervision using web data for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">From image vector to matrix: a straightforward image projection technique-impca vs. pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing Yu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1997" to="1999" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Ross Girshick, and Trevor Darrell. Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Integrating sift and cnn feature matching for partial-duplicate image detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingming</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="593" to="604" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8698" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
