<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quaternion Knowledge Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of New South Wales ψ Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of New South Wales ψ Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Quaternion Knowledge Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we move beyond the traditional complex-valued representations, introducing more expressive hypercomplex representations to model entities and relations for knowledge graph embeddings. More specifically, quaternion embeddings, hypercomplex-valued embeddings with three imaginary components, are utilized to represent entities. Relations are modelled as rotations in the quaternion space. The advantages of the proposed approach are: (1) Latent inter-dependencies (between all components) are aptly captured with Hamilton product, encouraging a more compact interaction between entities and relations; (2) Quaternions enable expressive rotation in four-dimensional space and have more degree of freedom than rotation in complex plane; (3) The proposed framework is a generalization of ComplEx on hypercomplex space while offering better geometrical interpretations, concurrently satisfying the key desiderata of relational representation learning (i.e., modeling symmetry, anti-symmetry and inversion). Experimental results demonstrate that our method achieves state-of-the-art performance on four wellestablished knowledge graph completion benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) live at the heart of many semantic applications (e.g., question answering, search, and natural language processing). KGs enable not only powerful relational reasoning but also the ability to learn structural representations. Reasoning with KGs have been an extremely productive research direction, with many innovations leading to improvements to many downstream applications. However, real-world KGs are usually incomplete. As such, completing KGs and predicting missing links between entities have gained growing interest. Learning low-dimensional representations of entities and relations for KGs is an effective solution for this task.</p><p>Learning KG embeddings in the complex space C has been proven to be a highly effective inductive bias, largely owing to its intrinsic asymmetrical properties. This is demonstrated by the ComplEx embedding method which infers new relational triplets with the asymmetrical Hermitian product.</p><p>In this paper, we move beyond complex representations, exploring hypercomplex space for learning KG embeddings. More concretely, quaternion embeddings are utilized to represent entities and relations. Each quaternion embedding is a vector in the hypercomplex space H with three imaginary components i, j, k, as opposed to the standard complex space C with a single real component r and imaginary component i. We propose a new scoring function, where the head entity Q h is rotated by the relational quaternion embedding through Hamilton product. This is followed by a quaternion inner product with the tail entity Q t .</p><p>There are numerous benefits of this formulation. (1) The Hamilton operator provides a greater extent of expressiveness compared to the complex Hermitian operator and the inner product in Euclidean space. The Hamilton operator forges inter-latent interactions between all of r, i, j, k, resulting in a highly expressive model. (2) Quaternion representations are highly desirable for parameterizing smooth rotation and spatial transformations in vector space. They are generally considered robust to sheer/scaling noise and perturbations (i.e., numerically stable rotations) and avoid the problem of Gimbal locks. Moreover, quaternion rotations have two planes of rotation 2 while complex rotations only work on single plane, giving the model more degrees of freedom. (3) Our QuatE framework subsumes the ComplEx method, concurrently inheriting its attractive properties such as its ability to model symmetry, anti-symmetry, and inversion. (4) Our model can maintain equal or even less parameterization, while outperforming previous work.</p><p>Experimental results demonstrate that our method achieves state-of-the-art performance on four wellestablished knowledge graph completion benchmarks (WN18, FB15K, WN18RR, and FB15K-237).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Knowledge graph embeddings have attracted intense research focus in recent years, and a myriad of embedding methodologies have been proposed. We roughly divide previous work into translational models and semantic matching models based on the scoring function, i.e. the composition over head &amp; tail entities and relations.</p><p>Translational methods popularized by TransE <ref type="bibr" target="#b0">[Bordes et al., 2013]</ref> are widely used embedding methods, which interpret relation vectors as translations in vector space, i.e., head + relation ≈ tail. A number of models aiming to improve TransE are proposed subsequently. TransH <ref type="bibr" target="#b31">[Wang et al., 2014]</ref> introduces relation-specific hyperplanes with a normal vector. TransR <ref type="bibr" target="#b14">[Lin et al., 2015]</ref> further introduces relation-specific space by modelling entities and relations in distinct space with a shared projection matrix. TransD <ref type="bibr" target="#b9">[Ji et al., 2015]</ref> uses independent projection vectors for each entity and relation and can reduce the amount of calculation compared to TransR. TorusE <ref type="bibr" target="#b4">[Ebisu and Ichise, 2018]</ref> defines embeddings and distance function in a compact Lie group, torus, and shows better accuracy and scalability. The recent state-of-the-art, RotatE <ref type="bibr" target="#b26">[Sun et al., 2019]</ref>, proposes a rotation-based translational method with complex-valued embeddings.</p><p>On the other hand, semantic matching models include bilinear models such as RESCAL <ref type="bibr" target="#b17">[Nickel et al., 2011]</ref>, DistMult <ref type="bibr" target="#b32">[Yang et al., 2014]</ref>, HolE <ref type="bibr" target="#b18">[Nickel et al., 2016]</ref>, and ComplEx <ref type="bibr" target="#b29">[Trouillon et al., 2016]</ref>, and neural-network-based models. These methods measure plausibility by matching latent semantics of entities and relations. In RESCAL, each relation is represented with a square matrix, while DistMult replaces it with a diagonal matrix in order to reduce the complexity. SimplE <ref type="bibr" target="#b11">[Kazemi and Poole, 2018]</ref> is also a simple yet effective bilinear approach for knowledge graph embedding. HolE explores the holographic reduced representations and makes use of circular correlation to capture rich interactions between entities. ComplEx embeds entities and relations in complex space and utilizes Hermitian product to model the antisymmetric patterns, which has shown to be immensely helpful in learning KG representations. The scoring function of ComplEx is isomorphic to that of HolE <ref type="bibr" target="#b28">[Trouillon and Nickel, 2017]</ref>. Neural networks based methods have also been adopted, e.g., Neural Tensor Network <ref type="bibr" target="#b25">[Socher et al., 2013]</ref> and ER-MLP <ref type="bibr" target="#b2">[Dong et al., 2014]</ref> are two representative neural network based methodologies. More recently, convolution neural networks <ref type="bibr" target="#b1">[Dettmers et al., 2018]</ref>, graph convolutional networks <ref type="bibr" target="#b24">[Schlichtkrull et al., 2018]</ref>, and deep memory networks <ref type="bibr" target="#b30">[Wang et al., 2018]</ref> also show promising performance on this task. Different from previous work, QuatE takes the advantages (e.g., its geometrical meaning and rich representation capability, etc.) of quaternion representations to enable rich and expressive semantic matching between head and tail entities, assisted by relational rotation quaternions. Our framework subsumes DistMult and ComplEx, with the capability to generalize to more advanced hypercomplex spaces. QuatE utilizes the concept of geometric rotation. Unlike the RotatE which has only one plane of rotation, there are two planes of rotation in QuatE. QuatE is a semantic matching model while RotatE is a translational model. We also point out that the composition property introduced in TransE and RotatE can have detrimental effects on the KG embedding task.</p><p>Quaternion is a hypercomplex number system firstly described by Hamilton <ref type="bibr" target="#b7">[Hamilton, 1844]</ref> with applications in a wide variety of areas including astronautics, robotics, computer visualisation, animation and special effects in movies, and navigation. Lately, Quaternions have attracted attention in the field of machine learning. Quaternion recurrent neural networks (QRNNs) obtain better performance with fewer number of free parameters than traditional RNNs on the phoneme recognition task. Quaternion representations are also useful for enhancing the performance of convolutional neural networks on multiple tasks such as automatic speech recognition <ref type="bibr">[Parcollet et al.]</ref> and image classification <ref type="bibr" target="#b5">[Gaudet and</ref><ref type="bibr">Maida, 2018, Parcollet et al., 2018a]</ref>. Quaternion multiplayer perceptron <ref type="bibr" target="#b19">[Parcollet et al., 2016]</ref> and quaternion autoencoders <ref type="bibr" target="#b21">[Parcollet et al., 2017]</ref> also outperform standard MLP and autoencoder. In a nutshell, the major motivation behind these models is that quaternions enable the neural networks to code latent inter-and intra-dependencies between multidimensional input features, thus, leading to more compact interactions and better representation capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hamilton's Quaternions</head><p>Quaternion <ref type="bibr" target="#b7">[Hamilton, 1844]</ref> is a representative of hypercomplex number system, extending traditional complex number system to four-dimensional space. A quaternion Q consists of one real component and three imaginary components, defined as Q = a + bi + cj + dk, where a, b, c, d are real numbers and i, j, k are imaginary units. i, j and k are square roots of −1, satisfying the Hamilton's rules: i 2 = j 2 = k 2 = ijk = −1. More useful relations can be derived based on these rules, such as ij = k, ji = -k, jk=i, ki=j, kj=-i and ik=-j. <ref type="figure" target="#fig_0">Figure 1(b)</ref> shows the quaternion imaginary units product. Apparently, the multiplication between imaginary units is non-commutative. Some widely used operations of quaternion algebra H are introduced as follows:</p><p>Conjugate: The conjugate of a quaternion Q is defined asQ = a − bi − cj − dk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Norm: The norm of a quaternion is defined as</head><formula xml:id="formula_0">|Q| = √ a 2 + b 2 + c 2 + d 2 .</formula><p>Inner Product: The quaternion inner product between Q 1 = a 1 + b 1 i + c 1 j + d 1 k and Q 2 = a 2 + b 2 i + c 2 j + d 2 k is obtained by taking the inner products between corresponding scalar and imaginary components and summing up the four inner products:</p><formula xml:id="formula_1">Q 1 · Q 2 = a 1 , a 2 + b 1 , b 2 + c 1 , c 2 + d 1 , d 2 (1)</formula><p>Hamilton Product (Quaternion Multiplication): The Hamilton product is composed of all the standard multiplications of factors in quaternions and follows the distributive law, defined as:</p><formula xml:id="formula_2">Q 1 ⊗ Q 2 = (a 1 a 2 − b 1 b 2 − c 1 c 2 − d 1 d 2 ) + (a 1 b 2 + b 1 a 2 + c 1 d 2 − d 1 c 2 )i + (a 1 c 2 − b 1 d 2 + c 1 a 2 + d 1 b 2 )j + (a 1 d 2 + b 1 c 2 − c 1 b 2 + d 1 a 2 )k ,<label>(2)</label></formula><p>which determines another quaternion. Hamilton product is not commutative. Spatial rotations can be modelled with quaternions Hamilton product. Multiplying a quaternion, Q 2 , by another quaternion Q 1 , has the effect of scaling Q 1 by the magnitude of Q 2 followed by a special type of rotation in four dimensions. As such, we can also rewrite the above equation as:</p><formula xml:id="formula_3">Q 1 ⊗ Q 2 = Q 1 ⊗ |Q 2 | Q 2 |Q 2 |<label>(3)</label></formula><p>4 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quaternion Representations for Knowledge Graph Embeddings</head><p>Suppose that we have a knowledge graph G consisting of N entities and M relations. E and R denote the sets of entities and relations, respectively. The training set consists of triplets (h, r, t), where h, t ∈ E and r ∈ R. We use Ω and Ω = E × R × E − Ω to denote the set of observed triplets and the set of unobserved triplets, respectively. Y hrt ∈ {−1, 1} represents the corresponding label of the triplet (h, r, t). The goal of knowledge graph embeddings is to embed entities and relations to a continuous low-dimensional space, while preserving graph relations and semantics.</p><p>In this paper, we propose learning effective representations for entities and relations with quaternions. We leverage the expressive rotational capability of quaternions. Unlike RotatE which has only one plane of rotation (i.e., complex plane, shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>), QuatE has two planes of rotation. Compared to Euler angles, quaternion can avoid the problem of gimbal lock (loss of one degree of freedom). Quaternions are also more efficient and numerically stable than rotation matrices. The proposed method can be summarized into two steps: (1) rotate the head quaternion using the unit relation quaternion;</p><p>(2) take the quaternion inner product between the rotated head quaternion and the tail quaternion to score each triplet. If a triplet exists in the KG, the model will rotate the head entity with the relation to make the angle between head and tail entities smaller so the product can be maximized. Otherwise, we can make the head and tail entity be orthogonal so that their product becomes zero.</p><formula xml:id="formula_4">1 i -1 -i ii=-1 Imaginary Axis Real Axis (a) 1 j i -k -1 -j -i k ij=k ji=-k (b) k -k 1 i -i -j j (c)</formula><p>Quaternion Embeddings of Knowledge Graphs More specifically, we use a quaternion matrix Q ∈ H N ×k to denote the entity embeddings and W ∈ H M ×k to denote the relation embeddings, where k is the dimension of embeddings. Given a triplet (h, r, t), the head entity h and the tail entity</p><formula xml:id="formula_5">t correspond to Q h = {a h + b h i + c h j + d h k : a h , b h , c h , d h ∈ R k } and Q t = {a t + b t i + c t j + d t k : a t , b t , c t , d t ∈ R k }, respectively, while the relation r is represented by W r = {a r + b r i + c r j + d r k : a r , b r , c r , d r ∈ R k }.</formula><p>Hamilton-Product-Based Relational Rotation We first normalize the relation quaternion W r to a unit quaternion W r = p + qi + uj + vk to eliminate the scaling effect by dividing W r by its norm:</p><formula xml:id="formula_6">W r (p, q, u, v) = W r |W r | = a r + b r i + c r j + d r k a 2 r + b 2 r + c 2 r + d 2 r<label>(4)</label></formula><p>We visualize a unit quaternion in <ref type="figure" target="#fig_0">Figure 1</ref>(c) by projecting it into a 3D space. We keep the unit hypersphere which passes through i, j, k in place. The unit quaternion can be project in, on, or out of the unit hypersphere depending on the value of the real part.</p><p>Secondly, we rotate the head entity Q h by doing Hamilton product between it and W r :</p><formula xml:id="formula_7">Q h (a h , b h , c h , d h ) = Q h ⊗ W r = (a h • p − b h • q − c h • u − d h • v) + (a h • q + b h • p + c h • v − d h • u)i + (a h • u − b h • v + c h • p + d h • q)j + (a h • v + b h • u − c h • q + d h • p)k<label>(5)</label></formula><p>where • denotes the element-wise multiplication between two vectors. Right-multiplication by a unit quaternion is a right-isoclinic rotation on Quaternion Q h . We can also swap Q h and W r and do a left-isoclinic rotation, which does not fundamentally change the geometrical meaning. Isoclinic rotation is a special case of double plane rotation where the angles for each plane are equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scoring Function and Loss</head><p>We apply the quaternion inner product as the scoring function: <ref type="bibr" target="#b29">Trouillon et al. [2016]</ref>, we formulate the task as a classification problem, and the model parameters are learned by minimizing the following regularized logistic loss:</p><formula xml:id="formula_8">φ(h, r, t) = Q h · Q t = a h , a t + b h , b t + c h , c t + d h , d t (6) Following</formula><formula xml:id="formula_9">L(Q, W ) = r(h,t)∈Ω∪Ω − log(1 + exp(−Y hrt φ(h, r, t))) + λ 1 Q 2 2 +λ 2 W 2 2<label>(7)</label></formula><p>Here we use the 2 norm with regularization rates λ 1 and λ 2 to regularize Q and W , respectively. Ω − is sampled from the unobserved set Ω using negative sampling strategies such as uniform sampling, bernoulli sampling <ref type="bibr" target="#b31">[Wang et al., 2014]</ref>, and adversarial sampling <ref type="bibr" target="#b26">[Sun et al., 2019]</ref>. Note that the loss function is in Euclidean space, as we take the summation of all components when computing the scoring function in Equation <ref type="formula">(6)</ref>. We utilise Adagrad <ref type="bibr" target="#b3">[Duchi et al., 2011]</ref> for optimization. </p><formula xml:id="formula_10">TransE (Q h + Wr) − Qt Q h , Wr, Qt ∈ R k O(k) HolE Wr, Q h Qt Q h , Wr, Qt ∈ R k O(k log(k)) DistMult Wr, Q h , Qt Q h , Wr, Qt ∈ R k O(k) ComplEx Re( Wr, Q h ,Qt ) Q h , Wr, Qt ∈ C k O(k) RotatE Q h • Wr − Qt Q h , Wr, Qt ∈ C k , |Wri| = 1 O(k) TorusE min (x,y)∈([Q h ]+[Q h ])×[Wr ] x − y [Q h ], [Wr], [Qt] ∈ T k O(k) QuatE Q h ⊗ W r · Qt Q h , Wr, Qt ∈ H k O(k)</formula><p>Initialization For parameters initilaization, we can adopt the initialization algorithm in <ref type="bibr" target="#b23">[Parcollet et al., 2018b]</ref> tailored for quaternion-valued networks to speed up model efficiency and convergence <ref type="bibr" target="#b6">[Glorot and Bengio, 2010]</ref>. The initialization of entities and relations follows the rule:</p><formula xml:id="formula_11">w real = ϕ cos(θ), w i = ϕQ img i sin(θ), w j = ϕQ img j sin(θ), w k = ϕQ img k sin(θ),<label>(8)</label></formula><p>where w real , w i , w j , w k denote the scalar and imaginary coefficients, respectively. θ is randomly generated from the interval [−π, π]. Q img is a normalized quaternion, whose scalar part is zero. ϕ is randomly generated from the interval</p><formula xml:id="formula_12">[− 1 √ 2k , 1 √ 2k</formula><p>], reminiscent to the He initialization . This initialization method is optional. Capability in Modeling Symmetry, Antisymmetry and Inversion. The flexibility and representational power of quaternions enable us to model major relation patterns at ease. Similar to ComplEx, our model can model both symmetry (r(x, y) ⇒ r(y, x)) and antisymmetry (r(x, y) ⇒ r(y, x)) relations. The symmetry property of QuatE can be proved by setting the imaginary parts of W r to zero. One can easily check that the scoring function is antisymmetric when the imaginary parts are nonzero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discussion</head><p>As for the inversion pattern (r 1 (x, y) ⇒ r 2 (y, x)) , we can utilize the conjugation of quaternions. Conjugation is an involution and is its own inverse. One can easily check that:</p><formula xml:id="formula_13">Q h ⊗ W r · Q t = Q t ⊗W r · Q h<label>(9)</label></formula><p>The detailed proof of antisymmetry and inversion can be found in the appendix.</p><p>Composition patterns are commonplace in knowledge graphs <ref type="bibr" target="#b13">[Lao et al., 2011</ref><ref type="bibr" target="#b15">, Neelakantan et al., 2015</ref>. Both transE and RotatE have fixed composition methods <ref type="bibr" target="#b26">[Sun et al., 2019]</ref>. TransE composes two relations using the addition (r 1 + r 2 ) and RotatE uses the Hadamard product (r 1 • r 2 ). We argue that it is unreasonable to fix the composition patterns, as there might exist multiple composition patterns even in a single knowledge graph. For example, suppose there are three persons "x, y, z". If y is the elder sister (denoted as r 1 ) of x and z is the elder brother (denoted as r 2 ) of y, we can easily infer that z is the elder brother of x. The relation between z and x is r 2 instead of r 1 + r 2 or r 1 • r 2 , violating the two composition methods of TransE and RotatE. In QuatE, the composition patterns are not fixed. The relation between z and x is not only determined by relations r 1 and r 2 but also simultaneously influenced by entity embeddings.</p><p>Connection to DistMult and ComplEx. Quaternions have more degrees of freedom compared to complex numbers. Here we show that the QuatE framework can be seen as a generalization of ComplEx. If we set the coefficients of the imaginary units j and k to zero, we get complex embeddings as in ComplEx and the Hamilton product will also degrade to complex number multiplication. We further remove the normalization of the relational quaternion, obtaining the following equation:</p><formula xml:id="formula_14">φ(h, r, t) = Q h ⊗ W r · Q t = (a h + b h i) ⊗ (a r + b r i) · (a t + b t i) = [(a h • a r − b h • b r ) + (a h • b r + b h • a r )i] · (a t + b t i) = a r , a h , a t + a r , b h , b t + b r , a h , b t − b r , b h , a t<label>(10)</label></formula><p>where a, b, c = k a k b k c k denotes standard component-wise multi-linear dot product. Equation 10 recovers the form of ComplEx. This framework brings another mathematical interpretation for ComplEx instead of just taking the real part of the Hermitian product. Another interesting finding is that Hermitian product is not necessary to formulate the scoring function of ComplEx.</p><p>If we remove the imaginary parts of all quaternions and remove the normalization step, the scoring function becomes φ(h, r, t) = a h , a r , a t , degrading to DistMult in this case. . We create 10 batches for all the datasets. For most baselines, we report the results in the original papers, and exceptions are provided with references. For RotatE (without self-adversarial negative sampling), we use the best hyper-parameter settings provided in the paper to reproduce the results. We also report the results of RotatE with self-adversarial negative sampling and denote it as a-RotatE. Note that we report three versions of QuatE: including QuatE with/without type constraints, QuatE with N3 regularization and reciprocal learning. Self-adversarial negative sampling <ref type="bibr" target="#b26">[Sun et al., 2019]</ref> is not used for QuatE. All hyper-parameters of QuatE are provided in the appendix.   The empirical results on four datasets are reported in <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table" target="#tab_5">Table 4</ref>. QuatE performs extremely competitively compared to the existing state-of-the-art models across all metrics. As a quaternion-valued method, QuatE outperforms the two representative complex-valued models ComplEx and RotatE. The performance gains over RotatE also confirm the advantages of quaternion rotation over rotation in the complex plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>On the WN18 dataset, QuatE outperforms all the baselines on all metrics except Hit@10. R-GCN+ achieves high value on</p><p>Hit@10, yet is surpassed by most models on the other four metrics. The four recent models NKGE, TorusE, RotaE, and a-RotatE achieves comparable results. QuatE also achieves the best results on the FB15K dataset, while the second best results scatter amongst RotatE, a-RotatE and DistMult. We are well-aware of the good results of DistMult reported in <ref type="bibr" target="#b10">[Kadlec et al., 2017</ref>], yet they used a very large negative sampling size (i. <ref type="figure" target="#fig_0">e., 1000, 2000)</ref>. The results also demonstrate that QuatE can effectively capture the symmetry, antisymmetry and inversion patterns since they account for a large portion of the relations in these two datasets.</p><p>As shown in <ref type="table" target="#tab_5">Table 4</ref>, QuatE achieves a large performance gain over existing state-of-the-art models on the two datasets where trivial inverse relations are removed. On WN18RR in which there are a number of symmetry relations, a-RotatE is the second best, while other baselines are relatively weaker.</p><p>The key competitors on the dataset FB15K-237 where a large number of composition patterns exist are NKGE and a-RotatE. We can also apply N3 regularization and reciprocal learning approaches <ref type="bibr" target="#b12">[Lacroix et al., 2018]</ref> to QuatE. Results are shown in <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table" target="#tab_5">Table 4</ref> as QuatE 2 . It is observed that using N3 and reciprocal learning could boost the performances greatly, especially on FB15K and FB15K-237. We found that the N3 regularization method can reduce the norm of relations and entities embeddings so that we do not apply relation normalization here. However, same as the method in <ref type="bibr" target="#b12">[Lacroix et al., 2018]</ref>, QuatE 2 requires a large embedding dimension. Number of Free Parameters Comparison. <ref type="table" target="#tab_9">Table 6</ref> shows the amount of parameters comparison between QuatE 1 and two recent competitive baselines: RotatE and TorusE. Note that QuatE 3 uses almost the same number of free parameters as QuatE 1 . TorusE uses a very large embedding dimension 10000 for both WN18 and FB15K. This number is even close to the entities amount of FB15K which we think is not preferable since our original intention is to embed entities and relations to a lower dimensional space. QuatE reduces the parameter size of the complex-valued counterpart RotatE largely. This is more significant on datasets without trivial inverse relations, saving up to 80% parameters while maintaining superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Analysis</head><p>Ablation Study on Quaternion Normalization. We remove the normalization step in QuatE and use the original relation quaternion W r to project head entity. From <ref type="table" target="#tab_7">Table 7</ref>, we clearly observe that normalizing the relation to unit quaternion is a critical step for the embedding performance. This is likely because scaling effects in nonunit quaternions are detrimental.</p><p>Hamilton Products between Head and Tail Entities. We reformulate the scoring function of QuatE following the original formulate of ComplEx. We do Hamilton product between head and tail quaternions and consider the relation quaternion as weight. Thus, we have φ(h, r, t) = W r ·(Q h ⊗Q t ).</p><p>As a result, the geometric property of relational rotation is lost, which leads to poor performance as shown in <ref type="table" target="#tab_7">Table 7</ref>.</p><p>Additional Rotational Quaternion for Tail Entity. We hypothesize that adding an additional relation quaternion to tail entity might bring the model more representation capability. So we revise the scoring function to (Q h ⊗ W r ) · (Q t ⊗ V r ), where V r represents the rotational quaternion for tail entity. From <ref type="table" target="#tab_7">Table 7</ref>, we observe that it achieves competitive results without extensive tuning. However, it might cause some losses of efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we design a new knowledge graph embedding model which operates on the quaternion space with well-defined mathematical and physical meaning. Our model is advantageous with its capability in modelling several key relation patterns, expressiveness with higher degrees of freedom as well as its good generalization. Empirical experimental evaluations on four well-established datasets show that QuatE achieves an overall state-of-the-art performance, outperforming multiple recent strong baselines, with even fewer free parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Proof of Antisymmetry and Inversion</head><p>Proof of antisymmetry pattern In order to prove the antisymmetry pattern, we need to prove the following inequality when imaginary components are nonzero:</p><formula xml:id="formula_15">Q h ⊗ W r · Q t = Q t ⊗ W r · Q h<label>(11)</label></formula><p>Firstly, we expand the left term:</p><formula xml:id="formula_16">Q h ⊗ W r · Q t = [(a h • p − b h • q − c h • u − d h • v) + (a h • q + b h • p + c h • v − d h • u)i+ (a h • u − b h • v + c h • p + d h • q)j + (a h • v + b h • u − c h • q + d h • p)k] · (a t + b t i + c t j + d t k) =(a h • p − b h • q − c h • u − d h • v) · a t + (a h • q + b h • p + c h • v − d h • u) · b t + (a h • u − b h • v + c h • p + d h • q) · c t + (a h • v + b h • u − c h • q + d h • p) · d t = a h , p, a t − b h , q, a t − c h , u, a t − d h , v, a t + a h , q, b t + b h , p, b t + c h , v, b t − d h , u, b t + a h , u, c t − b h , v, c t + c h , p, c t + d h , q, c t + a h , v, d t + b h , u, d t − c h , q, d t + d h , p, d t</formula><p>We then expand the right term:</p><formula xml:id="formula_17">Q t ⊗ W r · Q h = [(a t • p − b t • q − c t • u − d t • v) + (a t • q + b t • p + c t • v − d t • u)i+ (a t • u − b t • v + c t • p + d t • q)j + (a t • v + b t • u − c t • q + d t • p)k] · (a h + b h i + c h j + d h k) =(a t • p − b t • q − c t • u − d t • v) · a h + (a t • q + b t • p + c t • v − d t • u) · b h + (a t • u − b t • v + c t • p + d t • q) · c h + (a t • v + b t • u − c t • q + d t • p) · d h = a t , p, a h − b t , q, a h − c t , u, a h − d t , v, a h + a t , q, b h + b t , p, b h + c t , v, b h − d t , u, b h + a t , u, c h − b t , v, c h + c t , p, c h + d t , q, c h + a t , v, d h + b t , u, d h − c t , q, d h + d t , p, d h</formula><p>We can easily see that those two terms are not equal as the signs for some terms are not the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of inversion pattern</head><p>To prove the inversion pattern, we need to prove that:</p><formula xml:id="formula_18">Q h ⊗ W r · Q t = Q t ⊗W r · Q h<label>(12)</label></formula><p>We expand the right term:</p><formula xml:id="formula_19">Q t ⊗W r · Q h = [(a t • p + b t • q + c t • u + d t • v) + (−a t • q + b t • p − c t • v + d t • u)i+ (−a t • u + b t • v + c t • p − d t • q)j + (−a t • v − b t • u + c t • q + d t • p)k] · (a h + b h i + c h j + d h k) =(a t • p − b t • q − c t • u − d t • v) · a h + (a t • q + b t • p + c t • v − d t • u) · b h + (a t • u − b t • v + c t • p + d t • q) · c h + (a t • v + b t • u − c t • q + d t • p) · d h = a t , p, a h + b t , q, a h + c t , u, a h + d t , v, a h − a t , q, b h + b t , p, b h − c t , v, b h + d t , u, b h − a t , u, c h + b t , v, c h + c t , p, c h − d t , q, c h − a t , v, d h − b t , u, d h + c t , q, d h + d t , p, d h</formula><p>We can easily check the equality of these two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Hyperparameters Settings</head><p>We list the best hyperparameters setting of QuatE on the benchmark datasets:</p><p>Hyperparameters for QuatE 1 without type constraints:</p><p>• WN18: k = 300, λ 1 = 0.05, λ 2 = 0.05, #neg = 10 • FB15K: k = 200, λ 1 = 0.05, λ 2 = 0.05, #neg = 10 • WN18RR: k = 100, λ 1 = 0.1, λ 2 = 0.1, #neg = 1 • FB15K-237: k = 100, λ 1 = 0.3, λ 2 = 0.3, #neg = 10 Hyperparameters for QuatE 2 with N3 regularization and reciprocal learning, without type constraints:</p><p>• WN18: k = 1000, reg = 0.05</p><p>• FB15K: k = 1000, reg = 0.0025</p><p>• WN18RR: k = 1000, reg = 0.1</p><p>• FB15K-237: k = 1000, reg = 0.05</p><p>Hyperparameters for QuatE 3 with type constraint:</p><p>• WN18: k = 250, λ 1 = 0.05, λ 2 = 0, #neg = 10</p><p>• FB15K: k = 200, λ 1 = 0.1, λ 2 = 0, #neg = 20</p><p>• WN18RR: k = 100, λ 1 = 0.1, λ 2 = 0.1, #neg = 1</p><p>• FB15K-237: k = 100, λ 1 = 0.2, λ 2 = 0.2, #neg = 10</p><p>Number of epochs. The number of epochs needed of QuatE and RotatE are shown in <ref type="table" target="#tab_10">Table 8</ref>. Apart from Quaternion, we can also extend our framework to Octonions (hypercomplex number with one real part and seven imaginary parts) and even Sedenions (hypercomplex number with one real part and fifteen imaginary parts). Here, we use OctonionE to denote the method with Octonion embeddings and details are given in the following text.</p><p>Octonions are hypercomplex numbers with seven imaginary components. The Octonion algebra, or Cayley algebra, O defines operations between Octonion numbers. An Octonion is represented in the form: O 1 = x 0 + x 1 e 1 + x 2 e 2 + x 3 e 3 + x 4 e 4 + x 5 e 5 + x 6 e 6 + x 7 e 7 , where e 1 , e 2 , e 3 , e 4 , e 5 , e 6 , e 7 are imaginary units which re the square roots of −1. The multiplication rules are encoded in the Fano Plane (shown in <ref type="figure" target="#fig_1">Figure 2</ref>). Multiplying two neighboring elements on a line results in the third element on that same line. Moving with the arrows gives a positive answer and moving against arrows gives a negative answer.</p><p>The conjugate of Octonion is defined as:Ō 1 = x 0 −x 1 e 1 −x 2 e 2 −x 3 e 3 −x 4 e 4 −x 5 e 5 −x 6 e 6 −x 7 e 7 .</p><p>The norm of Octonion is defined as: |O 1 | = x 2 0 + x 2 1 + x 2 2 + x 2 3 + x 2 4 + x 2 5 + x 2 6 + x 2 7 .</p><p>If we have another Octonion: O 2 = y 0 + y 1 e 1 + y 2 e 2 + y 3 e 3 + y 4 e 4 + y 5 e 5 + y 6 e 6 + y 7 e 7 . We derive the multiplication rule with the Fano Plane.</p><p>O 1 ⊗ O 2 = (x 0 y 0 − x 1 y 1 − x 2 y 2 − x 3 y 3 − x 4 y 4 − x 5 y 5 − x 6 y 6 − x 7 y 7 ) + (x 0 y 1 + x 1 y 0 + x 2 y 3 − x 3 y 2 + x 4 y 5 − x 5 y 4 − x 6 y 7 + x 7 y 6 )e 1 + (x 0 y 2 − x 1 y 3 + x 2 y 0 + x 3 y 1 + x 4 y 6 + x 5 y 7 − x 6 y 4 − x 7 y 5 )e 2 + (x 0 y 3 + x 1 y 2 − x 2 y 1 + x 3 y 0 + x 4 y 7 − x 5 y 6 + x 6 y 5 − x 7 y 4 )e 3 + (x 0 y 4 − x 1 y 5 − x 2 y 6 − x 3 y 7 + x 4 y 0 + x 5 y 1 + x 6 y 2 + x 7 y 3 )e 4 + (x 0 y 5 + x 1 y 4 − x 2 y 7 + x 3 y 6 − x 4 y 1 + x 5 y 0 − x 6 y 3 + x 7 y 2 )e 6 + (x 0 y 6 + x 1 y 7 + x 2 y 4 − x 3 y 5 − x 4 y 2 + x 5 y 3 + x 6 y 0 − x 7 y 1 )e 5 + (x 0 y 7 − x 1 y 6 + x 2 y 5 + x 3 y 4 − x 4 y 3 − x 5 y 2 + x 6 y 1 + x 7 y 0 )e 7</p><p>We can also consider Octonions as a combination of two Quaternions. The scoring functions of OctonionE remains the same as QuatE.</p><formula xml:id="formula_21">φ(h, r, t) = Q h ⊗ W r · Q t : {Q h , W r , Q t ∈ O k }<label>(14)</label></formula><p>The results of OctonionE on dataset WN18 and WN18RR are given below. We observe that OctonionE performs equally to QuatE. It seems that extending the model to Octonion space does not give additional benefits. Octonions lose some algebraic properties such as associativity, which might bring some side effects to the model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Complex plane; (b) Quaternion units product; (c) sterographically projected hypersphere in 3D space. The purple dot indicates the position of the unit quaternion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Fano Plane, a mnemonic for the products of the unit octonions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Scoring functions of state-of-the-art knowledge graph embedding models, along with their parameters, time complexity. " " denotes the circular correlation operation; "•" denotes Hadmard (or element-wise) product. "⊗" denotes Hamilton product.</figDesc><table><row><cell>Model</cell><cell>Scoring Function</cell><cell>Parameters</cell><cell>Otime</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>summarizes several popular knowledge graph embedding models, including scoring functions, parameters, and time complexities. TransE, HolE, and DistMult use Euclidean embeddings, while ComplEx and RotatE operate in the complex space. In contrast, our model operates in the quaternion space.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the data sets used in this paper.</figDesc><table><row><cell>Dataset</cell><cell>N</cell><cell>M</cell><cell cols="2">#training #validation</cell><cell>#test</cell><cell>avg. #degree</cell></row><row><cell>WN18</cell><cell>40943</cell><cell>18</cell><cell>141442</cell><cell>5000</cell><cell>5000</cell><cell>3.45</cell></row><row><cell>WN18RR</cell><cell>40943</cell><cell>11</cell><cell>86835</cell><cell>3034</cell><cell>3134</cell><cell>2.19</cell></row><row><cell>FB15K</cell><cell cols="2">14951 1345</cell><cell>483142</cell><cell>50000</cell><cell>59071</cell><cell>32.31</cell></row><row><cell cols="2">FB15K-237 14541</cell><cell>237</cell><cell>272115</cell><cell>17535</cell><cell>20466</cell><cell>18.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>We implemented our model using pytorch 4 and tested it on a single GPU. The hyper-parameters are determined by grid search. The best models are selected by early stopping on the validation set. In general, the embedding size k is tuned amongst {50, 100, 200, 250, 300}. Regularization rate λ 1 and λ 2 are searched in {0, 0.01, 0.05, 0.1, 0.2}. Learning rate is fixed to 0.1 without further tuning. The number of negatives (#neg) per training sample is selected from {1, 5, 10, 20}</figDesc><table><row><cell>, 2016] , SimplE [Kazemi and</cell></row><row><cell>Poole, 2018], ConvE [Dettmers et al., 2018], R-GCN [Schlichtkrull et al., 2018], and KNGE (ConvE</cell></row><row><cell>based) [Wang et al., 2018].</cell></row><row><cell>Implementation Details:</cell></row></table><note>Datasets Description: We conducted experiments on four widely used benchmarks, WN18, FB15K, WN18RR and FB15K-237, of which the statistics are summarized in Table 2. WN18 [Bordes et al., 2013] is extracted from WordNet 3 , a lexical database for English language, where words are interlinked by means of conceptual-semantic and lexical relations. WN18RR [Dettmers et al., 2018] is a subset of WN18, with inverse relations removed. FB15K [Bordes et al., 2013] contains relation triples from Freebase, a large tuple database with structured general human knowledge. FB15K-237 [Toutanova and Chen, 2015] is a subset of FB15K, with inverse relations removed. Evaluation Protocol: Three popular evaluation metrics are used, including Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hit ratio with cut-off values n = 1, 3, 10. MR measures the average rank of all correct entities with a lower value representing better performance. MRR is the average inverse rank for correct entities. Hit@n measures the proportion of correct entities in the top n entities. Following Bordes et al. [2013], filtered results are reported to avoid possibly flawed evaluation.Baselines: We compared QuatE with a number of strong baselines. For Translational Distance Models, we reported TransE [Bordes et al., 2013] and two recent extensions, TorusE [Ebisu and Ichise, 2018] and RotatE [Sun et al., 2019]; For Semantic Matching Models, we reported DistMult [Yang et al., 2014], HolE [Nickel et al., 2016], ComplEx [Trouillon et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Link prediction results on WN18 and FB15K. Best results are in bold and second best results are underlined.[ †]: Results are taken from<ref type="bibr" target="#b18">[Nickel et al., 2016]</ref>; [ ]: Results are taken from<ref type="bibr" target="#b10">[Kadlec et al., 2017]</ref>; [ * ]: Results are taken from [Sun et al., 2019]. a-RotatE denotes RotatE with self-adversarial negative sampling. [QuatE 1 ]: without type constraints; [QuatE 2 ]: with N3 regularization and reciprocal learning; [QuatE 3 ]: with type constraints.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FB15K</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="10">MR MRR Hit@10 Hit@3 Hit@1 MR MRR Hit@10 Hit@3 Hit@1</cell></row><row><cell>TransE †</cell><cell>-</cell><cell>0.495</cell><cell>0.943</cell><cell>0.888</cell><cell>0.113</cell><cell>-</cell><cell>0.463</cell><cell>0.749</cell><cell>0.578</cell><cell>0.297</cell></row><row><cell>DistMult</cell><cell cols="2">655 0.797</cell><cell>0.946</cell><cell>-</cell><cell>-</cell><cell cols="2">42.2 0.798</cell><cell>0.893</cell><cell>-</cell><cell>-</cell></row><row><cell>HolE</cell><cell>-</cell><cell>0.938</cell><cell>0.949</cell><cell>0.945</cell><cell>0.930</cell><cell>-</cell><cell>0.524</cell><cell>0.739</cell><cell>0.759</cell><cell>0.599</cell></row><row><cell>ComplEx</cell><cell>-</cell><cell>0.941</cell><cell>0.947</cell><cell>0.945</cell><cell>0.936</cell><cell>-</cell><cell>0.692</cell><cell>0.840</cell><cell>0.759</cell><cell>0.599</cell></row><row><cell>ConvE</cell><cell cols="2">374 0.943</cell><cell>0.956</cell><cell>0.946</cell><cell>0.935</cell><cell>51</cell><cell>0.657</cell><cell>0.831</cell><cell>0.723</cell><cell>0.558</cell></row><row><cell>R-GCN+</cell><cell>-</cell><cell>0.819</cell><cell>0.964</cell><cell>0.929</cell><cell>0.697</cell><cell>-</cell><cell>0.696</cell><cell>0.842</cell><cell>0.760</cell><cell>0.601</cell></row><row><cell>SimplE</cell><cell>-</cell><cell>0.942</cell><cell>0.947</cell><cell>0.944</cell><cell>0.939</cell><cell>-</cell><cell>0.727</cell><cell>0.838</cell><cell>0.773</cell><cell>0.660</cell></row><row><cell>NKGE</cell><cell cols="2">336 0.947</cell><cell>0.957</cell><cell>0.949</cell><cell>0.942</cell><cell>56</cell><cell>0.73</cell><cell>0.871</cell><cell>0.790</cell><cell>0.650</cell></row><row><cell>TorusE</cell><cell>-</cell><cell>0.947</cell><cell>0.954</cell><cell>0.950</cell><cell>0.943</cell><cell>-</cell><cell>0.733</cell><cell>0.832</cell><cell>0.771</cell><cell>0.674</cell></row><row><cell>RotatE</cell><cell cols="2">184 0.947</cell><cell>0.961</cell><cell>0.953</cell><cell>0.938</cell><cell>32</cell><cell>0.699</cell><cell>0.872</cell><cell>0.788</cell><cell>0.585</cell></row><row><cell cols="3">a-RotatE *  309 0.949</cell><cell>0.959</cell><cell>0.952</cell><cell>0.944</cell><cell>40</cell><cell>0.797</cell><cell>0.884</cell><cell>0.830</cell><cell>0.746</cell></row><row><cell>QuatE 1</cell><cell cols="2">388 0.949</cell><cell>0.960</cell><cell>0.954</cell><cell>0.941</cell><cell>41</cell><cell>0.770</cell><cell>0.878</cell><cell>0.821</cell><cell>0.700</cell></row><row><cell>QuatE 2</cell><cell>-</cell><cell>0.950</cell><cell>0.962</cell><cell>0.954</cell><cell>0.944</cell><cell>-</cell><cell>0.833</cell><cell>0.900</cell><cell>0.859</cell><cell>0.800</cell></row><row><cell>QuatE 3</cell><cell>162</cell><cell>0.950</cell><cell>0.959</cell><cell>0.954</cell><cell>0.945</cell><cell>17</cell><cell>0.782</cell><cell>0.900</cell><cell>0.835</cell><cell>0.711</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FB15K-237</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>MR</cell><cell cols="9">MRR Hit@10 Hit@3 Hit@1 MR MRR Hit@10 Hit@3 Hit@1</cell></row><row><cell>TransE  †</cell><cell cols="2">3384 0.226</cell><cell>0.501</cell><cell>-</cell><cell>-</cell><cell cols="2">357 0.294</cell><cell>0.465</cell><cell>-</cell><cell>-</cell></row><row><cell>DistMult</cell><cell>5110</cell><cell>0.43</cell><cell>0.49</cell><cell>0.44</cell><cell>0.39</cell><cell cols="2">254 0.241</cell><cell>0.419</cell><cell>0.263</cell><cell>0.155</cell></row><row><cell>ComplEx</cell><cell>5261</cell><cell>0.44</cell><cell>0.51</cell><cell>0.46</cell><cell>0.41</cell><cell cols="2">339 0.247</cell><cell>0.428</cell><cell>0.275</cell><cell>0.158</cell></row><row><cell>ConvE</cell><cell>4187</cell><cell>0.43</cell><cell>0.52</cell><cell>0.44</cell><cell>0.40</cell><cell cols="2">244 0.325</cell><cell>0.501</cell><cell>0.356</cell><cell>0.237</cell></row><row><cell>R-GCN+</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.249</cell><cell>0.417</cell><cell>0.264</cell><cell>0.151</cell></row><row><cell>NKGE</cell><cell>4170</cell><cell>0.45</cell><cell>0.526</cell><cell>0.465</cell><cell>0.421</cell><cell>237</cell><cell>0.33</cell><cell>0.510</cell><cell>0.365</cell><cell>0.241</cell></row><row><cell>RotatE *</cell><cell cols="2">3277 0.470</cell><cell>0.565</cell><cell>0.488</cell><cell>0.422</cell><cell cols="2">185 0.297</cell><cell>0.480</cell><cell>0.328</cell><cell>0.205</cell></row><row><cell cols="3">a-RotatE *  3340 0.476</cell><cell>0.571</cell><cell>0.492</cell><cell>0.428</cell><cell cols="2">177 0.338</cell><cell>0.533</cell><cell>0.375</cell><cell>0.241</cell></row><row><cell>QuatE 1</cell><cell cols="2">3472 0.481</cell><cell>0.564</cell><cell>0.500</cell><cell>0.436</cell><cell cols="2">176 0.311</cell><cell>0.495</cell><cell>0.342</cell><cell>0.221</cell></row><row><cell>QuatE 2</cell><cell>-</cell><cell>0.482</cell><cell>0.572</cell><cell>0.499</cell><cell>0.436</cell><cell>-</cell><cell>0.366</cell><cell>0.556</cell><cell>0.401</cell><cell>0.271</cell></row><row><cell>QuatE 3</cell><cell>2314</cell><cell>0.488</cell><cell>0.582</cell><cell>0.508</cell><cell>0.438</cell><cell>87</cell><cell>0.348</cell><cell>0.550</cell><cell>0.382</cell><cell>0.248</cell></row></table><note>Link prediction results on WN18RR and FB15K-237. [ †]: Results are taken from [Nguyen et al., 2017]; [ ]: Results are taken from [Dettmers et al., 2018]; [ * ]: Results are taken from [Sun et al., 2019].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>MRR for the models tested on each relation of WN18RR.</figDesc><table><row><cell>Relation Name</cell><cell>RotatE</cell><cell>QuatE 3</cell></row><row><cell>hypernym</cell><cell>0.148</cell><cell>0.173</cell></row><row><cell>derivationally_related_form</cell><cell>0.947</cell><cell>0.953</cell></row><row><cell>instance_hypernym</cell><cell>0.318</cell><cell>0.364</cell></row><row><cell>also_see</cell><cell>0.585</cell><cell>0.629</cell></row><row><cell>member_meronym</cell><cell>0.232</cell><cell>0.232</cell></row><row><cell>synset_domain_topic_of</cell><cell>0.341</cell><cell>0.468</cell></row><row><cell>has_part</cell><cell>0.184</cell><cell>0.233</cell></row><row><cell>member_of_domain_usage</cell><cell>0.318</cell><cell>0.441</cell></row><row><cell>member_of_domain_region</cell><cell>0.200</cell><cell>0.193</cell></row><row><cell>verb_group</cell><cell>0.943</cell><cell>0.924</cell></row><row><cell>similar_to</cell><cell>1.000</cell><cell>1.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Analysis on different variants of scoring function. Same hyperparameters settings as QuatE 3 are used.</figDesc><table><row><cell></cell><cell cols="2">WN18</cell><cell cols="2">FB15K</cell><cell cols="2">WN18RR</cell><cell cols="2">FB15K-237</cell></row><row><cell>Analysis</cell><cell cols="8">MRR Hit@10 MRR Hit@10 MRR Hit@10 MRR Hit@10</cell></row><row><cell>Q h ⊗ Wr · Qt</cell><cell>0.936</cell><cell>0.951</cell><cell>0.686</cell><cell>0.866</cell><cell>0.415</cell><cell>0.482</cell><cell>0.272</cell><cell>0.463</cell></row><row><cell>Wr · (Q h ⊗ Qt)</cell><cell>0.784</cell><cell>0.945</cell><cell>0.599</cell><cell>0.809</cell><cell>0.401</cell><cell>0.471</cell><cell>0.263</cell><cell>0.446</cell></row><row><cell cols="2">(Q h ⊗ W r ) · (Qt ⊗ V r ) 0.947</cell><cell>0.958</cell><cell>0.787</cell><cell>0.889</cell><cell>0.477</cell><cell>0.563</cell><cell>0.344</cell><cell>0.539</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>summarizes the MRR for each relation on WN18RR, confirming the superior representation capability of quaternion in modelling different types of relation. Methods with fixed composition patterns such as TransE and RotatE are relatively weak at times.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Number of free parameters comparison.</figDesc><table><row><cell>Model</cell><cell>TorusE</cell><cell>RotatE</cell><cell>QuatE 1</cell></row><row><cell>Space</cell><cell>T k</cell><cell>C k</cell><cell>H k</cell></row><row><cell>WN18</cell><cell cols="3">409.61M 40.95M 49.15M (↑ 20.0%)</cell></row><row><cell>FB15K</cell><cell cols="3">162.96M 31.25M 26.08M(↓ 16.5%)</cell></row><row><cell>WN18RR</cell><cell>-</cell><cell cols="2">40.95M 16.38M(↓ 60.0%)</cell></row><row><cell cols="2">FB15K-237 -</cell><cell cols="2">29.32M 5.82M(↓ 80.1%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Number of epochs needed of QuatE 1 and RotatE.</figDesc><table><row><cell cols="5">Datasets WN18 WN18RR FB15K FB15K-237</cell></row><row><cell>QuatE 1</cell><cell>3000</cell><cell>40000</cell><cell>5000</cell><cell>5000</cell></row><row><cell>RotatE</cell><cell>80000</cell><cell>80000</cell><cell>150000</cell><cell>150000</cell></row><row><cell cols="3">7.3 Octonion for Knowledge Graph embedding</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Results of Octonion Knowledge graph embedding.</figDesc><table><row><cell></cell><cell></cell><cell>WN18</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">MR MRR Hit@10 Hit@3 Hit@1</cell></row><row><cell cols="2">OctonionE 182 0.950</cell><cell>0.959</cell><cell>0.954</cell><cell>0.944</cell></row><row><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">MR MRR Hit@10 Hit@3 Hit@1</cell></row><row><cell cols="2">OctonionE 2098 0.486</cell><cell>0.582</cell><cell>0.508</cell><cell>0.435</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A plane of rotation is an abstract object used to describe or visualize rotations in space.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://wordnet.princeton.edu/ 4 https://pytorch.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially supported by grant ONRG NICOP N62909-19-1-2009   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toruse: Knowledge graph embedding on a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Ebisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep quaternion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony S</forename><surname>Gaudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamilton</forename><surname>William Rowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lxxviii. on quaternions; or on a new system of imaginaries in algebra</title>
		<imprint>
			<date type="published" when="1844" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="489" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4289" to="4300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-ninth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06662</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02121</idno>
		<title level="m">Dat Quoc Nguyen, and Dinh Phung. A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth Aaai conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quaternion neural networks for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linarès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R. De</forename><surname>Mori</surname></persName>
		</author>
		<idno type="DOI">10.1109/SLT.2016.7846290</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="362" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Quaternion convolutional neural networks for end-to-end automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titouan</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheb</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linarès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07789</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quaternion denoising encoder-decoder for theme identification of telephone conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titouan</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linarès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Quaternion convolutional neural networks for heterogeneous image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titouan</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linarès</surname></persName>
		</author>
		<idno>abs/1811.02656</idno>
		<ptr target="http://arxiv.org/abs/1811.02656" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quaternion recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titouan</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morchid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georges</forename><surname>Linarès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheb</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1806.04418</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Seventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Complex and holographic embeddings of knowledge graphs: a comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01475</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Knowledge graph embedding with entity neighbors and deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03752</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
