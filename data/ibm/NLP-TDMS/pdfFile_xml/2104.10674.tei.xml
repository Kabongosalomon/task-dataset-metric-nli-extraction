<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation Low Level Imitator Module High Level Reasoning Module Sub-goal Visual Spatial Reasoning Textual Reasoning Cross-Modal Grounding Temporal Reasoning Neural Controller</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Zubair</forename><surname>Irshad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
						</author>
						<title level="a" type="main">Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation Low Level Imitator Module High Level Reasoning Module Sub-goal Visual Spatial Reasoning Textual Reasoning Cross-Modal Grounding Temporal Reasoning Neural Controller</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>CLS Turn Right ... Bedroom EOS</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Move forward through the corridor and turn right into the bedroom. Wait by the window on the right b. First person visual observations c. Decoupling reasoning and Imitation d. Hierarchical Cross-Modal Imitation a. Continuous Control Formulation for VLN First person Visual Observations Velocity output Stop output Time Frames Fig. 1: Overview: Robotics Vision-and-Language Navigation (Robo-VLN) task in continuous environments and our proposed Hierarchical Cross-Modal (HCM) agent. The agent decouples reasoning and imitation through a modularized training regime to solve the complex long-horizon Robo-VLN task.</p><p>Abstract-Deep Learning has revolutionized our ability to solve complex problems such as Vision-and-Language Navigation (VLN). This task requires the agent to navigate to a goal purely based on visual sensory inputs given natural language instructions. However, prior works formulate the problem as a navigation graph with a discrete action space. In this work, we lift the agent off the navigation graph and propose a more complex VLN setting in continuous 3D reconstructed environments. Our proposed setting, Robo-VLN, more closely mimics the challenges of real world navigation. Robo-VLN tasks have longer trajectory lengths, continuous action spaces, and challenges such as obstacles. We provide a suite of baselines inspired by state-of-the-art works in discrete VLN and show that they are less effective at this task. We further propose that decomposing the task into specialized highand low-level policies can more effectively tackle this task. With extensive experiments, we show that by using layered decision making, modularized training, and decoupling reasoning and imitation, our proposed Hierarchical Cross-Modal (HCM) agent outperforms existing baselines in all key metrics and sets a new benchmark for Robo-VLN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: Overview: Robotics Vision-and-Language Navigation (Robo-VLN) task in continuous environments and our proposed Hierarchical Cross-Modal (HCM) agent. The agent decouples reasoning and imitation through a modularized training regime to solve the complex long-horizon Robo-VLN task.</p><p>Abstract-Deep Learning has revolutionized our ability to solve complex problems such as Vision-and-Language Navigation (VLN). This task requires the agent to navigate to a goal purely based on visual sensory inputs given natural language instructions. However, prior works formulate the problem as a navigation graph with a discrete action space. In this work, we lift the agent off the navigation graph and propose a more complex VLN setting in continuous 3D reconstructed environments. Our proposed setting, Robo-VLN, more closely mimics the challenges of real world navigation. Robo-VLN tasks have longer trajectory lengths, continuous action spaces, and challenges such as obstacles. We provide a suite of baselines inspired by state-of-the-art works in discrete VLN and show that they are less effective at this task. We further propose that decomposing the task into specialized highand low-level policies can more effectively tackle this task. With extensive experiments, we show that by using layered decision making, modularized training, and decoupling reasoning and imitation, our proposed Hierarchical Cross-Modal (HCM) agent outperforms existing baselines in all key metrics and sets a new benchmark for Robo-VLN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The promise of personal assistant robots that can seamlessly follow human instructions in real life environments has long been sought after. Recent advancements in deep learning (to extract meaningful information from raw sensor data) and deep reinforcement learning (to learn effective decisionmaking policies) have enabled some progress towards this goal <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. Due to the difficulty of collecting data in these contexts, a great deal of work has been done using photo-realistic simulations such as those captured through Matterport3D panoramas in homes <ref type="bibr" target="#b3">[4]</ref> or point-cloud meshes * Georgia Institute of Technology (mirshad7,zkira)@gatech.edu â€  Now at Facebook cyma@fb.com in Gibson <ref type="bibr" target="#b4">[5]</ref>. For example, a number of works have investigated autonomous agents that can follow rich, naturallanguage instructions in such simulations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. Precisely defined, Vision-and-Language Navigation (VLN) is a task which requires the agent to navigate to a goal location purely based on visual inputs and provided instructions in the absence of a prior global map <ref type="bibr" target="#b8">[9]</ref>.</p><p>While increasingly effective neural network architectures have been developed for these tasks, many limitations still exist that prevent their applicability to real-world robotics problems. Specifically, previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> have focused on a simpler subset of this problem by defining the instruction-guided robot trajectories as either a discrete navigation graph <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref> or assuming the action space of the autonomous agent comprises of discrete values <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. These formulations assume known topology, perfect localization and deterministic navigation from one viewpoint to the next in the absence of any obstacles <ref type="bibr" target="#b12">[13]</ref>. Hence these assumptions significantly deviate from the real world both in terms of control and perception.</p><p>As a first contribution, we focus on a richer VLN formulation which is defined in continuous environments over long horizon trajectories. Our proposed setting, Robo-VLN (Robotics Vision-and-Language Navigation), is summarized in <ref type="figure">Figure 1</ref> and Section III. We lift the agent off the navigation graph, making the language guided navigation problem richer, more challenging, and closer to the real world.</p><p>In an attempt to solve the language-guided navigation (VLN) problem, recent learning-based approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> make use of sequence-to-sequence architectures <ref type="bibr" target="#b17">[18]</ref>. However, when tested for generalization performance in un- seen environments, these approaches (initially developed for shorter horizon nav-graph problems) translate poorly to more complex settings <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, as we also showed for Robo-VLN in Section VI. Hence, for our proposed continuous VLN setting over long-horizon trajectories, we present an approach utilizing hierarchical decomposition. Our proposed method leverages hierarchy to decouple cross-modal reasoning and imitation, thus equipping the agent with the following key abilities:</p><p>1. Decouple Reasoning and Imitation. The agent is comprised of a high-level policy and a corresponding lowlevel policy. The high-level policy is tasked with aligning the relevant instructions with observed visual cues as well as reasoning over which instructions have been completed, hence producing a sub-goal output through cross-modal grounding. The low-level policy imitates the feedback controller based on sub-goal information and observed visual states. A layered decision making allows spatially different reasoning at different levels in the hierarchy, hence specializing each policy with a dedicated reasoning abstraction level.</p><p>2. Modularized Training. Disentangling reasoning and controls allows fragmenting a complex long horizon problem into shorter time horizon problems. Since each policy is tasked with fulfilling a dedicated goal, each module utilizes separate end-to-end training with sparse communication between the hierarchy in terms of sub-goal information. In summary, we make the following contributions:</p><p>â€¢ To the best of our knowledge, we present the first work on formulating Vision-and-Language Navigation (VLN) as a continuous control problem in photo-realistic simulations, hence lifting the agent of the assumptions enforced by navigation graphs and discrete action spaces. â€¢ We formulate a novel hierarchical framework for Robo-VLN, referred to as Hierarchical Cross-Modal Agent (HCM) for effective attention between different input modalities through a modularized training regime, hence tackling a long-horizon and cross-modal task using layered decision making. â€¢ Provide a suite of baseline models in Robo-VLN inspired by recent state-of-the-art works in VLN and present a comprehensive comparison against our proposed hierarchical approach -Our work sets a new strong benchmark performance for a long horizon complex task, Robo-VLN, with over 13% improvement in absolute success rate in unseen validation environments.</p><p>II. RELATED WORK Vision-and-Language Navigation. Learning based navigation has been explored in both synthetic <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21]</ref> and photo-realistic <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b22">22]</ref> environments. For a navigation graph based formulation of the VLN problem (i.e. discrete action space), previous works have utilized hybrid reinforcement learning <ref type="bibr" target="#b23">[23]</ref>, behavior cloning <ref type="bibr" target="#b24">[24]</ref>, speaker-follower <ref type="bibr" target="#b25">[25]</ref> and sequence to sequence based approaches <ref type="bibr" target="#b8">[9]</ref>. Subsequent methods have focused on utilizing auxiliary losses <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">26]</ref>, backtracking <ref type="bibr" target="#b5">[6]</ref> and cross-modal attention techniques <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref> to improve the performance of VLN agents. Our work, in contrast to discrete VLN setting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> (see <ref type="table" target="#tab_0">Table I</ref>), focuses on a much richer VLN formulation, which is defined for continuous action spaces over long-horizon trajectories. We study the new continuous Robo-VLN setting and propose hierarchical cross-modal attention and modularized training regime for such task.</p><p>Hierarchical Decomposition. Hierarchical structure is most commonly utilized in the context of Reinforcement Learning over long-time horizons to improve sample efficiency <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32]</ref>. Our work closely relates to the options framework in Reinforcement Learning <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34]</ref> where the top-level policy identifies high-level decisions to be fulfilled by a bottom-level policy. In relation to other works which utilize sub-task decomposition for behaviour cloning <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b35">35]</ref>, we show that decomposing hierarchy based on reasoning and imitation are quite effective for longhorizon multi-modal tasks such as Robo-VLN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ROBOTICS VISION-AND-LANGUAGE NAVIGATION ENVIRONMENT (ROBO-VLN)</head><p>Different from existing VLN environments, we propose a new continuous environment for VLN that more closely mirrors the challenges of the real world, Robo-VLNa continuous control formulation for Vision-and-Language Navigation. Compared to navigation graph based <ref type="bibr" target="#b8">[9]</ref> and discrete VLN settings <ref type="bibr" target="#b12">[13]</ref>, Robo-VLN provides longer horizon trajectories (4.5x average number of steps), more visual frames (âˆ¼3.5M visual frames), and a balanced high-level action distribution (see <ref type="figure" target="#fig_1">Figure 2</ref>). Hence, making the problem more challenging and closer to the real-world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>Formally, consider an autonomous agentÃƒ in an unknown environmentáº¼. The goal of a Robo-VLN agent is to learn a policy a t = Ï€(x t , q t , Î¸) where the agent receives visual observations (x t ) from the environmentáº¼ at each time-step   <ref type="bibr" target="#b12">[13]</ref> and R2R <ref type="bibr" target="#b8">[9]</ref>. We provide longer horizon trajectories (4.5x average number of steps, over 3M visual frames, and a balanced high-level action distribution.</p><p>(t) while following a provided instruction (q) to navigate to a goal location G. Î¸ denotes the learnable parameters of the policy Ï€. The action space of the agent consists of continuous linear and angular velocity (v t , Ï‰ t ) and a discrete stop action (s t ). An episode (Ï„ ) is considered successful if agent's distance to the goal is less than a threshold (d a &lt; 3m) and the agent comes to a stop by either taking the stop action (s t ) or decreasing its angular velocity below a certain threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Constructing Continuous VLN in 3D Reconstructions</head><p>To make the continuous VLN formulation possible in 3D reconstructed environments, we port over human annotated instructions (q t ) corresponding to sparse way-points (z t ) along each instruction-trajectory pair in Room2Room (R2R) dataset <ref type="bibr" target="#b8">[9]</ref>, using a continuous control formulation. We do this in 2 stages as follows:</p><p>Ground-truth oracle feedback controller in 3D reconstructed environments. We consider the robotic agent to be a differential drive mobile robot, Locobot <ref type="bibr" target="#b36">[36]</ref>, with a specified radius and height. We develop A planner to compute high-level oracle actions (a h t ) along the shortest path to the goal and use a feedback controller <ref type="bibr" target="#b37">[37]</ref> to convert the discrete R2R trajectories <ref type="bibr" target="#b8">[9]</ref> into continuous ones. The low-level oracle controller (u t ) outputs velocity commands (v t , Ï‰ t ) given sparse way-points (z t ) along a given language-guided navigation trajectory from the R2R dataset <ref type="bibr" target="#b3">[4]</ref>. The converted continuous actions from the lowlevel controller will then be used as ground-truth low-level supervisions a l t when training the navigation agents. We create this continuous control formulation inside Matterport 3D environments <ref type="bibr" target="#b3">[4]</ref> by considering the Locobot robot as a 3D mesh inside 3D reconstructed environments (see <ref type="figure" target="#fig_5">Figure  5</ref>). We use the robot's dynamics <ref type="bibr" target="#b39">[38]</ref> to predict next state (x t+1 ) given current state (x t ) and controller actions (a l t ). Similar to Habitat <ref type="bibr" target="#b22">[22]</ref>, we render the mesh for any arbitrary viewpoint by taking the position generated by the dynamic model inside the 3D reconstruction.</p><p>Obtaining Navigable Instruction-Trajectory pairs. Given a feedback controller of the form a l t = u t (z t ) and high-level sparse viewpoints (z t = [z 1 , . . . , z N ] along the language guided navigation trajectory inside a reconstructed mesh, we search for the navigable space h nav (z t ) using collision detection. We find navigable space for all the trajectories present in the R2R dataset <ref type="bibr" target="#b8">[9]</ref>. This procedure ensures the transfer of only the navigable trajectories from R2R dataset to the continuous control formulation in Robo-VLN; hence, we eliminate non-navigable unrealistic paths for a mobile robot, such as climbing up the stairs and moving through obstacles. Through this approach, we transferred 71% of the trajectories from the discrete VLN setting (VLN-CE <ref type="bibr" target="#b12">[13]</ref>) while preserving all the environments in the Matterport3D dataset <ref type="bibr" target="#b3">[4]</ref>. At the end, Robo-VLN's expert demonstration provide first person RGB-D visual observations (i t ), human instructions (q t ), and oracle actions (a h t , a l t ) for each instruction-trajectory pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HIERARCHICAL CROSS-MODAL AGENT</head><p>Learning an effective policy (Ï€) for a long horizon continuous control problem entails preserving the temporal states as well as spatially reasoning about the surroundings. We therefore propose a hierarchical agent to tackle the Robo-VLN task as it effectively disentangles different dedicated tasks through layered decision making. Given states (X = {x}) and instructions (Q = {q}), our agent leverages these inputs and learns a high-level policy (Ï€ h Î¸ : X Ã— Q â†’ A s,t ) and a corresponding low-level policy (Ï€ l Î³ : X Ã— A s,t â†’ A l,t ). The high-level policy consistently reasons about the alignment between input textual and visual modalities to produce a sub-goal output (A s,t ). The low-level policy ensures that the high-level sub-goal is translated to low-level actions (A l,t ) effectively by imitating the expert controller through an imitation learning policy. Our approach is summarized in <ref type="figure" target="#fig_2">Figure 3</ref> and subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. High-Level Policy</head><p>The high-level policy (Ï€ h Î¸ ) decides a short-term goal (a h t ) based on the input instructions (q t ) and observed visual information x t = {r t , d t } from the environment at each timestep, where r t , d t denote the RGB and Depth sensor readings respectively. Ï€ h Î¸ consists of an encoder-decoder architecture with cross attention between the modules. Subsequent modules of the high-level policy (Ï€ h Î¸ ) are described below. Multi-Modal Cross Attention Encoder. Given a natural language instruction comprised of k words, we denote its feature representation as {q = q 1 with the residual connection from the previous block such that output of each individual block is LayerNorm(z + module(z )). Each Transformer block is computed as follows:</p><formula xml:id="formula_0">A M (Q, K, V ) = concat(h 1 , . . . , h k )W h , where h i = A QW Q i , KW K i , V W V i A(Q, K, V ) = softmax QK T âˆš d k V<label>(1)</label></formula><p>The Attention output (A) is a weighted sum of the values (V ) calculated using a similarity between projected Query (Q) and Key (K). A M represents stacked Attention blocks (A), and W Q i , W K i , W V i and W h are parameters to be learnt. We utilize Equation 1 to perform cross attention between visual spatial representation (RGB f r or Depth f d ) and language features (q t ) successively. We do this by utilising the sum of language features and sinusoidal Positional Encoding (PE <ref type="bibr" target="#b42">[40]</ref>) as query (Q = q t + PE(q t )) and visual representation as Key (K r = f r or K d = f d ) as well as Value (V r = f r or V d = f d ). The final outputs, which we denote as cross-attended context (from RGB or Depth), are computed using A M (Q, K, V ), e.g.,ÃŽ att q for RGB input and d att q for Depth input. These cross-attended contexts represent the matching between instructions and corresponding visual features at each time step t. Note that the learnable weights in the Transformer are not shared between the two modalities.</p><p>Multi-Modal Attention Decoder. To decide on which direction to go next and select the most optimal high-level action (a h t ) high-level policy preserves a temporal memory of the attended visual-linguistic contexts (ÃŽ att q ,d att q ), meanpooled visual features (v t ) and previous actions (a h tâˆ’1 ). We rely on a Recurrent Neural Network to preserve this temporal information across time.</p><formula xml:id="formula_1">h h t = LSTM ÃŽ att q ,d att q ,v t , a tâˆ’1 , h h tâˆ’1 v t = W i (g([f r , f d ]) + b i )<label>(2)</label></formula><p>where g(.) is mean adaptive pooling across the spatial dimensions. W i and b i are learned parameters of a fullyconnected layer.</p><p>The agent computes a probability (p h a ) of selecting the most optimal action (a t ) at each time-step by employing a feed-forward network followed by a sof tmax as follows:</p><formula xml:id="formula_2">p h a = sof tmax(W a ([h h t ] + b a ))<label>(3)</label></formula><p>where W a and b a are parameters to be learnt. High-level action a t comprises of the following navigable directions: move forward (0.25m), turn-left or turn-right (15 degrees) and stop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Low-level Policy</head><p>We employ an imitation policy for the low-level module. At each time-step t, the low-level policy (Ï€ l Ï† ) selects a lowlevel action (a l t )) given the sub-goal (a h t ), generated by the high-level policy and observed visual states (r t , d t ) from the environment. Low-level actions are comprised of agent's linear and angular velocity (v t , Ï‰ t ). Similar to the high-level module, we use mean pooled visual features (v t ) for the lowlevel policy and additionally condition the policy on the highlevel sub-goal (a h t ). Furthermore, we utilize stacked LSTM layers with respective fully-connected layers to generate both low-level action and stop probabilities (p l a , p s a ):</p><formula xml:id="formula_3">h l t = LSTM v t , a h t , h l tâˆ’1 (4) p h a = tanh(g a ([h l t , a l tâˆ’1 ])), p s a = Ïƒ(g s ([h l t , a l tâˆ’1 ])) (5)</formula><p>where g a (.) and g s (.) are one-layer Multi-Layer Perceptrons (MLP). Ïƒ and tanh are sigmoid and tanh activation functions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Details</head><p>We train both high-and low-level policies jointly with three different losses. We employ a multi-class cross-entropy loss computed between ground-truth high-level navigable action (y a t ) and the predicted action probability (p h a ) for  the high-level policy. We employ a mean squared error loss between ground-truth velocity commands (y v,Ï‰ t ) and predicted low-level action probabilities (p l a ). Lastly, we use a binary cross-entropy loss between ground-truth stopping actions (y s t ) and predicted stop probabilities (p s a ) as follows:</p><formula xml:id="formula_4">L loss = Î»</formula><p>High-Level Action Loss </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DATASET AND IMPLEMENTATIONS</head><p>Simulation and Dataset. We use Habitat simulator <ref type="bibr" target="#b22">[22]</ref> to perform our experiments. Our dataset, Robo-VLN, is built upon Matterport3D dataset <ref type="bibr" target="#b3">[4]</ref>, which is a collection of 90 environments captured through around 10k high-definition RGB-D panoramas. Robo-VLN provides 3177 trajectories, and each trajectory is associated with 3 instructions annotated by humans ported over from the R2R Dataset <ref type="bibr" target="#b8">[9]</ref>. Overall, the dataset comprises 9533 expert instruction-trajectory pairs with an average trajectory length of 326 steps compared to 55.8 in VLN-CE <ref type="bibr" target="#b12">[13]</ref> and 5 in R2R <ref type="bibr" target="#b8">[9]</ref>. The corresponding dataset is divided into train, validation seen and validation unseen splits.</p><p>Evaluation Metrics. We evaluate our experiments on the following key standard metrics described by Anderson et al. <ref type="bibr" target="#b44">[42]</ref> and Gabriel et al. <ref type="bibr" target="#b45">[43]</ref>: Success rate (SR), Success weighted by path length (SPL), Normalized Dynamic Time Warping (NDTW), Trajectory Length (TL) and Navigation Error (NE). We use SPL and NDTW as the primary metrics for comparison. Both of these metrics measure the deviation from ground-truth trajectories; SPL places more emphasis on reaching the goal location, whereas NDTW emphasises on following the complete ground-truth path.</p><p>Implementation Details. We use pre-trained ResNet-50 on ImageNet <ref type="bibr" target="#b46">[44]</ref> and pre-trained ConvNet on a large scale point-goal navigation task, DDPPO <ref type="bibr" target="#b47">[45]</ref> to extract spatial features for images and depth modalities successively. For transformer module, we use a hidden size (H = 256), number of Transformer heads (n h = 4), and the size of feed-forward layer (F F = 1024). We found that truncated backpropagation through time <ref type="bibr" target="#b48">[46]</ref> was invaluable to train longer sequence recurrent networks in our case. We used a truncation length of 100 to train attention decoders in both policies. We trained the network for 20 epochs and performed early stopping based on the performance of the model on validation seen dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS &amp; RESULTS</head><p>Flat Baselines. We introduce a suite of flat 1 baselines that are similar to the ones used in VLN-CE <ref type="bibr" target="#b12">[13]</ref>: (1) Sequenceto-Sequence (Seq2Seq): an encoder-decoder architecture trained using teacher-forcing <ref type="bibr" target="#b8">[9]</ref>, (2) Progress Monitor 1 Flat as in there is no explicit hierarchical design for agent's decision making of high-or low-level actions. Exit room and make a right. Head straight until you reach the entrance to the next room and wait.  (PM): an agent based on the Seq2Seq model but with an auxiliary loss for progress monitoring, conceptually similar to <ref type="bibr" target="#b15">[16]</ref>, and (3) Cross-Modal Attention (CMA): an crossmodality attention based agent that is conceptually similar to RCM <ref type="bibr" target="#b16">[17]</ref>. We adapt these baselines into our Robo-VLN task but with a single change: the output layers now predicts linear and angular velocities as well as the stop action, as opposed to the four actions (forward, turn-left, turn-right, and stop) used in VLN-CE. Note that baselines are without DAgger <ref type="bibr" target="#b49">[47]</ref> and data augmentation from <ref type="bibr" target="#b50">[48]</ref>.</p><p>Comparison with Flat Baselines. The results of our proposed HCM against baselines are summarized in <ref type="table" target="#tab_0">Table II</ref>. As shown in <ref type="table" target="#tab_0">Table II</ref> and <ref type="figure" target="#fig_4">Figure 4</ref>, our proposed approach, which uses a hierarchical structure to tackle the long-horizon Robo-VLN problem, consistently outperforms the strong baseline models. Specifically, our HCM agent shows superior validation unseen performance by achieving a 40% SPL and 46% SR; hence demonstrating an absolute 13% improvement in SR and 10% improvement in SPL over the best performing baseline on the validation unseen environments.</p><p>Ablation Study. In our ablation experiments, we empirically validate the significance of different design choices and modules in our proposed HCM agent. Our results are summarized in <ref type="table" target="#tab_0">Table III</ref>. First, we ablate vision (RGB and Depth) in our model. Our results show that an agent without vision performs as good as a random agent (i.e., 0.07 SPL, 0.07SR). It shows the effectiveness of vision for end-toend trainable agents in photo-realistic simulations. Second, we consider an architecture with early RGB and Depth fusion before cross attention with language. Our results show that separately aligning RGB and Depth with instructions performs much better than attending to the instructions corresponding to a fused RGB-D representation. We further ablate hierarchy to show the importance of hierarchy in our architecture. Our results are summarized as follows.</p><p>Is the source of improvement from hierarchy? Our method relies on decomposing the complex task into layered decision making; the top level predicts a sub-goal whereas the bottom level predicts low-level velocity commands. To confirm that hierarchy is indeed the source of improvement, we devise an experiment, in which we flattened the hierarchical model and provide auxiliary sub-goal supervision to the flattened model in addition to the low-level supervisions. This model effectively reduced to Seq2Seq baseline model but with high-level action supervision. The results are reported in <ref type="table" target="#tab_0">Table III</ref> (#2 vs #4). We show that, despite using same levels of supervisions, the flattened hierarchical model under-performs the hierarchical approach, e.g., 40% vs 46% in SR and 34% vs 40% in SPL. This comparison demonstrates that decoupling reasoning and imitation indeed plays a pivotal role in learning effective individual policies.</p><p>Qualitative Comparison. We qualitatively analyze the performance of hierarchical and flat agents in Robo-VLN. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, the hierarchical agent (top example) successfully predicts low-level velocity commands while reaching a desired goal location described by the instruction. The agent takes significantly more steps than discrete VLN settings (511 steps) to reach the goal location; hence showing the effectiveness of hierarchical agents to solve long horizon cross-modal trajectory following problem. The flat agent (bottom figure) fails to follow the trajectory and drives into obstacles multiple times. The episode ends after the agent is unsuccessful in reaching the goal at 1000 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>Despite the recent progress, existing VLN environments impose certain unrealistic assumptions such as perfect localization, known topology and deterministic navigation in the absence of any obstacles. In this work, we first propose the Robo-VLN setting that lifts off the unrealistic assumption of navigation graph and discrete action space and provides a suite of strong baselines inspired by the recent works in discrete VLN setting. We then take the next step to propose a Hierarchical Cross-Modal (HCM) agent that tackles the challenging long-horizon issue in Robo-VLN via a hierarchical model design. Our proposed HCM agent, with trained highand low-level policies, achieves significant performance improvement against the strong baselines. We believe that our new Robo-VLN setting and strong benchmarks would help build a stronger suite of autonomous agents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>visual fram es (M) Num ber of visual fram es</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Robo-VLN compared with discrete VLN settings: VLN-CE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Hierarchical Cross-Modal Agent (HCM): Our proposed agent consists of a high-level module and a corresponding low-level module. High-level module predicts the sub-goal output based on alignment between instructions and visual observations. Low-level module translates the high-level sub-goal output to linear and angular velocities using an imitation learning policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Comparison with strong flat baselines: Our proposed hierarchical method in comparison with strong flat baselines evaluated on the validation unseen dataset. Our approach shows superior performance and better generalization in unseen settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative Comparison: Inference performance of hierarchical and flat model in unseen environments within Robo-VLN. The hierarchical model successfully predicts low-level velocity commands to reach a goal location whereas flat model bumps into obstacles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparison between our proposed Robo-VLN setting and prior environments used for Vision-and-Language Navigation</figDesc><table><row><cell></cell><cell cols="2">-Simulation-</cell><cell></cell><cell cols="2">-Environment-</cell><cell cols="2">-Instructions-</cell></row><row><cell></cell><cell>Action space</cell><cell>Granularity</cell><cell>Agent</cell><cell>Navigation</cell><cell>Type</cell><cell>Richness</cell><cell>Generation</cell></row><row><cell>Touchdown [14], R2R [9]</cell><cell>Discrete</cell><cell>High</cell><cell>Virtual</cell><cell>Unconstrained</cell><cell>Photo-realistic</cell><cell>Complex</cell><cell>Human-annotated</cell></row><row><cell>Follow-net [10]</cell><cell>Discrete</cell><cell>High</cell><cell>Virtual</cell><cell>Constrained</cell><cell>Synthetic</cell><cell>Simple</cell><cell>Human-annotated</cell></row><row><cell>LANI [15]</cell><cell>Discrete</cell><cell>High</cell><cell>Virtual</cell><cell>Constrained</cell><cell>Synthetic</cell><cell>Simple</cell><cell>Template based</cell></row><row><cell>VLN-CE [13]</cell><cell>Discrete</cell><cell>High</cell><cell>Virtual</cell><cell>Unconstrained</cell><cell>Photo-realistic</cell><cell>Complex</cell><cell>Human-annotated</cell></row><row><cell>Robo-VLN (Ours)</cell><cell>Continuous</cell><cell>High/Low</cell><cell>Robotics</cell><cell>Unconstrained</cell><cell>Photo-realistic</cell><cell>Complex</cell><cell>Human-annotated</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Quantitative comparison: Comparison with strong baselines. Note that these baselines are reimplementations from VLN-CE<ref type="bibr" target="#b12">[13]</ref> with small changes (see Section VI for further details).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Validation Seen</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Validation Unseen</cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell>SR â†‘</cell><cell>SPL â†‘</cell><cell>NDTW â†‘</cell><cell>TL â†‘</cell><cell>NE â†“</cell><cell>SR â†‘</cell><cell>SPL â†‘</cell><cell>NDTW â†‘</cell><cell>TL â†‘</cell><cell>NE â†“</cell></row><row><cell>1</cell><cell>Random Agent</cell><cell>0.07</cell><cell>0.07</cell><cell>0.14</cell><cell>5.26</cell><cell>10.25</cell><cell>0.08</cell><cell>0.08</cell><cell>0.14</cell><cell>5.40</cell><cell>9.81</cell></row><row><cell>2</cell><cell>Seq2Seq [4]</cell><cell>0.36</cell><cell>0.34</cell><cell>0.32</cell><cell>11.84</cell><cell>8.63</cell><cell>0.33</cell><cell>0.30</cell><cell>0.28</cell><cell>11.92</cell><cell>8.97</cell></row><row><cell>3</cell><cell>PM [16]</cell><cell>0.32</cell><cell>0.27</cell><cell>0.23</cell><cell>14.12</cell><cell>9.33</cell><cell>0.28</cell><cell>0.24</cell><cell>0.22</cell><cell>13.85</cell><cell>9.82</cell></row><row><cell>4</cell><cell>CMA [17]</cell><cell>0.28</cell><cell>0.25</cell><cell>0.22</cell><cell>11.52</cell><cell>9.95</cell><cell>0.28</cell><cell>0.25</cell><cell>0.23</cell><cell>11.57</cell><cell>9.63</cell></row><row><cell></cell><cell>HCM (Ours)</cell><cell>0.49</cell><cell>0.43</cell><cell>0.35</cell><cell>13.53</cell><cell>7.48</cell><cell>0.46</cell><cell>0.40</cell><cell>0.35</cell><cell>14.06</cell><cell>7.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Ablation Study: Impact of different modules and design choices in our proposed Hierarchical agent.</figDesc><table><row><cell></cell><cell>Module</cell><cell></cell><cell></cell><cell></cell><cell>Validation Seen</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Validation Unseen</cell></row><row><cell></cell><cell># Vision Hierarchy</cell><cell>RGB-D Early fusion</cell><cell cols="3">SR â†‘ SPL â†‘ NDTW â†‘</cell><cell>TL â†‘</cell><cell cols="4">NE â†“â†‘ SR â†‘ SPL NDTW â†‘</cell><cell>TL â†‘</cell><cell>NE â†“</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell>0.07</cell><cell>0.07</cell><cell>0.14</cell><cell>4.82</cell><cell>10.34</cell><cell>0.07</cell><cell>0.07</cell><cell>0.14</cell><cell>10.2</cell><cell>4.81</cell></row><row><cell>Hierarchical</cell><cell>2</cell><cell></cell><cell>0.44</cell><cell>0.37</cell><cell>0.31</cell><cell>14.87</cell><cell>8.21</cell><cell>0.40</cell><cell>0.34</cell><cell>0.28</cell><cell>15.32</cell><cell>8.64</cell></row><row><cell>Agent</cell><cell>3</cell><cell></cell><cell>0.39</cell><cell>0.35</cell><cell>0.29</cell><cell>13.87</cell><cell>9.13</cell><cell>0.34</cell><cell>0.31</cell><cell>0.28</cell><cell>12.85</cell><cell>8.78</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell>0.49</cell><cell>0.43</cell><cell>0.35</cell><cell>13.53</cell><cell>7.48</cell><cell>0.46</cell><cell>0.40</cell><cell>0.35</cell><cell>14.06</cell><cell>7.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Exit room, go along the wall toward the piano, turn right before the piano, turn right into the bookshelf room and stop.... ... ...</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Oracle Path</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Agent's Path</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Start Location</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Goal Location</cell></row><row><cell></cell><cell>steps=1</cell><cell>...</cell><cell>steps=69</cell><cell></cell><cell>steps=122</cell><cell></cell><cell>steps=183</cell><cell></cell><cell></cell></row><row><cell>Flat (Failed Episode)</cell><cell>steps=1</cell><cell>...</cell><cell>steps=257</cell><cell>...</cell><cell>steps=447</cell><cell>...</cell><cell>steps=770</cell><cell>...</cell><cell>steps=1000</cell></row></table><note>steps=511</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , q 2 t , . . . , q k t }, where q i t is the encoded feature representation of the i th word using BERT embedding<ref type="bibr" target="#b40">[39]</ref> to extract meaningful representation of words in the sentence. To encode the observed RGB-D states (r t âˆˆ R hoÃ—woÃ—3 , d t âˆˆ R hoÃ—wo ), we generate a lowresolution spatial feature representations f r âˆˆ R HsÃ—WsÃ—Cs and f d âˆˆ R HsÃ—WsÃ—Cs by using a pre-trained ConvNet backbone, where H s = W s = 7 and C s = 2048. At each time-step t, we combine the individual RGB (f r ) and Depth (f d ) spatial features with encoded language representation (q t ) using a Transformer module<ref type="bibr" target="#b42">[40]</ref>. Each Transformer module is comprised of stacked multi-head attention block (A M ) followed by a position-wise feed-forward block. We utilize layer normalizations<ref type="bibr" target="#b43">[41]</ref> between these blocks along</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Talk2nav: Long-range vision-and-language navigation with dual attention and spatial memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving vision-andlanguage navigation with image-text pairs from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14973</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards learning a generic agent for vision-and-language navigation via pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matterport3D: Learning from RGB-D data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gibson env: real-world perception for embodied agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The regretful agent: Heuristic-aided navigation through progress estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reinforced crossmodal matching and self-supervised imitation learning for vision-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6622" to="6631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speaker-follower models for visionand-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>MontrÃ©al, Canada, S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="3318" to="3329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>SÃ¼nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Follownet: Robot navigation by following natural language directions with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Machine Learning in Planning and Control of Robot Motion Workshop at ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visionlanguage navigation with self-supervised auxiliary reasoning tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Beyond the nav-graph: Vision-and-language navigation in continuous environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vanderbilt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herrasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">AI2-THOR: An Interactive 3D Environment for Visual AI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mapping instructions to actions in 3D environments with visual goal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shatkhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="2667" to="2678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-monitoring navigation agent via auxiliary progress estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6629" to="6638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>ser. NIPS&apos;14</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Cambridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vizdoom: A doom-based AI research platform for visual reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kempka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Runc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Toczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jaskowski</surname></persName>
		</author>
		<idno>abs/1605.02097</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">AI2-THOR: an interactive 3d environment for visual AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1712.05474</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Building generalizable agents with a realistic and rich 3d environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Habitat: A Platform for Embodied AI Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Look before you leap: Bridging model-free and modelbased reinforcement learning for planned-ahead visionand-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part XVI, ser. Lecture Notes in Computer Science</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<meeting>Part XVI, ser. Lecture Notes in Computer Science<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11220</biblScope>
			<biblScope unit="page" from="38" to="55" />
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2018 -15th European Conference</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural Modular Control for Embodied Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Robot Learning (CoRL)</title>
		<meeting>the Conference on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speaker-follower models for visionand-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3314" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visionlanguage navigation with self-supervised auxiliary reasoning tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reinforced crossmodal matching and self-supervised imitation learning for vision-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6629" to="6638" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-modal discriminative model for visionand-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Perceive, transform, and act: Multimodal attention networks for vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Landi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Corsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning</title>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feudal networks for hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1703.01161</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical imitation and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>DaumÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iii</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ser. Proceedings of Machine Learning</title>
		<editor>Research, J. Dy and A. Krause</editor>
		<meeting><address><addrLine>Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>StockholmsmÃ¤ssan</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2917" to="2926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploration-Exploitation in MDPs with Options</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fruit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ser. Proceedings of Machine Learning</title>
		<editor>Research, A. Singh and J. Zhu</editor>
		<meeting><address><addrLine>Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-04-22" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional driving from natural language instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Robot Learning</title>
		<meeting>the Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Alwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08236</idno>
		<title level="m">Pyrobot: An open-source robotics framework for research and benchmarking</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emami-Naeini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Feedback Control of Dynamic Systems</title>
		<imprint/>
	</monogr>
	<note>7th Edition</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pearson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dudek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenkin</surname></persName>
		</author>
		<title level="m">Computational Principles of Mobile Robotics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>2nd ed. USA</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minnesota</forename><surname>Minneapolis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On evaluation of embodied navigation agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<idno>abs/1807.06757</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">General evaluation for instruction conditioned navigation using dynamic time warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Visually Grounded Interaction and Language (ViGIL) Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">University of</title>
		<meeting><address><addrLine>Toronto Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to noregret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning to navigate unseen environments: Back translation with environmental dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04195</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
