<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Orthogonal Recurrent Units: On Learning to Forget</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
							<email>gulcehrc@iro.umontreal.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">MILA -Universite de Montreal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peurifoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tegmark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Soljačić</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">MILA -Universite de Montreal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Gated Orthogonal Recurrent Units: On Learning to Forget</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel recurrent neural network (RNN) based model that combines the remembering ability of unitary RNNs with the ability of gated RNNs to effectively forget redundant/irrelevant information in its memory. We achieve this by extending unitary RNNs with a gating mechanism. Our model is able to outperform LSTMs, GRUs and Unitary RNNs on several long-term dependency benchmark tasks. We empirically both show the orthogonal/unitary RNNs lack the ability to forget and also the ability of GORU to simultaneously remember long term dependencies while forgetting irrelevant information. This plays an important role in recurrent neural networks. We provide competitive results along with an analysis of our model on many natural sequential tasks including the bAbI Question Answering, TIMIT speech spectrum prediction, Penn TreeBank, and synthetic tasks that involve long-term dependencies such as algorithmic, parenthesis, denoising and copying tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent Neural Networks with gating units -such as Long Short Term Memory (LSTMs) <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b8">Gers 2001)</ref> and Gated Recurrent Units (GRUs) <ref type="bibr" target="#b4">(Cho et al. 2014b</ref>) -have led to rapid progress in different areas of machine learning such as language modeling <ref type="bibr" target="#b9">(Graves, Wayne, and Danihelka 2014)</ref>, neural machine translation <ref type="bibr" target="#b4">(Cho et al. 2014b;</ref><ref type="bibr" target="#b22">Sutskever, Vinyals, and Le 2014)</ref>, and speech recognition <ref type="bibr" target="#b2">(Chan et al. 2016;</ref><ref type="bibr" target="#b5">Chorowski et al. 2015)</ref>. These works have proven the importance of gating units for Recurrent Neural Networks.</p><p>The main advantage of using these gated units in RNNs is primarily due to the ease of optimization of the models using them and to reduce the learning degeneracies such as vanishing gradients that can cripple conventional RNNs <ref type="bibr" target="#b21">(Pascanu, Mikolov, and Bengio 2013)</ref>. Most importantly, by designing special gates, it is easier to impose a particular behavior on the model, such as creating shortcut connections through time by using input and forget gates in LSTMs and resetting the memory via the reset gate of a GRU. This feature also brings modularity to the neural network design that seems to make training of those models easier. Gated RNNs are also empirically shown to achieve better results for a wide variety of real-world tasks.</p><p>Recently, using unitary and orthogonal matrices (instead of general matrices) in RNNs <ref type="bibr">(Arjovsky, Shah, and Ben-gio 2016;</ref><ref type="bibr" target="#b14">Jing et al. 2016;</ref><ref type="bibr" target="#b10">Henaff, Szlam, and LeCun 2016)</ref> have attracted an increasing amount of attention in the machine learning community. This trend was following the demonstration that these matrices can be effective in solving tasks involving long-term dependencies and gradients vanishing/exploding <ref type="bibr" target="#b1">(Bengio, Simard, and Frasconi 1994;</ref><ref type="bibr" target="#b12">Hochreiter 1991)</ref> problem. Thus a unitary/orthogonal RNN can capture long term dependencies more effectively in sequential data than a conventional RNN or LSTM. As a result, this type of model has been shown to perform well on tasks that would require rote memorization <ref type="bibr" target="#b12">(Hochreiter 1991)</ref> and simple reasoning, such as the copy task(Hochreiter and Schmidhuber 1997) and the sequential MNIST <ref type="bibr" target="#b17">(Le, Jaitly, and Hinton 2015)</ref>. Those models can just be viewed as an extension to vanilla RNNs <ref type="bibr" target="#b15">(Jordan 1986</ref>) that replaces the transition matrices with either unitary or orthogonal matrices.</p><p>In this paper, we refer the ability of a model to omit parts of the input sequence that contain redundant information and to filter out the noise input in general as the means of a forgetting mechanism. Previously <ref type="bibr" target="#b7">(Gers, Schmidhuber, and Cummins 1999)</ref> have shown the importance of the forgetting mechanism for LSTM networks and with very similar motivations, we discuss the utilization of a forgetting mechanism for RNNs with orthogonal transitions. The importance of forgetting for those networks is mainly due to that unitary/orthogonal RNNs can backpropagate the gradients without vanishing through time, and it is very easy for them to just have an output that depends on equal amounts of all the elements of the whole input sequence. From this perspective, learning to forget can be difficult with unitary/orthogonal RNNs and they can clog up the memory with useless information. However, most real-world applications and natural tasks require the model to filter out irrelevant or redundant information from the input sequence. We argue that difficulties of forgetting can cause unitary and orthogonal RNNs to perform badly on many realistic tasks, and demonstrate this empirically with a toy task.</p><p>We propose a new architecture, the Gated Orthogonal Recurrent Unit (GORU), which combines the advantages of the above two frameworks, namely (i) the ability to capture long term dependencies by using orthogonal matrices and (ii) the ability to "forget" by using a GRU structure. We demonstrate that GORU is able to learn long term dependencies effectively, even in complicated datasets which require a forgetting ability. In this work, we focus on implementation of orthogonal transition matrices which is just a subset of the unitary matrices.</p><p>GORU outperforms a recent variation of unitary RNN called EURNN <ref type="bibr" target="#b14">(Jing et al. 2016</ref>) on language modeling, denoising, parenthesis and the question answering tasks. We show that the unitary RNN fails catastrophically on a denoising task which requires the model to forget. On question answering, speech spectrum prediction, algorithmic, parenthesis and the denoising tasks, GORU achieves better accuracy on the test set over all other models that we compare against. We have attempted to use gates on the unitary matrices with complex numbers, but we encountered some training challenges of training gating mechanisms, thus we have decided to just to focus on orthogonal matrices for this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Given an input sequence x t ∈ R dx , t ∈ {1, 2, · · · , T }, a vanilla RNN defines a sequence of hidden states h t ∈ R d h updated at each time step according to the rule</p><formula xml:id="formula_0">h t = φ(W h h t−1 + W x x t + b),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">W h ∈ R d h ×d h , W x ∈ R dx×d h and b ∈ R d h are</formula><p>model parameters and φ is a nonlinear activation function.</p><p>RNNs have proven to be effective for solving sequential tasks due to their flexibility to . However, a well-known problem called gradient vanishing and gradient explosion has prevented RNNs from efficiently learning long-term dependencies <ref type="bibr" target="#b1">(Bengio, Simard, and Frasconi 1994)</ref>. Several approaches have been developed to solve this problem, with LSTMs and GRUs being the most successful and widely used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated Recurrent Unit</head><p>A big step forward from LSTM is the Gated Recurrent Unit (GRU), proposed by Cho et al, <ref type="bibr" target="#b3">(Cho et al. 2014a</ref>), which removed the extra memory state in LSTM. Specifically, the hidden state h t in a GRU is updated as follows:</p><formula xml:id="formula_2">h t = z t h t−1 + (1 − z t ) tanh(W x x t + r t W h h t−1 + b h ) (2) z t = sigmoid(W z [h t−1 , x t ] + b z ), (3) r t = sigmoid(W r [h t−1 , x t ] + b r ),<label>(4)</label></formula><p>where <ref type="figure" target="#fig_0">Figure 1</ref> demonstrated the architecture of GRU model. Although LSTMs and GRUs were proposed to solve the exploding and vanishing gradient problem <ref type="bibr" target="#b12">(Hochreiter 1991;</ref><ref type="bibr" target="#b1">Bengio, Simard, and Frasconi 1994)</ref> they can in practice still suffer from this issue for long-term tasks. As a result, gradient clipping <ref type="bibr" target="#b21">(Pascanu, Mikolov, and Bengio 2013)</ref> is usually required in the training process, although that only addresses the gradient explosion. and real-valued. Therefore, any vector x that multiplies a unitary or an orthogonal matrix satisfies:</p><formula xml:id="formula_3">W {z,r} ∈ R (d h +dx)×d h , W x ∈ R dx×d h , W h ∈ R d h ×d h and b {z,r,h} ∈ R d h .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unitary and Orthogonal Matrix RNN</head><formula xml:id="formula_4">||Ux|| = ||x||<label>(5)</label></formula><p>Thanks to this property, a unitary/orthogonal matrix is able to preserve the norm of vector flows through it and thus allow for the gradient to propagate through longer time steps. Recent papers from Arjovsky et al <ref type="bibr" target="#b0">(Arjovsky, Shah, and Bengio 2016;</ref><ref type="bibr" target="#b10">Henaff, Szlam, and LeCun 2016)</ref> pointed out that unitary/orthogonal matrices can effectively prevent the gradient vanishing/explosion problem in conventional RNNs. After this work, several other unitary/orthogonal RNN models have been proposed <ref type="bibr" target="#b14">(Jing et al. 2016;</ref><ref type="bibr" target="#b24">Wisdom et al. 2016;</ref><ref type="bibr" target="#b13">Hyland and Rtsch 2017;</ref><ref type="bibr" target="#b20">Mhammedi et al. 2016</ref>), all showing promising abilities in capturing long term dependencies in data. A unitary/orthogonal matrix RNN is simply defined as replacing the hidden to hidden matrices in a vanilla RNN by unitary/orthogonal matrices:</p><formula xml:id="formula_5">h t = σ(Uh t−1 + W x x t , b).<label>(6)</label></formula><p>For unitary matrices, the nonlinear activation function σ needs to handle complex-valued inputs. In this paper, we use the generalizations of the popular real-valued activation function ReLU(x) = max(0, x) known as</p><formula xml:id="formula_6">modReLU(z i , b i ) ≡ z i |z i | ReLU(|z i | + b i ),<label>(7)</label></formula><p>where b is a bias vector. This variant was found to perform effectively on a suite of benchmarks in <ref type="bibr" target="#b0">(Arjovsky, Shah, and Bengio 2016;</ref><ref type="bibr" target="#b14">Jing et al. 2016</ref>). Even though modReLU was developed for complex value models, it turns out this activation function fits unitary/orthogonal matrices best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Gated Orthogonal Recurrent Unit</head><p>The Problem of Forgetting in Orthogonal RNNs</p><p>First, we argue for the advantage of an RNN which can forget some of its past inputs. This is desirable because we seek a state representation which can capture the most important elements of the past sequence, and can throw away irrelevant details or noise. This ability becomes particularly critical when the dimensionality of the RNN state is smaller than the product of the sequence length with the input dimension; i.e., when some form of compression is necessary. For this compression to be most useful for further processing, it is likely that it requires a non-linear combination of the past input values, allowing the network to forget and ignore unnecessary elements from the past. Now consider an RNN whose state is obtained as a sequence of orthogonal transformations, with each transformation being a function of the input at a given time step. Let us focus on class of orthogonal transformations that are basically rotation for the simplicity of our analysis, which (noncommutativity aside) are analogous to addition in the space of angles. When we compose several orthogonal operators, we just add more angles together. So we forget in the mild sense that we get in the state a combination of several rotations (like adding the angles) and we lose track of exactly which individual rotations were applied. The advantage is that, in the space of angles, the derivative of the final angle to any of the individually added angle is 1, so there is no vanishing gradient. However, we cannot have complete forgetting, e.g., making the new state independent of the past inputs (or of some of them which we wish to forget): for a new rotation to cancel an old rotation, one would need the new rotation to "know" about the old rotation to cancel, i.e., it would need to be a function of the old rotation. But this is not what happens, because each new rotation is chosen before looking at the current state. Instead, in a regular RNN, the state update depends in a non-linear way on the past state, so that for example when a particular value of the state is reached, it can be reset to 0. This would not be possible with just the composition of orthogonal transformations. These considerations motivate an architecture in which we combine orthogonal or unitary transformations with non-linearities which can be trained to forget when and where it is appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GORU Architecture</head><p>This section introduces the Gated Orthogonal Recurrent Unit (GORU). In our architecture, we change the hidden state loop matrix into an orthogonal matrix and change the respective activation function to modReLU:</p><formula xml:id="formula_7">h t = z t h t−1 + (1 − z t ) modReLU(W x x t + r t (Uh t−1 ) + b h ), (8) z t = sigmoid(W z h t−1 + W z,x x t + b z ), (9) r t = sigmoid(W r h t−1 + W r,x x t + b r ),<label>(10)</label></formula><p>where σ is a suitable nonlinear activation function and W z ,</p><formula xml:id="formula_8">W r ∈ R d h ×d h , b z , b r , b h ∈ R d h , W z,x , W r,x , W x ∈ R dx×d h .</formula><p>r t and z t are the reset and update gates, respectively. U ∈ R d h ×d h is kept orthogonal.</p><p>In fact, we have only modified the main loop that absorbs new information to orthogonal while leaving the gates unchanged compared to the GRU. <ref type="figure">Figure 2</ref> demonstrated the architecture of GORU model. We enforce matrix U to be orthogonal by using parametrization method purposed in <ref type="bibr" target="#b14">(Jing et al. 2016)</ref>. U is decomposed into a sequence of 2-by-2 rotation matrices as shown in <ref type="figure">Figure 3</ref>. Each 2-by-2 rotation contains one trainable rotation parameter.</p><p>The update gates of the GORU help the model to filter out irrelevant or noise information coming from the input. It can be thought of as acting like a low-pass filter. The orthogonal transition matrices help the model to prevent the gradients to vanish through time. However, the ways an orthogonal transformation can interact with the hidden state of an RNN is limited to reflections and rotations. The reset gate enables the model to rescale the magnitude of the hidden state activations (h t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare GORU with unitary RNNs (using the EURNN parameterization purposed by Jing et al. <ref type="bibr" target="#b14">(Jing et al. 2016)</ref>) and two other well-known gatedRNNs (LSTMs and GRUs). Previous research on unitary RNNs has mainly focused on memorization tasks; in contrast, we focus on more realistic noisy tasks, which require the model to discard parts of the input sequence to be able to use its capacity efficiently. GORU is implemented in Tensorflow, available from https://github.com/jingli9111/ GORU-tensorflow</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Copying Memory Task</head><p>The first task we consider is the well known Copying Memory Task. The copying task is a synthetic task that is commonly used to test the network's ability to remember information seen T time steps earlier.</p><p>Specifically, the task is defined as follows. An alphabet consists of symbols {a i }, i ∈ {0, 1, · · · , n − 1, n, n + 1}, the first n of which represent data, and the remaining two representing "blank" and "marker", respectively. Here we choose n = 8. The input sequence contains 10 data steps, followed by "blank". The RNN model is supposed to output "blank" and give the original sequence once it sees the "marker". Note that each instance has a different location for these 10 elements of data.</p><p>In this experiment, we use RMSProp optimization with a learning rate of 0.001 and a decay rate of 0.9 for all models. The batch size is set to 128. All these models have roughly same number of hidden to hidden parameters, despite not having similar neuron layer sizes. Hidden state sizes are set to 128, 100, 90, 512, respectively to match total number of hidden to hidden parameters. GORU is the only gatedsystem to successfully solve this task while the GRU and LSTM both get stuck at the baseline. EURNN is seen to converges within hundreds of iterations.</p><p>This task only requires the model to efficiently overcome the gradient vanishing/explosion problem and does not require a forgetting ability. The EURNN performs perfectly and goes through to the baseline in no time -as previously seen. The GORU is the only gated-system to successfully solve this task while the GRU and LSTM get stuck at the baseline as shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denoise Task</head><p>We evaluate the forgetting ability of each RNN architecture on a synthetic "denoise" task. A list of data points are located randomly in a long noisy sequence. The RNN model is supposed to filter out the useless part ("noise") and output the remaining sequential labels.</p><p>Similarly to the labels of copying memory task above, an alphabet consists of symbols {a i }, i ∈ {0, 1, · · · , n − 1, n, n + 1}, the first n of which represent data, and the remaining two represent "noise" and "marker", respectively. The input sequence contains 10 randomly located data steps and the rest are filled by "noise". The RNN model is supposed to output those 10 data in a sequence after it sees the "marker". Just as in the previous experiment, we use RM-SProp optimization algorithm with a learning rate of 0.01 and a decay rate of 0.9 for all models. The batch size is set to 128. All these models have roughly same number of hidden to hidden parameters. Hidden state sizes are set to 128, 100, 90, 512, respectively to match total number of hidden to hidden parameters. EURNN get stuck at the baseline because of lacking forgetting mechanism, while GORU and GRU successfully solve the task.</p><p>This task requires both the ability of learning long dependencies but also the ability to forget the noisy input. The GORU and GRU both are able to successfully outperform LSTM in terms of both learning speed and final performances as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. EURNN, however, gets stuck at the baseline, just as we intuitively expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parenthesis Task</head><p>The parenthesis task <ref type="bibr" target="#b6">(Foerster et al. 2016</ref>) requires the RNN model to count the number of each type of unmatched parentheses at each time step, given that there are 10 types of parentheses. The input data contains 10 pairs of different parenthesis types -e.g. <ref type="bibr">'(, [, {, ¡, )</ref>, ], }, ¿, ...' -mixed with random noise/characters between them. The neural network outputs how many unmatched parentheses there are. For instance, given '(((', the neural network would output '123'.Note that there are never more than 10 unmatched parentheses in any category.</p><p>In our experiment, the total input length is set to 200. We used batch size 128 and RMSProp Optimizer with a learning rate 0.001, decay rate 0.9 on all models. Hidden state sizes are set to match their total numbers of hidden to hidden parameters.</p><p>This task requires learning long-term dependencies and forgetting of the noisy data. The GORU is able to successfully outperform GRU, LSTM and EURNN in terms of both learning speed and final performances as shown in <ref type="figure" target="#fig_4">Figure 6</ref>.</p><p>We also analyzed the activations of the update gates for GORU and GRU. According to the histogram of activations shown in <ref type="figure">Figure 7</ref>, both models behave very similarly, and when the model receives noise as input, the activations of its  <ref type="figure">Figure 7</ref>: The activations of the update gates for GORU and GRU on the parenthesis task. In those bar-charts we visualize the number of activations that are greater than 0.7 normalized by the total number of units in the gate,</p><formula xml:id="formula_9">d h i=0 1r i &gt;0.7 d h .</formula><p>As can be seen in the top two plots, the activations of the update gate peaks and becomes almost one when the input is noise when the magnitude of the noise input in the bottom plot is above the red bar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithmic Task</head><p>We tested the RNN models on algorithmic task as described in <ref type="bibr" target="#b18">(Li et al. 2015)</ref>. The model is fed with random graph as an input sequence and required to output the shortest path at the end of the sequence. We have used the exact same setup and use the data provided as in <ref type="bibr" target="#b18">(Li et al. 2015)</ref>.</p><p>We used batch size 50 and hidden size 128 for all models. The RNNs are trained with RMSProp optimizer with a learning rate of 0.001 and decay rate of 0.9.</p><p>We summarized the test set results in <ref type="table" target="#tab_0">Table 1</ref>. We found that the GORU performs averagely better than GRU/LSTM and EURNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy EURNN 66.3 ± 3.2 LSTM 60.9 ± 4.7 GRU 74.1 ± 4.8 GORU 75.0 ± 5.3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>bAbI: Episodic Question Answering</head><p>We tested the ability of our RNN models on a word-level episodic question answering task. The bAbI dataset <ref type="bibr">(Weston et al. )</ref> examines RNN's ability to understand language and perform basic logical reasoning. Each training example is a set of statements, that are logically related in some fashion. For instance, one training example consists of these three statements and a question: Mary went to the bathroom. John moved to the Hallway. Mary traveled to the office. Where is Mary? Answer: office. There are twenty different types of questions that can be asked -some requiring deduction between lines, some requiring association. The bAbI dataset is useful because it contains a small sized vocabulary, short sentences and requires one word answers for each story. Thus, it is a good benchmarking test because the word mapping layers are not the dominant sources of parameters.</p><p>We test each task with a uni-directional RNN without any attention mechanism. In detail, we word-embed and then feed one RNN the sequence of statements. Another RNN is fed the word-embedded question. Then, we concatenate the outputs of the two RNN's into a single input for a third RNN that then outputs the correct word.</p><p>We summarized the test set results as follows in <ref type="table" target="#tab_2">Table  2</ref>. We found that the GORU performs averagely better than GRU/LSTM and EURNN. We also show the big gains over EURNN by introducing the gates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Modeling: Character-level Prediction</head><p>We test each RNN on character-level language modeling. The RNN is fed by one character each step from a real context and supposed to output the prediction for the next character. We used the Penn Treebank corpus <ref type="bibr" target="#b19">(Marcus, Marcinkiewicz, and Santorini 1993)</ref>.</p><p>We use RMSProp with minibatch size of 32 and a learning rate of 0.001. Each training sequence is unfolded into 50 time steps. Similar to most work in language modeling, at the end of each sequence, the hidden state is saved and used  to initialize the hidden state for the next sequence. This allows the neural network to give consistent predictions even at the beginning of a sequence.</p><p>We show the final test performance in <ref type="table" target="#tab_4">Table 3</ref> by comparing their performance in terms of bits-per-character. GORU is performing comparable to LSTM and GRU in our experiments and it performs significantly better than EURNN. We have also done an ablation study with disabling reset and update gates. Since most of the relevant information for character-level prediction can be obtained by only using the recent rather than distant past <ref type="bibr" target="#b16">(Karpathy, Johnson, and Fei-Fei 2015)</ref>, the core of the character-prediction challenge does not involve the main strength of EURNN.  We only use single layer models. We choose the size of the models to match the number of parameters. GORU is able to outperform EURNN. We also tested the performance of restricted GORU which shows the necessity of both reset and update gates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech Spectrum Prediction</head><p>We tested the ability of our RNN models on real-world speech spectrum prediction task in short-time Fourier transform (STFT) <ref type="bibr" target="#b24">(Wisdom et al. 2016;</ref><ref type="bibr" target="#b14">Jing et al. 2016)</ref>. We used TIMIT dataset sampled in 8 kHz. The audio .wav file is initially divided into different time frames and then Fourier transformed into the frequency domain and finally normalized for training/testing. In our STFT operation we uses a Hann analysis window of 256 samples (32 milliseconds) and a window hop of 128 samples (16 milliseconds). In this task, the RNNs are required to predict th log-magnitude of the STFT frame at time t + 1, given all the log-magnitudes of STFT frames up to time t. We used a training set with 2400 utterances, a validation set of 600 utterances and 1000 utterances for test. We trained all RNNs for with the same batch size 32 using Adam optimization with a learning rate of 0.001.</p><p>We found GORU significantly outperforms all other models with same hidden size as shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have built a novel RNN that brings the benefits of orthogonal matrices to gated architectures: the Gated Orthogonal Recurrent Units (GORU). By replacing the hidden to hidden matrix in the reseting path of the GRU to be an orthogonal matrix, and replacing the non-linear activation to a mod-ReLU, GORU gains the advantage of unitary/orthogonal RNNs since the gradient can pass through long time steps without exploding. Our empirical results showed that GORU is the only model we found that could solve both the synthetic copying task and the denoise task. Moreover, GORU is able to outperform GRU and LSTM in several benchmark tasks.</p><p>These results suggest that the GORU is the first step in bringing an explicit forgetting mechanism to the class of unitary/orthogonal RNNs. Our method demonstrates how to incorporate orthogonal matrices into a variety of neural network architectures, and we are excited to open the gate the next level of neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>AFigure 1 :</head><label>1</label><figDesc>complex-valued matrix U is unitary when it satisfies UU * T = I. A matrix U is orthogonal if it is both unitary Illustration of GRU. r and z are reset and update gates. h is the hidden state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :UFigure 3 :</head><label>23</label><figDesc>Illustration of GORU. h is the hidden state. For GORU, r and z are reset and update gates. It uses modReLU activation function instead of tanh. Orthogonal matrix parametrization by 2-by-2 rotation matrices. Each row represents one neuron in the hidden state. Each junction represents a 2-by-2 rotation matrix on corresponding two neurons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Copying Memory Task with delay time T = 200 on GORU, GRU, LSTM and EURNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Denoise Task with sequence length T = 200 on GORU, GRU, LSTM and EURNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Parenthesis tasks with total sequence length T = 200 on GORU, GRU, LSTM and EURNN. Hidden state sizes are set to 128, 100, 90, 512, respectively to match total number of hidden to hidden parameters. Loss is shown in log scale. GORU is able to outperform all other models in both learning speed and final performance.update gate peaks. This behavior helps the model to forget and omit the noise input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Algorithmic Test on GORU, GRU, LSTM and EU-RNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Question Answering task on bAbI dataset. Test accuracy (%) on GORU, GRU, LSTM and EURNN. All models are performed on one-way RNN without extra memory or attention mechanism. GORU achieves highest average accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Penn Treebank character-level modeling Test on GORU, GRU, LSTM and EURNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="3">#parameters MSE(validation) MSE(test)</cell></row><row><cell>LSTM</cell><cell>98k</cell><cell>58.8</cell><cell>57.5</cell></row><row><cell>GRU</cell><cell>72k</cell><cell>58.9</cell><cell>57.3</cell></row><row><cell>EURNN</cell><cell>41k</cell><cell>51.8</cell><cell>51.9</cell></row><row><cell>GORU</cell><cell>59k</cell><cell>45.4</cell><cell>47.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Speech Spectrum prediction Test on GORU, GRU, LSTM and EURNN. We only use single layer models with same hidden size 128.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Listen, attend and spell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Intelligible language modeling with input switched affine networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sussillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09434</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXive preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<title level="m">Learning to forget: Continual prediction with lstm</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Long short-term memory in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Universitt Hannover</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent orthogonal networks and long-memory tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>Balcan, M. F., and Weinberger, K. Q.</editor>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2034" to="2042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen. Diploma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note type="report_type">Technische Universität München 91</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning unitary operators with help from u(n)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rtsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2050" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dubcek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skirlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soljacic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05231</idno>
		<title level="m">Tunable efficient unitary neural networks (eunn) and their application to rnns</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Serial order: A parallel distributed processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mhammedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hellicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>James</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00188</idno>
		<title level="m">Efficient orthogonal parametrisation of recurrent neural networks using householder reflections</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrinboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXive preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
