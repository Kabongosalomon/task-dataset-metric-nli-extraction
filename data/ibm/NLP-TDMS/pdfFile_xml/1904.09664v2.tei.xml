<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Hough Voting for 3D Object Detection in Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Hough Voting for 3D Object Detection in Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -samples from 2D manifolds in 3D space -we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The goal of 3D object detection is to localize and recognize objects in a 3D scene. More specifically, in this work, we aim to estimate oriented 3D bounding boxes as well as semantic classes of objects from point clouds.</p><p>Compared to images, 3D point clouds provide accurate geometry and robustness to illumination changes. On the other hand, point clouds are irregular. thus typical CNNs are not well suited to process them directly.</p><p>To avoid processing irregular point clouds, current 3D detection methods heavily rely on 2D-based detectors in various aspects. For example, <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b11">12]</ref> extend 2D detection frameworks such as the Faster/Mask R-CNN <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b10">11]</ref> to 3D. They voxelize the irregular point clouds to regular 3D grids and apply 3D CNN detectors, which fails to leverage sparsity in the data and suffer from high computation cost due to expensive 3D convolutions. Alternatively, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b55">55]</ref> project Voting from input point cloud 3D detection output <ref type="figure">Figure 1</ref>. 3D object detection in point clouds with a deep Hough voting model. Given a point cloud of a 3D scene, our VoteNet votes to object centers and then groups and aggregates the votes to predict 3D bounding boxes and semantic classes of objects.</p><p>Our code is open sourced at https://github.com/ facebookresearch/votenet points to regular 2D bird's eye view images and then apply 2D detectors to localize objects. This, however, sacrifices geometric details which may be critical in cluttered indoor environments. More recently, <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">34]</ref> proposed a cascaded two-step pipeline by firstly detecting objects in front-view images and then localizing objects in frustum point clouds extruded from the 2D boxes, which however is strictly dependent on the 2D detector and will miss an object entirely if it is not detected in 2D. In this work we introduce a point cloud focused 3D detection framework that directly processes raw data and does not depend on any 2D detectors neither in architecture nor in object proposal. Our detection network, VoteNet, is based on recent advances in 3D deep learning models for point clouds, and is inspired by the generalized Hough voting process for object detection <ref type="bibr" target="#b22">[23]</ref>.</p><p>We leverage PointNet++ <ref type="bibr" target="#b36">[36]</ref>, a hierarchical deep network for point cloud learning, to mitigates the need to convert point clouds to regular structures. By directly processing point clouds not only do we avoid information loss by a quantization process, but we also take advantage of the sparsity in point clouds by only computing on sensed points. While PointNet++ has shown success in object classification and semantic segmentation <ref type="bibr" target="#b36">[36]</ref>, few research study how to detect 3D objects in point clouds with such architectures. A naïve solution would be to follow common practice in 2D detectors and perform dense object proposal <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">37]</ref>, i.e. to propose 3D bounding boxes directly from the sensed points (with their learned features). However, the inherent sparsity of point clouds makes this approach unfavorable. In images there often exists a pixel near the object center, but it is often not the case in point clouds. As depth sensors only capture surfaces of objects, 3D object centers are likely to be in empty space, far away from any point. As a result, point based networks have difficulty aggregating scene context in the vicinity of object centers. Simply increasing the receptive field does not solve the problem because as the network captures larger context, it also causes more inclusion of nearby objects and clutter.</p><p>To this end, we propose to endow point cloud deep networks with a voting mechanism similar to the classical Hough voting. By voting we essentially generate new points that lie close to objects centers, which can be grouped and aggregated to generate box proposals.</p><p>In contrast to traditional Hough voting with multiple separate modules that are difficult to optimize jointly, VoteNet is end-to-end optimizable. Specifically, after passing the input point cloud through a backbone network, we sample a set of seed points and generate votes from their features. Votes are targeted to reach object centers. As a result, vote clusters emerge near object centers and in turn can be aggregated through a learned module to generate box proposals. The result is a powerful 3D object detector that is purely geometric and can be applied directly to point clouds.</p><p>We evaluate our approach on two challenging 3D object detection datasets: SUN RGB-D <ref type="bibr" target="#b40">[40]</ref> and ScanNet <ref type="bibr" target="#b4">[5]</ref>. On both datasets VoteNet, using geometry only, significantly outperforms prior arts that use both RGB and geometry or even multi-view RGB images. Our study shows that the voting scheme supports more effective context aggregation, and verifies that VoteNet offers the largest improvements when object centers are far from the object surface (e.g. tables, bathtubs, etc.).</p><p>In summary, the contributions of our work are:</p><p>• A reformulation of Hough voting in the context of deep learning through an end-to-end differentiable architecture, which we dub VoteNet.</p><p>• State-of-the-art 3D object detection performance on SUN RGB-D and ScanNet.</p><p>• An in-depth analysis of the importance of voting for 3D object detection in point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D object detection. Many previous methods were proposed to detect 3D bounding boxes of objects. Examples include: <ref type="bibr" target="#b26">[27]</ref> where a pair-wise semantic context potential helps guide the proposals objectness score; template-based methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b27">28]</ref>; Sliding-shapes <ref type="bibr" target="#b41">[41]</ref> and its deep learning-based successor <ref type="bibr" target="#b42">[42]</ref>; Clouds of Oriented Gradients (COG) <ref type="bibr" target="#b38">[38]</ref>; and the recent 3D-SIS <ref type="bibr" target="#b11">[12]</ref>. Due to the complexity of directly working in 3D, especially in large scenes, many methods resort to some type of projection. For example in MV3D <ref type="bibr" target="#b3">[4]</ref> and VoxelNet <ref type="bibr" target="#b55">[55]</ref>, the 3D data is first reduced to a bird's-eye view before proceeding to the rest of the pipeline. A reduction in search space by first processing a 2D input was demonstrated in both Frustum PointNets <ref type="bibr" target="#b34">[34]</ref> and <ref type="bibr" target="#b19">[20]</ref>. Similarly, in <ref type="bibr" target="#b15">[16]</ref> a segmentation hypothesis is verified using the 3D map. More recently, deep networks on point clouds are used to exploit sparsity of the data by GSPN <ref type="bibr" target="#b54">[54]</ref> and PointRCNN <ref type="bibr" target="#b39">[39]</ref>.</p><p>Hough voting for object detection. Originally introduced in the late 1950s, the Hough transform <ref type="bibr" target="#b12">[13]</ref> translates the problem of detecting simple patterns in point samples to detecting peaks in a parametric space. The Generalized Hough Transform <ref type="bibr" target="#b1">[2]</ref> further extends this technique to image patches as indicators for the existence of a complex object. Examples of using Hough voting include the seminal work of <ref type="bibr" target="#b23">[24]</ref> which introduced the implicit shape model, planes extraction from 3D point clouds <ref type="bibr" target="#b2">[3]</ref>, and 6D pose estimation <ref type="bibr" target="#b44">[44]</ref> to name a few.</p><p>Hough voting has also been previously combined with advanced learning techniques. In <ref type="bibr" target="#b30">[30]</ref> the votes were assigned with weights indicating their importance, which were learned using a max-margin framework. Hough forests for object detection were introduced in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>. More recently, <ref type="bibr" target="#b14">[15]</ref> demonstrated improved voting-based 6D pose estimation by using deep features extracted to build a codebook. Similarly <ref type="bibr" target="#b31">[31]</ref> learned deep features to build codebooks for segmentation in MRI and ultrasiounds images. In <ref type="bibr" target="#b13">[14]</ref> the classical Hough algorithm was used to extract circular patterns in car logos, which were then input to a deep classification network. <ref type="bibr" target="#b33">[33]</ref> proposed the semiconvolutional operator for 2D instance segmentation in images, which is also related to Hough voting.</p><p>There have also been works using Hough voting for 3D object detection <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b18">19]</ref>, which adopted a similar pipeline as in 2D detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning on point clouds.</head><p>Recently we see a surge of interest in designing deep network architectures suited for point clouds <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b51">51]</ref>, which showed remarkable performance in 3D object classification, object part segmentation, as well as scene segmentation. In the context of 3D object detection, VoxelNet <ref type="bibr" target="#b55">[55]</ref> learn voxel feature embeddings from points in voxels, while in <ref type="bibr" target="#b34">[34]</ref> PointNets are used to localize objects in a frustum point cloud extruded from a 2D bounding box. However, few methods studied how to directly propose and detect 3D objects in raw point cloud representation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Hough Voting</head><p>A traditional Hough voting 2D detector <ref type="bibr" target="#b23">[24]</ref> comprises an offline and an online step. First, given a collection of images with annotated object bounding boxes, a codebook is constructed with stored mappings between image patches (or their features) and their offsets to the corresponding object centers. At inference time, interest points are selected from the image to extract patches around them. These patches are then compared against patches in the codebook to retrieve offsets and compute votes. As object patches will tend to vote in agreement, clusters will form near object centers. Finally, the object boundaries are retrieved by tracing cluster votes back to their generating patches.</p><p>We identify two ways in which this technique is well suited to our problem of interest. First, voting-based detection is more compatible with sparse sets than regionproposal networks (RPN) <ref type="bibr" target="#b37">[37]</ref> is. For the latter, the RPN has to generate a proposal near an object center which is likely to be in an empty space, causing extra computation. Second, it is based on a bottom-up principle where small bits of partial information are accumulated to form a confident detection. Even though neural networks can potentially aggregate context from a large receptive field, it may be still beneficial to aggregate in the vote space.</p><p>However, as traditional Hough voting comprises multiple separated modules, integrating it into state-of-the-art point cloud networks is an open research topic. To this end, we propose the following adaptations to the different pipeline ingredients.</p><p>Interest points are described and selected by deep neural networks instead of depending on hand-crafted features. Vote generation is learned by a network instead of using a codebook. Levaraging larger receptive fields, voting can be made less ambiguous and thus more effective. In addition, a vote location can be augmented with a feature vector allowing for better aggregation. Vote aggregation is realized through point cloud processing layers with trainable parameters. Utilizing the vote features, the network can potentially filter out low quality votes and generate improved proposals. Object proposals in the form of: location, dimensions, orientation and even semantic classes can be directly generated from the aggregated features, mitigating the need to trace back votes' origins.</p><p>In what follows, we describe how to combine all the aforementioned ingredients into a single end-to-end trainable network named as VoteNet. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates our end-to-end detection network (VoteNet). The entire network can be split into two parts: one that processes existing points to generate votes; and the other part that operates on virtual points -the votes -to propose and classify objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">VoteNet Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Learning to Vote in Point Clouds</head><p>From an input point cloud of size N × 3, with a 3D coordinate for each of the N points, we aim to generate M votes, where each vote has both a 3D coordinate and a high dimensional feature vector. There are two major steps: point cloud feature learning through a backbone network and learned Hough voting from seed points. Point cloud feature learning. Generating an accurate vote requires geometric reasoning and contexts. Instead of relying on hand-crafted features, we leverage recently proposed deep networks <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b24">25]</ref> on point clouds for point feature learning. While our method is not restricted to any point cloud network, we adopt PointNet++ <ref type="bibr" target="#b36">[36]</ref> as our backbone due to its simplicity and demonstrated success on tasks ranging from normal estimation <ref type="bibr" target="#b9">[10]</ref>, semantic segmentation <ref type="bibr" target="#b20">[21]</ref> to 3D object localization <ref type="bibr" target="#b34">[34]</ref>.</p><p>The backbone network has several set-abstraction layers and feature propagation (upsampling) layers with skip connections, which outputs a subset of the input points with XYZ and an enriched C-dimensional feature vector. The results are M seed points of dimension (3 + C). Each seed point generates one vote 1 . Hough voting with deep networks. Compared to traditional Hough voting where the votes (offsets from local keypoints) are determined by look ups in a pre-computed codebook, we generate votes with a deep network based voting module, which is both more efficient (without kNN look ups) and more accurate as it is trained jointly with the rest of the pipeline.</p><p>Given a set of seed points</p><formula xml:id="formula_0">{s i } M i=1 where s i = [x i ; f i ] with x i ∈ R 3 and f i ∈ R C ,</formula><p>a shared voting module generates votes from each seed independently. Specifically, the voting module is realized with a multi-layer perceptron (MLP) network with fully connected layers, ReLU and batch normalization. The MLP takes seed feature f i and outputs the Euclidean space offset ∆x i ∈ R 3 and a feature offset ∆f i ∈ R C such that the vote v i = [y i ; g i ] generated from the seed s i has y i = x i + ∆x i and g i = f i + ∆f i .</p><p>The predicted 3D offset ∆x i is explicitly supervised by a regression loss</p><formula xml:id="formula_1">L vote-reg = 1 M pos i ∆x i − ∆x * i 1[s i on object],<label>(1)</label></formula><p>where 1[s i on object] indicates whether a seed point s i is on an object surface and M pos is the count of total number of seeds on object surface. ∆x * i is the ground truth displacement from the seed position x i to the bounding box center of the object it belongs to.</p><p>Votes are the same as seeds in tensor representation but are no longer grounded on object surfaces. A more essential difference though is their position -votes generated from seeds on the same object are now closer to each other than the seeds are, which makes it easier to combine cues from different parts of the object. Next we will take advantage of <ref type="bibr" target="#b0">1</ref> The case of more than one vote is discussed in the appendix. this semantic-aware locality to aggregate vote features for object proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Proposal and Classification from Votes</head><p>The votes create canonical "meeting points" for context aggregation from different parts of the objects. After clustering these votes we aggregate their features to generate object proposals and classify them. Vote clustering through sampling and grouping. While there can be many ways to cluster the votes, we opt for a simple strategy of uniform sampling and grouping according to spatial proximity. Specifically, from a set of votes</p><formula xml:id="formula_2">{v i = [y i ; g i ] ∈ R 3+C } M i=1 ,</formula><p>we sample a subset of K votes using farthest point sampling based on {y i } in 3D Euclidean space, to get {v i k } with k = 1, ..., K. Then we form K clusters by finding neighboring votes to each of the v i k 's 3D location:</p><formula xml:id="formula_3">C k = {v (k) i | v i − v i k ≤ r} for k = 1, ..., K.</formula><p>Though simple, this clustering technique is easy to integrate into an end-to-end pipeline and works well in practice. Proposal and classification from vote clusters. As a vote cluster is in essence a set of high-dim points, we can leverage a generic point set learning network to aggregate the votes in order to generate object proposals. Compared to the back-tracing step of traditional Hough voting for identifying the object boundary, this procedure allows to propose amodal boundaries even from partial observations, as well as predicting other parameters like orientation, class, etc.</p><p>In our implementation, we use a shared PointNet <ref type="bibr" target="#b35">[35]</ref> for vote aggregation and proposal in clusters. Given a vote cluster C = {w i } with i = 1, ..., n and its cluster center w j , where w i = [z i ; h i ] with z i ∈ R 3 as the vote location and h i ∈ R C as the vote feature. To enable usage of local vote geometry, we transform vote locations to a local normalized coordinate system by z i = (z i − z j )/r. Then an object proposal for this cluster p(C) is generated by passing the set input through a PointNet-like module:</p><formula xml:id="formula_4">p(C) = MLP 2 max i=1,...,n {MLP 1 ([z i ; h i ])}<label>(2)</label></formula><p>where votes from each cluster are independently processed by a MLP 1 before being max-pooled (channel-wise) to a single feature vector and passed to MLP 2 where information from different votes are further combined. We represent the proposal p as a multidimensional vector with an objectness score, bounding box parameters (center, heading and scale parameterized as in <ref type="bibr" target="#b34">[34]</ref>) and semantic classification scores. Loss function. The loss functions in the proposal and classification stage consist of objectness, bounding box estimation, and semantic classification losses. We supervise the objectness scores for votes that are located either close to a ground truth object center (within 0.3 meters) or far from any center (by more than 0.6 meters). We consider proposals generated from those votes as positive and negative proposals, respectively. Objectness predictions for other proposals are not penalized. Objectness is supervised via a cross entropy loss normalized by the number of non-ignored proposals in the batch. For positive proposals we further supervise the bounding box estimation and class prediction according to the closest ground truth bounding box. Specifically, we follow <ref type="bibr" target="#b34">[34]</ref> which decouples the box loss to center regression, heading angle estimation and box size estimation. For semantic classification we use the standard cross entropy loss. In all regression in the detection loss we use the Huber (smooth-L 1 <ref type="bibr" target="#b37">[37]</ref>) loss. Further details are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Input and data augmentation. Input to our detection network is a point cloud of N points randomly sub-sampled from either a popped-up depth image (N = 20k) or a 3D scan (mesh vertices, N = 40k). In addition to XY Z coordinates, we also include a height feature for each point indicating its distance to the floor. The floor height is estimated as the 1% percentile of all points' heights. To augment the training data, we randomly sub-sample the points from the scene points on-the-fly. We also randomly flip the point cloud in both horizontal direction, randomly rotate the scene points by Uniform[−5 • , 5 • ] around the upright-axis, and randomly scale the points by Uniform[0.9, 1.1]. Network architecture details. The backbone feature learning network is based on PointNet++ <ref type="bibr" target="#b36">[36]</ref>, which has four set abstraction (SA) layers and two feature propagation/upsamplng (FP) layers, where the SA layers have increasing receptive radius of 0.2, 0.4, 0.8 and 1.2 in meters while they sub-sample the input to 2048, 1024, 512 and 256 points respectively. The two FP layers up-sample the 4th SA layer's output back to 1024 points with 256-dim features and 3D coordinates (more details in the appendix).</p><p>The voting layer is realized through a multi-layer perceptron with FC output sizes of 256, 256, 259, where the last FC layer outputs XYZ offset and feature residuals.</p><p>The proposal module is implemented as a set abstraction layer with a post processing MLP 2 to generate proposals after the max-pooling. The SA uses radius 0.3 and MLP 1 with output sizes of 128, 128, 128. The max-pooled features are further processed by MLP 2 with output sizes of 128, 128, 5 + 2N H + 4N S + N C where the output consists of 2 objectness scores, 3 center regression values, 2N H numbers for heading regression (N H heading bins) and 4N S numbers for box size regression (N S box anchors) and N C numbers for semantic classification. Training the network. We train the entire network end-toend and from scratch with an Adam optimizer, batch size 8 and an initial learning rate of 0.001. The learning rate is decreased by 10× after 80 epochs and then decreased by another 10× after 120 epochs. Training the model to convergence on one Volta Quadro GP100 GPU takes around 10 hours on SUN RGB-D and less than 4 hours on ScanNetV2.</p><p>Inference. Our VoteNet is able to take point clouds of the entire scenes and generate proposals in one forward pass. The proposals are post-processed by a 3D NMS module with an IoU threshold of 0.25. The evaluation follows the same protocol as in <ref type="bibr" target="#b42">[42]</ref> using mean average precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we firstly compare our Hough voting based detector with previous state-of-the-art methods on two large 3D indoor object detection benchmarks (Sec. 5.1). We then provide analysis experiments to understand the importance of voting, the effects of different vote aggregation approaches and show our method's advantages in its compactness and efficiency (Sec. 5.2). Finally we show qualitative results of our detector (Sec. 5.3). More analysis and visualizations are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparing with State-of-the-art Methods</head><p>Dataset. SUN RGB-D <ref type="bibr" target="#b40">[40]</ref> is a single-view RGB-D dataset for 3D scene understanding. It consists of ∼5K RGB-D training images annotated with amodal oriented 3D bounding boxes for 37 object categories. To feed the data to our network, we firstly convert the depth images to point clouds using the provided camera parameters. We follow a standard evaluation protocol and report performance on the 10 most common categories.</p><p>ScanNetV2 <ref type="bibr" target="#b4">[5]</ref> is a richly annotated dataset of 3D reconstructed meshes of indoor scenes. It contains ∼1.2K training examples collected from hundreds of different rooms, and annotated with semantic and instance segmentation for 18 object categories. Compared to partial scans in SUN RGB-D, scenes in ScanNetV2 are more complete and cover larger areas with more objects on average. We sample vertices from the reconstructed meshes as our input point clouds. Since ScanNetV2 does not provide amodal or oriented bounding box annotation, we aim to predict axisaligned bounding boxes instead, as in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods in comparison.</head><p>We compare with a wide range of prior art methods. Deep sliding shapes (DSS) <ref type="bibr" target="#b42">[42]</ref> and 3D-SIS <ref type="bibr" target="#b11">[12]</ref> are both 3D CNN based detectors that combine geometry and RGB cues in object proposal and classification, based on the Faster R-CNN <ref type="bibr" target="#b37">[37]</ref> pipeline. Compared with DSS, 3D-SIS introduces a more sophisticated sensor fusion scheme (back-projecting RGB features to 3D voxels) and therefore is able to use multiple RGB views to improve performance. 2D-driven <ref type="bibr" target="#b19">[20]</ref> and F-PointNet <ref type="bibr" target="#b34">[34]</ref> are 2Dbased 3D detectors that rely on object detection in 2D images to reduce the 3D detection search space. Cloud of gra-  <ref type="bibr" target="#b40">[40]</ref>. Note that both COG <ref type="bibr" target="#b38">[38]</ref> and 2D-driven <ref type="bibr" target="#b19">[20]</ref> use room layout context to boost performance. To have fair comparison with previous methods, the evaluation is on the SUN RGB-D V1 data.</p><p>Input mAP@0.25 mAP@0.5 DSS <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b11">12]</ref> Geo + RGB 15.2 6.8 MRCNN 2D-3D <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>   <ref type="bibr" target="#b11">[12]</ref>. Mask R-CNN 2D-3D results are from <ref type="bibr" target="#b54">[54]</ref>. GSPN and 3D-SIS results are up-to-date numbers provided by the original authors.</p><p>dients <ref type="bibr" target="#b38">[38]</ref> is a sliding window based detector using a newly designed 3D HoG-like feature. MRCNN 2D-3D is a naïve baseline that directly projects Mask-RCNN <ref type="bibr" target="#b10">[11]</ref> instance segmentation results into 3D to get a bounding box estimation. GSPN <ref type="bibr" target="#b54">[54]</ref> is a recent instance segmentation method using a generative model to propose object instances, which is also based on a PointNet++ backbone.</p><p>Results are summarized in <ref type="table" target="#tab_1">Table 1</ref> and 2. VoteNet outperforms all previous methods by at least 3.7 and 18.4 mAP increase in SUN RGB-D and ScanNet respectively. Notably, we achieve such improvements when we use geometric input (point clouds) only while they used both geometry and RGB images. <ref type="table" target="#tab_1">Table 1</ref> shows that in the category "chair" with the most training samples, our method improves upon previous state of the art by more than 11 AP. <ref type="table" target="#tab_2">Table 2</ref> shows that when taking geometric input only, our method outperforms 3D CNN based method 3D-SIS by more than 33 AP. A per-category evaluation for ScanNet is provided in the appendix. Importantly, the same set of network hyperparameters was used in both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis Experiments</head><p>To Vote or Not To Vote? A straightforward baseline to VoteNet is a network that directly proposes boxes from sam- pled scene points. Such a baseline -which we refer to as BoxNet -is essential to distill the improvement due to voting. The BoxNet has the same backbone as the VoteNet but instead of voting, it directly generates boxes from the seed points (more details in appendix). <ref type="table" target="#tab_3">Table 3</ref> shows voting boosts the performance by a significant margin of ∼5 mAP on SUN RGB-D and &gt;13 mAP on ScanNet.</p><p>In what ways, then, does voting help? We argue that since in sparse 3D point clouds, existing scene points are often far from object centroids, a direct proposal may have lower confidence and inaccurate amodal boxes. Voting, instead, brings closer together these lower confidence points and allows to reinforce their hypothesis though aggregation. We demonstrate this phenomenon in <ref type="figure">Fig. 3</ref> on a typical ScanNetV2 scene. We overlay the scene with only those seed points which, if sampled, would generate an accurate proposal. As can be seen, VoteNet (right) offers a much broader coverage of "good" seed points compared to BoxNet (left), showing its robustness brought by voting.</p><p>We proceed with a second analysis in <ref type="figure" target="#fig_2">Fig. 4</ref> showing on the same plot (in separate scales), for each SUN RGB-D category: (in blue dots) gains in mAP between VoteNet and BoxNet, and (in red squares) closest distances between object points (on their surfaces) and their amodal box centers, averaged per category and normalized by the mean class size (a large distance means the object center is usually far from its surface). Sorting the categories according to the former, we see a strong correlation. Namely, when object points tend to be further from the amodal box center, voting helps much more. <ref type="figure">Figure 3</ref>. Voting helps increase detection contexts. Seed points that generate good boxes (BoxNet), or good votes (VoteNet) which in turn generate good boxes, are overlaid (in blue) on top of a representative ScanNet scene. As the voting step effectively increases context, VoteNet demonstrates a much denser cover of the scene, therefore increasing the likelihood of accurate detection.  Effect of Vote Aggregation Aggregation of votes is an important component in VoteNet as it allows communication between votes. Hence, it is useful to analyze how different aggregation schemes influence performance. In <ref type="figure" target="#fig_3">Fig. 5 (right)</ref>, we show that vote aggregation with a learned Pointnet and max pooling achieves far better results than manually aggregating the vote features in the local regions due to the existence of clutter votes (i.e. votes from non-object seeds). We test 3 types of those aggregations (first three rows): max, average, and RBF weighting (based on vote distance to the cluster center). In contrast to aggregation with Pointnet (Eq. 2), the vote features are directly pooled, e.g. for avg. pooling: p = MLP 2 {AVG{h i }}).</p><p>In <ref type="figure" target="#fig_3">Fig. 5 (left)</ref>, we show how vote aggregation radius affects detection (tested with Pointent using max pooling). As the aggregation radius increases, VoteNet improves until it peaks at around 0.2 radius. Attending to a larger region though introduces more clutter votes thus contaminating the good votes and results in decreased performance.</p><p>Model Size and Speed Our proposed model is very efficient since it leverages sparsity in point clouds and avoids search in empty space. Compared to previous best methods <ref type="table">(Table 4</ref>), our model is more than 4× smaller than F-PointNet (the prior art on SUN RGB-D) in size and more than 20× times faster than 3D-SIS (the prior art on Scan-NetV2) in speed. Note that the ScanNetV2 processing time by 3D-SIS is computed as averaged time in offline batch mode while ours is measured with sequential processing which can be realized in online applications. <ref type="figure" target="#fig_4">Fig. 6</ref> and <ref type="figure" target="#fig_5">Fig. 7</ref> show several representative examples of VoteNet detection results on ScanNet and SUN RGB-D scenes, respectively. As can be seen, the scenes are quite diverse and pose multiple challenges including clutter, partiality, scanning artifacts, etc. Despite these challenges, our network demonstrates quite robust results. See for example in <ref type="figure" target="#fig_4">Fig. 6</ref>, how the vast majority of chairs were correctly detected in the top scene. Our method was able to nicely distinguish between the attached sofa-chairs and the sofa in the bottom left scene; and predicted the complete bounding box of the much fragmented and cluttered desk at the bottom right scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Results and Discussion</head><p>There are still limitations in our method though. Com-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Model size SUN RGB-D ScanNetV2</head><p>F-PointNet <ref type="bibr" target="#b34">[34]</ref> 47.0MB 0.09s -3D-SIS <ref type="bibr" target="#b11">[12]</ref> 19.7MB -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.85s</head><p>VoteNet (ours) 11.2MB 0.10s 0.14s <ref type="table">Table 4</ref>. Model size and processing time (per frame or scan). Our method is more than 4× more compact in model size than <ref type="bibr" target="#b34">[34]</ref> and more than 20× faster than <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VoteNet prediction</head><p>Ground truth  mon failure cases include misses on very thin objects like doors, windows and pictures denoted in black bounding boxes in the top scene ( <ref type="figure" target="#fig_4">Fig. 6</ref>). As we do not make use of RGB information, detecting these categories is almost impossible. <ref type="figure" target="#fig_5">Fig. 7</ref> on SUN RGB-D also reveals the strengths of our method in partial scans with single-view depth images. For example, it detected more chairs in the top-left scene than were provided by the ground-truth. In the topright scene we can see how VoteNet can nicely hallucinate the amodal bounding box despite seeing only part of the sofa. A less successful amodal prediction is shown in the bottom right scene where an extremely partial observation of a very large table is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work we have introduced VoteNet: a simple, yet powerful 3D object detection model inspired by Hough voting. The network learns to vote to object centroids directly from point clouds and learns to aggregate votes through their features and local geometry to generate high-quality object proposals. Using only 3D point clouds, the model showed significant improvements over previous methods that utilize both depth and colored images.</p><p>In future work we intend to explore how to incorporate RGB images into our detection framework and to utilize our detector in downstream application such as 3D instance segmentation. We believe that the synergy of Hough voting and deep learning can be generalized to more applications such as 6D pose estimation, template based detection etc. and expect to see more future research along this line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>This appendix provides additional details on the network architectures and loss functions (Sec. A.1), more analysis experiment results (Sec. A.2), per-category results on Scan-Net (Sec. A.3), and finally more visualizations (Sec. A.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Details on Architectures and Loss Functions</head><p>VoteNet architecture details. As mentioned in the main paper, the VoteNet architecture composes of a backbone point feature learning network, a voting module and a proposal module.</p><p>The backbone network, based on the PointNet++ architecture <ref type="bibr" target="#b36">[36]</ref>, has four set abstraction layers and two feature up-sampling layers. The detailed layer parameters are shown in <ref type="table">Table 5</ref>. Each set abstraction (SA) layer has a receptive field specified by a ball-region radius r, a MLP network for point feature transform M LP [c 1 , ..., c k ] where c i is output channel number of the i-th layer in the MLP. The SA layer also subsamples the input point cloud with farthest point sampling to n points. Each SA layer is specified by (n, r, [c 1 , ..., c k ]) as shown in the <ref type="table">Table 5</ref>. Compared to <ref type="bibr" target="#b36">[36]</ref>, we also normalize the XYZ scale of points in each local region by the region radius.</p><p>Each set feature propagation (FP) layer upsamples the point features by interpolating the features on input points to output points (each output point's feature is weighted average of its three nearest input points' features). It also combines the skip-linked features through a MLP (interpolated features and skip-linked features are concatenated before fed into the MLP). Each FP layer is specified by [c 1 , ..., c k ] where c i is the output of the i-th layer in the MLP.</p><p>The voting module as mentioned in the main paper is a MLP that transforms seeds' features to votes including a XYZ offset and a feature offset. The seed points are outputs of the fp2 layer. The voting module MLP has output sizes of 256, 256, 259 for its fully connected layers. The last fully connected layer does not have ReLU or BatchNorm.</p><p>The proposal module as mentioned in the main paper is a SA layer followed by another MLP after the max-pooling in each local region. We follow <ref type="bibr" target="#b34">[34]</ref> on how to parameterize the oriented 3D bounding boxes. The layer's output has 5 + 2N H + 4N S + N C channels where N H is the number of heading bins (we predict a classification score for each heading bin and a regression offset for each bin -relative to the bin center and normalized by the bin size), N S is the number of size templates (we predict a classification score for each size template and 3 scale regression offsets for height, width and length) and N C is the number of semantic classes. In SUN RGB-D: N H = 12, N S = N C = 10, in ScanNet: N H = 12, N S = N C = 18. In the first 5 channels, the first two are for objectness classification and the rest three are for center regression (relative to the vote cluster center).</p><p>VoteNet loss function details. The network is trained end-to-end with a multi-task loss including a voting loss, an objectness loss, a 3D bounding box estimation loss and a semantic classification loss. We weight the losses such that they are in similar scales with λ 1 = 0.5, λ 2 = 1 and λ 3 = 0.1.</p><formula xml:id="formula_5">L VoteNet = L vote-reg + λ 1 L obj-cls + λ 2 L box + λ 3 L sem-cls (3)</formula><p>Among the losses, the vote regression loss is as defined in the main paper (with L1 distance). For ScanNet we compute the ground truth votes as offset from the mesh vertices of an instances to the centers of the axis-aligned tight bounding boxes of the instances. Note that since the bounding box is not amodal, they can vary in sizes due to scan completeness (e.g. a chair may have a floating bounding box if its leg is not recovered from the reconstruction). For SUN RGB-D since the dataset does not provide instance segmentation annotations but only amodal bounding boxes, we cannot compute a ground truth vote directly (as we don't know which points are on objects). Instead, we consider any point inside an annotated bounding box as an object point (required to vote) and compute its offset to the box center as the ground truth. In cases that a point is in multiple ground truth boxes, we keep a set of up to three ground truth votes, and consider the minimum distance between the predicted vote and any ground truth vote in the set during vote regression on this point.</p><p>The objectness loss is just a cross-entropy loss for two classes and the semantic classification loss is also a crossentropy loss of N C classes.</p><p>The box loss follows <ref type="bibr" target="#b34">[34]</ref> (but without the corner loss regularization for simplicity) and is composed of center regression, heading estimation and size estimation sub-losses. In all regression in the box loss we use the robust L1smooth loss. Both the box and semantic losses are only computed on positive vote clusters and normalized by the number of positive clusters. We refer readers to <ref type="bibr" target="#b34">[34]</ref> for more details.</p><formula xml:id="formula_6">L box = L center-reg + 0.1L angle-cls + L angle-reg + 0.1L size-cls + L size-reg<label>(4)</label></formula><p>One difference though is that, instead of a naive regression loss, we use a Chamfer loss <ref type="bibr" target="#b5">[6]</ref> for L center-reg (between regressed centers and ground truth box centers). It requires that each positive proposal is close to a ground truth object and each ground truth object center has a proposal near it. The latter part also influences the voting in the sense that it encourages non-object seed points near the object to also vote for the center of the object, which helps further increase contexts in detection. BoxNet loss function details. BoxNet has the same loss function as VoteNet, except it is not supervised by vote regression. There is also a slight difference in how objectness labels (used to supervise objectness classification) are computed. As seed points (on object surfaces) are often far from object centroids, it no longer works well to use the distances between seed points and object centroids to compute the objectness labels. In BoxNet, we assign positive objectness labels to seed points that are on objects (those belonging to the semantic categories we consider) and negative labels to all the other seed points on clutter (e.g. walls, floors).</p><formula xml:id="formula_7">L BoxNet = λ 1 L obj-cls + λ 2 L box + λ 3 L sem-cls<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. More Analysis Experiments</head><p>Average precision and recall plots <ref type="figure" target="#fig_7">Fig. 8</ref> shows how average precision (AP) and average recall (AR) change as we increase the number of proposals. The AP and AR are both averaged across 10 categories on SUN RGB-D. We report two ways of using the proposals: joint and per-class. For the joint proposal we propose K objects' bounding boxes for all the 10 categories, where we consider each proposal as the semantic class it has the largest confidence in, and use their objectness scores to rank them. For the per-class proposal we duplicate the K proposal 10 times thus have K proposals per class where we use the multiplication of semantic probability for that class and the objectness probability to rank them. The latter way of using proposals gives us a slight improvement on AP and a big boost on AR.  The AP and AR are averaged across the 10 classes. The recall is maximum recall given a fixed number of detection per scene. The "joint proposal" means that we assign each proposal to a single class (the class with the highest classification score); The "per-class proposal" means that we assign each proposal to all the 10 classes (the objectness score is multipled by the semantic classification probability).</p><p>We see that with as few as 10 proposals our VoteNet can achieve a decent AP of around 45% while having 100 proposals already pushes the AP to above 57%. With a thousand proposals, our network can achieve around 78.7% recall with joint proposal and around 87.7% recall with perclass proposal.</p><p>Context of voting One difference of a deep Hough voting scheme with the traditional Hough voting is that we can take advantage of deep features, which can provide more context knowledge for voting. In <ref type="table" target="#tab_5">Table 8</ref> we show how features from different levels of the PointNet++ affects detection performance (from SA2 to FP3, the network has increasing contexts for voting). FP3 layer is extended from the FP2 with a MLP of output sizes 256 and 256 with 2048 output points (the same set of XYZ as that output by SA1).</p><p>It is surprising to find that voting from even SA2 can achieve reasonable detection results (mAP 51.2%) while voting from FP2 achieves the best performance. Having larger context (e.g. FP3) than FP2 does not show further improvements on the performance.</p><p>Multiple votes per seed In default we just generate one vote per seed since we find that with large enough context there is little need to generate more than one vote to resolve ambiguous cases. However, it is still possible to generate cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP 3DSIS 5views <ref type="bibr" target="#b11">[12]</ref>   <ref type="table">Table 9</ref>. Effects of number of votes per seed. Evaluation metric is mAP@0.25 on SUN RGB-D. If random number is on, we concatenate a random number to the seed feature before voting, which helps break symmetry in the case of multiple votes per seed. more than one vote with our network architecture. Yet to break the symmetry in multiple vote generation, one has to introduce some bias to different votes to prevent then from pointing to the same place.</p><p>In experiments, we find that one vote per seed achieves the best results, as shown in <ref type="table">Table 9</ref>. We ablate by using a vote factor of 3, where the voting module generates 3 votes per seed with a MLP layer spec: [256, 256, 259 * 3]). In computing the vote regression loss on a seed point, we consider the minimum distance between any predicted votes to the ground truth vote (in case of SUN RGB-D where we may have a set of ground truth votes for a seed, we compute the minimum distance among any pair of predicted vote and ground truth vote).</p><p>To break symmetry, we generate 3 random numbers and inject them to the second last features from the MLP layer. We show results both with and without this procedure which shows no observable difference.</p><p>On proposal sampling In the proposal step, to generate K proposals from the votes, we need to select K vote clusters. How to select those clusters is a design choice we study here (each cluster is simply a group of votes near a center <ref type="figure">Figure 9</ref>. Vote meeting point. Left: ScanNet scene with votes coming from object points. Right: vote offsets from source seedpoints to target-votes. Object votes are colored green, and nonobject ones are colored red. See how object points from all-parts of the object vote to form a cluster near the center. Non-object points, however, either vote "nowhere" and therefore lack structure, or are near object and have gathered enough context to also vote properly. vote). In <ref type="table" target="#tab_1">Table 10</ref>, we report mAP results on SUN RGB-D with 256 proposals (joint proposal) using cluster sampling strategies of vote FPS, seed FPS and random sampling, where FPS means farthest point sampling. From 1024 vote clusters, vote FPS samples K clusters based on votes' XYZ. Seed FPS firstly samples on seed XYZ and then finds the votes corresponding to the sampled seeds -it enables a direct comparison with BoxNet as it uses the same sampling scheme, making the two techniques similar up to the space in which the points are grouped: VoteNet groups votes according to vote XYZ, while BoxNet groups seeds according to seed XYZ. Random sampling simply selects a random set of K votes and take their neighborhoods for proposal generation. Note that the results from We can see that while seed FPS gets the best number in mAP, the difference caused by different sampling strategies is small, showing the robustness of our method.</p><p>Effects of the height feature In point clouds from indoor scans, point height is a useful feature in recognition. As mentioned in the main paper, we can use 1% of the Z values (Z-axis is up-right) of all points from a scan as an approximate as the floor height z floor , and then compute the a point (x, y, z)'s height as z − z floor . In <ref type="table" target="#tab_1">Table 11</ref> we show how this extra height feature affect detection performance. We see that adding the height feature consistently improves performance in both SUN RGB-D and ScanNet. <ref type="table">Table 6</ref> and <ref type="table">Table 7</ref> report per-class average precision on 18 classes of ScanNetV2 with 0.25 and 0.5 box IoU thresholds respectively. Relying on purely geonetric data, our method excels (esp. with mAP@0.25) in detecting objects like bed, chair, table, desk etc. where geometry is a strong cue for recognition; and struggles with objects best recognized by texture and color like pictures. <ref type="figure">Fig. 9</ref> shows (a subset of) votes predicted from our VoteNet in a typical ScanNet scene. We clearly see that seed points on objects (bed, sofa etc.) vote to object centers while clutter points vote either to object center as well (if the clutter point is close to the object) or to nowhere due to lack of structure in the clutter area (e.g. a wall).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. ScanNet Per-class Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Visualization of Votes</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the VoteNet architecture for 3D object detection in point clouds. Given an input point cloud of N points with XYZ coordinates, a backbone network (implemented with PointNet++ [36] layers) subsamples and learns deep features on the points and outputs a subset of M points but extended by C-dim features. This subset of points are considered as seed points. Each seed independently generates a vote through a voting module. Then the votes are grouped into clusters and processed by the proposal module to generate the final proposals. The classified and NMSed proposals become the final 3D bounding boxes output. Image best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Voting helps more in cases where object points are far from object centers. We show for each category: voting accuracy gain (in blue dots) of VoteNet w.r.t our direct proposal baseline BoxNet; and (in red squares) average object-center distance, normalized by the mean class size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Vote aggregation analysis. Left: mAP@0.25 on SUN RGB-D for varying aggregation radii when aggregating via Pointnet (max). Right: Comparisons of different aggregation methods (radius = 0.3 for all methods). Using a learned vote aggregation is far more effective than manually pooling the features in a local neighborhood.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results of 3D object detection in ScanNetV2. Left: our VoteNet, Right: ground-truth. See Section 5.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results on SUN RGB-D. Both left and right panels show (from left to right): an image of the scene (not used by our network), 3D object detection by VoteNet, and ground-truth annotations. See Section 5.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Number of proposals per scene v.s. Average Precision (AP) and Average Recall (AR) on SUN RGB-D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>3D object detection results on SUN RGB-D val set. Evaluation metric is average precision with 3D IoU threshold 0.25 as proposed by</figDesc><table><row><cell></cell><cell>Input</cell><cell>bathtub</cell><cell>bed</cell><cell cols="2">bookshelf chair</cell><cell>desk</cell><cell cols="3">dresser nightstand sofa</cell><cell>table</cell><cell cols="2">toilet mAP</cell></row><row><cell>DSS [42]</cell><cell>Geo + RGB</cell><cell>44.2</cell><cell>78.8</cell><cell>11.9</cell><cell>61.2</cell><cell>20.5</cell><cell>6.4</cell><cell>15.4</cell><cell>53.5</cell><cell>50.3</cell><cell>78.9</cell><cell>42.1</cell></row><row><cell>COG [38]</cell><cell>Geo + RGB</cell><cell>58.3</cell><cell>63.7</cell><cell>31.8</cell><cell>62.2</cell><cell>45.2</cell><cell>15.5</cell><cell>27.4</cell><cell>51.0</cell><cell>51.3</cell><cell>70.1</cell><cell>47.6</cell></row><row><cell cols="2">2D-driven [20] Geo + RGB</cell><cell>43.5</cell><cell>64.5</cell><cell>31.4</cell><cell>48.3</cell><cell>27.9</cell><cell>25.9</cell><cell>41.9</cell><cell>50.4</cell><cell>37.0</cell><cell>80.4</cell><cell>45.1</cell></row><row><cell cols="2">F-PointNet [34] Geo + RGB</cell><cell>43.3</cell><cell>81.1</cell><cell>33.3</cell><cell>64.2</cell><cell>24.7</cell><cell>32.0</cell><cell>58.1</cell><cell>61.1</cell><cell>51.1</cell><cell>90.9</cell><cell>54.0</cell></row><row><cell>VoteNet (ours)</cell><cell>Geo only</cell><cell>74.4</cell><cell>83.0</cell><cell>28.8</cell><cell>75.3</cell><cell>22.0</cell><cell>29.8</cell><cell>62.2</cell><cell>64.0</cell><cell>47.3</cell><cell>90.1</cell><cell>57.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Geo + RGB</cell><cell>17.3</cell><cell>10.5</cell></row><row><cell>F-PointNet [34, 12]</cell><cell>Geo + RGB</cell><cell>19.8</cell><cell>10.8</cell></row><row><cell>GSPN [54]</cell><cell>Geo + RGB</cell><cell>30.6</cell><cell>17.7</cell></row><row><cell>3D-SIS [12]</cell><cell>Geo + 1 view</cell><cell>35.1</cell><cell>18.7</cell></row><row><cell>3D-SIS [12]</cell><cell>Geo + 3 views</cell><cell>36.6</cell><cell>19.0</cell></row><row><cell>3D-SIS [12]</cell><cell>Geo + 5 views</cell><cell>40.2</cell><cell>22.5</cell></row><row><cell>3D-SIS [12]</cell><cell>Geo only</cell><cell>25.4</cell><cell>14.6</cell></row><row><cell>VoteNet (ours)</cell><cell>Geo only</cell><cell>58.6</cell><cell>33.5</cell></row></table><note>. 3D object detection results on ScanNetV2 val set. DSS and F-PointNet results are from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>mAP@0.25</cell><cell></cell></row><row><cell></cell><cell cols="2">SUN RGB-D ScanNet</cell></row><row><cell>BoxNet (ours)</cell><cell>53.0</cell><cell>45.4</cell></row><row><cell>VoteNet (ours)</cell><cell>57.7</cell><cell>58.6</cell></row></table><note>. Comparing VoteNet with a no-vote baseline. Metric is 3D object detection mAP. VoteNet estimate object bounding boxes from vote clusters. BoxNet proposes boxes directly from seed points on object surfaces without voting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>19.76 69.71 66.15 71.81 36.06 30.64 10.88 27.34 0.00 10.00 46.93 14.06 53.76 35.96 87.60 42.98 84.30 16.20 40.23 3DSIS Geo [12] 12.75 63.14 65.98 46.33 26.91 7.95 2.79 2.30 0.00 6.92 33.34 2.47 10.42 12.17 74.51 22.87 58.66 7.05 25.36 VoteNet ours 36.27 87.92 88.71 89.62 58.77 47.32 38.10 44.62 7.83 56.13 71.69 47.23 45.37 57.13 94.94 54.70 92.11 37.20 58.65 Table 6. 3D object detection scores per category on the ScanNetV2 dataset, evaluated with mAP@0.25 IoU. cab bed chair sofa tabl door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP 3DSIS 5views [12] 5.73 50.28 52.59 55.43 21.96 10.88 0.00 13.18 0.00 0.00 23.62 2.61 24.54 0.82 71.79 8.94 56.40 6.87 22.53 3DSIS Geo [12] 5.06 42.19 50.11 31.75 15.12 1.38 0.00 1.44 0.00 0.00 13.66 0.00 2.63 3.00 56.75 8.68 28.52 2.55 14.60 VoteNet (ours) 8.07 76.06 67.23 68.82 42.36 15.34 6.43 28.00 1.25 9.52 37.52 11.55 27.80 9.96 86.53 16.76 78.87 11.69 33.54 Table 7. 3D object detection scores per category on the ScanNetV2 dataset, evaluated with mAP@0.5 IoU. Effects of seed context for 3D detection. Evaluation metric is mAP@0.25 on SUN RGB-D.</figDesc><table><row><cell cols="5">Seed layer SA2 SA3 SA4 FP1 FP2 FP3</cell></row><row><cell>mAP</cell><cell cols="4">51.2 56.3 55.1 56.6 57.7 57.1</cell></row><row><cell></cell><cell>Vote factor</cell><cell>1</cell><cell>3</cell><cell>3</cell></row><row><cell></cell><cell>Random number</cell><cell>N</cell><cell>N</cell><cell>Y</cell></row><row><cell></cell><cell>mAP</cell><cell cols="3">57.7 55.8 55.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 Table 10 .Table 11 .</head><label>101011</label><figDesc>are from the same model trained with vote FPS to select proposals. Farthest point sampling on votes 57.2 Farthest point sampling on seeds 57.7 Effects of proposal sampling. Evaluation metric is mAP@0.25 on SUN RGB-D. 256 proposals are used for all evaluations. Our method is not sensitive to how we choose centers for vote groups/clusters. Effects of the height feature. Evaluation metric is mAP@0.25 on both datasets.</figDesc><table><row><cell cols="2">Proposal sampling</cell><cell>mAP</cell></row><row><cell cols="2">Random sampling</cell><cell>57.5</cell></row><row><cell>Dataset</cell><cell cols="2">with height without height</cell></row><row><cell>SUN RGB-D</cell><cell>57.7</cell><cell>57.0</cell></row><row><cell>ScanNet</cell><cell>58.6</cell><cell>58.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ackownledgements. This work was supported in part by ONR MURI grant N00014-13-1-0341, NSF grant IIS-1763268 and a Vannevar Bush Faculty Fellowship. We thank Daniel Huber, Justin Johnson, Georgia Gkioxari and Jitendra Malik for valuable discussions and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalizing the hough transform to detect arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="122" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The 3d hough transform for plane detection in point clouds: A review and a new accumulator design. 3D Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorit</forename><surname>Borrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Elseberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Lingemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Nüchter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Class-specific hough forests for object detection. In Decision forests for computer vision and medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="143" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hough forests for object detection, tracking, and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2188" to="2202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pcpnet learning local shape properties from raw point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanir</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maks</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="75" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Mask R-CNN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">3D-SIS: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07003</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Machine analysis of bubble chamber pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">C</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Proc</title>
		<imprint>
			<date type="published" when="1959" />
			<biblScope unit="volume">590914</biblScope>
			<biblScope unit="page" from="554" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vehicle logo retrieval based on hough transform and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Yujian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="967" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accurate localization of 3d objects from rgb-d data using segmentation hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung-Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shili</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3182" to="3189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Orientation invariant 3d object classification using hough transform based methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukta</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM workshop on 3D object retrieval</title>
		<meeting>the ACM workshop on 3D object retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene cut: Class-specific object detection and segmentation in 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukta</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">2d-driven 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4622" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointgrid: A deep network for 3d shape understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9204" to="9214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Combined object categorization and segmentation with an implicit shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Bastian Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on statistical learning in computer vision, ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Bastian Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Database-assisted object retrieval for real-time 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Asist: automatic semantically invariant scene transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lior Shapira, Alex Bronstein, and Ran Gal</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="284" to="299" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Reed; Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Object detection using a max-margin hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hough-cnn: deep learning for segmentation of deep brain regions in mri and ultrasound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Kroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annika</forename><surname>Plate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rozanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliana</forename><surname>Maiostre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birgit</forename><surname>Ertl-Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Bötzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="102" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A search-classify approach for cluttered indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Sharf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH Asia</title>
		<meeting>SIGGRAPH Asia</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-convolutional operators for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="86" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04244</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="634" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Depth-encoded hough voting for joint object detection and shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="658" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09438</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Implicit shape models for object detection in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Velizhev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society of Photogrammetry and Remote Sensing Congress</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Demisting the hough transform for 3d shape recognition and registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Tri</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="332" to="341" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Frank Perbet, and Björn Stenger</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03320</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
