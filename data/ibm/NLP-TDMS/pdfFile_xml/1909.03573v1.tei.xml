<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LCSCNet: Linear Compressing Based Skip-Connecting Network for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingmin</forename><surname>Liao</surname></persName>
						</author>
						<title level="a" type="main">LCSCNet: Linear Compressing Based Skip-Connecting Network for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Single-image super-resolution</term>
					<term>deep convolu- tional neural networks</term>
					<term>skip connection</term>
					<term>feature fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we develop a concise but efficient network architecture called linear compressing based skipconnecting network (LCSCNet) for image super-resolution. Compared with two representative network architectures with skip connections, ResNet and DenseNet, a linear compressing layer is designed in LCSCNet for skip connection, which connects former feature maps and distinguishes them from newly-explored feature maps. In this way, the proposed LCSCNet enjoys the merits of the distinguish feature treatment of DenseNet and the parametereconomic form of ResNet. Moreover, to better exploit hierarchical information from both low and high levels of various receptive fields in deep models, inspired by gate units in LSTM, we also propose an adaptive element-wise fusion strategy with multisupervised training. Experimental results in comparison with state-of-the-art algorithms validate the effectiveness of LCSCNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>image textures. Neighbor embedding methods <ref type="bibr" target="#b9">[10]</ref> proposed by Chang et al. took advantage of similar local geometry between LR and HR to restore HR image patches. Inspired by the sparse signal recovery theory, researchers applied sparse coding methods <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b14">[15]</ref> to SR. Random forest <ref type="bibr" target="#b15">[16]</ref> has also been used to improve the reconstruction performance.</p><p>Recently, remarkable performance has been achieved for SR by deep models, especially deep network architectures, which are elaborated for high-level tasks in computer vision. Notably, residual network (ResNet) and densely connected network (DenseNet) are two widely-used architectures, which use skip connections to alleviate gradient problems and degradation phenomena in training. Chen et al. <ref type="bibr" target="#b16">[17]</ref> analyzed ResNet and DenseNet in the HORNN framework <ref type="bibr" target="#b17">[18]</ref> and concluded that ResNet enables feature re-usage while DenseNet enables feature exploration, both important to learn powerful representations.</p><p>Through extensive experiments, <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b19">[20]</ref> implied that ResNet shows an ensemble-like behavior within its structure. Yang et al. <ref type="bibr" target="#b20">[21]</ref> showed that ResNet applied in SR would lead to output with a layer-by-layer progressive effect, and Huang et al. <ref type="bibr" target="#b21">[22]</ref> argued that this might restrict ResNet from reaching more feasible solutions. Although DenseNet explores as many new features as possible by directly utilizing any former features, its excessive skip connections among intermediate layers increase the number of parameters and burden the hardware during training.</p><p>In this paper, we propose Linear Compressing Based Skip-Connecting Network (LCSCNet), as a framework for SR, which takes advantages of ResNet's parameter-economic feature reusage and DenseNet's distinguishing feature exploration, as well as mitigating difficulties of restricted structures of ResNet and parameter burden of DenseNet.</p><p>As the network depth grows, the features produced by different intermediate layers would be hierarchical with different receptive fields. Among deep SR models, DRCN <ref type="bibr" target="#b22">[23]</ref> and MemNet <ref type="bibr" target="#b23">[24]</ref> used these intermediate features with multisupervised methods, in which each feature corresponded to a raw SR output, and then fused these intermediate SR outputs by a list of trained scalars. Such a fusion strategy has two flaws: 1) once the weight scalars are determined in training, it will not change with different inputs; 2) using a single scalar to weight SR output fails to take pixel-wise differences into consideration, i.e., it would be better to weight different parts distinguishingly in an adaptive way. To overcome these shortcomings, inspired by the gate units in LSTM <ref type="bibr" target="#b24">[25]</ref>, we develop an adaptive element-wise fusion strategy in a progressive constructive way arXiv:1909.03573v1 [eess.IV] 9 Sep 2019 to maintain the element-wise convex weighted pattern, aiming at making better use of hierarchical information with different receptive fields.</p><p>In the end, we composite the Basic LCSCNet architecture with the adaptive element-wise fusion strategy gracefully for SR. Analysis and experiments in the following sections will illustrate the rationality of the proposed methods.</p><p>The main contributions of this work are three-fold: 1) We propose an accurate and efficient Linear Compressing Based Skip-Connecting Network (LCSCNet) architecture, which inherits the advantage of DenseNet in treating features of different levels distinguishingly while reducing its parameter size by exploiting the parameter-economic strength of ResNet. Moreover, we develop an Enhanced LCSCNet (E-LCSCNet) to further alleviate difficulties of training large-scale networks.</p><p>2) Differently from the traditional stationary fusion strategy, we take the input differences as well as the element-wise variation into consideration and propose an adaptive elementwise fusion strategy to further utilize hierarchical information.</p><p>3) When compared with the state-of-the-art models trained on the widely-used 291 dataset and those light networks trained on the DIV2K dataset, our proposed framework achieves the state-of-the-art performance. When compared with large models trained on DIV2K, our E-LCSCNet is among the state-of-theart with apparent parametric efficiency.</p><p>The rest of the paper is organized as follows. Section II reviews recent related work. Section III presents a detailed description of the proposed architecture, mainly on the configuration of Basic LCSCNet and the adaptive element-wise fusion algorithm. Section IV illustrates several intriguing properties of LCSCNet, which could explain the rationality of LCSCNet. Section V conducts ablation studies to further probe into the proposed framework. Section VI presents experimental results in comparison with other relevant methods. Section VII concludes the paper and envisages some future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Because our proposed methods include the Basic LCSCNet architecture and the adaptive element-wise fusion strategy, in this section we review related work mainly from the aspects of basic SISR reconstruction and sub-output fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basic SISR Reconstruction</head><p>Dong et al. pioneeringly proposed a three-layer superresolution convolutional neural networks (SRCNN) <ref type="bibr" target="#b26">[27]</ref>, predicting the end-to-end nonlinear mapping between LR and HR spaces. This first trial significantly outperformed other algorithms at that time. To combine the benefits of the natural sparsity of images and deep neural network architectures, Wang et al. proposed the Cascaded Sparse Coding Network (CSCN) <ref type="bibr" target="#b27">[28]</ref>, which had a higher visual quality than previous work. After SRCNN, Dong et al. further proposed FSRCNN <ref type="bibr" target="#b28">[29]</ref> improving SRCNN mainly by leveraging deconvolution layers, which reduced computation significantly by increasing the resolution only at the end of network. In the meantime, the Efficient Sub-Pixel Convolution Neural Network (ESPCN) <ref type="bibr" target="#b25">[26]</ref> was proposed by Shi et al., replacing the traditional deconvolution layer by an efficient sub-pixel convolution layer and further reducing computation.</p><p>Inspired by the success that very deep neural networks with sophisticated architectures and training strategies achieved in some high-level tasks in computer vision <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr">Kim</ref>   <ref type="bibr" target="#b36">[37]</ref> to progressively reconstruct the sub-band residuals of higherresolution images, which was especially effective for large scale factors. Motivated by explicitly mining persistent memory through an adaptive learning process and further mitigating the difficulties of training deeper networks, Tai et al. proposed an 80-layer network for image restoration, named as Persistent Memory Network (MemNet) <ref type="bibr" target="#b23">[24]</ref>.</p><p>Very recently, to further explore the power of example-based SISR with abundant training data, a new dataset DIV2K <ref type="bibr" target="#b37">[38]</ref> consisting of 800 2K resolution images was established. Based on this powerful dataset, many new architectures were proposed for performance improvement. Among them, by removing Batch-Normalization (BN) <ref type="bibr" target="#b38">[39]</ref> and applying residual scaling, Lim et al. proposed the Enhanced Deep Residual Network (EDSR) <ref type="bibr" target="#b39">[40]</ref>, which significantly improved performance. Then the Deep Back-Projection Network (DBPN) <ref type="bibr" target="#b40">[41]</ref> was proposed by Haris et al. to combine the merits of deep neural networks with the back-projection procedure, proven to be very effective for large scale factors. By making full use of local and global information from deep architectures, the Residual Dense Network (RDN) <ref type="bibr" target="#b41">[42]</ref> proposed by Zhang et al. exhibits comparable performance to EDSR, with fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sub-output Fusion</head><p>Features from different depths with different receptive fields specialize in different patterns in SISR. From the perspective of ensemble learning, a better result can be acquired by adaptively fusing the outputs from different-level features. Based on this concept, several fusion strategies were proposed. Among them, two representative weighted-summation methods were the vectorized weighted fusion strategy <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and MSCN <ref type="bibr" target="#b42">[43]</ref>. In the vectorized weighted fusion, a trainable positive vector For models trained on the 291 dataset, this part is the traditional deconv layer consisting of "nearst-neighborhood upsampling + conv-ReLU + conv-ReLU + conv"; for models trained on the DIV2K dataset, we use ESPCN <ref type="bibr" target="#b25">[26]</ref> instead. To be specific, we only use ESPCN as U&amp;RNet in Section V-E and the E-LCSCNet in <ref type="table" target="#tab_2">Table VIII.</ref> whose 1 norm is 1 is applied, and each element in this vector controls how much of the current sub-output contributes to the final one. To regularize each sub-output and stabilize training, multi-supervised training is adopted. In MSCN, an extra CNN module takes LR as input and outputs several tensors with the same shape as the HR. These tensors can be viewed as adaptive element-wise weights for raw HR outputs. Then the weight module and the basic SISR module are trained jointly by optimizing the fused results in an end-to-end manner. Both of the two fusing strategies above have shortcomings. The vectorized approach does not take the diversity of input and pixel-wise differences into consideration, while in MSCN the summation of coefficients at each pixel is not normalized, which is incongruous. Therefore, in this paper we aim to propose a normalized adaptive element-wise fusion strategy to overcome the shortcomings of the two previous fusion methods.</p><p>The above-mentioned deep methods mainly minimized the mean squared error (MSE), which tended to be blurry, oversmoothing and perceptually unsatisfying, especially in the case of large scale factors. Recently, some inspiring deep learningbased works concentrated on the exploration of more effective loss functions for SR. In <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b44">[45]</ref>, the perceptual loss using high-level feature maps of VGG made HR outputs more visually pleasing; <ref type="bibr" target="#b45">[46]</ref> introduced amortized MAP inference to the loss function to get more plausible results; <ref type="bibr" target="#b46">[47]</ref> and <ref type="bibr" target="#b47">[48]</ref> used the adversarial loss to produce photo-realistic HR outputs. Although these methods produced high-quality images with rich texture details, the details in their outputs may be quite different from original images. As we mainly aim to develop efficient deep models with fewer pixel-wise errors, our work does not belong to this group. Readers can refer to <ref type="bibr" target="#b48">[49]</ref> for an elaborated survey on deep learning based SISR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LINEAR COMPRESSING BASED SKIP-CONNECTING NETWORK (LCSCNET AND E-LCSCNET)</head><p>Our work has two main technical contributions: an (enhanced) linear compressing based skip-connecting structure for developing extremely deep efficient neural networks, and an adaptive fusion strategy for further utilizing intermediate features. In order to better clarify the contribution and function for each of them, here we briefly specify four architectures used in later discussions and ablation studies:</p><p>Basic LCSCNet: as shown in <ref type="figure" target="#fig_0">Fig.1</ref>(a) (without red-line parts), it firstly extracts features and then sends them to a series of LCSCBlocks, and the final results are obtained from the upsampling and reconstruction part;</p><p>Basic E-LCSCNet: quite similar to Basic LCSCNet except for the replacement of LCSCBlock by E-LCSCBlock ( <ref type="figure" target="#fig_0">Fig.1(a)</ref>) and the extra additive skip connections with initial features;</p><p>LCSCNet and E-LCSCNet: applying the proposed adaptive fusion strategy to Basic LCSCNet and Basic E-LCSCNet respectively, as shown in <ref type="figure" target="#fig_0">Fig.1(b)</ref>.</p><p>Because the structure of the above two basic networks are quite simple and we mainly use LCSCNet (E-LCSCNet) to compare with other state-of-the-art works, we will focus on the detailed descriptions on LCSCNet (E-LCSCNet).</p><p>As shown in <ref type="figure" target="#fig_0">Fig.1(b)</ref>, our LCSCNet and E-LCSCNet both mainly consist of four parts: 1) a preliminary feature extraction net (PFENet), 2) linear compressing based skip-connecting blocks (LCSCBlocks) or enhanced linear compressing based skip-connecting blocks (E-LCSCBlocks) for deep feature exploration, 3) a upsampling and reconstruction net (U&amp;RNet), and 4) an adaptive element-wise fusion of all intermediate outputs. Many previous works <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref> learned the residue between HR and its bicubic interpolation and argued that this helps stabilize training and improves performance. When we compare LCSCNet with these works, as shown in <ref type="figure" target="#fig_0">Fig.1</ref>, the input I in is LR, and the output I out is the residue. Meanwhile, many recent works <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref> just learned the mapping between LR and HR. When we compare E-LCSCNet with these methods, we also follow this routine for fairness.</p><p>Our PFENet uses a single 3 × 3 convolution layer to conduct preliminary feature extraction:</p><formula xml:id="formula_0">F 0 = f P F E (I in ),<label>(1)</label></formula><p>where F 0 denotes extracted features from the LR input.  <ref type="figure" target="#fig_1">Fig.2</ref>, where LU i,j denotes the j-th unit in the i-th LCSCBlock, Y in denotes the input feature maps of this unit and Y out denotes the output feature maps, both maps with n channels. In <ref type="figure" target="#fig_1">Fig.2</ref>, the upper convolution operator named as linear compressing (LC) layer is of size 1 × 1 with n 1 output channels. We denote the LC layer in LU i,j as K L i,j . Motivated by <ref type="bibr" target="#b34">[35]</ref>, the nonlinear operator in the lower part of <ref type="figure" target="#fig_1">Fig.2</ref> consists of two parts: ReLU and the convolution operator denoted as K N L i,j of size 3×3 with n 2 output channels. Here the superscripts L and N L denote the convolution kernels for linear and nonlinear transformations, respectively. Then the output of K L i,j and K N L i,j are concatenated to form a n-channel output feature maps. For simplicity, bias is omitted and convolution is replaced by matrix multiplication 1 , then the whole process of LCSCUnit can be formulated as</p><formula xml:id="formula_1">Y out = concat K L i,j Y in , K N L i,j ReLU (Y in ) .<label>(2)</label></formula><p>Furthermore, features and convolution kernels in LCSCUnits can be separated by their properties. As for features, Y out can be divided into n 1 -channel Y L out and n 2 -channel Y N L out , where superscripts L and N L in features denote features produced by linear and nonlinear operations, respectively. For convolution kernels, K L i,j can be divided according to the output channel into K L,L i,j and K L,N L i,j , where superscript L,L means the part of the linear-transforming kernel K L i,j operating on Y L in and L,N L means the part operating on Y N L in . Notably, although the LC layer with 1 × 1 convolution resembles the bottleneck layer that is widely used to reduce dimensions of feature maps <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b49">[50]</ref>, the main difference between them is that the bottleneck layer is placed before the nonlinear operator in a cascading manner, while the LC layer parallels the nonlinear operator.</p><p>Skip connections in a neural network structure create short paths from early layers to latter layers, which are considered as an effective way to ease the difficulties in training deep neural networks. In all LCSCNet, we implement skip connections mainly by the LC layer in each basic unit. In LCSCUnit, there is a parameter which controls the proportion of the number of linear output channel n 1 and the nonlinear output channel n 2 . This parameter, which can affect the performance of the network, is defined as</p><formula xml:id="formula_2">ρ = n 2 n 1 + n 2 .<label>(3)</label></formula><p>We find that a fixed ρ for each LCSCUnit throughout the network can already offer a quite good performance. Alternatively, we can set up LCSCUnits with different ρ, and the LCSCUnits with the same ρ are connected consecutively and can be divided into different LCSCBlocks. For simplicity, we let each LCSCBlock contain the same number of LCSCUnit. Suppose there are N LCSCBlocks stacked to explore deep features, and M LCSCUnits in an LCSCBlock. Let LB ρ d d denote the d-th LCSCBlock with specific ρ d , F d−1 denote its input features and F d its output features. The mapping of LCSCUnits in this block are denoted by {LU d,1 (·), LU d,2 (·), . . . , LU d,M (·)}, then the whole process of this block can be formulated as</p><formula xml:id="formula_3">F d = LB ρ d d (F d−1 ) = LU d,M (LU d,M−1 (· · · (LU d,1 (F d−1 )) · · · )),<label>(4)</label></formula><p>and it follows that</p><formula xml:id="formula_4">F N = LB ρ N N (LB ρ N −1 N −1 (· · · (LB ρ1 1 (F 0 )) · · · )).<label>(5)</label></formula><p>Furthermore, we investigate how the ordinal position of blocks with different ρ effects the final performance. Detailed discussions and relative comparative experiments will be demonstrated in Section V-C.</p><p>2) E-LCSCBlock: As mentioned in <ref type="bibr" target="#b39">[40]</ref>, the simplest way to enhance performance via increasing the number of parameters is to increase the width of deep architectures. However, a deep wide network is extremely hard to train. Inspired by the longterm memory connection in <ref type="bibr" target="#b23">[24]</ref>, we find that if we further concatenate the input and the output of LCSCBlock and then use a 1 × 1 bottleneck layer to maintain the compactness of the output channel, it will alleviate the difficulty of training a large LCSCNet. We denote the LCSCBlock with such a long-term memory connection as E-LCSCBlock. Compared with (4), the mapping of E-LCSCBlock can be written as</p><formula xml:id="formula_5">ELB ρ d d (F d−1 ) = bottle(concat(F d−1 , LB ρ d d (F d−1 ))),<label>(6)</label></formula><p>where ELB ρ d d denotes the d-th E-LCSCBlock with specific ρ d , and bottle(·) denotes the 1 × 1 bottleneck layer. Moreover, we find that deep models of moderate scales using E-LCSCBlocks also perform slightly better than the ones using LCSCBlocks. Further discussions and ablation studies on E-LCSCBlock will be presented in Section V-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Upsampling and Reconstruction Net (U&amp;RNet)</head><p>Sajjadi et al. <ref type="bibr" target="#b47">[48]</ref> reported that adding convolution layers after the nearest-neighbor upsamping layer can help alleviate artifacts in SR. We follow this way in our models trained on the 291 dataset using the nearest-neighbor upsampling layer followed by three 3×3 convolution kernels (except the last one) with ReLU. When we develop models aiming to compare with models trained on DIV2K, we use ESPCN as the U&amp;RNet, as EDSR and RDN did, for fair comparison.</p><p>In LCSCNet, the deep features {F 1 , F 2 , . . . , F N }, explored hierarchically in its second part by LCSCBlocks, are then sent to U&amp;RNet U R(·), which maps feature F d to output Y d :</p><formula xml:id="formula_6">Y d = U R(F d ), 1 ≤ d ≤ N.<label>(7)</label></formula><p>In E-LCSCNet, like EDSR and RDN, even without directly learning the residue between the HR and its bicubic version, the global residual learning is implemented by adding initial features F 0 to F d before upsampling. That is, the U&amp;RNet EU R(·) in E-LCSCNet has input and output as</p><formula xml:id="formula_7">Y d = EU R(F d + F 0 ), 1 ≤ d ≤ N.<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adaptive Element-wise Fusion Strategy</head><p>Feature maps of different receptive fields are sensitive to features of different sizes, which are often fused to enhance the performance in various computer vision tasks. In our case, we develop an adaptive element-wise fusion strategy.</p><formula xml:id="formula_8">With N intermediate results {Y 1 , Y 2 , . . . , Y N } mapped from {F 1 , F 2 , . . . , F N } through U&amp;RNet,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a list of weight tensors</head><p>Algorithm 1 Adaptive Element-wise Fusion Strategy.</p><formula xml:id="formula_9">Input: Intermediate outputs {Y 1 , Y 2 , . . . , Y N }. Output: The final fused feature maps M . 1: Initialize M with Y 1 : M = Y 1 ; 2: for each i ∈ [1, N − 1] do 3: Concatenate SR output X = concat(M, Y i+1 ) 4:</formula><p>Convolve with 1×1 tensor: </p><formula xml:id="formula_10">α i = C i X, C i is the i-th 1×1 tensor 5: Use sigmoid activation: α i = sigmoid(α i ) 6: Update M = α i M + (I − α i )Y i+1 7: end for 8: return M {W 1 , W 2 , . . . , W N } with the same size of output are deter- mined by {Y 1 , Y 2 , . . . , Y N },</formula><formula xml:id="formula_11">W i = f i (Y 1 , Y 2 , . . . , Y N ), i = 1, 2, . . . , N,<label>(9)</label></formula><p>where f i is the mapping from</p><formula xml:id="formula_12">{Y 1 , Y 2 , . . . , Y N } to W i ; Trait 2:</formula><p>The value of each point in the weight tensor is between 0 and 1, and</p><formula xml:id="formula_13">N i=1 W i = I,<label>(10)</label></formula><p>where I is the tensor with all elements being 1.</p><p>The final fused output M is a convex weighted average of intermediate outputs {Y 1 , Y 2 , . . . , Y N }:</p><formula xml:id="formula_14">I out = M = N d=1 W d Y d .<label>(11)</label></formula><p>Inspired by the gate unit in LSTM, by adopting a series of 1 × 1 convolution kernels followed by sigmoid activation functions, we develop a heuristic algorithm to construct the fused output M , in which weight tensors satisfy the above two traits, as summarized in Algorithm 1. A sketch for Algorithm 1 is plotted in <ref type="figure" target="#fig_2">Fig.3</ref>, in which intermediate variable tensor</p><formula xml:id="formula_15">α i (i = 1, . . . , N − 1)</formula><p>is generated progressively given current SR outputs {Y 1 , . . . , Y i }, 1×1 convolution kernel C i and sigmoid activation function. The use of sigmoid activation functions ensures the element-wise value of α i to be between 0 and 1. The updating step (Step 6) ensures the output to be a convex weighted average of current inputs {Y 1 , . . . , Y i }. From Algorithm 1, {W 1 , W 2 , . . . , W N } can be obtained as</p><formula xml:id="formula_16">W k =                N −1 i=1 α i , k = 1; I − α k−1 N −1 i=k α i , 2 ≤ k &lt; N ; I − α N −1 , k = N ,<label>(12)</label></formula><p>where α N −1 contains the information of {Y 1 , Y 2 , . . . , Y N }. As <ref type="bibr" target="#b11">(12)</ref> shows that every W k contains α N −1 , the first trait in <ref type="formula" target="#formula_11">(9)</ref> is satisfied; with simple algebra, the second trait in (10) is also verified, and hence the rationality of the proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Function for Training</head><p>During training, we minimize the 1 loss L 1 (x, y) = |x − y| over the training set of M samples. Let X (i) denote the i-th ground-truth HR label in the training set and I (i) out denote the corresponding output of network. Then the loss function l is</p><formula xml:id="formula_17">l(I out , X) = 1 M M i=1 L 1 (I (i) out , X (i) ).<label>(13)</label></formula><p>When we apply the adaptive element-wise fusion strategy, we use the multi-supervised methods mentioned in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> to train our model. The loss function of multi-supervised LCSCNet can be formulated as</p><formula xml:id="formula_18">L(Θ) = l(I out , X) + β N d=1 l(Y d , X),<label>(14)</label></formula><p>where I out and {Y 1 , . . . , Y N } are defined in <ref type="bibr" target="#b10">(11)</ref>, and β is a trade-off parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSIONS</head><p>In this section, we mainly discuss the motivation and characteristics of Basic LCSCNet by showing its connections to DenseNet and its differences from ResNet and DenseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basic LCSCNet as an Efficient Variant of DenseNet</head><p>In this sub-section, we illustrate that Basic LCSCNet can be transformed from DenseNet with small changes on topology: we first show the redundancy in DenseNet and then introduce Basic LCSCNet as a remedy for this redundancy.</p><p>Skip connection in DenseNet is implemented by directly concatenating all former features to be the input of current layer. For illustration, a 4-layer DenseBlock is depicted in <ref type="figure" target="#fig_3">Fig.4(a)</ref>, in which Y 0 is a k-channel input feature, Y i is the newly-explored feature after nonlinear mapping f i (i = 1, 2, 3), where f i consists of a ReLU followed by a 3×3 convolution kernel with k 0 output channels (k 0 is also called growth rate in DenseNet). The last nonlinear mapping f 4 acts as a transition layer; C means a concatenation operator in the channel dimension.</p><p>To have a better understanding of DenseBlock in <ref type="figure" target="#fig_3">Fig.4(a)</ref>, we can simplify <ref type="figure" target="#fig_3">Fig.4(a)</ref> into its equivalent form in <ref type="figure" target="#fig_3">Fig.4(b)</ref>, where Y i = concat(Y 0 , . . . , Y i ) (i = 0, 1, 2, 3). By denoting the concatenation of former features as Y i , excessive skip connections in <ref type="figure" target="#fig_3">Fig.4(a)</ref> are simplified into concise adjacent skip connections. For simplicity, unless otherwise specified, we take the DenseBlock in the form of <ref type="figure" target="#fig_3">Fig.4(b)</ref> as the basic DenseBlock structure.</p><p>As shown in <ref type="figure" target="#fig_3">Fig.4(a)</ref> and <ref type="figure" target="#fig_3">Fig.4(b)</ref>, when depth increases, the number of parameters of the convolution kernel in DenseNet also increases. To reduce the parameter amount, the authors of DenseNet applied the bottleneck layer 2 before every nonlinear mapping and called it B-DenseNet. <ref type="figure" target="#fig_3">Fig.4(c)</ref> is the bottleneck version of <ref type="figure" target="#fig_3">Fig.4(b)</ref>, where B i (i = 1, 2, 3) is the 1 × 1 convolution kernel with b output channels. We can see that the parameter amount of every convolution kernel f i for nonlinear mapping is a constant now, and only the parameter amount of bottleneck layer with fewer parameters increases with depth.</p><p>Although B-DenseBlock has reduced the parameter amount to a great extent, the parameter amount of each basic unit in B-DenseBlock still increases with depth. To further control the parameter amount, we make the parameter amount of each unit in B-DenseBlock a constant. One simple but effective solution is to move forward the bottleneck layer in each unit, reducing the number of channels of input feature to b by the bottleneck layer before they are sent to the concatenation part, as shown in <ref type="figure" target="#fig_3">Fig.4(d)</ref>. We can set k = b + k 0 to make channels of each feature unchanged. In this case, if we re-depict <ref type="figure" target="#fig_3">Fig.4(d)</ref> by allocating the bottleneck layer to each branch and using a nonlinear mapping f i to replace B i • f i , the structure of Basic LCSCNet reemerges, as shown in <ref type="figure" target="#fig_3">Fig.4(e)</ref>.</p><p>From the analysis above we can see that the N -layer Basic LCSCNet with an extra transition layer and the (N + 1)-layer B-DenseNet share a strong relationship. This transition layer can be replaced by subsequent nonlinear operators and omitted. If it is replaced by a compressing layer located at the end of DenseBlock, it becomes BC-DenseNet. Since this compressing layer is to compress features generated by each block, when we simplify B-DenseUnit into LCSCUnit, the output channel of each LCSCUnit is already a constant, it is unnecessary to compress features again. From this perspective, BC-DenseNet can also be transferred to Basic LCSCNet in a similar way. Now look back into <ref type="figure" target="#fig_1">Fig.2</ref>: the nonlinear output channel is just the growth rate in DenseNet, denoting how many new features are explored, and 1 − ρ = n1 n1+n2 acts as some kind "compress ratio" denoting how many former features have flowed to the current stage through skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Differences from ResNet and DenseNet</head><p>This sub-section aims to illustrate the differences between Basic LCSCNet and ResNet/DenseNet as well as the novelty of our proposed network. It is still an open problem to compare different deep architectures. When different ways of skip connections are employed to alleviate training difficulties, the output features explored by nonlinear mapping with different skip connections have different constitutions. We suppose that by comparing different constitutions of the feature maps, we could get some useful information about the properties of different skip-connection architectures.</p><p>1) Feature maps of ResNet: We use the structure in <ref type="bibr" target="#b34">[35]</ref>. Let Y k and Y k+1 denote the input and output of block k, respectively, and let f R k (·) denote the nonlinear transformation in block k. Then the mathematical formulation of block k is</p><formula xml:id="formula_19">Y k+1 = Y k + f R k (Y k ) = Y j + k i=j f R i (Y i ), 1 ≤ j ≤ k.<label>(15)</label></formula><p>From <ref type="formula" target="#formula_0">(15)</ref>, we can see that in ResNet, skip connection is implemented by element-wise summation between adjacent features. Compared with traditional plain architecture, any former maps Y j (j = 1, . . . , k) can be added to the current state Y k+1 , creating many short paths for more "smooth" gradient flow during back-propagation. Moreover, it is extremely concise because no extra parameter is required for this skip connection.</p><p>2) Feature maps of DenseNet: We employ the structure shown in <ref type="figure" target="#fig_3">Fig.4(b)</ref> to illustrate the properties of feature maps in DenseNet. Let Y k and Y k+1 denote the input and output of unit k in a DenseBlock, respectively, and f D k (·) the nonlinear transformation. Then the formulation of unit k is</p><formula xml:id="formula_20">Y k+1 = concat(Y k , f D k (Y k )),<label>(16)</label></formula><p>and it follows that</p><formula xml:id="formula_21">Y k+1 = concat(Y j , f D j (Y j ), . . . , f D k (Y k )), 1 ≤ j ≤ k.<label>(17)</label></formula><p>Like ResNet, all the former features in DenseNet can be fused into the current stage, but instead of summation, all the feature maps are concatenated in the channel dimension. Such a skip connection has both advantages and disadvantages compared with ResNet. One obvious advantage is that when features produced in DenseNet are sent to follow-up convolution kernels to explore new features, the features from different stages use different convolution kernels, while in ResNet the reused parts and newly-explored ones share the same convolution kernel. From this perspective, connecting features by elementwise summation may restrict a network from reaching better solutions in some cases. As for disadvantage, concatenating features need more following convolution kernels. As shown in <ref type="figure" target="#fig_3">Fig.4(a)</ref> and <ref type="figure" target="#fig_3">Fig.4(b)</ref>, the parameter amount of DenseUnit increases with depth. When a DenseNet is very deep, even a small growth rate may lead to a large parameter amount.</p><p>3) Feature maps of Basic LCSCNet: From the analysis above, we can conclude that the feature re-usage of ResNet benefits from its concise skip connection between adjacent basic blocks and the new feature exploration of DenseNet mainly benefits from its little relevance between newly-explored feature maps and former ones. We have already seen that in Basic LCSCNet, former features are firstly compressed and then concatenated with the newly-explored features. Now we examine how the former features are combined in the current stage. Let Y k and Y k+1 denote the input and output of the k-th LCSCUnit, and K L k and K N L k its convolution kernels. From <ref type="figure" target="#fig_1">Fig.2 and (2)</ref>, we can derive the formulation of 1 × 1 convolution in the LC layer as</p><formula xml:id="formula_22">Y L k+1 (c o ) = n ci=1 K L k (c o , c i )Y k (c i ) = n1 ci=1 K L k (c o , c i )Y L k (c i ) + n ci=n1+1 K L k (c o , c i )Y N L k (c i − n 1 ) = n1 ci=1 K L,L k (c o , c i )Y L k (c i ) + n2 ci=1 K L,N L k (c o , c i )Y N L k (c i ),<label>(18)</label></formula><p>where c i denotes the input channel and c o the output channel. For simplicity, <ref type="bibr" target="#b17">(18)</ref> can be rewritten as</p><formula xml:id="formula_23">Y L k+1 = K L,L k Y L k + K L,N L k Y N L k .<label>(19)</label></formula><p>Applying the same approach to the convolution kernel K N L k in nonlinear transformation, we have <ref type="bibr" target="#b19">(20)</ref> where A 'global' form of <ref type="bibr" target="#b18">(19)</ref> is</p><formula xml:id="formula_24">Y N L k+1 = K N L,L k ReLU (Y L k ) + K N L,N L k ReLU (Y N L k ),</formula><formula xml:id="formula_25">Y L k+1 = P L k+1 + P N L k+1 ,<label>(21)</label></formula><formula xml:id="formula_26">P L k+1 = ( k i=1 K L,L i )Y L 1 ,<label>(22)</label></formula><formula xml:id="formula_27">P N L k+1 = k−1 i=1 (K L,N L i k j=i+1 K L,L j )Y N L i + K L,N L k Y N L k .<label>(23)</label></formula><p>From <ref type="formula" target="#formula_0">(19)</ref>, we can see that Y L k+1 restores the information of all former feature maps in the form of weighted summation. From <ref type="bibr" target="#b19">(20)</ref>, we can see that Y N L k+1 is the new features explored by new nonlinear transformation. Among the deep features produced by deep architectures, newly-explored parts are thought to be more important. In Basic LCSCNet, we concatenate newly-explored features with the former ones like DenseNet, ensuring that features of different kinds can be treated differently. Meanwhile, as former features in the current stage are mainly aimed to create paths for training deep networks, instead of concatenating each former features separately, we compress all the former features and then concatenate them with the newly-explored ones, making it quite parameter-economic like ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATION STUDIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison with ResNet and DenseNet</head><p>In this sub-section, we replace LCSCUnit in our basic LCSCNet by ResBlock or DenseUnit with the bottleneck layer. The three networks for comparison are all 34-layer, where Basic LCSCNet and DenseNet both have 30 units while ResNet has 15 blocks. As discussed before, the growth rate in DenseNet plays a similar role to the channel number of the nonlinear output. To compare fairly, if we set all output feature channels to 64 and ρ of every LCSCUnit to 0.5, then the growth rate of DenseNet is 32 and the output channel of the bottleneck is 64. As for BC-DenseNet, for example, BC-Dense B3 U10 means dividing the network into 3 blocks uniformly and add a compressing layer at the end of each block, whose output channel is 64. Here we train the above three models with the 291 dataset for ×3 scale and the results are shown in <ref type="table" target="#tab_2">Table I</ref>. We use PSNR/SSIM <ref type="bibr" target="#b50">[51]</ref> to measure reconstruction, and parameter amounts to measure storage efficiency. We can see that Basic LCSCNet has the least parameters and the competitive performance to DenseNet both better than ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Efficiency Brought by the LC layer</head><p>Here we discuss the rationale behind implementing the LC layer with 1 × 1 convolution and its advantage on parameter efficiency. It is known that increasing receptive fields is essential for exploring deeper features. From Section IV we can see that the LC layer helps transport the previous features and does not produce newly-explored features directly. Hence, we do not need to use 3 × 3 convolution to increase receptive fields in the LC layer and 1 × 1 convolution is sufficient. To support  <ref type="table" target="#tab_2">Table II</ref>, we can see that the LC layer with 3 × 3 convolution indeed does not achieve better performance. The usage of 1 × 1 convolution as the LC layer also makes the proposed architecture more parameter-economic compared with ResNet and DenseNet. Firstly we compare Basic LCSCNet with ResNet. A basic unit in ResNet with n 1 input channels, n 2 output channels and a k × k nonlinear transformation convolution kernel has n 1 n 2 k 2 parameters. The number of parameters of a basic unit in Basic LCSCNet, with n 1 input channels, n 2 output channels, a k × k nonlinear transformation convolution kernel and parameter ρ 0 , is n 1 n 2 (k 2 ρ 0 + 1 − ρ 0 ). The ratio of parameter amounts of these two units with the same n 1 , n 2 and k is</p><formula xml:id="formula_28">p L/R (n 1 , n 2 , k) = ρ 0 + 1 k 2 (1 − ρ 0 ).<label>(24)</label></formula><p>As illustrated before, good performance can be obtained when ρ 0 is around 0.5. In practice, the size of a convolution kernel for feature extraction is usually an odd bigger than 3. So when ρ 0 is 0.5, p L/R (n 1 , n 2 , k) &lt; 55.7%, which means the parameter amount of Basic LCSCNet is just half of the ResNet's. As for DenseNet, the parameter amount of a basic unit increases with depth. We take B-DenseNet as example: if the output channel of nonlinear mapping in LCSCUnit and DenseUnit is both n 2 , the output channel of 1 × 1 compressing layer is both n 1 , the nonlinear kernel size is k × k, then the parameter amount of LCSCUnit is always (n 1 + n 2 )(k 2 n 2 + n 1 ), while the parameter amount of the p-th DenseUnit is (pn 2 n 1 + k 2 n 1 n 2 ). If such Basic LCSCNet and DenseNet both have L nonlinear mapping layers, the ratio of parameter amounts of the two networks with the same n 1 , n 2 and k is</p><formula xml:id="formula_29">p L/D (L; n 1 , n 2 , k) = 2 2k 2 + L + 1 ( 1 ρ 0 + k 2 1 1 − ρ 0 ).<label>(25)</label></formula><p>From <ref type="formula" target="#formula_1">(25)</ref>, we can see the advantage of Basic LCSCNet is more remarkable when the network goes deeper. When we compare Basic LCSCNet with an L-layer BC-DenseNet of N blocks, if the transition layer is omitted for simplicity, the ratio can be obtained by replacing L with L N in <ref type="bibr" target="#b24">(25)</ref>.</p><p>C. Investigation into Parameter ρ 1) Fixed ρ throughout the network: In this situation, we find when ρ is around 0.5, the best performance could be achieved. <ref type="table" target="#tab_2">Table III</ref> shows 34-layer Basic LCSCNets for ×3 scale with different fixed ρ. As here we mainly focus on the effect of ρ, the experiments are conducted without adaptive element-wise fusion. Firstly, we consider two special cases of ρ. When ρ is 0, the feature exploration part is a linear transformation; if the upsampling and reconstruction part is taken into account, the whole network has just two nonlinear convolution layers, whose fitting capacity for complex functions is relatively poor. In contrast, when ρ is 1, Basic LCSCNet becomes the traditional feedforward neural network without skip connections, which is difficult to train. Hence ρ balances the fitting capacity and the training ease of Basic LCSCNet. As <ref type="table" target="#tab_2">Table III</ref> shows, when ρ = 0.25, the performance is suboptimal because of the restricted fitting capacity; when ρ = 0.75, the performance is suboptimal mainly because the LC layers output fewer feature maps. As we discussed before, the output feature maps of LC layers restore the information of former features, insufficiency of which leads to insufficient skip connections and thus training difficulty increase and performance decline.</p><p>2) Different ρ throughout the network: In this situation, the LCSCUnits with the same ρ form an LCSCBlock and different LCSCBlocks have different ρs. We find that as depth increases, the ρ of an LCSCBlock should be decreased slightly to improve performance. <ref type="table" target="#tab_2">Table IV</ref> shows the relevant experimental results on the 34-layer Basic LCSCNets with different ordinal positions of ρ list for ×3 scale. One possible reason for this phenomenon is that as depth increases, exploring higher-level features becomes harder, so there is less room for newly-explored features. Meanwhile, as information on former feature maps accumulates, more room is needed for reusing former features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies on Different Fusion Strategies</head><p>Table V compares properties of the vectorized fusion <ref type="bibr" target="#b22">[23]</ref>, MSCN <ref type="bibr" target="#b42">[43]</ref> and our proposed fusion method. We can see that our method incorporates the advantages of the vectorized fusion and MSCN. We also compare these fusion strategies quantitatively. We train 34-layer LCSCNets for ×2 scale with ρ list [0.75, 0.6875, 0.625, 0.5625, 0.5], and every six LCSCUnits with the same ρ form a LCSCBlock. In <ref type="table" target="#tab_2">Table VI</ref>, Basic LCSCNet, LCSCNet S, LCSCNet M and LCSCNet denote the LCSCNet without any fusion, with vectorized fusion, with MSCN and with our proposed method, respectively. Here we note that our implementation of MSCN is slightly different from the original one. In the original MSCN, the input of the weight module is the bicubic of LR, while in our LCSCNet we use the upsampled LR input. This small difference should have little influence on final results. As <ref type="table" target="#tab_2">Table VI</ref> shows, when combined with Basic LCSCNet, our fusion strategy performs better than the other two fusion benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Studies on LCSCBlock and E-LCSCBlock</head><p>Firstly, we show that when we use LCSCUnits to construct deep models of moderate scales, the advantage of E-LCSCBlock is mild. With the 291 dataset, we train 34-layer, 44layer and 54-layer Basic LCSCNets for ×3 scale, ρ of each unit is 0.5, and the number of feature channels is 64. For comparison, we use 10 LCSCUnits to constitute an E-LCSCBlock and train 37-layer, 48-layer and 59-layer Basic E-LCSCNets, respectively. As <ref type="table" target="#tab_2">Table VII</ref> shows, every Basic E-LCSCNet performs better than its corresponding Basic LCSCNet.     Then we show that when we aim to develop an extremely deep and wide network, E-LCSCBlock can make up the deficiencies of LCSCBlock. With the DIV2K dataset we train a Basic LCSCNet for ×2 scale of ρ list [0.75, 0.71875, 0.6875, 0.65625, 0.625, 0.59375, 0.5625, 0.53125, 0.5], every sixteen LCSCUnits with the same ρ form a LCSCBlock, and the output channel of each feature is 128. Its convergence curve is the blue one in <ref type="figure" target="#fig_5">Fig.5</ref>, indicating quite poor performance. For comparison, we train the LCSCNet with the same setting, and its performance (the green curve in <ref type="figure" target="#fig_5">Fig.5</ref>) is significantly better than Basic LCSCNet. We contribute this obvious improvement to the extra short paths created by the adaptive fusion strategy, which suggests that more short paths may further help in this case. The experimental results shown in <ref type="figure" target="#fig_5">Fig.5</ref> also support this view: when we evolve (Basic) LCSCNet into (Basic) E-LCSCNet, the performance of deep architecture booms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison with State-of-the-Art Models</head><p>It is well known that the training set and the parameter amount largely influence the final performance of a model. To compare with various representative models fairly, we divide these models into three categories: models trained on the 291 dataset <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b51">[52]</ref>, light models (Params &lt; 2M) trained on the DIV2K dataset <ref type="bibr" target="#b37">[38]</ref> and large models (Params &gt; 10M) on DIV2K. When compared with models on the 291 dataset such as VDSR <ref type="bibr" target="#b30">[31]</ref>, DRCN <ref type="bibr" target="#b22">[23]</ref>, LapSR <ref type="bibr" target="#b36">[37]</ref>, DRRN <ref type="bibr" target="#b33">[34]</ref> and MemNet <ref type="bibr" target="#b23">[24]</ref>, we train a 76-layer LCSCNet with the proposed fusion strategy, ρ list is also [0.75, 0.71875, 0.6875, 0.65625, 0.625, 0.59375, 0.5625, 0.53125, 0.5] but every eight units with the same ρ form a block, denoted by LCSC 76 291. When compared with light models on DIV2K and similarly large datasets such as SelNet <ref type="bibr" target="#b52">[53]</ref>, SRDenseNet <ref type="bibr" target="#b35">[36]</ref>, CARN <ref type="bibr" target="#b53">[54]</ref> and FALSR-A <ref type="bibr" target="#b54">[55]</ref>, because the fusion part is quite computationconsuming, our light models was developed just based on Basic E-LCSCNet. Our light models share the same ρ list with LCSC 76 291, but every six units with the same ρ form a block, denoted by BE-LCSC L. When compared with large models on DIV2K such as EDSR <ref type="bibr" target="#b39">[40]</ref> and RDN <ref type="bibr" target="#b41">[42]</ref>, the E-LCSCNet mentioned in Section V-E is adopted.</p><p>Quantitative comparisons on BE-LCSC L are listed in <ref type="table" target="#tab_2">Table VIII</ref>. Because operations in neural networks for SISR are mainly multiplication along with addition, we use the number of composite multiply-accumulate operations in CARN, denoted by Mult&amp;Adds, to measure computational efficiency, and we also assume that the HR image is 1280 × 720. From <ref type="table" target="#tab_2">Table VIII</ref>, we can see that among the models trained on 291,  Representative qualitative comparisons are shown in Figs.6-8. In <ref type="figure">Fig.6</ref>, our model restores the grid structure more precisely with fewer artifacts than other models. In <ref type="figure">Fig.7</ref>, compared with blurry characters generated by other models, our result has sharper edges. In <ref type="figure" target="#fig_6">Fig.8</ref>, compared with EDSR and RDN, E-LCSCNet recovers the line with the least blurry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>For training LCSCNet, we augment data (90 • , 180 • and 270 • rotation), and then downsample the LR input with the desired scaling factor. Like many methods trained on 291, we only take the luminance component for training. Ground truth for training is the residue between the bicubic of LR image and the original HR image, and all inputs are scaled into [-1, 1]. When trained on 291, training images are split into patches of sizes 18 2 /36 2 , 12 2 /36 2 and 21 2 /84 2 , respectively. We initialize all the convolution kernels as suggested by <ref type="bibr" target="#b55">[56]</ref>. All intermediate feature maps have 64 channels. For optimization, we use Adam <ref type="bibr" target="#b56">[57]</ref> with its default settings. Learning rate is initialized as 10 −4 , and is divided by 10 every 15 epochs over the whole augmented dataset and the training is stopped after 60 epochs. For training, we use Keras <ref type="bibr" target="#b57">[58]</ref>; for testing, we use MatConvNet <ref type="bibr" target="#b58">[59]</ref>.</p><p>The training of BE-LCSC L and E-LCSCNet is based on the PyTorch <ref type="bibr" target="#b59">[60]</ref> version of EDSR with the same setting of EDSR except that the batch size is 32, and training is terminated after 650 epochs. The codes are available from https://github.com/XuechenZhang123/LCSC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORKS</head><p>In this paper, we propose the linear compressing based skipconnecting network (LCSCNet) for image SR, which combines the merits of the parameter-economic form of ResNet and the effective feature exploration of DenseNet. Linear compressing layers are adapted to implement skip connections, connecting former features and separating them from the newly-explored features. Compared with previous deep models with skip connections, our LCSCNet can explore relatively more new features with lower computational costs. Based on LCSCNet, to improve the performance of extremely deep and wide networks, the Enhanced LCSCNet is developed. An adaptive element-wise fusion strategy is also proposed, not only for further exploiting hierarchical information from diverse levels of deep models, but also for stabilizing the training deep models by adding extra paths for gradient flows. Comprehensive experiments and discussions are presented in this paper and demonstrate the rationality and superiority of the proposed methods.</p><p>Future work can be mainly explored from the following two aspects: 1) it would be worthwhile to try to apply LCSCNet and E-LCSCNet or their basic units to other computer vision tasks; and 2) in terms of Mult&amp;Adds in <ref type="table" target="#tab_2">Table VIII</ref>, we can see the computational cost for this part is still somewhat high despite that we have managed to control its complexity; therefore, further efforts can be made to further improve its efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Overall architecture of Basic LCSCNet (E-LCSCNet) (b) Overall architecture of LCSCNet (E-LCSCNet) The overall architectures of (a) Basic LCSCNet (E-LCSCNet) and (b) LCSCNet (E-LCSCNet). In (b), ⊗ means element-wise multiplication; {Y1, . . . , YN } are the intermediate HR outputs reconstructed from {F1, . . . , FN }. When E-LSCSNet is employed, red lined parts are activated. For fair comparison, the upsampling and reconstruction part of (Basic) LCSCNet varies with the training dataset:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The configuration of LCSCUnit. A. Configurations of LCSCUnit, LCSCBlock and E-LCSCBlock 1) LCSCUnit and LCSCBlock: The features extracted by the PFENet are then transmitted to the second part of overall network, which uses LCSCBlocks to explore complicated features progressively. An LCSCBlock comprises a fixed number of linear compressing based skip-connecting units (LCSCUnit) with the same configuration. The basic configuration of the LCSCUnit is depicted in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A sketch of the adaptive element-wise fusion strategy, where N = 4 and Mi (i = 1, 2, 3) are the current fused outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>DenseBlock with adjacent skip connection, equivalent to (a) (c) DenseBlock with bottleneck (B-DenseBlock) (d) Move forward every bottleneck layer (e) Equivalent form of (d), Basic LCSCNet Sketch on how a DenseBlock can be simplified into a Basic LCSCNet. For a better understanding, the channel number of each feature is marked beside the feature, and the kernel size of each convolution kernel is marked beside the kernel in form of "input chanenel × kernel width × kernel height × output channel".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Convergence comparison between deep wide (Basic)LCSCNet and (Basic) E-LCSCNet on the DIV2K validation set for scale ×2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Results of large models for upscaling factor 3 onUrban100-img019</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al. employed the VGG architecture and high learning rate with gradient clipping to stack a very deep (20 layers) convolutional neural network (VDSR) [31] and gained a remarkable improvement. Mao et al. proposed a deep fully convolutional auto-encoder network with symmetric skip connections [32]. To handle the issue of large numbers of parameters brought by very deep architectures, Kim et al. proposed the Deeply-Recursive Convolutional Network (DRCN) [23], which was also 20layer but with 16 recursions among its intermediate layers. To further exploit the advantages from deepening neural networks, motivated by the success of [33], Tai et al. proposed the Deep Recursive Residual Network (DRRN) [34], a 54-layer convolutional neural network for SR, in which they utilized the residual network architecture (ResNet) [35] in both global and local manners. Inspired by the Dense Connected Network (DenseNet) [22] proposed by Huang et al., Tong et al. introduced dense skip connections to their deep architecture [36]. Based on the correlations among the HR outputs with different scale factors and a heuristic methodology, Lai et al. proposed the Laplacian Pyramid Super-Resolution Network (LapSRN)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>which control how much of each raw result contributes to the final fused output. Here the adaptive weight tensors satisfy two traits: Trait 1: Each adaptive tensor is determined by all intermediate outputs together, which can be formulated as</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table I :</head><label>I</label><figDesc>Quantitative comparisons on ×3 SISR among the ResNet, B-DenseNet, BC-DenseNet and Basic LCSCNet of the same depth. Blue indicates the least parameters. Red indicates the best quantitative performance.</figDesc><table><row><cell>Model</cell><cell>Parameters</cell><cell>Set5</cell><cell>Set14</cell><cell>BSD100</cell><cell>Urban100</cell></row><row><cell>ResNet</cell><cell>118.1K</cell><cell cols="4">33.90/0.9233 29.84/0.8328 28.85/0.7987 27.12/0.8303</cell></row><row><cell>B-DenseNet</cell><cell>219.8K</cell><cell cols="4">33.98/0.9241 29.87/0.8338 28.87/0.7997 27.25/0.8326</cell></row><row><cell>BC-Dense B3 U10</cell><cell>102.7K</cell><cell cols="4">33.90/0.9234 29.90/0.8336 28.88/0.7991 27.22/0.8310</cell></row><row><cell>BC-Dense B5 U6</cell><cell>90.4K</cell><cell cols="4">33.92/0.9234 29.90/0.8334 28.87/0.7990 27.21/0.8307</cell></row><row><cell>Basic LCSCNet</cell><cell>68.9K</cell><cell cols="4">33.99/0.9241 29.87/0.8337 28.87/0.7994 27.24/0.8324</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II :</head><label>II</label><figDesc>Quantitative comparisons on ×3 SISR between the original Basic LCSCNet and the Basic LCSCNet with 3 × 3 LC layers. Red indicates the best quantitative performance. Basic LCSCNet 33.99/0.9241 29.87/0.8337 28.87/0.7994 27.24/0.8324 Basic LCSCNet of 3 × 3 LC 33.94/0.9238 29.88/0.8334 28.87/0.7989 27.17/0.8320 this view, we apply 3 × 3 convolution to the LC layer of the Basic LCSCNet mentioned in Section V-A. From</figDesc><table><row><cell>Model</cell><cell>Set5</cell><cell>Set14</cell><cell>BSD100</cell><cell>Urban100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III :</head><label>III</label><figDesc>Average ×3 PSNR/SSIM for Basic LCSCNets with different ρ on the Set5, Set14, BSD100 and Urban100 datasets, respectively. Red color indicates the best performance. Set5 32.66/0.9103 33.86/0.9229 33.97/0.9242 33.99/0.9241 33.94/0.9241 33.92/0.9237 31.78/0.8941 Set14 29.27/0.8208 29.82/0.8330 29.90/0.8337 29.87/0.837 29.93/0.8340 29.85/0.8333 28.57/0.8012 BSD100 28.41/0.7858 28.85/0.7984 28.88/0.7997 28.87/0.7994 28.87/0.7994 28.85/0.7990 27.92/0.7648</figDesc><table><row><cell>ρ</cell><cell>0</cell><cell>0.25</cell><cell>0.375</cell><cell>0.5</cell><cell>0.625</cell><cell>0.75</cell><cell>1</cell></row><row><cell>Urban100</cell><cell cols="7">26.21/0.8011 27.16/0.8296 27.25/0.8329 27.24/0.8324 27.24/0.8321 27.20/0.8312 25.50/0.7761</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table IV :</head><label>IV</label><figDesc>The effect of ordinal position of block with different ρ on average ×3 PSNR/SSIM for the Set5, Set14, BSD100 and Urban100 datasets. Each block has the same number of LCSCUnits.</figDesc><table><row><cell>ρ list</cell><cell>[0.5,0.75]</cell><cell cols="2">[0.75,0.5] [0.5,0.625,0.75] [0.75,0.625,0.5]</cell></row><row><cell>Set5</cell><cell cols="2">33.97/0.9240 34.02/0.9244 33.89/0.9234</cell><cell>33.95/0.9239</cell></row><row><cell>Set14</cell><cell cols="2">29.91/0.8341 29.91/0.8343 29.86/0.8332</cell><cell>29.86/0.8336</cell></row><row><cell cols="3">BSD100 28.88/0.7998 28.89/0.8001 28.86/0.7993</cell><cell>28.88/0.7994</cell></row><row><cell cols="3">Urban100 27.28/0.8336 27.31/0.8343 27.20/0.8317</cell><cell>27.25/0.8323</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table V :</head><label>V</label><figDesc>Brief comparisons among different fusion strategies.</figDesc><table><row><cell></cell><cell cols="3">Vectorized fusion [23] MSCN [43] Our method</cell></row><row><cell>Adaptiveness</cell><cell>×</cell><cell>√</cell><cell>√</cell></row><row><cell>Pixel-wise</cell><cell>×</cell><cell>√</cell><cell>√</cell></row><row><cell>Normalization</cell><cell>√</cell><cell>×</cell><cell>√</cell></row><row><cell>Multi-supervised training</cell><cell>√</cell><cell>×</cell><cell>√</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table VI :</head><label>VI</label><figDesc>Average ×2 PSNR/SSIM for LCSCNets with different fusions on the Set5, Set14, BSD100 and Urban100 datasets, respectively. Red indicates the best results.</figDesc><table><row><cell></cell><cell>Basic LCSCNet LCSCNet S LCSCNet M LCSCNet</cell></row><row><cell>Set5</cell><cell>37.77/0.0.9558 37.80/0.9560 37.79/0.9559 37.84/0.9559</cell></row><row><cell>Set14</cell><cell>33.23/0.9140 33.26/0.9144 33.25/0.9142 33.31/0.9144</cell></row><row><cell>BSD100</cell><cell>32.06/0.8980 32.05/0.8981 32.07/0.8981 32.08/0.8984</cell></row><row><cell>Urban100</cell><cell>31.15/0.9182 31.26/0.9197 31.23/0.9190 31.31/0.9200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table VII :</head><label>VII</label><figDesc>Average ×3 PSNR/SSIM for Basic LCSCNet and its corresponding Basic E-LCSCNet on Set5, Set14, BSD100 and Urban100. All the models are of moderate scales (Parameter amount &lt; 150K). .9241 34.01/0.9248 34.02/0.9244 34.05/0.9251 34.03/0.9244 34.08/0.9248 Set14 29.87/0.8337 29.92/0.8349 29.85/0.8334 29.90/0.8345 29.88/0.8340 29.89/0.8339 BSD100 28.87/0.7994 28.89/0.8002 28.87/0.7996 28.90/0.8004 28.89/0.7998 28.89/0.7998</figDesc><table><row><cell></cell><cell></cell><cell>LC 34</cell><cell>E-LC 37</cell><cell>LC 44</cell><cell>E-LC 48</cell><cell>LC 54</cell><cell>E-LC 59</cell></row><row><cell></cell><cell cols="3">Set5 33.99/0Urban100 27.24/0.8324 27.28/0.8340</cell><cell cols="2">27.23/0.8326 27.29/0.8343</cell><cell>27.27/0.8330 27.32/0.8347</cell></row><row><cell>(a) HR</cell><cell>(b) VDSR</cell><cell>(c) DRCN</cell><cell cols="2">(d) LapSR</cell><cell></cell></row><row><cell>(e) DRRN</cell><cell>(f) MemNet</cell><cell>(g) CARN</cell><cell cols="2">(h) BE-LCSC L</cell><cell></cell></row><row><cell cols="5">Figure 6: Results for upscaling factor 3 on image Set14-barbara</cell><cell></cell></row><row><cell>(a) HR</cell><cell>(b) VDSR</cell><cell>(c) DRCN</cell><cell cols="2">(d) LapSR</cell><cell></cell></row><row><cell>(e) DRRN</cell><cell>(f) MemNet</cell><cell>(g) CARN</cell><cell cols="2">(h) BE-LCSC L</cell><cell></cell></row><row><cell cols="5">Figure 7: Results for upscaling factor 3 on image Set14-ppt</cell><cell></cell></row><row><cell cols="5">LCSC 76 291 achieves better accuracy than MemNet. As for</cell><cell></cell></row><row><cell cols="5">efficiency, MemNet has fewer parameters due to its recursive</cell><cell></cell></row><row><cell cols="5">structure, but LCSC 76 291 is more computation-efficient</cell><cell></cell></row><row><cell cols="5">than MemNet. When compared with SelNet and CARN, BE-</cell><cell></cell></row><row><cell cols="5">LCSC L is moderately computation-consuming but achieves</cell><cell></cell></row><row><cell cols="5">obvious improvement. Among large models on DIV2K, our</cell><cell></cell></row></table><note>E-LCSCNet has the fewest Params for every scale. For ×2 scale, our E-LCSCNet holds the same level with RDN but with a clear advantage in Mult&amp;Adds. For ×3 and ×4 scale, our E-LCSCNet performs better than EDSR and RDN, but is somehow more computation-consuming than RDN due to its fusion part.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table VIII :</head><label>VIII</label><figDesc>Quantitative comparisons among mainstream deep models for SISR. To compare fairly, we divide models into three categories: models trained on 291, light models (Params &lt; 2M) trained on DIV2K, and large models trained on DIV2K. For each scale, we compare the models within the same category, and the best performance is highlighted in Red. In DRCN, MemNet, LCSC 76 291 and E-LCSCNet, extra Mult&amp;Adds of the multi-supervised fusion part are added after the Mult&amp;Adds of the basic structure.</figDesc><table><row><cell>Scale</cell><cell>Model</cell><cell>Training data</cell><cell>Params</cell><cell>Mult&amp;Adds</cell><cell>Set5</cell><cell>Set14</cell><cell>BSD100</cell><cell>Urban100</cell></row><row><cell></cell><cell>VDSR</cell><cell>291</cell><cell>665K</cell><cell>612.6G</cell><cell>37.53/0.9587</cell><cell>33.03/0.9124</cell><cell>31.90/0.8960</cell><cell>30.76/0.9140</cell></row><row><cell></cell><cell>DRCN</cell><cell>291</cell><cell>1774K</cell><cell>9243.0G+8731.3G</cell><cell>37.63/0.9588</cell><cell>33.04/0.9118</cell><cell>31.85/0.8942</cell><cell>30.75/0.9133</cell></row><row><cell></cell><cell>LapSRN</cell><cell>291</cell><cell>813K</cell><cell>29.9G</cell><cell>37.52/0.9591</cell><cell>33.08/0.9130</cell><cell>31.80/0.8950</cell><cell>30.41/0.9101</cell></row><row><cell></cell><cell>DRRN</cell><cell>291</cell><cell>297K</cell><cell>6796.9G</cell><cell>37.74/0.9591</cell><cell>33.23/0.9136</cell><cell>32.05/0.8973</cell><cell>31.23/0.9188</cell></row><row><cell></cell><cell>MemNet</cell><cell>291</cell><cell>667K</cell><cell>2261.8G+3.2G</cell><cell>37.78/0.9597</cell><cell>33.28/0.9142</cell><cell>32.08/0.8978</cell><cell>31.31/0.9195</cell></row><row><cell></cell><cell>LCSC 76 291</cell><cell>291</cell><cell>1844K</cell><cell>407.8G+616.3G</cell><cell>37.86/0.9600</cell><cell>33.34/0.9146</cell><cell>32.10/0.8985</cell><cell>31.34/0.9204</cell></row><row><cell>×2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SelNet</cell><cell>DIV2K</cell><cell>974K</cell><cell>225.7G</cell><cell>37.89/0.9598</cell><cell>33.61/0.9160</cell><cell>32.08/0.8984</cell><cell>-/-</cell></row><row><cell></cell><cell>CARN</cell><cell>DIV2K</cell><cell>1592K</cell><cell>222.8G</cell><cell>37.76/0.9590</cell><cell>33.52/0.9166</cell><cell>32.09/0.8978</cell><cell>31.92/0.9256</cell></row><row><cell></cell><cell>FALSR-A</cell><cell>DIV2K</cell><cell>1021K</cell><cell>234.7G</cell><cell>37.82/0.9595</cell><cell>33.55/0.9168</cell><cell>32.12/0.8987</cell><cell>31.93/0.9256</cell></row><row><cell></cell><cell>BE-LCSC L</cell><cell>DIV2K</cell><cell>1552K</cell><cell>358.6G</cell><cell>38.01/0.9600</cell><cell>33.67/0.9160</cell><cell>32.23/0.9002</cell><cell>32.31/0.9297</cell></row><row><cell></cell><cell>EDSR</cell><cell>DIV2K</cell><cell>40.7M</cell><cell>9379.4G</cell><cell>38.11/0.9602</cell><cell>33.92/0.9195</cell><cell>32.32/0.9013</cell><cell>32.93/0.9351</cell></row><row><cell></cell><cell>D DBPN</cell><cell>DIV2K+Flickr</cell><cell>5876.3K</cell><cell>3429.0G</cell><cell>38.09/0.9600</cell><cell>33.87/0.9191</cell><cell>32.27/0.9000</cell><cell>32.55/0.9324</cell></row><row><cell></cell><cell>RDN</cell><cell>DIV2K</cell><cell>22.1M</cell><cell>5096.2G</cell><cell>38.24/0.9614</cell><cell>34.01/0.9212</cell><cell>32.34/0.9017</cell><cell>32.89/0.9353</cell></row><row><cell></cell><cell>E-LCSCNet</cell><cell>DIV2K</cell><cell>14.2M</cell><cell>3126.4G+1251.7G</cell><cell>38.23/0.9608</cell><cell>33.85/0.9180</cell><cell>32.36/0/9018</cell><cell>32.93/0.9351</cell></row><row><cell></cell><cell>VDSR</cell><cell>291</cell><cell>665K</cell><cell>612.6G</cell><cell>33.66/0.9213</cell><cell>29.77/0.8314</cell><cell>28.82/0.7976</cell><cell>27.14/0.8279</cell></row><row><cell></cell><cell>DRCN</cell><cell>291</cell><cell>1774K</cell><cell>9243.0G+8731.3G</cell><cell>33.82/0.9226</cell><cell>29.76/0.8311</cell><cell>28.80/0.7963</cell><cell>27.15/0.8276</cell></row><row><cell></cell><cell>LapSRN</cell><cell>291</cell><cell>813K</cell><cell>29.9G</cell><cell>33.82/0.9227</cell><cell>29.79/0.8320</cell><cell>28.82/0.7973</cell><cell>27.07/0.8272</cell></row><row><cell></cell><cell>DRRN</cell><cell>291</cell><cell>297K</cell><cell>6796.9G</cell><cell>34.03/0.9244</cell><cell>29.96/0.8349</cell><cell>28.95/0.8004</cell><cell>27.53/0.8378</cell></row><row><cell></cell><cell>MemNet</cell><cell>291</cell><cell>667K</cell><cell>2261.8G+3.2G</cell><cell>34.09/0.9248</cell><cell>30.00/0.8350</cell><cell>28.96/0.8001</cell><cell>27.56/0.8376</cell></row><row><cell></cell><cell>LCSC 76 291</cell><cell>291</cell><cell>1844K</cell><cell>181.3G+616.3G</cell><cell>34.13/0.9254</cell><cell>29.95/0.8348</cell><cell>28.97/0.8014</cell><cell>27.53/0.8377</cell></row><row><cell>×3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SelNet</cell><cell>DIV2K</cell><cell>1159K</cell><cell>120.0G</cell><cell>34.27/0.9257</cell><cell>30.30/0.8399</cell><cell>28.97/0.8025</cell><cell>-/-</cell></row><row><cell></cell><cell>CARN</cell><cell>DIV2K</cell><cell>1592K</cell><cell>118.8G</cell><cell>34.29/0.9255</cell><cell>30.29/0.8407</cell><cell>29.06/0.8034</cell><cell>28.06/0.8493</cell></row><row><cell></cell><cell>BE-LCSC L</cell><cell>DIV2K</cell><cell>1736K</cell><cell>179.1G</cell><cell>34.39/0.9265</cell><cell>30.33/0.8395</cell><cell>29.12/0.8065</cell><cell>28.25/0.8540</cell></row><row><cell></cell><cell>EDSR</cell><cell>DIV2K</cell><cell>43.7M</cell><cell>4471.8G</cell><cell>34.65/0.9280</cell><cell>30.52/0.8462</cell><cell>29.25/0.8093</cell><cell>28.80/0.8653</cell></row><row><cell></cell><cell>D DBPN</cell><cell>DIV2K+Flickr</cell><cell>-</cell><cell>-</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell><cell>-/-</cell></row><row><cell></cell><cell>RDN</cell><cell>DIV2K</cell><cell>22.3M</cell><cell>2284.7G</cell><cell>34.71/0.9296</cell><cell>30.57/0.8468</cell><cell>29.26/0.8093</cell><cell>28.80/0.8653</cell></row><row><cell></cell><cell>E-LCSCNet</cell><cell>DIV2K</cell><cell>14.9M</cell><cell>1389.5G+1251.7G</cell><cell>34.71/0.9286</cell><cell>30.56/0.8460</cell><cell>29.27/0.8104</cell><cell>28.83/0.8658</cell></row><row><cell></cell><cell>VDSR</cell><cell>291</cell><cell>665K</cell><cell>612.6G</cell><cell>31.35/0.8838</cell><cell>28.01/0.7674</cell><cell>27.29/0.7251</cell><cell>25.18/0.7524</cell></row><row><cell></cell><cell>DRCN</cell><cell>291</cell><cell>1774K</cell><cell>9243.0G+8731.3G</cell><cell>31.53/0.8854</cell><cell>28.02/0.7670</cell><cell>27.23/0.7233</cell><cell>25.14/0.7510</cell></row><row><cell></cell><cell>LapSRN</cell><cell>291</cell><cell>813K</cell><cell>29.9G</cell><cell>31.54/0.8855</cell><cell>28.19/0.7720</cell><cell>27.32/0.7280</cell><cell>25.21/0.7553</cell></row><row><cell></cell><cell>DRRN</cell><cell>291</cell><cell>297K</cell><cell>6796.9G</cell><cell>31.68/0.8888</cell><cell>28.21/0.7721</cell><cell>27.38/0.7284</cell><cell>25.44/0.7638</cell></row><row><cell></cell><cell>MemNet</cell><cell>291</cell><cell>667K</cell><cell>2261.8G+3.2G</cell><cell>31.74/0.8893</cell><cell>28.26/0.7723</cell><cell>27.40/0.7281</cell><cell>25.50/0.7638</cell></row><row><cell></cell><cell>LCSC 76 291</cell><cell>291</cell><cell>1844K</cell><cell>110.0G+616.3G</cell><cell>31.76/0.8899</cell><cell>28.20/0.7731</cell><cell>27.36/0.7293</cell><cell>25.38/0.7643</cell></row><row><cell>×4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SRDenseNet</cell><cell>ImageNet Subset</cell><cell>2015K</cell><cell>389.9G</cell><cell>32.02/0.8934</cell><cell>28.50/0.7782</cell><cell>27.53/0.7337</cell><cell>26.05/0.7819</cell></row><row><cell></cell><cell>SelNet</cell><cell>DIV2K</cell><cell>1417K</cell><cell>83.1G</cell><cell>32.00/0.8931</cell><cell>28.49/0.7783</cell><cell>27.44/0.7325</cell><cell>-/-</cell></row><row><cell></cell><cell>CARN</cell><cell>DIV2K</cell><cell>1592K</cell><cell>90.9G</cell><cell>32.13/0.8937</cell><cell>28.60/0.7806</cell><cell>27.58/0.7349</cell><cell>26.07/0.7837</cell></row><row><cell></cell><cell>BE-LCSC L</cell><cell>DIV2K</cell><cell>1699K</cell><cell>124.8G</cell><cell>32.20/0.8948</cell><cell>28.66/0.7806</cell><cell>27.62/0.7390</cell><cell>26.22/0.7908</cell></row><row><cell></cell><cell>EDSR</cell><cell>DIV2K</cell><cell>43.1M</cell><cell>2890.0G</cell><cell>32.46/0.8968</cell><cell>28.80/0.7876</cell><cell>27.71/0.7420</cell><cell>26.64/0.8033</cell></row><row><cell></cell><cell>D DBPN</cell><cell>DIV2K+Flickr</cell><cell>10.3M</cell><cell>5715.4G</cell><cell>32.47/0.8980</cell><cell>28.82/0.7860</cell><cell>27.72/0.7400</cell><cell>26.38/0.7946</cell></row><row><cell></cell><cell>RDN</cell><cell>DIV2K</cell><cell>22.6M</cell><cell>1300.7G</cell><cell>32.47/0.8990</cell><cell>28.81/0.7871</cell><cell>27.72/0.7419</cell><cell>26.61/0.8028</cell></row><row><cell></cell><cell>E-LCSCNet</cell><cell>DIV2K</cell><cell>14.8M</cell><cell>781.6G+1700.7G</cell><cell>32.51/0.8984</cell><cell>28.81/0/7871</cell><cell>27.73/0.7433</cell><cell>26.64/0.8033</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">e.g. the convolution operation X * Y is rewritten as XY for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Many works add nonlinear activation before a 1 × 1 convolution kernel to make the bottleneck layer; here we take the 1 × 1 convolution kernel as the bottleneck layer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank the authors of <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b53">[54]</ref> for releasing their source codes and models for comparison. We would also like to thank the Associate Editor and anonymous reviewers for their selfless dedication and constructive suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single-image super-resolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super-resolution image reconstruction: a technical overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="36" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cubic convolution interpolation for digital image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Keys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lanczos filtering in one and two dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Duchon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Meteorology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1016" to="1022" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SoftCuts: a soft edge smoothness prior for color image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="969" to="981" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image super-resolution using gradient profile prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Single image superresolution based on gradient profile sharpness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3187" to="3202" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image super-resolution by TVregularization and Bregman iteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marquina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="367" to="382" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Example-based superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Curves and Surfaces</title>
		<meeting>the International Conference on Curves and Surfaces</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Consistent coding scheme for single-image super-resolution via independent dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="325" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3791" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4470" to="4478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Higher order recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.00064</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Residual networks behave like ensembles of relatively shallow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep edge guided recurrent residual learning for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5895" to="5907" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MemNet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4809" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">NTIRE 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep backprojection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning a mixture of deep networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Super-resolution with deep convolutional sufficient statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05666</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Amortised map inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04490</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">EnhanceNet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep learning for single image super-resolution: A brief review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2019.2919431</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A deep convolutional neural network with selection units for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1150" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fast, accurate, and lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="252" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Fast, accurate and lightweight super-resolution with neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">MatConvNet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">PyTorch</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
