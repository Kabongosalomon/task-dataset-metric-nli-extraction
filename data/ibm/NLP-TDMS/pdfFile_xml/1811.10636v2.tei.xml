<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evolving Space-Time Neural Architectures for Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
							<email>anelia@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
							<email>toshev@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
							<email>mryoo@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<title level="a" type="main">Evolving Space-Time Neural Architectures for Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new method for finding video CNN architectures that capture rich spatio-temporal information in videos. Previous work, taking advantage of 3D convolutions, obtained promising results by manually designing video CNN architectures. We here develop a novel evolutionary search algorithm that automatically explores models with different types and combinations of layers to jointly learn interactions between spatial and temporal aspects of video representations. We demonstrate the generality of this algorithm by applying it to two meta-architectures, obtaining new architectures superior to manually designed architectures. Further, we propose a new component, the iTGM layer, which more efficiently utilizes its parameters to allow learning of space-time interactions over longer time horizons. The iTGM layer is often preferred by the evolutionary algorithm and allows building cost-efficient networks. The proposed approach discovers new and diverse video architectures that were previously unknown. More importantly they are both more accurate and faster than prior models, and outperform the state-of-the-art results on multiple datasets we test, including HMDB, Kinetics, and Moments in Time. We will open source the code and models, to encourage future model development 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video understanding tasks, such as video object detection and activity recognition, are important for many societal applications of computer vision including robot perception, smart cities, medical analysis, and more. Convolutional neural networks (CNNs) have been popular for video understanding, with many successful prior approaches, including C3D <ref type="bibr">[30]</ref>, I3D <ref type="bibr">[1]</ref>, R(2+1)D <ref type="bibr">[33]</ref>, S3D <ref type="bibr">[38]</ref>, and others <ref type="bibr">[3,</ref><ref type="bibr">9]</ref>. These approaches focus on manually designing CNN architectures specialized for videos, for example by extending known 2D architectures such as Inception <ref type="bibr">[28]</ref> and ResNet <ref type="bibr">[5]</ref> to 3D <ref type="bibr">[1,</ref><ref type="bibr">33]</ref>. However, designing new, larger or more advanced architectures is a challenging <ref type="bibr">1</ref> Code and models: https://sites.google.com/corp/view/evanet-video problem, especially as the complexity of video tasks necessitates deeper and wider architectures and more complex sub-modules. Furthermore, the existing networks, which are mostly inspired by single-image based tasks, might not sufficiently capture the rich spatio-temporal interactions in video data.</p><p>In this work, we present a video architecture evolution approach to harness the rich spatio-temporal information present in videos. Neural architecture search and evolution have been previously applied for text and image classification <ref type="bibr">[29,</ref><ref type="bibr">41]</ref>. A naive extension of the above approaches to video is infeasible due to the large search space of possible architectures operating on 3D inputs.</p><p>To address these challenges we propose a novel evolution algorithm for video architecture search. We introduce a hybrid meta-architecture ('fill-in-the-blanks') model for which the high level connectivity between modules is fixed, but the individual modules can evolve. We apply this successfully to both Inception and ResNet based metaarchitectures. We design the search space specifically for video CNN architectures that jointly capture various spatial and temporal interactions in videos. We encourage exploration of more diverse architectures by applying multiple nontrivial mutations at the earlier stages of evolution while constraining the mutations at the later stages. This enables discovering multiple, very different but similarly good architectures, allowing us to form a better ensemble by combining them.</p><p>Furthermore, to enrich the search space for video inputs, we propose a new key element which is specifically designed to capture space-time features' interactions. We introduce an Inflated Temporal Gaussian Mixture (iTGM) layer as part of the evolution search space. The iTGM is motivated by the original 1D TGM <ref type="bibr">[21]</ref>. For our iTGM, we learn 2D spatial filters in addition to the temporal Gaussian mixture values, and inflate the 2D filter temporally to allow learning of joint features in 3D. The 2D filter is inflated non-uniformly, by following the weights according to the learned 1-D temporal Gaussian mixture pattern. This allows to explore space-time interactions more effectively and with much fewer parameters, while at the same time capture longer temporal information in videos.  The proposed algorithm results in novel architectures which comprise interesting sub-modules (see <ref type="figure" target="#fig_0">Fig. 1</ref> and 2). It discovers complex substructures, including modules with multiple parallel space-time conv/pooling layers focusing on different temporal resolutions of video representations. Other findings include: multiple different types of layers combined in the same module e.g., an iTGM layer jointly with (2+1)D convolutions and pooling layers; heterogeneous modules at different levels of the architecture, which is in contrast to previous handcrafted models. Furthermore, the evolution itself generates a diverse set of accurate models. By ensembling them, recognition accuracy increases beyond other homogeneous-architecture ensembles.</p><p>Our approach discovers models which outperform the state-of-the-art on public datasets we tested, including HMDB, Kinetics, and Moments in time. This is done with a generic evolutionary algorithm and no per-data hyperparamter tuning. Furthermore, the best found models are very fast, running at about 100 ms for a single model, and 250ms for an ensemble, both being considerably faster than prior models.</p><p>The main technical contributions of this paper are: 1) We propose a novel evolutionary approach for developing space-time CNN architectures, specifically designed for videos. We design the search space to specifically explore different space-time convolutional layers and their combinations and encourage diversity. 2) We introduce a new space-time convolutional layer, the Inflated TGM layer, designed to capture longer-term temporal information.</p><p>3) The discovered models achieve state-of-the-art performance on several video datasets and are among the fastest models for videos. We provide new diverse architectures, ensembles and components which can be reused for future work. To our knowledge this is the first automated neural architecture search algorithm for video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>CNNs for video understanding. Approaches considering a video as a space-time volume have been particularly successful <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">30,</ref><ref type="bibr">31]</ref>, with a direct application of 3D CNNs to videos. C3D <ref type="bibr">[30]</ref> learned 3x3x3 XYT filters, which was not only applied to action recognition but also to video object recognition. I3D <ref type="bibr">[1]</ref> extended the Inception architecture to 3D, obtaining successful results on multiple activity recognition video datasets including Kinetics. S3D <ref type="bibr">[38]</ref> investigated the usage of 1D and 2D convolutional layers in addition to the 3D layers. R(2+1)D <ref type="bibr">[33]</ref> used the 2D conv. layers followed by 1D conv. layers while following the ResNet structure. Two-stream CNN design is also widely adopted in action recognition, which takes optical flow inputs in addition to raw RGBs <ref type="bibr">[3,</ref><ref type="bibr">27]</ref>. There are also works focusing on capturing longer temporal information in continuous videos using pooling <ref type="bibr">[19]</ref>, attention <ref type="bibr">[20]</ref>, and convolution <ref type="bibr">[9]</ref>. Recurrent neural networks (e.g., LSTMs) are also used to sequentially represent videos <ref type="bibr">[19,</ref><ref type="bibr">39]</ref>.</p><p>Neural architecture search. Neural network architectures have advanced significantly since the early convolutional neural network concepts of LeCun et al. <ref type="bibr">[13]</ref> and Krizhevsky et al. <ref type="bibr">[11]</ref>: from developing wider modules, e.g., Inception <ref type="bibr">[28]</ref>, or introducing duplicated modules <ref type="bibr">[14]</ref>, residual connections <ref type="bibr">[5,</ref><ref type="bibr">37]</ref>, densely connected networks <ref type="bibr">[6,</ref><ref type="bibr">7]</ref>, or multi-task architectures: e.g., Faster-RCNN and RetinaNet for detection, and many others <ref type="bibr">[15,</ref><ref type="bibr">16,</ref><ref type="bibr">24]</ref>. Recently several ground-breaking approaches have been proposed for automated learning/searching of neural network architectures, rather than manually designing them <ref type="bibr">[23,</ref><ref type="bibr">29,</ref><ref type="bibr">41,</ref><ref type="bibr" target="#b41">42]</ref>. Successful architecture search has been demonstrated for images and text <ref type="bibr">[41,</ref><ref type="bibr" target="#b41">42]</ref>, including object classification. Tran et al. <ref type="bibr">[32]</ref> analyze action recognition experiments with different settings, e.g., input resolution, frame rate, number of frames, network depth, all within the 3D ResNet architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Convolutional layers for action recognition</head><p>We first review standard convolutional layers for videos and then introduce the new iTGM layer to learn longer temporal structures with fewer parameters and lower computational cost. Video CNNs are analogous to standard CNNs, with the difference of an additional temporal dimension in the input and all intermediate feature maps. In more detail, both input and feature maps are represented as 4D tensors XYTC with two spatial dimensions, one temporal and one for the pixel values or features (i.e., channels). Several forms of convolution on such tensors have been explored. 3D convolutional layer learns a standard 3D convolutional kernel over space and time <ref type="bibr">[8]</ref>. It applies C out kernels of dimension L × H × W × C in on a tensor of size T × Y × X × C in to produce a tensor of size T × Y × X × C out . This layer has LHW C in C out parameters, which is an order of magnitude larger than CNNs and becomes prohibitive in many cases. Further, expanding 2D kernels to 3D has been explored <ref type="bibr">[17]</ref>. I3D expanded kernels by stacking the 2D kernels L times, results in state-of-the-art performance <ref type="bibr">[1]</ref>. (2+1)D convolutional layer decomposes a 3D kernel into a composition of a 2D spatial kernel followed by a 1D temporal kernel <ref type="bibr">[33,</ref><ref type="bibr">38]</ref>. It has HW C in C out + LC out C out parameters, and as such is more efficient than 3D convolution. However, it still depends on the time dimension L which limits the temporal size of the filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Inflated TGM layer</head><p>The recently introduced Temporal Gaussian Mixture layer (TGM) <ref type="bibr">[21]</ref> is a specialized 1D convolutional layer designed to overcome the limitations of standard 1D convolutional layers. In contrast to the standard 1D temporal convolutional layer, which was often used in video CNNs such as R(2+1)D, a TGM layer represents its filter as a mixture of 1D Gaussians. This makes the number of its learnable parameters independent of the temporal filter size; with a TGM layer, one does not have to handle all kernel weights but only the Gaussian mixture parameters.</p><p>In this work, we employ the above idea to define a 3D space-time kernel directly, named Inflated Temporal Gaussian Mixture layer (iTGM). We 'inflate' the 2D spatial kernels to 3D by representing 3D kernel as a product of two kernels:</p><p>S K where S is the 'inflated' 2D convolution and K is a temporal 1D kernel defined using a mixture of Gaussians (see <ref type="figure" target="#fig_2">Fig. 3</ref>). The Gaussian mixture kernel K is defined as follows. Denote by µ m and width σ m the center and width of M Gaussians, m ∈ {0, . . . , M }. Further, denote by a im , i ∈ {0, . . . , C out } soft-attention mixing-weights. The temporal Gaussian kernels read:</p><formula xml:id="formula_0">K ml = 1 Z exp − (l − µ m ) 2 2σ 2 m<label>(1)</label></formula><p>where Z is a normalization: L l=0K ml = 1. Then, the a mixture of the above Gaussian kernels is:</p><formula xml:id="formula_1">K il = exp (a im ) j exp (a ij )K ml .<label>(2)</label></formula><p>This results in K being a C out ×L kernel; i. e., a temporal kernel with C out output channels. We apply this kernel on the output of the spatial kernel. Thus, we obtain a L × H × W × C in × C out kernel, using only HW C in C out + 2M + M C out parameters.</p><p>In practice, µ is constrained to be in [0, L), µ = (1/2)(L − 1) tanh (μ) + 1. and σ is positive, σ 2 = exp (σ). Further, M is a hyperparameter, typically smaller than L.</p><p>The parameters of the iTGM layer -spatial kernel parameters, µ m , σ m , and a im -are all differentiable, and are learned from data for the specified task. The above layer behaves exactly like the standard 3D XYT convolution. Note that this layer learns fewer parameters than both 3D and (2+1)D convolution, and can learn temporally longer kernels as the number of parameters is independent of the length, L. Examples of inflated TGMs are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Neural architecture evolution for videos</head><p>We design our neural architecture search specifically for videos, and propose the following:</p><p>• Use of 'fill-in-the-blanks' meta-architectures to limit the search space and generate both trainable and highperforming architectures. • Search among combinations of six different types of space-time convolution/pooling layer concatenations where their temporal duration can vary in large ranges. • We specially design mutation operations to more effectively explore the large space of possible architectures. • We propose an evolutionary sampling strategy which encourages more diverse architectures early in the search. Neural architecture evolution finds better-performing architectures by iteratively modifying a pool of architectures. Starting from a set of random architectures, it mutates them over multiple rounds, while only retaining the better performing ones. Recent studies <ref type="bibr">[22]</ref> show that evolutionary algorithms can find good image architectures from a smaller number of samples, as opposed to model search algorithms using reinforcement learning <ref type="bibr">[41]</ref>. This makes evolution more suitable for video architecture search, as video CNNs are expensive to train. Further, it allows for mutating architectures by selecting and combining various space-time layers which more effectively process inputs with much larger dimensionality. The evolution also enables obtaining multiple different architectures instead of a single architecture which we use to build a powerful ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Search space and base architecture</head><p>We evolve our architectures to have heterogeneous modules, motivated by the recent observations that video CNN architectures may need differently sized temporal filters at different layers, e.g., bottom-heavy vs. top-heavy <ref type="bibr">[38]</ref>. In order to keep the entire search space manageable while evolving modules heterogeneously, we use a meta-architecture where internal sub-modules are allowed to evolve without constraints but the high level architecture has a fixed number of total modules. We used both an Inception-like and ResNet-like meta-architecture. The Inception meta-architecture follows the popular Inception architecture, with five layers forming the 'stem' followed by Inception modules whose structure is evolved. The ResNetlike meta-architecture is illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. This metaarchitecture is composed of two fixed convolutional layers (i.e., the 'stem') followed by four residual Inception mod-ules interspersed with max-pooling layers. Each residual Inception module can be repeated R times and has a residual connection. <ref type="figure" target="#fig_4">Figure 5</ref> shows an example module.</p><p>Each module can have multiple parallel convolutional or pooling layers and its specific form is chosen through evolution. We constrain the complexity of the connections between the layers within a module while making the evolution explore temporal aspects of the modules. More specifically, we make each module have 1-6 parallel 'streams' with four different stream types: (i) one 1x1x1 conv., (ii) one space-time conv. layer after one 1x1x1 layer, (iii) two space-time conv. layers after one 1x1x1, and (iv) a spacetime pooling followed by one 1x1x1. <ref type="figure" target="#fig_4">Figure 5</ref> shows the four types. The architecture evolution focuses on modifying each module: selecting layer types and its parameters, selecting the number of parallel layers, and for the residual ones, how many times should each module be repeated.</p><p>The convolutional layers have {1, 3, 5, 7, 9, 11} as the set of possible temporal kernel sizes. As a result, the architec-</p><formula xml:id="formula_2">ture search space size is O((3×6+1) 5+B×N +(6+1) D×N )</formula><p>where B and D are the maximum of number of spacetime conv and pooling layers we allow in each module, and N = 4 or 9 is the number modules in the meta-architecture. There are 2 or 5 individual layers (often also called a 'stem') before the modules. Each space-time conv. layer has 3 × 6 possible options and each space-time pooling has 6 options. Also, there is the option to add/omit the layer, making the total number of choices 3 × 6 + 1 and 6 + 1. For the ResNetlike models, we allow modules to be repeated up to 6 times. We fix the spatial size of the kernels to be 3 × 3. Although the search space is very big, the idea is that an exhaustive search is not necessary and it is possible to find good local optima by evolving from various initial samples (i.e., architectures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evolutionary algorithm</head><p>Algorithm 1 summarizes the architecture search. In a standard genetic algorithm setting, we maintain a population of size P , where each individual in the population is a particular architecture. Initial architectures are obtained by randomly sampling from our large search space, encouraging diversity and exploration. At each round of the evolution, the algorithm randomly selects S number of samples from the entire population and compares their recognition performance. The architecture with the highest fitness (i.e., validation accuracy) becomes the 'parent', and mutation operators are applied to the selected parent to generate a new 'child' architecture to be added to the population. Whenever a new architecture is added, it is trained with the training set for a number of iterations, and is evaluated with a separate validation set (different from the actual test and validation sets) to measure the recognition accuracy. This performance becomes the 'fitness' of the architecture. Having S where 1 &lt; S ≤ P controls the randomness in of the parent selection. It avoids the algorithm repeatedly selecting the same parent, which might already be at a local maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Evolutionary search algorithm</head><p>function SEARCH Randomly initialize the population, P Evaluate each individual in P for i &lt; number of evolutionary rounds do S = random sample of 25 individuals parent = the most fit individual in S child = parent for max( d − i r , 1) do child = mutate(child) end for evaluate child and add to population remove least fit individual from population end for end function Mutations. The mutation operators modify the parent architecture to generate a new child architecture. In order to explore the architecture search space we describe in Section 4.1 efficiently, we consider the following 4 mutation operators: (i) Select a space-time conv. layer within the parent architecture, and change its 'type'. (ii) Select a space-time conv. layer or a pooling layer, and change its temporal size (i.e., L) . (iii) Select a module from the parent architecture, and add/remove a parallel layer stream. We constrain the number of parallel layer streams to be 1-6. We additionally constrain each module to have a fixed number of output filters which are evenly divided between the parallel layers. repeated. <ref type="figure" target="#fig_5">Figure 6</ref> illustrates examples of our mutation operators applied to layers of a module. Diversity. Importantly, we design the mutation in our algorithm to happen by applying multiple randomly chosen mutation operators. In order to encourage more diverse architectures, we develop the strategy of applying many mutation operators in the early stage of the evolution while reducing the amount of mutations in the later stages, which is analogous to controlling the learning rate in a CNN model learning. As described in Algorithm 1, we apply max(d − i r , 1) number of mutation operators where d is the maximum number of operators we want to apply in the beginning, and r controls how quickly we want to decrease their numbers linearly. Once a child architecture is added to the population, in order to maintain the size of the population to P , the evolutionary algorithm selects an individual to discard from the pool. We tried different removal criteria including the lowest fitness and the oldest (i.e., <ref type="bibr">[22]</ref>), which did not make much difference in our case. Ensemble. We obtain a number of top performing architectures after the evolutionary search is completed, thanks to our evolutionary algorithm promoting populations with diverse individual architectures. Thus, we are able to construct a strong ensemble from the diverse models by averaging the outputs of their softmax layers: F * (x) = i F i (x) where x is the input video and F i are the top models. In the experiments, we found our approach obtains very diverse, top performing architectures. Ensembling further improves the overall recognition. We named our final ensemble network as EvaNet (Evolved Video Architecture).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Although our evolutionary architecture search is applicable to various different video understanding tasks, here we focus on human activity recognition. The video CNN architectures are evolved using public datasets. Fitness of the architectures during evolution is measured on a subset of the training data. In all experiments, the evolutionary algorithm has no access to the test set during training and evolution. In more detail, we use following datasets: HMDB <ref type="bibr">[12]</ref> is a dataset of human motion videos collected from a variety of sources. It is a common datasets for video classification and has ∼7K videos of 51 action classes. Kinetics <ref type="bibr">[10]</ref> is a large challenging video dataset with 225,946 training and 18,584 validation videos. We use the currently available version (Kinetics-400 dataset), which has about 25k fewer training videos than original Kinetics dataset (i.e., missing about 10% of train/val/test data). This makes the dataset more difficult to train, and not comparable to the previous version. Charades <ref type="bibr">[26]</ref> is an activity recognition dataset with ∼10K videos, whose durations are 30 seconds on average. We chose Charades to particularly confirm whether our architecture evolution finds structures different from those found with shorter videos like Kinetics. We use the standard classification evaluation protocol. Moments in Time <ref type="bibr">[18]</ref> is a large-scale dataset for understanding of actions and events in videos (339 classes, 802,264 training, 33,900 validation videos).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setup</head><p>Architecture evolution is done in parallel on smaller input size and fewer number of iterations. Details can be found in the appendix. We perform evolution for 2000 rounds: generating, mutating, training/evaluating, and discarding 2000 CNN architectures. Note that ∼300 rounds were often sufficient to find good architectures <ref type="figure" target="#fig_6">(Figure 7</ref>). Once the architecture evolution is complete and the top performing models are found, they are trained on full inputs. Baselines. We compare our results to state-of-the-art activity recognition methods. We train (1) the original I3D <ref type="bibr">[1]</ref> with standard 3D conv. layers. We also train an Inception model with: (2) 3D conv. layers with L = 3, (3) (2+1)D conv. layers, and (4) the proposed iTGM layers. The difference between (1) and (2) is that (1) uses L = 7 in the first 3D conv. layer and L = 3 in all the other 3D layers (a handcrafted design), while (2) uses L = 3 in all its layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>Next, we report the results of the proposed method and compare with baselines and prior work. This is not only done in terms of recognition accuracy but also in terms of computational efficiency. As shown in <ref type="table">Table 7</ref>, our individual models are 4x faster and the ensemble (EvaNet) is 1.6x Two-stream <ref type="bibr">[27]</ref> 59.4 Two-stream+IDT <ref type="bibr">[3]</ref> 69.2 R(2+1)D <ref type="bibr">[33]</ref> 78.7 Two-stream I3D <ref type="bibr">[1]</ref> 80.9 PoTion <ref type="bibr">[2]</ref> 80.9 Dicrim. Pooling <ref type="bibr">[35]</ref> 81.3 DSP <ref type="bibr">[34]</ref> 81. faster than standard methods like ResNet-50. Both of our meta-architectures perform similarly. Below, we report results of the ResNet-like architecture (see suppl. material for further results). HMDB: <ref type="table" target="#tab_1">Table 1</ref> shows the accuracy of the evolved CNNs compared to the baseline architectures, where the evaluation is done on 'split 1'. We see improved accuracy of our individual models as well as ensembles. We also confirm that the EvaNet ensemble is superior to the ensembles obtained by combining other architectures (e.g., 3D ResNet). <ref type="table">Table 2</ref> compares our performance with the previous state-of-the-arts on all three splits following the standard protocols. As seen, our EvaNet models have strong performances outperfoming the state-of-the-art.</p><p>Kinetics: <ref type="table" target="#tab_1">Table 13</ref> shows the classification accuracy of our algorithm on Kinetics-400, and compares with baselines, other ensembles, and the state-of-the-art. The architecture evolution finds better performing models than any prior model. Further the ensemble of 3 models (EvaNet) improves the performance and outperforms other ensembles, including and ensemble of diverse, standard architectures. <ref type="table">Table 3</ref>. Performances on Kinetics-400 Nov. 2018 version. Note that this set is ∼10% smaller (in training/validation set size) than the initial version of Kinetics-400. We report the numbers based on models trained on this newest version. Baselines are shown on top, followed by the state-of-the-arts, and then our methods. Charades: We also test our approach on the popular Charades dataset. <ref type="table" target="#tab_3">Table 4</ref> compares against the previously reported results (we use Kinetics pre-training as in <ref type="bibr">[36]</ref>). As shown, we outperform the state-of-the-art and establish a new one with our EvaNet. Our CNNs only use RGB input (i.e., one-stream) in this experiment.</p><p>Transfer learned architectures -Moments in Time: We evaluate the models evolved on Kinetics by training it on another dataset: Moments in Time <ref type="bibr">[18]</ref>. <ref type="table" target="#tab_4">Table 5</ref> shows the results, where we see that the models outperform prior methods and baselines. This is particularly appealing as the evolution is done on another dataset and successfully transfers to a new dataset. Ensembling and runtime. One key benefit of evolving model architectures is that the resulting models are naturally diverse, as they are evolved from very different initial random models. As shown in <ref type="table" target="#tab_1">Table 13</ref>, we compared with an ensemble of three different baselines (3D Conv + (2+1)D + iTGM) and with an ensemble of different architectures (e.g., I3D + ResNet-50 + ResNet-101). Both are outperformed by EvaNet, although the base models are individually strong.</p><p>Furthermore, our evolved models are very efficient performing inference on a video in ∼100 ms ( <ref type="table">Table 7)</ref>. Note that even an ensemble is faster, 258 ms, than previous individual models which makes the proposed approach very suitable for practical use with higher accuracy and faster runtimes. This gain in runtime is due to the use of parallel shallower layers and the use of iTGM layers, which is by itself faster than prior layers (274ms vs 337ms). Architecture findings. <ref type="figure" target="#fig_0">Figures 1 and 2</ref> show examples of the architectures found. Interesting substructures discovered include: (1) modules combining multiple spacetime pooling layers with different temporal intervals and <ref type="bibr">(2)</ref>  modules heavily relying on Inflated TGM or (2+1)D conv. layers instead of standard 3D conv. layers. Such modules were commonly observed at most of the locations in the architectures, while being very diverse and heterogeneous. Video CNN architectures may evolve differently depending on the datasets. This is as expected, and we were able to explicitly confirm this. The architectures have many more layers with longer space-time filters (e.g., 9 or 11) when evolved for Charades, while they only had a small number of them when evolved for HMDB or Kinetics. An average activity duration in Charades videos are around 12 seconds, while HMDB and Kinetics videos are on the average of 3 to 5 seconds. Different architectures are needed for different datasets/tasks, and we are providing an evolutionary approach to automate the architecture design. <ref type="table">Table 8</ref> further shows that both Inception-like and ResNet-like meta-architectures are successful, and a combination of them is even more successful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>Effectiveness of iTGM models. In <ref type="table">Table 9</ref>, we show the layer statistics for the best models. In the EvaNet architecture, iTGM layers have the longest average length (8.6). Further, our models have quite large temporal resolution of 368 frames on average (compared to I3D/S3D with 99 frames). To further confirm the usefulness of the iTGM layer, we conduct several experiments. In <ref type="table" target="#tab_1">Table 10</ref>, we show the results using iTGM layers with various temporal durations. Since we can increase the temporal length without changing the number of parameters, we can improve performance by simply taking longer temporal durations. We also compare to replacing all iTGM layers with (2+1)D layers and performing the architecture search without the iTGM layer as an option. Both restrictions degrade performance, confirming that iTGMs are needed. We also note that iTGM layers are most common in the best models (Table 9), further confirming their importance.  a model from the Kinetics dataset and 'stretch' the iTGM layers and apply it to Charades, which has activities with much longer temporal duration. In <ref type="table" target="#tab_1">Table 11</ref>, we show the results using models with L = 3 on Kinetics and stretched to L = 11 on Charades, which shows similar performance. Evolution vs. random search. We compared our architecture evolution with random architecture search ( <ref type="figure" target="#fig_6">Figure 7</ref>). We observe that both the evolution and the random search accuracies improve as they explore more samples (benefiting from the search space designed). However, the architecture evolution obtains much higher accuracy and much more quickly with few initial rounds of evolution, suggesting the mutations are being effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present a novel evolutionary algorithm that automatically constructs architectures of layers exploring space-time interactions for videos. The discovered architectures are accurate, diverse and very efficient. Ensembling such models leads to further accuracy gains and yields faster and more accurate solutions than previous state-of-the-art models. Evolved models can be used across datasets and to build more powerful models for video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inception-like meta-architecture</head><p>Similar to <ref type="figure" target="#fig_3">Fig. 4</ref> of the main paper describing ResNetlike meta-architecture, we also describe the Inception-like meta-architecture we used. <ref type="figure" target="#fig_7">Fig. 9</ref> describes the architecture with nine modifiable modules, it has a 'stem' of five modules. Note that the modules are evolved to be heterogeneous, having different combination of multiple conv. and pooling layers for each module. We describe multiple examples of such evolved architectures following the Inception-like meta-architectures in Figs. <ref type="bibr">10, 11, 12, 13, 14, 15, 16, 17, and 18.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evolution training details</head><p>Our architecture evolution was done with 50 parallel workers. Each worker selects S = 25 random samples from the population to generate one new child architecture based on the individual with the highest fitness (i.e., the parent). The architecture is trained using 12 GPUs on the training data. As training video CNNs is computationally expensive, during the search, we train the models with video segments of size 32 × 176 × 176 (for HMDB and Kinetics) or 64 × 176 × 176 (for Charades) where 32 and 64 are the number of frames. We use the batch size of 144 (12 per GPU). Each newly generated child architecture is trained for 1000 iterations (i.e., it looks at 144000 samples), then evaluated with a separate validation set of 1000 examples. The classification accuracy measured using the validation becomes the 'fitness' used in our algorithm. We observed that relative recognition performances of the models (on the validation set) is stable after training for 1000 iterations, and we used this setting in our evolutionary algorithm to reduce the model training time necessary for the architecture evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mutation rate</head><p>In <ref type="figure">Fig. 8</ref>, we compare the architecture evolution done with a constant mutation rate of 1 or 3 (per round) and our annealed mutation rate. As we described in the main section of the paper, our evolutionary algorithm applies a set of random mutation operators at each round. In our annealed mutation rate strategy, the number of the mutation operators to apply is decided based on the evolution round i: it starts with d = 7 mutations initially and it is linearly decreased by i/r where r is 100 in our experimental setting. That is, at the ith round, a total of max( d − i/r , 1) random mutations were applied to the parent. We find that the annealed mutation rate performs the best. Our strategy allows the search to explore more diverse architectures based on the best initial models, but then refine the top performing models after many evolution rounds.  <ref type="figure">Figure 8</ref>. Comparison of the architecture search with various mutation rates. We observe that the constant rate takes longer to reach higher performance while the higher mutation rate initially learns faster, but plateaus at a lower value. Annealing the mutation rate based on the number of architectures evaluated provides the best performance. The x-axis is the number of evolutionary rounds and the y-axis is the accuracy after training for 1000 iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Supplementary results and experiments</head><p>In addition to the experimental results in the main paper, we below provide additional tables comparing our EvaNet against more detailed baselines sharing the same metaarchitecture as our EvaNet. <ref type="table" target="#tab_1">Table 12</ref> compares baseline and EvaNet models only using RGB input, optical flow input, and both. <ref type="table" target="#tab_1">Table 13</ref> illustrates the performances of our EvaNet as well as previous works on two different settings of Kinetics-400. Note that Kinetics is periodically removing some of its training/validation/testing videos, and the accuracies of the approach thus changes depending on which version they were trained/tested on, thus the results are not directly comparable to the ones published on the larger set.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discovered architectures</head><p>We here present diverse architectures evolved in the following figures. The color of each layer corresponds to a specific layer type. Check <ref type="figure" target="#fig_0">Fig. 1</ref> of the main paper for the illustration.</p><p>In <ref type="figure" target="#fig_0">Figures 10, 11</ref>, and 12, we show the Inception-like architectures found when searching on Kinetics using RGB inputs. We observe that the networks learn quite different architectures. For example, the third inception module is quite different in all three networks. In <ref type="figure" target="#fig_0">Figures 13, 14</ref>, and 15, we illustrate the models found when searching on Kinetics using optical flow as input. When using optical flow as input, we observe that the architectures perfer to use layers with shorter temporal durations, using very few layers with size 11 and 9 when compared to the RGB networks. (2+1)D conv layers and iTGM layers were used much more commonly in both RGB and optical flow architectures. Parallel space-time conv and pooling layers with different temporal lengths were also very commonly observed. In <ref type="figure" target="#fig_0">Figures 16, 17, and 18</ref>, we illustrate the Inceptionlike architectures evolved on Charades. We observe that on Charades, the architectures generally capture longer temporal intervals (e.g., the first layer has size 11) and many layers contain longer kernels (i.e., 9 and 11) especially compared to the architectures found on Kinetics.</p><p>In addition, we also illustrate evolved architectures based on our ResNet-like meta-architecture. Similar to the above    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example of a video architecture obtained with evolution. Inception-like architecture. The color encodes the type of the layer, as indicated on the right. The numbers indicate the temporal size of the filters in each module. See text for discussion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Three different ResNet-like architectures obtained for the Kinetics dataset. Modules are repeated R times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The iTGM layer. Example of inflated TGM kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Our ResNet-like 'fill-in-the-blanks' meta-architecture: each heterogeneous module is repeated R times, based on the evolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>A example structure of the a residual Inception module with 4 layer streams. There could be 1-6 parallel streams (with 4 types) and a residual connection from input to output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>(iv) Select a module and change the number of times it is Example mutations applied to a module, including (a) layer type change, (b) filter length change, and (c) layer addition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>'Stretching' of iTGM layer Since the number of parameters of the iTGM layer is independent of length, we use Random search vs. evolutionary algorithm on HMDB.X axis is number of rounds, Y axis is accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Inception-like meta-architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 18 .</head><label>18</label><figDesc>mentioned figures, we show three examples of evolved architectures per Kinetics-RGB and Kinetics-Flow. Figs. 19, Figs. 20, Figs. 21, Figs. 22, Figs. 23, and Figs. 24 show the architectures. Charades RGB Top 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 19 .Figure 20 .</head><label>1920</label><figDesc>Kinetics RGB Top 1 with ResNet Meta-Architecture. Kinetics RGB Top 2 with ResNet Meta-Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 21 .Figure 22 .Figure 23 .</head><label>212223</label><figDesc>Kinetics RGB Top 3 with ResNet Meta-Architecture. Kinetics optical flow Top 1 with ResNet Meta-Architecture. Kinetics optical flow Top 2 with ResNet Meta-Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 Figure 24 .</head><label>124</label><figDesc>Kinetics optical flow Top 3 with ResNet Meta-Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Video</cell><cell></cell><cell>Stem</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BxTxHxWxC</cell><cell>Lx7x7 Stride 2</cell><cell>Lx3x3 Max-Pool Stride 1,2,2</cell><cell>Residual Inception Module 1</cell><cell>Lx3x3 Max-Pool Stride 2,2,2</cell><cell>Residual Inception Module 2</cell><cell>Lx2x2 Max-Pool Stride 2,2,2</cell><cell>Residual Inception Module 3</cell><cell>Lx2x2 Max-Pool Stride 2,2,2</cell><cell>Residual Inception Module 4</cell><cell>2x7x7 Avg-Pool</cell><cell>1x1x1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>HMDB split 1 comparison to baselines, with and without Kinetics pre-training. The models were all initialized with Ima-geNet weights.</figDesc><table><row><cell></cell><cell>HMDB</cell><cell>HMDB(pre-train)</cell></row><row><cell></cell><cell cols="2">RGB Flow RGB+F RGB Flow RGB+F</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell></row><row><cell>I3D</cell><cell cols="2">49.5 61.9 66.4 74.8 77.1 80.1</cell></row><row><cell>3D Conv</cell><cell cols="2">47.4 60.5 65.9 74.3 76.8 79.9</cell></row><row><cell>(2+1)D Conv</cell><cell cols="2">27.8 56.4 51.8 74.4 76.5 79.9</cell></row><row><cell>iTGM Conv</cell><cell cols="2">56.5 62.5 68.2 74.6 76.7 79.9</cell></row><row><cell>3D-Ensemble</cell><cell>67.6</cell><cell>80.4</cell></row><row><cell>iTGM-Ensemble</cell><cell>69.5</cell><cell>80.6</cell></row><row><cell cols="2">Top individual models from evolution</cell><cell></cell></row><row><cell>Top 1</cell><cell cols="2">60.7 63.2 70.3 74.4 78.7 81.4</cell></row><row><cell>Top 2</cell><cell cols="2">63.4 62.5 71.2 75.8 78.4 80.6</cell></row><row><cell>Top 3</cell><cell cols="2">60.5 63.1 70.5 75.4 78.9 79.7</cell></row><row><cell>EvaNet</cell><cell>72.8</cell><cell>82.7</cell></row></table><note>Table 2. HMDB performances averaged over the 3 splits.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Charades classification results against state-of-the-arts.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Accuracy</cell></row><row><cell>3D Conv</cell><cell></cell><cell>72.6</cell></row><row><cell>(2+1)D Conv</cell><cell></cell><cell>74.3</cell></row><row><cell>iTGM Conv</cell><cell></cell><cell>74.4</cell></row><row><cell>ResNet-50 (2+1)D</cell><cell></cell><cell>72.1</cell></row><row><cell>ResNet-101 (2+1)D</cell><cell></cell><cell>72.8</cell></row><row><cell>3D-Ensemble</cell><cell></cell><cell>74.6</cell></row><row><cell>iTGM-Ensemble</cell><cell></cell><cell>74.7</cell></row><row><cell>Diverse Ensemble (3D, (2+1)D, iTGM)</cell><cell></cell><cell>75.3</cell></row><row><cell>Two-stream I3D [1]</cell><cell></cell><cell>72.6</cell></row><row><cell>Two-stream S3D-G [38]</cell><cell></cell><cell>76.2</cell></row><row><cell>ResNet-50 + Non-local [36]</cell><cell></cell><cell>73.5</cell></row><row><cell cols="2">Arch. Ensemble (I3D, ResNet-50, ResNet-101)</cell><cell>75.4</cell></row><row><cell>Top 1 (Individual, ours)</cell><cell></cell><cell>76.4</cell></row><row><cell>Top 2 (Individual, ours)</cell><cell></cell><cell>75.5</cell></row><row><cell>Top 3 (Individual, ours)</cell><cell></cell><cell>75.7</cell></row><row><cell>Random Ensemble</cell><cell></cell><cell>72.6</cell></row><row><cell>EvaNet (Ensemble, ours)</cell><cell></cell><cell>77.2</cell></row><row><cell></cell><cell>mAP</cell></row><row><cell>Two-Stream [25]</cell><cell>18.6</cell></row><row><cell>Two-Stream + LSTM [25]</cell><cell>17.8</cell></row><row><cell>Async-TF [25]</cell><cell>22.4</cell></row><row><cell>TRN [40]</cell><cell>25.2</cell></row><row><cell>Dicrim. Pooling [35]</cell><cell>26.7</cell></row><row><cell>Non-local NN [36]</cell><cell>37.5</cell></row><row><cell>3D-Ensemble (baseline)</cell><cell>35.2</cell></row><row><cell cols="2">iTGM-Ensemble (baseline) 35.7</cell></row><row><cell>Top 1 (Individual, ours)</cell><cell>37.3</cell></row><row><cell>Top 2 (Individual, ours)</cell><cell>36.8</cell></row><row><cell>Top 3 (Individual, ours)</cell><cell>36.6</cell></row><row><cell>EvaNet (Ensemble, ours)</cell><cell>38.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Moments in time. We show that models evolved on Kinetics transfer to similar datasets.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell></row><row><cell>I3D [18]</cell><cell></cell><cell></cell><cell>29.5</cell><cell></cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell>30.5</cell><cell></cell></row><row><cell>ResNet-50 + NL [36]</cell><cell></cell><cell></cell><cell>30.7</cell><cell></cell></row><row><cell cols="4">Arch. Ensemble (I3D, ResNet-50, ResNet-101) 30.9</cell><cell></cell></row><row><cell>Top 1 (Individual, ours)</cell><cell></cell><cell></cell><cell>30.5</cell><cell></cell></row><row><cell cols="2">EvaNet (Ensemble, ours)</cell><cell></cell><cell>31.8</cell><cell></cell></row><row><cell cols="5">Table 6. Test accuracy across datasets for a model evolved on a</cell></row><row><cell>single dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Kinetics Charades HMDB MiT</cell></row><row><cell>Evolved on Kinetics</cell><cell>77.2</cell><cell>37.8</cell><cell>82.3</cell><cell>31.8</cell></row><row><cell>Evolved on Charades</cell><cell>76.5</cell><cell>38.1</cell><cell>81.8</cell><cell>31.1</cell></row><row><cell>Evolved on HMDB</cell><cell>77.0</cell><cell>37.5</cell><cell>82.3</cell><cell>31.6</cell></row><row><cell>Best without evolution</cell><cell>76.2</cell><cell>37.5</cell><cell>81.5</cell><cell>30.7</cell></row><row><cell cols="5">Table 7. Runtime measured on a V100 GPU. Accuracy numbers on</cell></row><row><cell cols="5">Kinetics-400 are added for context. These numbers are evaluation</cell></row><row><cell cols="2">time for 1 128 frame clip at 224x224.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="3">Accuracy Runtime</cell></row><row><cell>I3D</cell><cell></cell><cell>72.6</cell><cell cols="2">337ms</cell></row><row><cell>S3D</cell><cell></cell><cell>75.2</cell><cell cols="2">439ms</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>71.9</cell><cell cols="2">526ms</cell></row><row><cell cols="2">ResNet-50 + Non-local</cell><cell>73.5</cell><cell cols="2">572ms</cell></row><row><cell>I3D iTGM (ours)</cell><cell></cell><cell>74.4</cell><cell cols="2">274ms</cell></row><row><cell cols="2">Individual learned model (ours)</cell><cell>75.5</cell><cell cols="2">108ms</cell></row><row><cell cols="2">EvaNet (Ensemble, ours)</cell><cell>77.2</cell><cell cols="2">258ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Comparison between models from different hybrid metaarchitectures. Kinetics dataset. Statistics of the top models. iTGM layers are most common and have longest temporal duration. Kinetics dataset.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell></cell></row><row><cell cols="6">EvaNet Inception (Ensemble, ours)</cell><cell>76.8</cell><cell></cell></row><row><cell cols="6">EvaNet ResNet (Ensemble, ours)</cell><cell>77.2</cell><cell></cell></row><row><cell cols="6">EvaNet Combined (Ensemble, ours)</cell><cell>77.4</cell><cell></cell></row><row><cell></cell><cell cols="3">Number of Layers</cell><cell></cell><cell cols="2">Ave. Temporal Length</cell><cell></cell></row><row><cell></cell><cell cols="3">3D (2+1)D iTGM</cell><cell>3D</cell><cell cols="3">(2+1)D iTGM Pool</cell></row><row><cell>Top 1</cell><cell>2</cell><cell>6</cell><cell>16</cell><cell>5</cell><cell>7.2</cell><cell>7.2</cell><cell>6.0</cell></row><row><cell>Top 2</cell><cell>6</cell><cell>7</cell><cell>12</cell><cell>7.8</cell><cell>8.1</cell><cell>8.6</cell><cell>5.7</cell></row><row><cell>Top 3</cell><cell>2</cell><cell>6</cell><cell>15</cell><cell>6</cell><cell>7.8</cell><cell>8.5</cell><cell>6.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 .</head><label>10</label><figDesc>Table 11. Stretching iTGM kernels from Kinetics to Charades.</figDesc><table><row><cell cols="2">Experiments evaluating effect of iTGM layer on Kinet-</cell></row><row><cell>ics.</cell><cell></cell></row><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>iTGM (L = 3)</cell><cell>74.4</cell></row><row><cell>iTGM (L = 11)</cell><cell>74.9</cell></row><row><cell>EvaNet replacing iTGM with (2+1)D</cell><cell>76.6</cell></row><row><cell>Arch Search without iTGM in space</cell><cell>76.8</cell></row><row><cell>EvaNet</cell><cell>77.2</cell></row><row><cell>Model</cell><cell>mAP</cell></row><row><cell>iTGM Baseline (L = 3)</cell><cell>33.8</cell></row><row><cell>iTGM Stretched (L = 11)</cell><cell>34.2</cell></row><row><cell>Kinetics EvaNet</cell><cell>37.7</cell></row><row><cell cols="2">Kinetics EvaNet Stretched (L = 11) 38.1</cell></row><row><cell>Charades EvaNet</cell><cell>38.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 12 .</head><label>12</label><figDesc>Kinetics performance comparison to baselines.</figDesc><table><row><cell></cell><cell cols="2">RGB Flow RGB+F</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell></row><row><cell>3D Conv</cell><cell>70.6 62.1</cell><cell>72.6</cell></row><row><cell>(2+1)D Conv</cell><cell>71.1 62.5</cell><cell>74.3</cell></row><row><cell>iTGM Conv</cell><cell>71.2 62.8</cell><cell>74.4</cell></row><row><cell>3D-Ensemble</cell><cell></cell><cell>74.6</cell></row><row><cell>iTGM-Ensemble</cell><cell></cell><cell>74.7</cell></row><row><cell cols="2">Top individual models from evolution</cell><cell></cell></row><row><cell>Top 1</cell><cell>71.9 63.8</cell><cell>76.4</cell></row><row><cell>Top 2</cell><cell>71.7 64.9</cell><cell>75.5</cell></row><row><cell>Top 3</cell><cell>72.9 64.8</cell><cell>75.7</cell></row><row><cell>EvaNet</cell><cell></cell><cell>77.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 14 compares</head><label>14</label><figDesc>EvaNet with various baselines similar to Table 12, this time using the Charades dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Stem</cell><cell></cell><cell></cell></row><row><cell>Video</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BxTxYxXxC</cell><cell></cell><cell>Lx3x3</cell><cell></cell><cell></cell><cell>Lx3x3</cell></row><row><cell></cell><cell>Lx7x7</cell><cell>Max-Pool</cell><cell>1x1x1</cell><cell>Lx7x7</cell><cell>Max-Pool</cell><cell>Inception</cell></row><row><cell></cell><cell>Stride 2</cell><cell>Stride</cell><cell>Stride 1</cell><cell>Stride 1</cell><cell>Stride</cell><cell>Module 1</cell></row><row><cell></cell><cell></cell><cell>1,2,2</cell><cell></cell><cell></cell><cell>1,2,2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lx3x3</cell></row><row><cell>Inception</cell><cell>Inception</cell><cell cols="2">Inception</cell><cell>Inception</cell><cell>Max-Pool</cell><cell>Inception</cell></row><row><cell>Module 2</cell><cell>Module 2</cell><cell cols="2">Module 2</cell><cell>Module 2</cell><cell>Stride</cell><cell>Module 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2,2,2</cell></row><row><cell></cell><cell>Lx2x2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inception Module 2</cell><cell>Max-Pool Stride</cell><cell cols="2">Inception Module 2</cell><cell>Inception Module 2</cell><cell>2x7x7 Avg-Pool</cell><cell>1x1x1</cell></row><row><cell></cell><cell>2,2,2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13 .</head><label>13</label><figDesc>Kinetics-400 accuracy. Note that * are the reported numbers on the initial Kinetics dataset, which is no longer available. We report the numbers based on the new Kinetics version from Nov 2018. The new version has 8% less training/validation videos.</figDesc><table><row><cell>Method</cell><cell cols="2">Kinetics-400</cell></row><row><cell></cell><cell>new</cell><cell>old</cell></row><row><cell>Two-stream I3D [1]</cell><cell cols="2">72.6 74.1  *</cell></row><row><cell>Two-stream (2+1)D [33]</cell><cell>-</cell><cell>75.4  *</cell></row><row><cell>ResNet-50 (2+1)D</cell><cell>72.1</cell><cell>-</cell></row><row><cell>ResNet-101 (2+1)D</cell><cell>72.8</cell><cell>-</cell></row><row><cell cols="3">Two-stream S3D-G [38] 76.2 77.2  *</cell></row><row><cell>Non-local NN [36]</cell><cell>-</cell><cell>77.7  *</cell></row><row><cell>ResNet-50 + Non-local</cell><cell>73.5</cell><cell>-</cell></row><row><cell>EvaNet (ours)</cell><cell>77.2</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 14 .</head><label>14</label><figDesc>Charades performance comparison to baselines, all initialized with ImageNet or Kinetics weights.</figDesc><table><row><cell></cell><cell cols="2">ImageNet Kinetics</cell></row><row><cell>Baselines</cell><cell></cell><cell></cell></row><row><cell>3D Conv</cell><cell>17.2</cell><cell>34.6</cell></row><row><cell>(2+1)D Conv</cell><cell>17.1</cell><cell>34.7</cell></row><row><cell>iTGM Conv</cell><cell>17.2</cell><cell>34.9</cell></row><row><cell>3D-Ensemble</cell><cell>17.4</cell><cell>35.2</cell></row><row><cell>iTGM-Ensemble</cell><cell>17.8</cell><cell>35.7</cell></row><row><cell cols="3">Top individual models from evolution</cell></row><row><cell>Top 1</cell><cell>22.3</cell><cell>37.3</cell></row><row><cell>Top 2</cell><cell>24.1</cell><cell>36.8</cell></row><row><cell>Top 3</cell><cell>23.2</cell><cell>36.6</cell></row><row><cell>EvaNet</cell><cell>26.6</cell><cell>38.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition</title>
		<meeting>the ICCV Workshop on Action, Gesture, and Emotion Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">One hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.07274</idno>
		<title level="m">Initialization strategies of spatio-temporal convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning latent sub-events in activity videos using temporal attention filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyou</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Association for Artificial Intelligence (AAAI)</title>
		<meeting>the American Association for Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporal gaussian mixture layer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka Leon Suematsu Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06371</idno>
		<title level="m">Asynchronous temporal fields for action recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Platform-aware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">C3d: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning discriminative video representations using adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video representation learning using discriminative pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08496</idno>
		<title level="m">Temporal relational reasoning in videos</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition</title>
		<meeting>the ICCV Workshop on Action, Gesture, and Emotion Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">One hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.07274</idno>
		<title level="m">Initialization strategies of spatio-temporal convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning latent sub-events in activity videos using temporal attention filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyou</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Association for Artificial Intelligence (AAAI)</title>
		<meeting>the American Association for Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Temporal gaussian mixture layer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka Leon Suematsu Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06371</idno>
		<title level="m">Asynchronous temporal fields for action recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11626</idno>
		<title level="m">Platform-aware neural architecture search for mobile</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">C3d: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning discriminative video representations using adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Video representation learning using discriminative pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08496</idno>
		<title level="m">Temporal relational reasoning in videos</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Neural architecture search with</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
