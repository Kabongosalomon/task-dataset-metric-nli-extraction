<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wu</surname></persName>
							<email>xdwupeng@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Sun</surname></persName>
							<email>yjsun@stu.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Yang</surname></persName>
							<email>zwyang97@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Violence Detection</term>
					<term>Multimodality</term>
					<term>Weak Supervision</term>
					<term>Re- lation Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Violence detection has been studied in computer vision for years. However, previous work are either superficial, e.g., classification of short-clips, and the single scenario, or undersupplied, e.g., the single modality, and hand-crafted features based multimodality. To address this problem, in this work we first release a large-scale and multi-scene dataset named XD-Violence with a total duration of 217 hours, containing 4754 untrimmed videos with audio signals and weak labels. Then we propose a neural network containing three parallel branches to capture different relations among video snippets and integrate features, where holistic branch captures long-range dependencies using similarity prior, localized branch captures local positional relation using proximity prior, and score branch dynamically captures the closeness of predicted score. Besides, our method also includes an approximator to meet the needs of online detection. Our method outperforms other state-of-the-art methods on our released dataset and other existing benchmark. Moreover, extensive experimental results also show the positive effect of multimodal (audiovisual) input and modeling relationships. The code and dataset will be released in https://roc-ng.github.io/XD-Violence/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Everyone hopes for peaceful life, and it is our duty to safeguard peace and oppose violence. Violence detection <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30]</ref> in videos has been studied in computer vision community for years. However, due to limited application and challenging nature, this specific task received far less attention than other popular tasks, e.g., video classification <ref type="bibr" target="#b41">[42]</ref>, action recognition <ref type="bibr" target="#b10">[11]</ref>, and temporal action detection <ref type="bibr" target="#b11">[12]</ref>, in the past decades. Along with the advance in video technology in recent years, the application of violence detection is becoming more and more extensive. For example, violence detection is not only used for real-world scenarios, e.g., intelligent surveillance, but also used for Internet, e.g., video content review (VCR). Violence detection aims to timely locate the start and the end of violent events with minimum human resource cost.</p><p>The earliest task of violence detection <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15]</ref> can be considered as video classification. Within this context, most methods assume well-trimmed videos, where violent events last for nearly the entire video. However, such solutions restrict their scope to short clips and cannot be generalized to locate violent events in untrimmed videos, therefore they render a limited use in practice. A small step towards addressing violence detection is to develop algorithms to focus on untrimmed videos. Such as the violent scene detection (VSD) task on MediaEval <ref type="bibr" target="#b3">[4]</ref>, and the Fighting detector <ref type="bibr" target="#b29">[30]</ref>. However, assigning frame-level annotations to videos is a time-consuming procedure which is adverse to building large-scale datasets.</p><p>Recently, several research <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b50">51]</ref> focus on weakly supervised violence detection, where only video-level labels are available in the training set. Compared with annotating frame-level labels, assigning video-level labels is labor-saving. Thus, forming large-scale datasets of untrimmed videos and training a datadriven and practical system is no longer a difficult challenge. In this paper, we aim to study weakly supervised violence detection.</p><p>Furthermore, we leverage multimodal cue to address violence detection, namely, incorporating both visual and audio information. Multimodal input is beneficial for violence detection as compared to unimodal input. In most cases, visual cue can precisely discriminate and locate events. At times, visual signals are ineffective and audio signals can separate visually ambiguous events. For example, it is difficult for visual signals to figure out what happen in the violently shaking video accompanied by the sounds of explosion, rather, audio signals are the prime discriminators in this case. Therefore, audiovisual fusion can make full use of complementary information and become an extensive tendency in computer vision and speech recognition communities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>. We are certainly not the first to attempt to detect violence by multimodal signals, there were precedents for multimodal violence detection before, such as, <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b28">29]</ref>. However, the above methods have several drawbacks, e.g., relying on small-scale datasets, using subjective hand-crafted features, and the single scene, that indicates a pragmatic system with high generalization is still in the cradle. Unlike these, we are intended to design a reliable neural network based algorithm on large-scale data.</p><p>To support research toward leveraging multimodal information (vision and audio) to detect violent events in weakly supervised perspective, we first release a large-scale video violence dataset consisting of 4754 untrimmed videos. Unlike previous datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref>, our dataset has audio signals and is collected from both movies and in-the-wild scenarios. With the dataset in hands, we then view weakly supervised violence detection as a multiple instance learning (MIL) task; that is, a video is cast as a bag, which consists of several instances (snippets), and instance-level annotations are learned via bag-level labels. Based on this, we attempt to learn more powerful representations to remedy weak labels. Therefore, we propose a holistic and localized network (HL-Net) that explicitly exploits relations of snippets and learns powerful representations based on these relations, where holistic branch captures long-range dependencies by similarity prior of snippets, and localized branch models short-range interactions within a local neighborhood. In addition, we introduce a holistic and localized cue (HLC) approximator for online violence detection since HL-Net need the whole video to compute relations of snippets. The HLC approximator only processes a local neighborhood and learns precise prediction guided by HL-Net. Even better, HLC approximator brings a dynamic score branch paralleling to holistic branch and localized branch, which computes the response at a position by a weighted sum of all features, and weights depend on predicted scores.</p><p>To summarize, contributions of this paper are threefold, We release a audio-visual violence dataset termed XD-Violence, which consists of 4754 untrimmed videos and covers six common types of violence. To our knowledge, XD-Violence is by far the largest scale violence dataset, with a total of 217 hours. Unlike previous datasets, the videos of XD-Violence are captured from multi scenarios, e.g. movies and YouTube.</p><p>We introduce a HL-Net to simultaneously capture long-range relations and local distance relations, of which these two relations are based on similarity prior and proximity prior, respectively. In addition, we also propose an HLC approximator for online detection. Based on this, we use a score branch to dynamically obtain an additional holistic relation.</p><p>We conducted extensive experiments to verify the effectiveness of our proposed method, and our method shows clear advantage over existing baselines on two benchmarks, i.e., XD-Violence (Ours), and UCF-Crime. Furthermore, experimental results also demonstrate the superiority of multimodal information as compared with the unimodality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Violence detection. In the last years, many researchers proposed different methods for violence detection. For instance, Bermejo et al. <ref type="bibr" target="#b24">[25]</ref> released two well-known fighting datasets. Gao et al. <ref type="bibr" target="#b14">[15]</ref> proposed violent flow descriptors to detect violence in crowded videos. Mohammadi et al. <ref type="bibr" target="#b23">[24]</ref> proposed a new behavior heuristic based approach to classify violent and non-violent videos. Most of prior work utilized hand-crafted features to detect violence on small-scale datasets. Common features include, scale-invariant feature transform (SIFT), spatial-temporal interest point (STIP), histogram of oriented gradient (HOG), histograms of oriented optical flow (HOF), motion intensity, and so on.</p><p>With the rise of deep convolutional neural networks (CNNs), many work have looked into designing effective deep convolutional neural networks for violence detection. For example, Sudhakaran and Lanz <ref type="bibr" target="#b34">[35]</ref> used a convolutional long short-term memory (LSTM) network for the purpose of recognizing violent videos. Similarly, Hanson et al. <ref type="bibr" target="#b12">[13]</ref> built a bidirectional convolutional LSTM architecture for violence detection in videos. Peixoto et al. <ref type="bibr" target="#b27">[28]</ref> used two deep neural network frameworks to learn the spatial-temporal information under different scenarios, then aggregated them by training a shallow neural network to describe violence. Recently, an interesting research <ref type="bibr" target="#b33">[34]</ref> was proposed, which used a scatter net hybrid deep learning network for violence detection in drone surveillance videos.</p><p>There are several attempts to detect violence with multimodality or audio <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. To our knowledge, vast majority of methods use hand-crafted features to extract audio information, e.g., spectrogram, energy entropy, audio energy, chroma, Mel-scale frequency cepstral coefficients (MFCC), zero-crossing rate (ZCR), pitch, etc. Hand-crafted features are easy to extract but are low-level and not robust. Unlike them, our method uses a CNN based model to extract high-level features. Relation networks. Several work apply the graph neural networks (GCNs) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref> over the graph to model relations among different nodes and learn powerful representations for computer vision. For instance, GCN are used for temporal action localization <ref type="bibr" target="#b49">[50]</ref>, video classification <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41]</ref>, anomaly detection <ref type="bibr" target="#b50">[51]</ref>, skeleton-based action recognition <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>, point cloud semantic segmentation <ref type="bibr" target="#b20">[21]</ref>, image captioning <ref type="bibr" target="#b45">[46]</ref>, and so on. Besides GCN, temporal relation networks <ref type="bibr" target="#b51">[52]</ref>, designed to learn and reason about temporal dependencies between video frames, are proposed to address video classification. Recently, self-attention networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref> have been successfully applied in vision problems. An attention operation can affect an individual element by aggregating information from a set of elements, where the aggregation weights are automatically learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">XD-Violence Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Selecting Violence Categories</head><p>The World Health Organization (WHO) defines violence as "the intentional use of physical force or power, threatened or actual, against oneself, another person, or against a group or community, which either results in or has a high likelihood of resulting in injury, death, psychological harm, maldevelopment, or deprivation." Because of the multiple facets of violence, no common and generic enough definition for violent events was ever proposed, even when restricting ourselves to physical violence. However, establishing a clear definition of violence is a key issue because human annotators can rely on a ground truth reference to reduce ambiguity. To mitigate the problem, we consider six physically violent classes, namely, Abuse,Car Accident, Explosion, Fighting, Riot, and Shooting. We take these violence into account due to clear definition, frequent occurrence, widespread use <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>, and adverse impact to safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Collection and Annotation</head><p>Video collection. Previous violence datasets are collected from either movies or in-the-wild scenes, almost no dataset is collected from both. Unlike them, our dataset is collected from both movies and YouTube (in-the-wild scenes). There is a total of 91 movies, of which violent movies are used to collect both violent and non-violent events, and non-violent movies are only used to collect non-violent events. We also collect in-the-wild videos by YouTube. We search and download a mass of video candidates using text search queries. In order to prevent violence detection systems from discriminating violence based on the background of scenarios rather than occurrences, we specifically collect large amounts of non-violent videos whose background is consistent with that of violent videos. A dataset with video-level labels is completed after elaborate efforts of several months. Several example videos from each category are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. More details are given in the supplementary material. Video annotation. Our dataset has a total of 4754 videos, which consists of 2405 violent videos and 2349 non-violent videos. We split it into two parts: the training set containing 3954 videos and the test set including 800 videos, where the test set consists of 500 violent videos and 300 non-violent videos. To evaluate the performance of violence detection methods, we need to make frame-level (temporal) annotations for test videos. To be specific, for each violent video of the test set, we mark the start and ending frames of violent events. As <ref type="bibr" target="#b35">[36]</ref>, we also assign the same videos to multiple annotators to label the temporal extent of each violence and average annotations of different annotators to make final temporal annotations more precise. Both training and test sets contain all 6 kinds of violence at various temporal locations in the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Statistics</head><p>Multi-scenario includes but not limited to the following sources: movies, cartoons, sports, games, music, fitness, news, live scenes, captured by CCTV cameras, captured by hand-held cameras, captured by car driving recorders, etc. (some of them may be overlapped.) We also assign multi violent labels (1 ≤ #labels ≤ 3) to each violent video owing to the co-occurrence of violent events.   The order of labels of each video is based on the importance of different violent events occurring in the video. The distribution of the violent videos in terms of the number of labels is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In addition, our dataset consists of untrimmed videos, therefore, we show the distribution of videos in terms of length in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. We also present the percentage of violence in each test video in <ref type="figure" target="#fig_2">Fig. 3</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dataset Comparisons</head><p>In order to highlight the traits of our dataset, we compare our dataset with other widely-used datasets for violence detection. These datasets can be split into three types: small scale, medium scale, and large scale, of which Hockey <ref type="bibr" target="#b24">[25]</ref>, Movie <ref type="bibr" target="#b24">[25]</ref>, and Violent-Flows <ref type="bibr" target="#b14">[15]</ref> are small-scale, VSD <ref type="bibr" target="#b3">[4]</ref> and CCTV-Fights <ref type="bibr" target="#b29">[30]</ref> are medium-scale, and the remaining UCF-Crime <ref type="bibr" target="#b35">[36]</ref> is large-scale. <ref type="table" target="#tab_1">Table 1</ref> compares several characteristics of these datasets. Our dataset is by far the Overall, our dataset has three good traits: 1) large scale, which is beneficial for training generalizable methods for violence detection; 2) diversity of scenarios, so that violence detection methods actively respond to complicated and diverse environments and are more robust; 3) containing audio-visual signals, making algorithms leverage multimodal information and more confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multimodal Fusion</head><p>Our proposed method is summarized in <ref type="figure" target="#fig_3">Fig. 4</ref>. Consider that we have a training set of videos, we denote an untrimmed video and the corresponding label as V and y, where y ∈ {0, 1}, y = 1 denotes V covers violent events. With a video V in hands, we use feature extractors F V and F A to extract visual and audio feature matrix X V and X A using the sliding window mechanism, where X A are first concatenated in the channels, then the concatenation passes through two stacked fully connected layers (FC) with 512 and 128 nodes respectively, where each FC layer is followed by ReLU and dropout. Finally, the output is taken as the fusion features, which is denoted by X F .</p><formula xml:id="formula_0">X V ∈ R T ×d V , X A ∈ R T ×d A ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Holistic and Localized Networks</head><p>Revisit relations. We first recap the long-range dependencies of neural networks, which can be acquired by two prevalent types of networks, namely, GCNs and non-local networks (NL-Net, self-attention networks). A general graph convolution operation can be formulated as follows <ref type="bibr" target="#b20">[21]</ref>,</p><formula xml:id="formula_1">X l+1 = U pdate Aggregate(X l , W agg l ), W update l<label>(1)</label></formula><p>which contains two essential operations, aggregation and update, and corresponding learnable weights, where the aggregation operation is used to compile information from the global vertices (long-range dependencies), while update functions perform a non-linear transform to compute new representations. An instantiated non-local operation can be formulated as follows <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b46">47]</ref>,</p><formula xml:id="formula_2">X l+1 = sof tmax(X T l W T θ W φ X l )(W g X l ) W ψ<label>(2)</label></formula><p>Although they have different original intentions (GCN is mainly used to address problems with non-Euclidean data, capturing long-range dependencies is an avocation), they are similar on capturing long-range dependencies. Because the term within the outer brackets in Eq. (2) can be viewed as an aggregation operation based on feature similarity, which is followed by the update operation. Holistic branch implementation. Inspired by the GCN for video understanding <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50]</ref>, we define the holistic relation matrix by feature similarity prior, as shown below:</p><formula xml:id="formula_3">A H ij = g (f (x i , x j ))<label>(3)</label></formula><p>where A H ∈ T × T , A H ij measures the feature similarity between the i th and j th features. g is the normalization function, and function f computes the similarity of a pair of features, we define f as follows,</p><formula xml:id="formula_4">f (x i , x j ) = x T i x j x i 2 · x j 2<label>(4)</label></formula><p>we also define other versions of f , and present them on the supplementary material. f bounds the similarity to the range of (0, 1] for the sake of the thresholding that filters weak relations and strengthens correlations of more similar pairs. The thresholding operation can be defined as follows,</p><formula xml:id="formula_5">f (x i , x j ) = f (x i , x j ) f (x i , x j ) &gt; τ 0 f (x i , x j ) ≤ τ<label>(5)</label></formula><p>where τ is the threshold. After that, we adopt the softmax as the normalization function g to make sure the sum of each row of A is 1, as shown in Eq. <ref type="formula" target="#formula_6">(6)</ref>:</p><formula xml:id="formula_6">A H ij = exp(A H ij ) T k=1 exp(A H ik )<label>(6)</label></formula><p>We emphasize that X used in Eq. <ref type="formula" target="#formula_3">(3)</ref> is the concatenation of raw features (X A and X V ) to capture the original feature prior.</p><p>In order to capture long-range dependencies, we follow the GCN paradigm and design the holistic layer as,</p><formula xml:id="formula_7">X H l+1 = Dropout ReLU (A H X H l W H l )<label>(7)</label></formula><p>which allows us to compute the response of a position defined by the similarity prior based on the global filed rather than its neighbors. Localized branch implementation. Holistic branch captures long-range dependencies directly by computing interactions between any two positions, regardless of their positional distance. However, positional distance has positive effects on temporal events detection <ref type="bibr" target="#b50">[51]</ref>, and to retain it, we devise the local relation matrix based on proximity prior as,</p><formula xml:id="formula_8">A L ij = exp −|i − j| γ σ<label>(8)</label></formula><p>which only depends on temporal positions of the i th and j th features, and where γ and σ are hyper-parameters to control the range of influence of distance relation. Likewise, X L l+1 is the output of the (l + 1) th localized layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Online Detection</head><p>As we mentioned, a violence detection system is not only applied for offline detection (Internet VCR), but also online detection (surveillance system). However, online detection by the above HL-Net is impeded by a major obstacle: HL-Net needs the whole video to obtain long-range dependencies. To jump out of the dilemma, we propose an HLC approximator, only taking previous video snippets as input, to generate precise predictions guided by HL-Net. Two stacked FC layers followed by ReLU and a 1D causal convolution layer constitute HLC approximator. The 1D causal convolution layer has kernel size 5 in time with stride 1, sliding convolutional filters over time. The 1D causal convolution layer also acts as the classifier, whose output is the violent activation denoted as C S of shape T . Even better, this operation introduces an additional branch named dynamic score branch to extend HL-Net, which depends on C S . Score branch implementation. The main role of this branch is to compute the response at a position as a weighted sum of the features at all positions, where weights depend on the closeness of the scores. Different from the relation matrices of holistic and localized branches, the relation matrix of score branch is updated in each iteration, and depends on predicted scores rather than the prior. Formally, the relation matrix of score branch is devised as follows,</p><formula xml:id="formula_9">A S ij = ρ 1 − s(C S i ) − s(C S j ) (9) ρ(x) = 1 1 + exp(− x−0.5 0.1 )<label>(10)</label></formula><p>where function s is sigmoid, and function ρ is used to enhance (and weak) the pairwise relation where the closeness of the scores is greater (and less) than 0.5, and softmax is also used for the normalization. Analogously, X S l+1 is the output of the (l + 1) th score layer, where X S 0 = (X H 0 = X L 0 ) = X F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training Based on MIL</head><p>We use an FC layer with 1 node to project the concatenation representations to the label space (1D space), and the violent activations we obtain after this projection can be represented as follows,</p><formula xml:id="formula_10">C P = (X H X L X S )W<label>(11)</label></formula><p>where denotes the concatenation operation, and C P ∈ R T denotes the violent activations.</p><p>Following the principles of MIL <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b26">27]</ref>, we use the average of K-max activation ( C P and C S ) over the temporal dimension rather than the whole activations to compute y P and y S , and K is defined as T q + 1 . The instances corresponding to the K-max activation in the positive bag is most likely to be true positive instances (violence). The instances corresponding to the K-max activation in the negative bag is hard instances. We expect these two types of instances to be as far as possible.</p><p>We define the classification loss, L BCE and L BCE2 , as the binary crossentropy between the predicted labels (y P and y S ) and ground truth y. In addition, we also use the knowledge distillation loss to encourage the output of the HLC approximator to approximate the output of HL-Net. <ref type="bibr" target="#b11">12)</ref> where N is the batch size. Finally, the total loss is the weighted sum of the above three loss, which is shown as follows,</p><formula xml:id="formula_11">L DIST ILL = N j=1 − i s(C P i )log s(C S i ) j<label>(</label></formula><formula xml:id="formula_12">L T OT AL = L BCE + L BCE2 + λL DIST ILL (13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Inference</head><p>Aiming at different requests, our method can choose offline or online manners to efficiently detect violent events. Sigmoid functions follow the violent activations C P and C S and generate the violence confidence (score) that is bounded in the range of [0, 1]. Note that, in the online inference, only HLC approximator works, and HL-Net can be removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Metric</head><p>we utilize the frame-level precision-recall curve (PRC) and corresponding area under the curve (average precision, AP) <ref type="bibr" target="#b29">[30]</ref> rather than receiver operating characteristic curve (ROC) and corresponding area under the curve (AUC) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b42">43]</ref> since AUC usually shows an optimistic result when dealing with class-imbalanced data, and PRC and AP focus on positive samples (violence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>Visual features. We utilize two mainstream networks as our visual feature extractor F V , namely, C3D <ref type="bibr" target="#b37">[38]</ref> and I3D <ref type="bibr" target="#b2">[3]</ref> networks. We extract fc6 features from C3D that is pretrained on the Sports-1M dataset, and extract global pool features from I3D pre-trained on Kinetics-400 dataset. I3D is a two-stream model, therefore, the visual feature has two versions, RGB and optical flow. We use the GPU implementation of TV-L1 <ref type="bibr" target="#b47">[48]</ref> to compute the optical flow. We fix the frame rate to 24 FPS for all videos, and set the length of sliding window as 16 frames.</p><p>Audio features. we leverage the VGGish <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref> network pretrained on a large  YouTube dataset as the audio feature extractor F A due to its remarkable performance on audio classification. The audio is divided into overlapped 960-ms segments, of which each segment has the only corresponding video snippet with aligned end time. The log-mel spectrogram patches of 96 × 64 bins, computed from segments, form the input to VGGish. Unless otherwise stated, we use the fusion of RGB feature of I3D and VGGish feature by default. HL-Net architecture. Generalized HL-Net is formed from holistic branch, localized branch, and an additional score branch. Each branch is a stack of two layers, where the number of output channels for each layer is 32. Furthermore, taking inspiration from <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21]</ref>, we add a residual connection for each layer, which enables GCN to reliably converge in the training stage. Training details. We implement the network based on PyTorch. For hyperparameters, without otherwise stated, we set τ as 0.7, γ and σ of A L as 1, q as 16, dropout rate as 0.7, and λ as 5. For network optimization, Adam is used as the optimizer. The initial learning rate is set as 10 −3 , and is divided by 10 at the 10 th epoch and 30 th epoch. The network is trained for 50 epochs in total, and the batch size is 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>The effect of modality. Most of work still focus on the unimodality for video event detection, and audio-visual input is one advantage of our work. Therefore, we conduct experiments on the XD-Violence dataset to verify the superiority of multimodality. We try five different inputs, i.e., audio modality, optical flow (Flow) modality, RGB modality, the fusion of audio and Flow modalities, the fusion of audio and RGB modalities, (For the unimodality, the two stacked FC layers of multimodal fusion module are still retained.) and report the results in <ref type="figure" target="#fig_4">Fig. 5</ref>. For the unimodality, a key observation is that visual modality is significantly superior to audio, which is not hard to understand: visual signals contain more rich information, which make algorithms see farther and more. More importantly, the fusion of multimodality shows clear advantage over the single modality, to be specific, the fusion of audio and Flow achieves clear improve-ments against the Flow-only input by 3.5% on AP, and the fusion of audio and RGB outperforms the RGB-only input by 3.2% on AP. This matches the expectation that audio provides complementary information for visual signals.</p><p>The effect of holistic, localized and score branches. As depicted in Section 4, there are three parallel branches in our HL-Net. We manually delete one or two of the branches and show their performance in <ref type="table" target="#tab_2">Table 2</ref>. We observe that: 1) three separate branches achieve similar performance; 2) removing any one of the three branches will harm the performance; 3) HL-Net achieves the best performance with all three branches work together. This demonstrates that all branches play an irreplaceable role in our HL-Net.</p><p>Online detection vs Offline detection. We show the performance comparison between online detection and offline detection in <ref type="table">Table 3</ref>. It is observed that offline detection outperforms online detection by 5% on AP. The performance improvement benefits from the powerful ability of HL-Net, i.e., computing the response by three different types of feature aggregation. We also use the models saved in different training stages to inference, and show the performance changing curves. We can see from <ref type="figure" target="#fig_5">Fig. 6</ref> that offline detection outperforms online detection except for in the initial training stage, we argue that this is because HLC approximator is a lightweight module that is easy to train and can find a good solution in the early stage, and HL-Net has relatively more parameters, needing more time to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparisons with State-of-the-Arts</head><p>We compare our method with several baselines on XD-Violence, and show the results in <ref type="table">Table 3</ref>. It is obvious that our method can outperform current state-ofthe-art methods. We also show the performance of the fusion of C3D and audio features in <ref type="table">Table 3</ref>, and observe that C3D is inferior to I3D by a large margin in our violence detection task. Note that all baselines in <ref type="table">Table 3</ref> take the fusion of the RGB features of I3D and VGGish features as input. PRC on the XD-Violence and results on the UCF-Crime dataset are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Qualitative Results</head><p>We present several qualitative examples in <ref type="figure">Fig. 7</ref>. (a)-(c) and (e)-(g) are violent videos, and our method successfully detects violent events. (d) and (h) are nonviolent videos, and our method generates very low violence scores. As can be seen from the 1 st row, multimodal input can localize violence more precisely than the RGB-only input with lower false positives and false negatives, especially in videos with audio. For instance, in (c), Audio+RGB detects the explosion according to not just fire, but explosive sound. From the 2 nd row it is evident that online detection is slightly worse than offline detection, and with higher false alarms, this is because of the lack of contextual information. Besides, an interesting finding is that our method considers there are violent events in the second half of (g), but this part is not marked as violence. After watching the video, we find out the reason: the accident process is finished, but there is still an overturned car. More qualitative results are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we study the large-scale violence detection with audio-visual modalities under weak supervision. Due to lack of applicable datasets, we first release a large-scale violence dataset to fill the gap. Then we propose a method to explicitly modeling relationships among video snippets and learn powerful representations. Extensive experiments show, 1) our dataset is applicable; 2) multimodality significantly improves the performance; 3) explicitly exploiting relations is highly effective. In the further work, we will add some audio-dominated violence classes (e.g., scream), and our research will naturally extend to multiclass violence detection as XD-Violence is a multi-label dataset. In addition, more powerful online detection is yet to be further explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Video Collection</head><p>Our dataset is collected from both movies and YouTube (in-the-wild scenes).</p><p>To generate high-quality video clips from movies, we first search multiple types of movies, e.g., action movies, military movies, blood movies, literary movies, romantic movies, cartoons, etc. Then we invite eight annotators having high levels of computer expertise to watch movies, randomly cut sections of different length that contain clear violent or non-violent events and make video-level labels. Finally, annotators perform two checks to correct wrong videos and remove ill-suited videos annotated by others. We also collect in-the-wild videos by YouTube. We first search and download a mass of video candidates using text search queries. In order to prevent violence detection systems from discriminating violence based on the background of scenarios rather than occurrences, we specifically collect large amounts of non-violent videos whose background is consistent with that of violent videos. After that, we remove videos which fall into any of the following conditions: soundless, only containing background sounds, ambiguity, blurry scenes, and containing very little violence. Besides, we randomly split our dataset into training and test sets, repeat this process multiple times, and keep the best one with suitable proportion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset Comparisons</head><p>We list violence types of common datasets in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Similarity Computation Functions</head><p>Two other versions of f are defined as follows,  [Version 2]</p><formula xml:id="formula_13">f (x i , x j ) = (wx) T i (w x) j (wx) i 2 · (w x) j 2 (1) [Version 3] f (x i , x j ) = exp (x i · x j − max(x i · X))<label>(2)</label></formula><p>From <ref type="table" target="#tab_2">Table 2</ref>, we observe that three versions achieve similar performance, and the Version 2 outperforms other two versions by a narrow margin since the version 2 has learnable weights and can learn better similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Effect of Length of Sampling</head><p>Untrimmed videos have large variance in length, from a few seconds to several hours. On the one hand, we need to process the entire video at once because we only have video-level labels. On the other hand, it is impractical to directly process a very long video due to GPU memory constraints. We use a simple yet effective sampling. Consider a video V and corresponding features X F , we process the entire video if its feature length T is less than the pre-defined Γ length necessary to meet the GPU bandwidth. Otherwise, we uniformly extract a segment of length Γ from X F to represent the whole video. In this paper, we set Γ as 200 because this is a good tradeoff between accuracy and computation burden.</p><p>Results from <ref type="table">Table 3</ref> show that with the increase of threshold, the run time of each training epoch increases, but the performance increases firstly and then fluctuates slightly. Therefore, we choose 200 as the pre-defined Γ in this paper due to the good tradeoff between accuracy and computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Investigating Perclass Performance with Different Multimodal Cues</head><p>Following [?], we show comparison results in <ref type="table">Table 4</ref>. As for per-class breakdown, we observe that 1) compared single signal, Audio+RGB improves the performance of perclass (except for the abuse, possible reason is that the number of abuse samples is small); 2) adding audio gets clear performance boosts for some classes, e.g., Shoot, Riot, Car Accident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Comparisons with State-of-the-Arts</head><p>We compare our method with several baselines on the UCF-Crime dataset, and show the results in <ref type="table" target="#tab_6">Table 5</ref>, respectively. It is obvious that our method can outperform current state-of-the-art methods.</p><p>We also show the PRC on the XD-Violence dataset as <ref type="figure" target="#fig_0">Fig. 1</ref>. As <ref type="figure" target="#fig_0">Fig. 1</ref> shows, the curve of our method completely encloses others, which means our method is superior to the competitors at various thresholds. Besides, online detection and RGB-only do not obtain the maximum area under curve due to lacks of contextual information and audio information, respectively.  <ref type="bibr" target="#b13">[14]</ref> 50.60 Lu et al. <ref type="bibr" target="#b22">[23]</ref> 65.51 Sultani et al. <ref type="bibr" target="#b35">[36]</ref> 75.51 Ours 82.44 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">More Qualitative Results</head><p>We present several qualitative examples in <ref type="figure" target="#fig_1">Fig. 2</ref>. As we can see, RGB-only input produces many false alarms when: scene keeps changing in the live video (a), playing football looks like a fight (b), and an airplane plummet through the sky (c). For the false alarm in (d), we find the possible cause is that there is a mirror on the ceiling, which confuses our method. We argue that the missed alarm of offline detection in (e) is caused by over-smoothing, which usually occurs</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Sample videos from the XD-Violence dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Dataset Statistics. (a) Distribution of the number of videos belonging to each category according to multi-label. (b) Distribution of the number of videos belonging to each category according to the first label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Dataset Statistics. (a) Distribution of videos according to length (minutes). (b) Distribution of violent videos according to percentage of violence (in each video) in test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>T is the length of feature matrix, x V i and x A i are visual and audio features of the i th snippet, respectively.Quite a few fusion manners have been proposed for multimodal input, we there opt to the simple yet effective concatenation fusion. More precisely, X V and The pipeline of our proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>AP comparison of different modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>AP changing curves using models saved in different epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>PRC on the XD-Violence dataset. Qualitative results of our method on test videos. The 1 st row shows qualitative comparisons between Audio+RGB and RGB-only input. The 2 nd row shows qualitative comparisons between offline detection and online detection. Colored window shows the ground truth of violent regions. [Best viewed in color.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of different violence datasets. * means quite a few videos are silent or only contain background music. Yes largest dataset, which is more than 300 times than the total of small datasets, 4 times than the total of medium datasets, and almost 2 times than the UCF-Crime dataset. Besides, variations in scenes of previous datasets are also limited, by contrast, our dataset embraces a wide variety of scenarios. In addition, Hockey, Movie, Violent-Flows and CCTV-Fights are only used for fighting detection. Intriguingly, though Violent-Flows and CCTV-Fights contain audio signals, in fact, there are quite a few videos that are silent or only contain background music, which is inoperative or even harmful for training detection methods taking multimodality as input. The UCF-Crime dataset is used to detect violence in surveillance videos, but lacks audio.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Videos Length</cell><cell>Source of scenarios</cell><cell>#Violence</cell><cell>Audio</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>types</cell><cell></cell></row><row><cell>Hockey [25]</cell><cell>1000</cell><cell>27 min</cell><cell>Ice hockey</cell><cell>1</cell><cell>No</cell></row><row><cell>Movie [25]</cell><cell>200</cell><cell>6 min</cell><cell>Movies and sports</cell><cell>1</cell><cell>No</cell></row><row><cell>Violent-Flows [15]</cell><cell>246</cell><cell>15 min</cell><cell>Streets, school, and sports</cell><cell>1</cell><cell>Yes  *</cell></row><row><cell>CCTV-Fights [30]</cell><cell>1000</cell><cell cols="2">18 hours CCTV and mobile cameras</cell><cell>1</cell><cell>Yes  *</cell></row><row><cell>VSD [4]</cell><cell>25</cell><cell>35 hours</cell><cell>Movies</cell><cell>8</cell><cell>Yes</cell></row><row><cell>UCF-Crime [36]</cell><cell>1900</cell><cell>128 hours</cell><cell>CCTV camera</cell><cell>9</cell><cell>No</cell></row><row><cell>XD-Violence (Ours)</cell><cell>4754</cell><cell>217 hours</cell><cell>Movies, sports, games,</cell><cell>6</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>hand-held cameras, CCTV,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>car cameras, etc.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>AP comparison of different branches. Qualitative results of our method on test videos. Colored window shows the ground truth of violent regions. [Best viewed in color.]</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Table 3. AP comparison on the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">XD-Violence dataset.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Holisitic</cell><cell>Localized</cell><cell cols="2">Score</cell><cell>AP(%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>branch</cell><cell>branch</cell><cell cols="2">branch</cell><cell></cell><cell></cell><cell>Method</cell><cell>AP(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>75.44</cell><cell></cell><cell>SVM baseline</cell><cell>50.78</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76.60</cell><cell></cell><cell>OCSVM [31]</cell><cell>27.25</cell></row><row><cell>(c)</cell><cell>Violence Score</cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell>75.40 77.23 77.05</cell><cell>(c)</cell><cell>Hasan et al. [14] 30.77 Ours (C3D) 67.19 Sultani et al. [36] 73.20 (d)</cell><cell>non-violence</cell></row><row><cell>(f)</cell><cell>Violence Score</cell><cell>(e)</cell><cell></cell><cell></cell><cell>(f)</cell><cell></cell><cell>77.70 78.64</cell><cell>(g)</cell><cell>Ours (Online) Ours</cell><cell>73.67 78.64 (h)</cell><cell>non-violence</cell></row><row><cell>(i)</cell><cell>failure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Score</cell><cell>(a)</cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell>(c)</cell><cell></cell><cell>(d)</cell><cell>non-violence</cell></row><row><cell></cell><cell>Violence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Score</cell><cell>(e)</cell><cell></cell><cell>(f)</cell><cell></cell><cell></cell><cell>(g)</cell><cell></cell><cell>(h)</cell><cell>non-violence</cell></row><row><cell></cell><cell>Violence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>non-violence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Score Fig. 7. (a)</cell><cell>non-violence</cell><cell></cell><cell>(b)</cell><cell cols="2">non-violence</cell><cell>(c)</cell></row><row><cell></cell><cell>Violence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(d)</cell><cell cols="2">non-violence</cell><cell>(e)</cell><cell></cell><cell></cell><cell>(f)</cell></row><row><cell></cell><cell>Score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Violence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of violence types.</figDesc><table><row><cell>Corresponding author: neouma@163.com</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>AP comparison of different similarity computation functions on the XD-Violence dataset.</figDesc><table><row><cell>Function AP (%)</cell></row><row><cell>Version 1 78.64</cell></row><row><cell>Version 2 79.04</cell></row><row><cell>Version 3 77.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Performance comparisons with respect to length of sampling on the XD-Violence dataset. Perclass AP comparison of different multimodal cues.</figDesc><table><row><cell cols="3">Threshold AP (%) Run Time /s</cell></row><row><cell>100</cell><cell>78.30</cell><cell>69</cell></row><row><cell>200</cell><cell>78.64</cell><cell>71</cell></row><row><cell>300</cell><cell>78.04</cell><cell>72</cell></row><row><cell>400</cell><cell>77.94</cell><cell>75</cell></row><row><cell>500</cell><cell>78.32</cell><cell>78</cell></row><row><cell>Class</cell><cell cols="2">Audio RGB Audio+RGB</cell></row><row><cell>Fighting</cell><cell>85.04 85.97</cell><cell>88.02</cell></row><row><cell>Shooting</cell><cell>71.53 83.51</cell><cell>90.30</cell></row><row><cell>Riot</cell><cell>65.07 70.54</cell><cell>76.42</cell></row><row><cell>Abuse</cell><cell>76.48 90.10</cell><cell>83.43</cell></row><row><cell cols="2">Car Accident 65.21 68.83</cell><cell>74.89</cell></row><row><cell>Explosion</cell><cell>68.36 84.04</cell><cell>86.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>AUC comparisons on the UCF-Crime dataset.</figDesc><table><row><cell>Method</cell><cell>AUC (%)</cell></row><row><cell>SVM baseline</cell><cell>50.00</cell></row><row><cell>Hasan et al.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">in GCN. Specifically, the violent features are smoothed by non-violent features since violent segment accounts for a little part of the entire video.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="892" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00932</idno>
		<title level="m">See, hear, and read: Deep aligned representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vsd, a public dataset for the detection of violent scenes in movies: design, annotation, analysis and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Penet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="7379" to="7404" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Audio-visual fusion for detecting violent scenes in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Perantonis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hellenic conference on artificial intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multi-class audio classification method with respect to violent content in movies using bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pikrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 9th Workshop on Multimedia Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="90" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A multimodal approach to violence detection in video sharing sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pikrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3244" to="3247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning individual styles of conversational gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3497" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ava: A video dataset of spatiotemporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bidirectional convolutional lstm for the detection of violence in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pnvr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Violent flows: Real-time detection of violent crowd behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Itcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audio-visual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5492" to="5501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly-supervised violence detection in movies with audio and video based co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Conference on Multimedia</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="930" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Angry crowds: Detecting violent events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Violence detection in video using computer vision techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Nievas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">D</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="332" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech2face: Learning the face behind a voice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7539" to="7548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">W-talc: Weakly-supervised temporal activity localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="563" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Toward subjective violence detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P P</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8276" to="8280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multimodal information fusion and temporal integration for violence detection in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Penet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2393" to="2396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detection of real-world fights in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2662" to="2666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="582" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to localize sound source in visual scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senocak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4358" to="4366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Eye in the sky: Real-time drone surveillance system (dss) for violent individuals identification using scatternet hybrid deep learning network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Omkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1629" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to detect violent videos using convolutional long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Longterm feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast sparse coding networks for anomaly detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107515</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A deep one-class neural network for anomalous event detection in complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Compact generalized nonlocal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6510" to="6519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cassandra: audio-video sensor fusion for aggression detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zajdel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Krijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Andringa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="200" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1237" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
