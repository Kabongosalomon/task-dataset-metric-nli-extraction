<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BERTje: A Dutch BERT Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wietse</forename><surname>De Vries</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CLCG</orgName>
								<orgName type="institution" key="instit2">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Van Cranenburgh</surname></persName>
							<email>a.w.van.cranenburgh@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CLCG</orgName>
								<orgName type="institution" key="instit2">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
							<email>a.bisazza@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CLCG</orgName>
								<orgName type="institution" key="instit2">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
							<email>t.caselli@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CLCG</orgName>
								<orgName type="institution" key="instit2">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
							<email>g.j.m.van.noord@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CLCG</orgName>
								<orgName type="institution" key="instit2">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
							<email>m.nissim@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CLCG</orgName>
								<orgName type="institution" key="instit2">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BERTje: A Dutch BERT Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The transformer-based pre-trained language model BERT has helped to improve state-of-the-art performance on many natural language processing (NLP) tasks. Using the same architecture and parameters, we developed and evaluated a monolingual Dutch BERT model called BERTje. Compared to the multilingual BERT model, which includes Dutch but is only based on Wikipedia text, BERTje is based on a large and diverse dataset of 2.4 billion tokens. BERTje consistently outperforms the equally-sized multilingual BERT model on downstream NLP tasks (part-of-speech tagging, named-entity recognition, semantic role labeling, and sentiment analysis). Our pre-trained Dutch BERT model is made available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the field of computational linguistics there has been a major transition from the development of taskspecific models built from scratch to fine-tuning approaches based on large general-purpose language models <ref type="bibr" target="#b3">(Howard and Ruder, 2018;</ref><ref type="bibr" target="#b12">Peters et al., 2018)</ref>. Currently, the most commonly used pre-trained model of this type is BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>. This model and its derivatives are based on the transformer architecture <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref>. Many state-of-the-art results on benchmark natural language processing (NLP) tasks have been improved by fine-tuned versions of BERT and BERT-derived models.</p><p>The BERT model is pre-trained with two learning objectives that force the model to learn semantic information within and between sentences <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>. The masked language modeling (MLM) task forces the BERT model to embed each word based on the surrounding words. The next sentence prediction (NSP) task, on the other hand, forces the model to learn semantic coherence between sentences. For BERT, NSP is implemented through a binary prediction task where two sentences are either consecutive (positive instance) or the second sentence is completely random (negative instance). It has however been shown that this method is ineffective <ref type="bibr" target="#b8">(Liu et al., 2019)</ref>. The NSP was intended to learn inter-sentence coherence, but apparently BERT actually learned topic similarity. Indeed, if the next sentence is random, it is not just a matter of coherence: crucially, the topic is likely different. Because of this, the authors of RoBERTa removed the NSP task from the pre-training process <ref type="bibr" target="#b8">(Liu et al., 2019)</ref>. The developers of ALBERT, instead, implemented a different solution by replacing the NSP task with a sentence order prediction (SOP) task <ref type="bibr" target="#b6">(Lan et al., 2019)</ref>. In SOP, two sentences are either consecutive or swapped. This change has resulted in improved downstream task performance.</p><p>The success of BERT on NLP tasks has mostly been limited to the English language since the main BERT model is trained on English <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>. For other languages, one could either train language-specific models with the same BERT architecture, or use the existing multilingual BERT model. 1 This is a model trained on all Wikipedia pages of 104 different languages, including Dutch. However, a monolingual model may perform better at tasks in a specific language and Wikipedia is a specific domain that is not representative of general language use. Monolingual models with the BERT architecture have been developed for Italian <ref type="bibr" target="#b13">(Polignano et al., 2019)</ref>, French <ref type="bibr" target="#b7">(Le et al., 2019)</ref>, German, 2 Finnish <ref type="bibr" target="#b18">(Virtanen et al., 2019)</ref>, and Japanese. 3 The Italian model is pre-trained on Twitter data, which may not be representative for general use of language and is only trained on the MLM objective, as the NSP task is barely applicable to tweets. The other models are pre-trained on a combination of Wikipedia with additional data from for instance online news articles. <ref type="bibr">4</ref> To demonstrate the effectiveness of using multi-genre data in a monolingual model, and to equip NLP research on Dutch with a high-performing model, we developed a Dutch BERT model which we call BERTje. 5 In this paper we describe the training process of BERTje and evaluate its performance by fine-tuning the model on several Dutch NLP tasks. We compare the performance on all tasks to that achieved using multilingual BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pre-training data and parameters</head><p>To facilitate comparison and due to limited resources, we opt to train a single Dutch BERT-based model that is architecturally equivalent to the BERT BASE model with 12 transformer blocks <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>. However, the pre-training data is of course different and other pre-training data generation modifications were made based on later derivations of BERT. Nevertheless, we aimed to collect a dataset of similar size and diversity as used for the English BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>For pre-training, we combined several corpora of high quality Dutch text, listed below. The sizes in parentheses are the uncompressed text sizes after cleaning. Documents that originate from chats or Twitter were removed from the SoNaR corpus because of quality considerations. We also removed the Wikipedia documents from SoNaR to avoid overlap with the full Wikipedia dump. Finally, in order to avoid any overlap with texts that we want to use as test data, we removed all documents from SoNaR-500 that are included in the manually annotated SoNaR-1 and Lassy Small <ref type="bibr" target="#b9">(van Noord et al., 2013)</ref> datasets. As a result, the final pre-training dataset contains 12GB of uncompressed text which amounts to about 2.4B tokens.</p><p>Like BERT, we constructed a WordPiece vocabulary with a vocabulary size of 30K tokens. A Sentence-Piece model <ref type="bibr" target="#b5">(Kudo and Richardson, 2018)</ref> was created for this based on the raw pre-training dataset. The resulting vocabulary is translated to WordPiece format for compatibility with the original BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-training procedure</head><p>BERT was pre-trained with two objectives: next sentence prediction (NSP) and masked language modeling (MLM). Based on findings after the initial release of BERT, we made modifications in the pre-training data generation procedure for both tasks.</p><p>Because of the demonstrated ineffectiveness of the NSP task during pre-training, BERTje is trained with the SOP objective. This means that the second sentence in each training example is either the next or the previous sentence. We also apply a different strategy for the MLM objective. Many words are split into multiple WordPiece tokens and some suffixes of words are too easy to predict <ref type="bibr" target="#b6">(Lan et al., 2019)</ref>. Therefore, instead of randomly masking single word pieces, we mask consecutive word pieces that belong to the same word. We masked 15% of all tokens using this strategy. Of these selected tokens, 80% are replaced with a special mask token, 10% are replaced by a completely random token, and 10% are left as-is. This strategy is used to ensure that the model also accurately embeds unmasked words.</p><p>BERTje is pre-trained for 1 million iterations. To gauge the effect of the number of iterations on the performance of downstream tasks, we also evaluate fine-tuning performance at the 850k iterations checkpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tasks and test data</head><p>To evaluate the effectiveness of BERTje for use on downstream tasks, the model is fine-tuned for several NLP tasks. We use annotated data from three sources for this.</p><p>First, we use the Dutch CoNLL-2002 named-entity recognition (NER) data (Tjong Kim <ref type="bibr" target="#b16">Sang, 2002)</ref>. This is a four-class BIO-encoded named-entity classification task with the following four classes: person, organisation, location and miscellaneous. Second, we evaluate on the 16 universal part-of-speech (POS) tags in the Lassy Small treebank <ref type="bibr" target="#b9">(van Noord et al., 2013)</ref> part of Universal Dependencies v2.5 <ref type="bibr" target="#b20">(Zeman et al., 2019)</ref>. Both datasets are already split into train, development, and test sets.</p><p>Third, we evaluate on several classification tasks that originate from the SoNaR-1 corpus of written Dutch <ref type="bibr" target="#b1">(Delaere et al., 2009)</ref>. We evaluate on token-level NER (6 labels), coarse POS tags (12 labels) and fine-grained POS tags (241 labels in total of which 223 are present in the training data). The fine-grained POS tags contain many labels that are used only once. In addition to NER and POS tags, we extract three other annotation types from SoNaR-1:</p><p>Semantic Role predicate-argument structures: The semantic role label annotations in SoNaR contain predicate-argument relations. We only extract predicate and argument labels. Just the highest level labels are used, so entire subordinate clauses are considered to be a single argument and the arguments and predicates within these subordinate clauses are ignored. Semantic Role modifiers: Modifier phrases for semantic roles are often short and non-overlapping. The labels for this task are the modifier phrase types, regardless of the predicate they belong to. Spatio-temporal Relations: A subset of spatio-temporal annotations are extracted including geographical relations and verb tenses.</p><p>Each of these annotations are flattened from hierarchical annotations to token-level classifications. We use 80% of the resulting documents for training, 10% for validation, and 10% for testing. The extracted annotations are split on document level, so there is no document overlap between splits. Each of the previous tasks describe a low level linguistic type of information. However, we want to test BERTje on a more high-level, downstream task, such as sentiment analysis. For this, we use the 110k Dutch Book Reviews Dataset (van der Burgh and Verberne, 2019), a balanced collection of positive and negative reviews which lends itself to a binary sentiment classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>For each of the previously described tasks, three models are fine-tuned: multilingual BERT base, BERTje at the 850K checkpoint (BERTje 850k ) and the fully trained BERTje model (1M checkpoint). All models are fine-tuned for four epochs on the training data for each task with the same hyperparameters. Longer training has shown degradation of performance on some validation data and increase of performance after the fourth epoch has not been observed.    Part-of-Speech tagging <ref type="table" target="#tab_1">Table 2</ref> illustrates POS tagging performance of our models. BERTje does outperform multilingual BERT consistently, but the 850K checkpoint model appears to perform just as well as the fully trained BERTje model. For all three tag sets, the difference between the 850K checkpoint and the fully pre-trained BERTje model is at most 0.3 percentage points. This indicates that the model has already learned the relevant information before the 850K checkpoint. This is important to acknowledge since the previously mentioned NER results shows that the model does learn new information that is relevant for named-entity recognition after this checkpoint. For the Lassy Small dataset, BERTje outperforms the 95.98% accuracy score achieved by UDPipe 2.0 <ref type="bibr" target="#b14">(Straka, 2018)</ref>. These scores are not strictly comparable, since they evaluate on UD 2.2, while we evaluate on UD 2.5; however, the differences can be assumed to be minimal. <ref type="table" target="#tab_2">Table 3</ref> show that BERTje outperforms multilingual BERT for the semantic role labeling (SRL) and spatio-temporal relation (STR) based test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named-Entity Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Roles and Spatio-Temporal Relations The results in</head><p>However, for these tasks the model has not really improved after the 850K checkpoint. For evaluation of the SRL data, the CoNLL-2002 evaluation script is used in order to take chunk overlap of multi-token expressions into account. The results on these tasks are stand-alone since we are not aware of the existence of similar systems.</p><p>The results in <ref type="table" target="#tab_0">Table 1 and Table 3</ref> both show a similar pattern where the scores on training data are higher than the development and test results. This indicates that BERTje may be prone to overfitting just like other models. Therefore, hyper-parameter tuning for specific tasks may help to improve performance. <ref type="table" target="#tab_3">Table 4</ref> shows the sentiment analysis accuracy scores on the 110k Dutch Book Reviews Dataset. Without hyperparameter tuning, BERTje comes close to the 93.8% score that van der Burgh and Verberne (2019) obtain with manual hyperparameter tuning of an ULMFiT model <ref type="bibr" target="#b3">(Howard and Ruder, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have successfully pre-trained, fine-tuned and evaluated a Dutch BERT-based model called BERTje. This model consistently outperforms multilingual BERT on word-level NLP tasks. Even though multilingual BERT has been shown to perform well on Dutch NLP tasks <ref type="bibr" target="#b19">(Wu and Dredze, 2019)</ref>, our results indicate that a monolingual model should be preferred.</p><p>In addition to the comparison with multilingual BERT, we see that lower level linguistic structure like part-of-speech tags appear to be learned earlier during pre-training than higher level information. Low-level linguistic tasks do not benefit from longer pre-training after 850K epochs, but the higher-level entity recognition task does benefit from longer pre-training. This gives an indication that higher level structures in language are only properly learned after lower level structures have been encoded. Therefore, it is important that large pre-trained language models are trained for enough iterations to properly encode high level structures. It has been observed that English BERT encodes higher level linguistic structures in later layers <ref type="bibr" target="#b4">(Jawahar et al., 2019;</ref><ref type="bibr" target="#b15">Tenney et al., 2019)</ref> and this may be the case for BERTje too.</p><p>In future work, the encoding of different layers of linguistic abstraction within BERTje should be explored in order to fully understand and evaluate how well BERTje has learned different types of information. It also needs to be investigated how well BERTje performs on sentence-level tasks that require coherence information between sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Books: a collection of contemporary and historical fiction novels (4.4GB) • TwNC (Ordelman et al., 2007): a Multifaceted Dutch News Corpus (2.4GB) • SoNaR-500 (Oostdijk et al., 2013): a multi-genre reference corpus (2.2GB) • Web news: all articles of 4 Dutch news websites from January 1, 2015 to October 1, 2019 (1.6GB) • Wikipedia: the October 2019 dump (1.5GB)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>shows the span-based F1 scores of the fine-tuned models. For both the CoNLL-2002 data as well as the SoNaR-1 data, it is clear that BERTje outperforms the multilingual BERT model. Additionally, the BERTje model has improved after the 850K checkpoint. Our models do not outperform the state-of-the-art test score of 90.9% of Wu and Dredze (2019) on the CoNLL-2002 test data. This model is a well optimized fine-tuned large multilingual BERT model. Based on the performance difference between multilingual BERT and BERTje, it is likely that replicating their approach with a monolingual Dutch BERT model would improve the state-of-the-art performance.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">CoNLL-2002</cell><cell></cell><cell>SoNaR-1</cell></row><row><cell>Model</cell><cell></cell><cell cols="6">Train Dev Test Train Dev Test</cell></row><row><cell cols="2">Wu and Dredze (2019)</cell><cell>-</cell><cell>-</cell><cell>90.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Multilingual BERT</cell><cell cols="6">95.4 81.3 80.7 95.3 85.0 79.7</cell></row><row><cell cols="2">BERTje 850k</cell><cell cols="6">97.7 87.7 87.6 95.9 85.2 81.1</cell></row><row><cell>BERTje</cell><cell></cell><cell cols="6">98.0 87.8 88.3 96.8 86.1 82.1</cell></row><row><cell cols="8">Table 1: NER F1 scores according to the CoNLL-2002 evaluation script (Tjong Kim Sang, 2002).</cell></row><row><cell></cell><cell cols="2">UD-LassySmall</cell><cell cols="5">SoNaR-1 (coarse) SoNaR-1 (fine-grained)</cell></row><row><cell>Model</cell><cell cols="7">Train Dev Test Train Dev Test Train Dev</cell><cell>Test</cell></row><row><cell cols="8">Multilingual BERT 95.1 92.9 92.5 99.7 98.1 98.3 98.8 96.4</cell><cell>96.2</cell></row><row><cell>BERTje 850k</cell><cell cols="7">99.6 96.8 96.6 99.8 98.6 98.6 99.4 97.0</cell><cell>96.6</cell></row><row><cell>BERTje</cell><cell cols="7">99.6 96.7 96.3 99.8 98.6 98.5 99.5 97.0</cell><cell>96.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Part-of-speech tagging accuracy scores for Lassy Small and SoNaR.</figDesc><table><row><cell></cell><cell cols="2">SRL Predicate-arguments</cell><cell>SRL Modifiers</cell><cell>STR</cell></row><row><cell>Model</cell><cell>Train Dev</cell><cell>Test</cell><cell cols="2">Train Dev Test Train Dev Test</cell></row><row><cell cols="2">Multilingual BERT 90.8 79.3</cell><cell>80.4</cell><cell cols="2">77.5 61.8 62.4 67.9 63.0 57.3</cell></row><row><cell>BERTje 850k</cell><cell>96.4 84.0</cell><cell>85.2</cell><cell cols="2">88.5 66.0 67.3 81.9 65.6 62.5</cell></row><row><cell>BERTje</cell><cell>96.3 84.3</cell><cell>85.3</cell><cell cols="2">88.5 66.2 67.2 81.9 68.5 64.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Semantic Role Labeling (SRL) F1 scores according to the CoNLL-2002 evaluation script (Tjong Kim Sang, 2002) and Spatio-Temporal Relation (STR) macro F1 scores.</figDesc><table><row><cell>Model</cell><cell cols="2">Train Test</cell></row><row><cell>ULMFiT, van der Burgh and Verberne (2019)</cell><cell>-</cell><cell>93.8</cell></row><row><cell>Multilingual BERT</cell><cell cols="2">86.5 89.1</cell></row><row><cell>BERTje 850k</cell><cell cols="2">93.8 92.8</cell></row><row><cell>BERTje</cell><cell cols="2">93.6 93.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Sentiment Analysis accuracy scores on the 110k Dutch Book Reviews Dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://deepset.ai/german-bert 3 https://github.com/cl-tohoku/bert-japanese 4 A monolingual Dutch model has also been made available at http://textdata.nl, but this this model was consistently significantly outperformed by multilingual BERT in our experiments.5  The suffix -je is used to form diminutives in Dutch; it is also used with names in an affectionate sense. BERTje is pronounced ["bErÙ@].</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Daniel de Kok for sharing the Wikipedia data. BERTje was trained with Cloud TPUs from Google's TensorFlow Research Cloud (TFRC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The merits of universal language model fine-tuning for small datasets-a case with Dutch book reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzan</forename><surname>Benjamin Van Der Burgh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verberne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arXiv preprint 1910.00896</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cultivating trees: Adding several semantic layers to the Lassy treebank in SoNaR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Delaere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronique</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Monachesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International workshop on Treebanks and Linguistic Theories (TLT-7)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
		<respStmt>
			<orgName>LOT (Landelijke Onderzoekschool Taalwetenschap</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1356</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP: System Demonstrations</title>
		<meeting>EMNLP: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Albert: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno>1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, and Didier Schwab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Vial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jibril</forename><surname>Frej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Segonne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>FlauBERT: Unsupervised language model pre-training for French</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jelmer van der Linde, Ineke Schuurman, Erik Tjong Kim Sang, and Vincent Vandeghinste</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gosse</forename><surname>Gertjan Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniël</forename><surname>Van Eynde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Kok</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-30910-6_9</idno>
	</analytic>
	<monogr>
		<title level="m">Essential Speech and Language Technology for Dutch: Results by the STEVIN programme</title>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="147" to="164" />
		</imprint>
	</monogr>
	<note>Peter Spyns and Jan Odijk</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The construction of a 500-million-word reference corpus of contemporary written Dutch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Reynaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Véronique</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ineke</forename><surname>Schuurman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-30910-6_13</idno>
	</analytic>
	<monogr>
		<title level="m">Essential Speech and Language Technology for Dutch: Results by the STEVIN programme</title>
		<editor>Peter Spyns and Jan Odijk</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="219" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TwNC: a multifaceted Dutch news corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Roeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franciska</forename><forename type="middle">M G</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrianus</forename><forename type="middle">J</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H W</forename><surname>Van Hessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hondorp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ELRA Newsletter</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Alberto: Italian BERT language understanding model for NLP challenging tasks based on tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Polignano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierpaolo</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>De Gemmis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Italian Conference on Computational Linguistics</title>
		<meeting>the Sixth Italian Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UDPipe 2.0 prototype at CoNLL 2018 UD shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-2020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="197" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT rediscovers the classical NLP pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1452</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jouni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<idno>1912.07076</idno>
		<title level="m">Multilingual is not enough: BERT for Finnish</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="833" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal dependencies 2.5. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (ÚFAL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Faculty of Mathematics and Physics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
