<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Look-into-Object: Self-supervised Structure Modeling for Object Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
							<email>ylbai@outlook.comwzhang.cu@gmail.comtjzhao@hit.edu.cntmei@jd.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD AI Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Look-into-Object: Self-supervised Structure Modeling for Object Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most object recognition approaches predominantly focus on learning discriminative visual patterns while overlooking the holistic object structure. Though important, structure modeling usually requires significant manual annotations and therefore is labor-intensive. In this paper, we propose to "look into object" (explicitly yet intrinsically model the object structure) through incorporating selfsupervisions into the traditional framework. We show the recognition backbone can be substantially enhanced for more robust representation learning, without any cost of extra annotation and inference speed. Specifically, we first propose an object-extent learning module for localizing the object according to the visual patterns shared among the instances in the same category. We then design a spatial context learning module for modeling the internal structures of the object, through predicting the relative positions within the extent. These two modules can be easily plugged into any backbone networks during training and detached at inference time. Extensive experiments show that our lookinto-object approach (LIO) achieves large performance</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object recognition is one of the most fundamental tasks in computer vision, which has achieved steady progress with the efforts from deep neural network design and abundant data annotations. However, recognizing visually similar objects is still challenging in practical applications, es- pecially when there exist diverse visual appearances, poses, background clutter, and so on.</p><p>Suffering from complex visual appearance, it is not always reliable to correctly recognize objects purely based on discriminative regions, even with a large-scale humanlabeled dataset. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, a well-trained ResNet-50 (the third column) can still misclassify objects by looking at the wrong parts.</p><p>Existing object recognition approaches can be roughly grouped into two groups. One group optimizes the network architecture to learn high-quality representations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref>, while the other line of research introduces extra modules to highlight the salient parts explicitly (by boundingbox <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>) or implicitly (by attention <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36]</ref>). Apparently, the latter one costs more on either annotation (e.g. bounding boxes / part locations) or calculation (attentions /  <ref type="figure">Figure 2</ref>. Our proposed Look-into-Object (LIO) approach. Object Extent Learning (OEL) and Spatial Context Learning (SCL) enforce the backbone to learn object extent and internal structure respectively. detection modules). However, all these methods predominantly focus on learning salient patterns while ignoring the holistic structural composition.</p><p>In this paper, we argue that correctly identifying discriminative regions largely depends on the holistic structure of objects. Traditional deep learning based methods can be easily fooled in many cases, e.g., distinguishing front and rear tires of a car, localizing legs of a bird among twigs. It is mainly due to the lack of cognitive ability for structures of objects. Therefore, it is crucial to learn the structure of objects beyond simple visual patterns. Though important, it still remains challenging to systematically learn the object structural composition, especially without additional annotation and extra inference time cost.</p><p>In this work, we propose to model the holistic object structure without additional annotation and extra inference time. Specifically, we propose to "look-into-objects" (in short "LIO) to understand the object structure in images by automatically modeling the context information among regions. From the psychological point of view, recognizing an object can be naturally regarded into two stages: 1) roughly localizing the object extent (the whole extent of the object rather than object part) in the image, and 2) parsing the structure among parts within the object.</p><p>Accordingly, we design two modules to mimic such a psychological process of object recognition. We propose a novel and generic scheme for object recognition by embedding two additional modules into a traditional backbone network, as shown in <ref type="figure">Fig. 2</ref>. The first one is Object-Extent Learning Module (OEL) for object extent localization, while the second is Spatial Context Learning Module (SCL) for structure learning within the object.</p><p>Naturally, a prerequisite for object structure modeling is that the object extent can be localized. The OEL module enforces the backbone to learn object extent using a pseudo mask. We first measure the region-level correlation between the target image and other positive images in the same category. The regions belonging to the main object would have high correlations, owing to the commonality among images from the same category. As a result, a pseudo mask of object extent can be constructed according to the correlation scores without additional annotation besides the original image labels. Then, the backbone network is trained to regress the pseudo mask for localizing the object. With the end-to-end training, the capacity of object-extent localization for backbone network can be further reinforced.</p><p>The SCL module predicts the spatial relationships among regions within the object extent in a self-supervised manner. Given the localized extent learned by the OEL module, the SCL mainly focuses on the internal structure among regions. Specifically, we enforce the backbone network to predict the relative polar coordinates among pairs of regions, as shown in <ref type="figure">Fig. 2</ref>. In this way, the structural composition of object parts can be modeled. This selfsupervised signal can benefit the classification network for the object structure understanding by end-to-end training. Obviously, localize the discriminative regions in a wellparsed structure is much easier than in the raw feature maps.</p><p>Note that all these modules take the feature representations generated by the classification backbone network as input and operate at a regional level, which leads to a delicate Look-into-Object (LIO) framework. Training with such objectives enforces the feature learning of the backbone network by the end-to-end back-propagation. Ideally, both object extent and structure information can be injected into the backbone network to improve object recognition without additional annotations. Furthermore, both modules can be disabled during inference time.</p><p>The main contributions can be summarized as follows: 1. A generic LIO paradigm with two novel modules: object-extent learning for object-extent localization, and self-supervised spatial context learning module for modeling object structural compositions.</p><p>2. Experimental results on generic object recognition, fine-grained recognition, object detection, and semantic segmentation tasks demonstrate the effectiveness and generalization ability of LIO.</p><p>3. From the perspective of practical application, our proposed methods do not need additional annotation and introduce no computational overhead at inference time. Moreover, the proposed modules can be plugged into any CNN based recognition models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generic Object Recognition: General image classification was popularized by the appearance of ILSVRC <ref type="bibr" target="#b26">[27]</ref>. With the extraordinary improvement achieved by AlexNet <ref type="bibr" target="#b31">[32]</ref>, deep learning wave started in the field of computer vision. Since then, a series works, e.g. VGGNet <ref type="bibr" target="#b29">[30]</ref>, GoogLeNet <ref type="bibr" target="#b32">[33]</ref>, ResNet <ref type="bibr" target="#b12">[13]</ref>, Inception Net <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>, SENet <ref type="bibr" target="#b14">[15]</ref>, etc. are proposed to learn better representation for image recognition.</p><p>However, general object recognition models still suffer from easy confusion among visually similar objects <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>The class confusion patterns usually follow a hierarchical structure over the classes. General object recognition networks usually can well separate high-level groups of classes, but it is quite costly to learn specialized feature detectors that separate individual classes. The reason is that the global geometry and appearances of the classes in the same hierarchy can be very similar. As a result, how to identify their subtle differences in the discriminative regions is of vital importance. Fine-Grained Object Recognition: Different from general object recognition, delicate feature representation of object parts play a more critical role in fine-grained object recognition. Existing fine-grained image classification methods can be concluded in two directions. The first one is to enhance the detailed feature representation ability of the backbone network <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>. The second one is to introduce part locations or object bounding box annotations as an additional optimization objective or supervision besides basic classification network <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. Similar to general object recognition, deep learning based feature representations achieved great success on fine-grained image recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>. After that, secondorder bilinear feature representation learning methods <ref type="bibr" target="#b20">[21]</ref> and a series of extensions <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42]</ref> were proposed for learning local pairwise feature interactions in a translation invariant manner.</p><p>However, recognizing objects from a fine-grained category requires the neural network to focus more on the discriminative parts <ref type="bibr" target="#b39">[40]</ref>. To address this problem, a large amount of part localization based fine-grained recognition methods are proposed. Most of these methods applied attention mechanism to obtain discriminative regions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>. Zheng et al. <ref type="bibr" target="#b43">[44]</ref> tried to generate multiple parts by clustering, then classified these parts to predict the category. Compared with earlier part based methods, some recent works tend to use weak supervisions or even no annotation of parts or key areas <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41]</ref>. In particular, Peng et al. <ref type="bibr" target="#b25">[26]</ref> proposed a part spatial constraint to make sure that the model could select discriminative regions, and a specialized clustering algorithm is used to integrate the features of these regions. Yang et al. <ref type="bibr" target="#b40">[41]</ref> introduced a method to detect informative regions and then scrutinizes them for final predictions. These previous works aim to search for key regions from pixel-level images directly. However, to correctly detect discriminative parts, the deep understanding of the structures of objects and the spatial contextual information of key regions are essential. In turn, the location information of regions in images can enhance the visual representation of neural networks <ref type="bibr" target="#b23">[24]</ref>, which has been demonstrated on unsupervised feature learning.</p><p>Different from previous works, our proposed method focuses on modeling spatial connections among object parts for understanding object structure and localizing discriminative regions. Inspired by the studies that contextual information among objects influences the accuracy and efficiency of object recognition <ref type="bibr" target="#b13">[14]</ref>, the spatial information among regions within objects also benefits the localization of discriminative regions. Thus we introduce two modules in our proposed method; the first one aims to detect the main objects, and the second one inferences the spatial dependency among regions in objects. The experimental results show that our method can improve the performance of both general object recognition and fine-grained object recognition. Moreover, our method has no additional overhead except the backbone network feedforward during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we introduce our proposed LIO approach. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, our network is mainly organized by three modules: Given an image I and its ground truth one-hot label l, we can get the feature maps f (I) of size N × N × C from one of the convolutional layers, and the probability vector y(I) from the classification network. C is the channel size of that layer, and N × N is the size of each feature map in f (I). The loss function of the classification module (CM) L cls can be written as:</p><formula xml:id="formula_0">L cls = − I∈I l · log y(I),<label>(1)</label></formula><p>where I is the image set for training.</p><p>The object-extent learning module and spatial context learning module are designed to help our backbone classification network learn representations beneficial to structure understanding and object localization. These two modules are light-weighted, and only a few learnable parameters are introduced. Furthermore, OEL and SCL are disabled at inference time, and only the classification module is needed for computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Object-Extent Learning (OEL)</head><p>Localizing the extent of the object in an image is a prerequisite for understanding the object structure. A typical approach is to introduce bounding boxes or segmentation</p><formula xml:id="formula_1">... 1 ′ ′ Positive Images ′ ... ... ℒ ℒ ℒ Spatial Context Learning Object Extent Learning ( , ′) ′( ) ( ) ( ) Classification Module Backbone ( ) ′, ′ 1 ′ ′ ( 1 ′ ) ( ′ ) Backbone Backbone</formula><p>Inactive during backpropagation Supervision for each module annotations, which cost much on data collection. For typical image recognition task that lacks localization or segmentation annotations, we propose a new module called Object-Extent Learning to help the backbone network distinguish the foreground and background. We can partition the feature maps f (I) into N × N feature vector f (I) i,j ∈ R 1×C , where i and j are the horizontal and vertical indices respectively (1 ≤ i, j ≤ N ). Each feature vector centrally responds to a certain region in input image I.</p><p>Inspired by the principle that objects in the image from the same category always share some commonality, and the commonality, in turn, help the model recognize objects, we sample a positive image set I = {I 1 , I 2 , · · · , I P } with the same label l of image I, and then measure the region-level correlations between f (I) i,j and each image I ∈ I by</p><formula xml:id="formula_2">ϕ i,j (I, I ) = 1 C max 1≤i ,j ≤N f (I) i,j , f (I ) i ,j ,<label>(2)</label></formula><p>where ·, · denotes dot product.</p><p>Jointly trained with the classification objective L cls , the correlation score ϕ i,j is usually positively correlated with the semantic relevance to l.</p><p>After that, we can construct a N × N semantic mask matrix ϕ(I, I ) for the object extent in I.</p><p>Therefore, the commonality of images from the same category can be well captured by this semantic correlation mask ϕ, and the values in ϕ distinguish the main object area and background naturally, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>Taking the impact of viewpoint variation and deformation into account, we use multiple positive images to localize the main area of an object. Therefore, we get a weakly supervisory pseudo label to mimic the object localization  </p><p>Also, M (I, I ) can be regarded as the representations of the commonality shared among images from the same category. The primary purpose of the OEL module is to enrich the classification network from the commonality and infer the semantic mask of the object extent. Thus we equip a simple stream after f (I) to fuse all feature maps in f (I) with weights. The features are processed by a 1 × 1 convolution to obtain outputs with one channel m (I). Different from traditional attention that aims to detect some specific parts or regions, our OEL module is trained for gathering all regions within the object and neglect the background or other irrelevant objects.</p><p>The loss of OEL module L oel can be defined as the distance between pseudo mask M (I, I ) of the object extent and m (I), which can be expressed as:</p><formula xml:id="formula_4">L oel = I∈I MSE m (I), M (I, I ) ,<label>(4)</label></formula><p>where MSE is defined as a mean-square-error loss function. L oel is helpful to learn a better representation of the object extent according to visual commonality among images in the same category. By end-to-end training, the objectextent learning module can enrich the backbone network by detecting the main object extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Context Learning (SCL)</head><p>Structural information plays a significant role in image comprehension. Classical general convolutional neural networks use convolutional kernels to extract structural information in the image, and fuse the multi-level information by stacking layers. We propose a self-supervised module called Spatial Context Learning to strengthen the structural information for the backbone network by learning the spatial context information in objects.</p><p>Given an image I, our SCL module also acts on the feature maps f (I) and aims to learn the structural relationships among regions. Firstly, the feature map is processed by a 1 × 1 convolution plus a ReLU such that we get the new map h(I) ∈ R N ×N ×C1 , describing the spatial information of different feature cells. Each cell in h(I) centrally represents the semantic information of an area of the image I. The structural relationships among different parts of an object can be easily modeled by building spatial connections among different regions.</p><p>In this paper, we apply polar coordinates for measuring the spatial connections among different regions. Given a reference region R o = R x,y whose indices are (x, y) in N × N plane, and a reference horizontal direction, the polar coordinates of region R i,j can be written as (Γ i,j , θ i,j ):</p><formula xml:id="formula_5">Γ i,j = (x − i) 2 + (y − j) 2 / √ 2N θ i,j = (atan2(y − j, x − i) + π)/2π,<label>(5)</label></formula><p>where 0 &lt; Γ i,j ≤ 1 measures the relative distance between R o and R i,j , atan2(·) returns a unambiguous value in range of (−π, π] for the angle converting from Cartesian coordinates to polar coordinates, and θ i,j measures the polar angle of R i,j corresponding to the horizontal direction. It is worth noting that, to ensure a wide range of the distribution of the values of θ, ideally, the region within the object extent should be selected as the reference region. In this paper, the region who respond to the maximum value in m(I) is selected:</p><formula xml:id="formula_6">R o = R x,y , where (x, y) = arg max 1≤x,y≤N m (I) i,j (6)</formula><p>This ground-truth polar coordinates is regarded as supervision for guiding the SCL module training. Specifically, the SCL module is designed for predicting the polar coordinates of region R i,j by jointly considering the representations of target region R i,j and reference region R o from h(I). We first apply channel-wise concatenation for h(I) i,j and h(I) x,y , then the outputs are handled by a fully-connected layer with ReLU to get the predicted polar coordinates (Γ i,j , θ i,j ) . Since our proposed modules mainly focus on modeling the spatial structures of different parts within the object, the object-extent mask m (I) learned from the OEL module is also adapted in the SCL module.</p><p>There are two objectives in the SCL module. The first one measures the relative distance differences of all regions with object:</p><formula xml:id="formula_7">L dis = I∈I 1≤i,j≤N m (I) i,j (Γ i,j − Γ i,j ) 2 m (I) .<label>(7)</label></formula><p>The other one measures the polar angle differences of regions inside the object. Considering the structural information for an object should be rotation invariant, and robust to various appearances and poses of the object, we measure the polar angle difference L ∠ according to the standard deviation of gaps between predicted polar angles and groundtruth polar angles:</p><formula xml:id="formula_8">L ∠ = I∈I 1≤i,j≤N m (I) i,j θ ∆i,j − θ ∆ 2 m (I) , θ ∆i,j = θ i,j − θ i,j , if θ i,j − θ i,j ≥ 0 1 + θ i,j − θ i,j , otherwise,<label>(8)</label></formula><p>where θ ∆ = 1 m (I) 1≤i,j≤N m (I) i,j θ ∆i,j is the mean of the gaps between predicted polar angles and ground-truth polar angles. In this way, our SCL could focus on modeling the relative structure among parts of the object rather than the absolute position of regions that is sensitive to the reference direction. Moreover, owing to the usage of predicted semantic mask m (I), other visual information except for the main object, e.g., background, is ignored during regressing polar coordinates.</p><p>Overall, the loss function of the Spatial Context Learning Module can be written as:</p><formula xml:id="formula_9">L scl = L dis + L ∠ .<label>(9)</label></formula><p>With L scl , the backbone network can recognize the pattern structures, i.e., the composition of the object. By end-toend training, the spatial context learning module can empower the backbone network to model the spatial dependence among parts of the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Joint Structure Learning</head><p>In our framework, the classification, object-extent learning and spatial context learning modules are trained in an end-to-end manner, in which the network can leverage both enhanced object localization and object structural information. The whole framework is trained by minimizing the following objective:</p><formula xml:id="formula_10">L = L cls + αL oel + βL scl .<label>(10)</label></formula><p>We set α = β = 0.1 for all experimental results reported in this paper. During inference, both the SCL and OEL are removed, and only the Classification Module is kept. Thus, the framework does not introduce additional computational overhead at inference time and runs faster for practical product deployment.</p><p>Moreover, the object-extent learning module and spatial context learning module can be attached to different stages of feature maps generated from different convolutional layers of the Classification Module. Thus we can model the structural information of the object in different granularity levels. Together, the overall training method is named as multi-stage LIO. For example, we can jointly optimize our framework by the combination of L 7×7 (extracting feature maps from the last convolutional layer with N = 7) and L 14×14 (from the penultimate convolutional layer with N = 14) for ResNet-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To show the superiority of our proposed look-into-object framework, we evaluate the performance on two object recognition settings: fine-grained object recognition and generic image classification. Furthermore, we also explore our LIO framework in other tasks, such as object detection and segmentation, to study its generalization ability.</p><p>Unless specially mentioned, the spatial context learning module and object-extent learning module are applied on the feature map of the last stage in the backbone classification network, and three positive images are used for training procedure by default. For all of these tasks, we did not use any additional annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fine-grained Object Recognition</head><p>For fine-grained object recognition, we test LIO on three different standard benchmarks: CUB-200-2011 (CUB) <ref type="bibr" target="#b3">[4]</ref>, Stanford Cars (CAR) <ref type="bibr" target="#b18">[19]</ref> and FGVC-Aircraft (AIR) <ref type="bibr" target="#b22">[23]</ref>.</p><p>We first initialize LIO with ResNet-50 backbone pretrained on ImageNet classification task, and then finetune our framework on the datasets above-mentioned. The input images are resized to a fixed size of 512×512 and randomly cropped into 448 × 448 for scale normalization. We adopt random rotation and horizontal flip for data augmentation. All above transformations are standard in the literature. Both ResNet-50 baseline and LIO/ResNet-50 are trained for 240 epochs to ensure complete convergence. SGD is used to optimize the training loss as defined in Equation <ref type="bibr" target="#b9">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy (%) CUB CAR AIR CoSeq (+BBox) <ref type="bibr" target="#b17">[18]</ref> 82.8 92.8 -FCAN (+BBox) <ref type="bibr" target="#b21">[22]</ref> 84.7 93.1 -B-CNN <ref type="bibr" target="#b20">[21]</ref> 84.1 91. During testing, only the backbone network is applied for classification. The input images are centrally cropped and then fed into the backbone classification network for final predictions.</p><p>Detailed results are summarized in <ref type="table">Table 1</ref>. Besides plugging the OEL and SCL to the last stage feature map of size 7 × 7, we also tested these two modules on the penultimate stage 14 × 14 output, and the antepenultimate stage 28 × 28 output. Then these three different stages of models are combined into a multi-stage LIO. As in <ref type="table">Table 1</ref>, the LIO embedded ResNet-50 can achieve significantly better accuracy than baseline ResNet-50. Moreover, the multistage LIO achieves significant performance improvements on all three benchmarks, which proves the effectiveness of the proposed region-level structure learning framework.</p><p>It worthy note that LIO and our previous work DCL <ref type="bibr" target="#b5">[6]</ref> target at different research lines in the fine-grained recognition task. DCL aims to learn discriminative local regions, while LIO tries to understand the structure of the whole object. Both of these two kinds of methods can benefit the fine-grained object recognition, while LIO works better on recognition of flexible objects (CUB), and can be further expanded into generic object recognition (Sec. 4.2), object detection and segmentation (Sec. 4.3) since object structure information plays an essential role in those tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generic Object Recognition on ImageNet</head><p>We also evaluate the performance of our proposed LIO on large-scale general object recognition dataset ImageNet- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top-1 err. Top-5 err. ResNet-50 <ref type="bibr" target="#b12">[13]</ref> 24.80 7.48 LIO/ResNet-50 (7 × 7)</p><p>23.63 7.12 LIO/ResNet-50 <ref type="bibr">(14 × 14)</ref> 23.60 7.10 LIO/ResNet-50 (multi-stage)</p><p>22.87 6.64 1K (ILSVRC-2012), which including 1.28 million images in 1000 classes. For compatibility test, we evaluate our method on the commonly used backbone network ResNet-50. Following standard practices, we perform data augmentation with random cropping to a size of 224 × 224 pixels and perform random horizontal flipping. The optimization is performed using SGD with momentum 0.9 and a minibatch size of 256.</p><p>The experimental results are reported in <ref type="table" target="#tab_2">Table 2</ref>. We can find that LIO boosts the performance of three different backbone networks on the ImageNet-1K validation set, which further demonstrates the generality ability of our proposed object recognition framework. With a lightweight LIO plugin, the performance of typical ResNet-50 can even achieve the performance of SE-ResNet-50 <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object Detection and Segmentation on COCO</head><p>Meanwhile, considering the object structure information would be helpful for object detection and segmentation tasks, we also investigate our proposed LIO on the object detection/segmentation task on MS COCO dataset <ref type="bibr" target="#b19">[20]</ref>. We adopt the basic Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> and plug the LIO behind the Region Proposal Network, such that the structural information of each object can be well modeled. The SCL module can directly act on the object features after ROI pooling, thus the OEL module is disabled. We implemented the novel detection/segmentation network based on mmdetection <ref type="bibr" target="#b4">[5]</ref> toolbox and keep all hyper-parameters as default.</p><p>We apply the LIO module on the basic baseline of ResNet-50-C4 and a higher baseline of ResNeXt-101-FPN. The models are trained on COCO train2017 set and  <ref type="figure">Figure 6</ref>. Visualization of feature maps by using OEL and SCL respectively. OEL enforce the backbone focus on object extent. SCL is helpful for not only searching discriminative region in object extent, but also completing the object extent localized by OEL. evaluated in the COCO val2017 set. We report the standard COCO metrics including AP , AP 50 , AP 75 (averaged precision over multiple IoU thresholds), and AP S , AP M , AP L (AP across scales). Experimental results described in <ref type="table">Table 3</ref> show that modeling structural compositions benefit object understanding and lead to better results on semantic segmentation. This demonstrated the effectiveness and generalization ability of our LIO for object structural compositions learning. Some examples of results by our basic ResNeXt-101-FPN and our approach are given in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>To demonstrate the effects of the OEL module and SCL module, we perform the module separation experiments on CUB <ref type="bibr" target="#b3">[4]</ref> and CAR <ref type="bibr" target="#b18">[19]</ref>. Both OEL and SCL act on the last stage feature map from the ResNet-50 backbone. The results are shown in <ref type="table">Table 4</ref>. We can find that both modules improve performance significantly. In detail, as we show in <ref type="figure">Fig. 6</ref>, the SCL provides a principled way to learn the spatial structure, which is helpful for mining discriminative regions in an object. Moreover, the object extent can be localized by the OEL module according to the in-class region correlations and further defeats the negative influences from the diverse poses, appearance and background clutter. Together, the overall performance can be further improved owing to the complementary of nature SCL and OEL. Moreover, we also try to replace the pseudo semantic mask M (I, I ) with the ground-truth mask for LIO. The results show that our learning based method can construct a high-quality semantic mask, which is even very close to the ground-truth mask (87.3% vs. 87.4% accuracy on CUB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussions</head><p>Number of Positive Images: The number P of positive images in a batch is an important parameter for the objectextent learning Module. We visualized the pseudo mask M (I, I ) by given different number of positive images P in <ref type="figure">Fig. 7</ref>. We also evaluate our method on CUB and CAR with different numbers of positive images, and the recognition accuracy is shown in <ref type="table" target="#tab_5">Table 5</ref>. With more positive images used, the framework gets better in structural learning and result in better performance. Finally, the performance will stop rising or falling and become steady. For a rigid object structure, such as CAR, we only need a few positive images for generating a reasonable pseudo extent mask.</p><p>In general, feeding only one positive image may let the backbone learn fragmentary object extent for viewpoint diversity. The increase of P leads to rapidly rising memory #Positive = 1 #Positive = 3 #Positive = 5 #Positive = 3 #Positive = 5 #Positive = 1 <ref type="figure">Figure 7</ref>. Visualization of the changes of pseudo segmentation masks given different number of positive images. usage. Thus we use P = 3 for experiments in this paper to trade-off between final performance and computation cost.</p><p>Model Efficiency: During training time, our LIO introduced three additional layers besides the backbone network, including one convolutional layer in the OEL module, one convolutional layer and one fully-connected layer in the SCL module. For LIO/ResNet-50 (28x28), there are only 0.26 million new parameters introduced in our LIO, which is 1.01% of #Params of original ResNet-50. An important property is that both OEL and SCL modules can be disabled during testing. It means that the final classification model size is the same as the original backbone network. The baseline backbone network can be significantly improved without any computation overhead at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a Look-into-Object (LIO) framework to learn structure information for enhancing object recognition. We show that supervised object recognition could largely benefit from "additional but free" selfsupervision, where geometric spatial relationship significantly rectifies the localization of discriminative regions and even result in better object detection and segmentation. Structural information, which was overlooked in prior literature, reliably prevents the network from falling into local confusion. Moreover, our plug-in style design can be widely adopted for injecting extra supervision into the backbone network without additional computational overhead for model deployment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Feature map visualization based on the last convolutional layer of ResNet-50 backbone. The first column shows the original images, while the second and the third columns show the maximally responding feature maps from the ground-truth and the predicted labels, respectively. The last column shows the feature maps by plugging our proposed LIO on ResNet-50. Object extend and discriminative regions are all correctly localized owing to the holistic structure modeling. (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>Classification Module (CM): the backbone classification network that extracts basic image representations and produces the final object category. • Object-Extent Learning Module (OEL): a module for localizing the main object in a given image. • Spatial Context Learning Module (SCL): a selfsupervised module to strengthen the connections among regions through interactions among feature cells in CM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The overall pipeline of our Look-into-object (LIO) framework. The feature maps f (I) extracted from the classification module are further fed into spatial context learning module and object-extent learning module. After end-to-end training, the backpropagation signals from spatial context learning module and object-extent learning module can jointly optimize the representation learning of the backbone network in classification module. Only the classification module (in the green box) is activated during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Correlation calculation helps to localize object extent. masks: M (I, I ) = 1 P P p=1 ϕ(I, I p ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative examples for COCO object detection and instance segmentation. Our LIO based method can help improve the performance according to object structure information in three aspects: (a) reducing incorrect object label prediction. (b) neglecting noisy segmentation mask. (c) completing fragmentary segmentation mask. Best viewed in electronic version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Single-crop error rates (%) of single model on the</cell></row><row><cell>ImageNet-1K validation set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>White Throated Sparrow Fox Sparrow White Throated Sparrow White Throated Sparrow Song Sparrow Laysan Albatross Groove Billed Ani Song Sparrow Least Auklet Rhinoceros Auklet Shiny Cowbird Shiny Cowbird Sage Thrasher Shiny Cowbird Sage Thrasher Sage Thrasher</head><label></label><figDesc></figDesc><table><row><cell>Ground Truth</cell><cell>ResNet-50 baseline</cell><cell>OEL / ResNet-50</cell><cell>LIO / ResNet-50</cell></row><row><cell>Yellow Throated Vireo</cell><cell>Blue Winged Warbler</cell><cell>Canada Warbler</cell><cell>Yellow Throated Vireo</cell></row><row><cell>Wilson Warbler</cell><cell>Worm Eating Warbler</cell><cell>Prothonotary Warbler</cell><cell>Wilson Warbler</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Object detection and segmentation results on COCO val2017 set. Ablation studies conducted on the proposed framework. ResNet-50: Basic ResNet-50 neural network trained by L cls .</figDesc><table><row><cell>Method</cell><cell>AP</cell><cell cols="5">Object Detection AP50 AP75 APS APM APL AP</cell><cell cols="5">Semantic Segmentation AP50 AP75 APS APM APL</cell></row><row><cell>ResNet-50-C4</cell><cell>35.9</cell><cell>56.1</cell><cell>38.9</cell><cell>18.0</cell><cell>40.1</cell><cell>49.7 31.5</cell><cell>52.8</cell><cell>33.0</cell><cell>12.1</cell><cell>34.7</cell><cell>49.3</cell></row><row><cell>LIO/ResNet-50-C4</cell><cell>37.6</cell><cell>57.5</cell><cell>41.0</cell><cell>21.0</cell><cell>41.8</cell><cell>52.0 32.6</cell><cell>54.1</cell><cell>34.7</cell><cell>14.3</cell><cell>35.7</cell><cell>51.3</cell></row><row><cell>ResNeXT-101-FPN</cell><cell>41.1</cell><cell>62.8</cell><cell>45.0</cell><cell>24.0</cell><cell>45.4</cell><cell>52.6 37.1</cell><cell>59.4</cell><cell>39.7</cell><cell>17.7</cell><cell>40.5</cell><cell>53.8</cell></row><row><cell cols="2">LIO/ResNeXT-101-FPN 42.0</cell><cell>63.3</cell><cell>46.0</cell><cell>24.7</cell><cell>46.1</cell><cell>54.3 37.9</cell><cell>60.0</cell><cell>40.6</cell><cell>18.1</cell><cell>41.1</cell><cell>54.8</cell></row><row><cell>Method</cell><cell cols="3">Accuracy (%) CUB CAR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">ResNet-50 [13] 85.50 92.73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SCL</cell><cell cols="3">86.74 93.82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OEL</cell><cell cols="3">86.99 93.83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LIO</cell><cell cols="3">87.31 93.89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LIO w/ GM</cell><cell cols="2">87.37</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>OEL: Model trained by L cls + αL oel . SCL: Model trained by L cls + βL scl . LIO: Model trained by L. GM: Ground truth se- mantic segmentation annotations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The effect of the number of positive images on accuracy.</figDesc><table><row><cell>Dataset</cell><cell># Positive Images 1 3 5</cell></row><row><cell>CUB</cell><cell>86.83 87.31 87.30</cell></row><row><cell>CAR</cell><cell>93.81 93.89 93.89</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Do convolutional neural networks learn class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alsallakh</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for finegrained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The caltech-ucsd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Catherine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Serge</surname></persName>
		</author>
		<idno>birds-200-2011 dataset. (CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What does classifying more than 10,000 image categories tell us?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kostas Daniilidis, Petros Maragos, and Nikos Paragios</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2010</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepkspd: Learning kernel-matrix-based spd representation for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melih</forename><surname>Engin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="612" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contextual relations: the influence of familiarity, physical plausibility, and belongingness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whitehurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="8" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Partstacked cnn for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fully convolutional attention networks for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06765</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft. CoRR, abs/1306</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5151</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Errui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object-part attention model for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep cooccurrence feature learning for visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Fang</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang-Ming</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4123" to="4132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond object recognition: Visual sentiment analysis with deep coupled adjective and noun neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3484" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Grassmann pooling as compact homogeneous bilinear pooling for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaojian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
