<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EMPIRICAL BAYES TRANSDUCTIVE META-LEARNING WITH SYNTHETIC GRADIENTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ponts ParisTech</orgName>
								<address>
									<settlement>Champs-sur-Marne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
							<email>yang.xiao@enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Ponts ParisTech</orgName>
								<address>
									<settlement>Champs-sur-Marne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
							<email>xi.shen@enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Ponts ParisTech</orgName>
								<address>
									<settlement>Champs-sur-Marne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
							<email>guillaume.obozinski@epfl.ch</email>
							<affiliation key="aff2">
								<orgName type="department">Swiss Data Science Center Lausanne</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
							<email>damianou@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="department">Amazon Cambridge</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EMPIRICAL BAYES TRANSDUCTIVE META-LEARNING WITH SYNTHETIC GRADIENTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a meta-learning approach that learns from multiple tasks in a transductive setting, by leveraging the unlabeled query set in addition to the support set to generate a more powerful model for each task. To develop our framework, we revisit the empirical Bayes formulation for multi-task learning. The evidence lower bound of the marginal log-likelihood of empirical Bayes decomposes as a sum of local KL divergences between the variational posterior and the true posterior on the query set of each task. We derive a novel amortized variational inference that couples all the variational posteriors via a meta-model, which consists of a synthetic gradient network and an initialization network. Each variational posterior is derived from synthetic gradient descent to approximate the true posterior on the query set, although where we do not have access to the true gradient. Our results on the Mini-ImageNet and CIFAR-FS benchmarks for episodic few-shot classification outperform previous state-of-the-art methods. Besides, we conduct two zero-shot learning experiments to further explore the potential of the synthetic gradient.</p><p>Published as a conference paper at ICLR 2020</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support set</head><p>Query set</p><p>Meta-training Meta-testing <ref type="table">Table 1</ref>: The setup of few-shot learning. If task t is used for meta-testing, y t is not given to the model. <ref type="bibr" target="#b30">Nichol et al. (2018)</ref> notice that most of the existing meta-learning methods involve transduction unintentionally since they use x t implicitly via the batch normalization <ref type="bibr" target="#b17">(Ioffe &amp; Szegedy, 2015)</ref>. Explicit transduction is less explored in meta-learning, the exception is Liu et al. <ref type="formula">(2018)</ref>, who adapted the idea of label propagation (Zhu et al., 2003)  from graph-based semi-supervised learning methods. We take a totally different path that meta-learn the "gradient" descent on x t without using y t .</p><p>Due to the hierarchical structure of the data, it is natural to formulate meta-learning by a hierarchical Bayes (HB) model <ref type="bibr" target="#b14">(Good, 1980;</ref><ref type="bibr" target="#b3">Berger, 1985)</ref>, or alternatively, an empirical Bayes (EB) model <ref type="bibr" target="#b36">(Robbins, 1985;</ref><ref type="bibr" target="#b22">Kucukelbir &amp; Blei, 2014)</ref>. The difference is that the latter restricts the learning of meta-parameters to point estimates. In this paper, we focus on the EB model, as it largely simplifies the training and testing without losing the strength of the HB formulation.</p><p>The idea of using HB or EB for meta-learning is not new: Amit &amp; Meir (2018) derive an objective similar to that of HB using PAC-Bayesian analysis; <ref type="bibr" target="#b15">Grant et al. (2018)</ref> show that MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref> can be understood as a EB method; Ravi &amp; Beatson (2018) consider a HB extension to MAML and compute posteriors via amortized variational inference. However, unlike our proposal, these methods do not make full use of the unlabeled data in query set. Roughly speaking, they construct the variational posterior as a function of the labeled set d l t without taking advantage of the unlabeled set x t . The situation is similar in gradient based meta-learning methods <ref type="bibr" target="#b8">(Finn et al., 2017;</ref><ref type="bibr" target="#b34">Ravi &amp; Larochelle, 2016;</ref><ref type="bibr" target="#b27">Li et al., 2017b;</ref><ref type="bibr" target="#b30">Nichol et al., 2018;</ref><ref type="bibr" target="#b9">Flennerhag et al., 2019)</ref> and many other meta-learning methods <ref type="bibr" target="#b45">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b41">Snell et al., 2017;</ref>, where the mechanisms used to generate the task-specific parameters rely on groundtruth labels, thus, there is no place for the unlabeled set to contribute. We argue that this is a suboptimal choice, which may lead to overfitting when the labeled set is small and hinder the possibility of zero-shot learning (when the labeled set is not provided).</p><p>2. In section 4, we show in theory that a transductive variational posterior yields better generalization performance. The generalization analysis is done through the connection between empirical Bayes formulation and a multitask extension of the information bottleneck principle. In light of this, we name our method synthetic information bottleneck (SIB).</p><p>3. In section 5, we verify our proposal empirically. Our experimental results demonstrate that our method significantly outperforms the state-of-the-art meta-learning methods on few-shot classification benchmarks under the one-shot setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>While supervised learning of deep neural networks can achieve or even surpass human-level performance <ref type="bibr" target="#b16">(He et al., 2015;</ref><ref type="bibr" target="#b7">Devlin et al., 2018)</ref>, they can hardly extrapolate the learned knowledge beyond the domain where the supervision is provided. The problem of solving rapidly a new task after learning several other similar tasks is called meta-learning <ref type="bibr" target="#b40">(Schmidhuber, 1987;</ref><ref type="bibr" target="#b2">Bengio et al., 1991;</ref><ref type="bibr" target="#b43">Thrun &amp; Pratt, 1998)</ref>; typically, the data is presented in a two-level hierarchy such that each data point at the higher level is itself a dataset associated with a task, and the goal is to learn a meta-model that generalizes across tasks. In this paper, we mainly focus on few-shot learning <ref type="bibr" target="#b45">(Vinyals et al., 2016)</ref>, an instance of meta-learning problems, where a task t consists of a query set d t := {(x t,i , y t,i )} n i=1 serving as the test-set of the task and a support set d l t :={(x l t,i ,y l t,i )} n l i=1 serving as the train-set. In meta-testing 1 , one is given the support set and the inputs of the query set x t := {x t,i } n i=1 , and asked to predict the labels y t := {y t,i } n i=1 . In meta-training, y t is provided as the ground truth. The setup of few-shot learning is summarized in <ref type="table">Table 1.</ref> A important distinction to make is whether a task is solved in a transductive or inductive manner, that is, whether x t is used. The inductive setting is what was originally proposed by <ref type="bibr" target="#b45">Vinyals et al. (2016)</ref>, in which only d l t is used to generate a model. The transductive setting, as an alternative, has the advantage of being able to see partial or all points in x t before making predictions. In fact,  <ref type="figure">Figure 1</ref>: (a) The generative and inference processes of the empirical Bayes model are depicted in solid and dashed arrows respectively, where the meta-parameters are denoted by dashed circles due to the point estimates. A comparison between MAML (6) and our method (SIB) (10) is shown in (b) and (c). MAML is an inductive method since, for a task t, it first constructs the variational posterior (with parameter θ K ) as a function of the support set d l t , and then test on the unlabeled x t ; while SIB uses a better variational posterior as a function of both d l t and x t : it starts from an initialization θ 0 t (d l t ) generated using d l t , and then yields θ K t by running K synthetic gradient steps on x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">META-LEARNING WITH TRANSDUCTIVE INFERENCE</head><p>The goal of meta-learning is to train a meta-model on a collection of tasks, such that it works well on another disjoint collection of tasks. Suppose that we are given a collection of N tasks for training. The associated data is denoted by D := {d t := (x t , y t )} N t=1 . In the case of few-shot learning, we are given in addition a support set d l t in each task. In this section, we revisit the classical empirical Bayes model for meta-learning. Then, we propose to use a transductive scheme in the variational inference by implementing the variational posterior as a function of x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">EMPIRICAL BAYES MODEL</head><p>Due to the hierarchical structure among data, it is natural to consider a hierarchical Bayes model with the marginal likelihood</p><formula xml:id="formula_0">p f (D) = ψ p f (D|ψ)p(ψ) = ψ N t=1 wt p f (d t |w t )p(w t |ψ) p(ψ).</formula><p>(1)</p><p>The generative process is illustrated in <ref type="figure">Figure 1</ref> (a, in red arrows): first, a meta-parameter ψ (i.e., hyper-parameter) is sampled from the hyper-prior p(ψ); then, for each task, a task-specific parameter w t is sampled from the prior p(w t |ψ); finally, the dataset is drawn from the likelihood p f (d t |w t ).</p><p>Without loss of generality, we assume the log-likelihood model factorizes as</p><formula xml:id="formula_1">log p f (d t |w t ) = n i=1 log p f (y t,i |x t,i , w t ) + log p(x t,i |w t ), = n i=1 − 1 n t ŷ t,i (f (x t,i ), w t ), y t,i + constant.<label>(2)</label></formula><p>It is the setting advocated by <ref type="bibr" target="#b29">Minka (2005)</ref>, in which the generative model p(x t,i |w t ) can be safely ignored since it is irrelevant to the prediction of y t . To simplify the presentation, we still keep the notation p f (d t |w t ) for the likelihood of the task t and use t to specify the discriminative model, which is also referred to as the task-specific loss, e.g., the cross entropy loss. The first argument in t is the prediction, denoted byŷ t,i =ŷ t,i (f (x t,i ), w t ), which depends on the feature representation f (x t,i ) and the task-specific weight w t .</p><p>Note that rather than following a fully Bayesian approach, we leave some random variables to be estimated in a frequentist way, e.g., f is a meta-parameter of the likelihood model shared by all tasks, for which we use a point estimate. As such, the posterior inference about these variables will be largely simplified. For the same reason, we derive the empirical Bayes <ref type="bibr" target="#b36">(Robbins, 1985;</ref><ref type="bibr" target="#b22">Kucukelbir &amp; Blei, 2014)</ref> by taking a point estimate on ψ. The marginal likelihood now reads as</p><formula xml:id="formula_2">p ψ,f (D) = N t=1 wt p f (d t |w t )p ψ (w t ).<label>(3)</label></formula><p>We highlight the meta-parameters as subscripts of the corresponding distributions to distinguish from random variables. Indeed, we are not the first to formulate meta-learning as empirical Bayes. The overall model formulation is essentially the same as the ones considered by <ref type="bibr" target="#b1">Amit &amp; Meir (2018)</ref>; <ref type="bibr" target="#b15">Grant et al. (2018)</ref>; <ref type="bibr" target="#b33">Ravi &amp; Beatson (2018)</ref>. Our contribution lies in the variational inference for empirical Bayes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AMORTIZED INFERENCE WITH TRANSDUCTION</head><p>As in standard probabilistic modeling, we derive an evidence lower bound (ELBO) on the log version of (3) by introducing a variational distribution q θt (w t ) for each task with parameter θ t :</p><formula xml:id="formula_3">log p ψ,f (D) ≥ N t=1 E wt∼q θ t log p f (d t |w t ) − D KL q θt (w t ) p ψ (w t ) .<label>(4)</label></formula><p>The variational inference amounts to maximizing the ELBO with respect to θ 1 , . . . , θ N , which together with the maximum likelihood estimation of the meta-parameters, we have the following optimization problem:</p><formula xml:id="formula_4">min ψ,f min θ1,...,θ N 1 N N t=1 E wt∼q θ t − log p f (d t |w t ) + D KL q θt (w t ) p ψ (w t ) .<label>(5)</label></formula><p>However, the optimization in (5), as N increases, becomes more and more expensive in terms of the memory footprint and the computational cost. We therefore wish to bypass this heavy optimization and to take advantage of the fact that individual KL terms indeed share the same structure. To this end, instead of introducing N different variational distributions, we consider a parameterized family of distributions in the form of q φ(·) , which is defined implicitly by a deep neural network φ taking as input either d l t or d l t plus x t , that is, q φ(d l t ) or q φ(d l t ,xt) . Note that we cannot use entire d t , since we do not have access to y t during meta-testing. This amortization technique was first introduced in the case of variational autoencoders <ref type="bibr" target="#b21">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b35">Rezende et al., 2014)</ref>, and has been extended to Bayesian inference in the case of neural processes <ref type="bibr" target="#b10">(Garnelo et al., 2018)</ref>.</p><p>Since d l t and x t are disjoint, the inference scheme is inductive for a variational posterior q φ(d l t ) . As an example, MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref> takes q φ(d l t ) as the Dirac delta distribution, where φ(d l t ) = θ K t , is the K-th iterate of the stochastic gradient descent</p><formula xml:id="formula_5">θ k+1 t = θ k t + η ∇ θ E wt∼q θ k t log p(d l t |w t ) with θ 0 t = φ, a learnable initialization.<label>(6)</label></formula><p>In this work, we consider a transductive inference scheme with variational posterior q φ(d l t ,xt) . The inference process is shown in <ref type="figure">Figure 1</ref>(a, in green arrows). Replacing each q θt in (5) by q φ(d l t ,xt) , the optimization problem becomes</p><formula xml:id="formula_6">min ψ,f min φ 1 N N t=1 E wt∼q φ(d l t ,x t ) − log p f (d t |w t ) + D KL q φ(d l t ,xt) (w t ) p ψ (w t ) .<label>(7)</label></formula><p>In a nutshell, the meta-model to be optimized includes the feature network f , the hyper-parameter ψ from the empirical Bayes formulation and the amortization network φ from the variational inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UNROLLING EXACT INFERENCE WITH SYNTHETIC GRADIENTS</head><p>It is however non-trivial to design a proper network architecture for φ(d l t , x t ), since d l t and x t are both sets. The strategy adopted by neural processes <ref type="bibr" target="#b10">(Garnelo et al., 2018)</ref> is to aggregate the information from all individual examples via an averaging function. However, as pointed out by <ref type="bibr" target="#b19">Kim et al. (2019)</ref>, such a strategy tends to underfit x t because the aggregation does not necessarily attain the most relevant information for identifying the task-specific parameter. Extensions, such as attentive neural process <ref type="bibr" target="#b19">(Kim et al., 2019)</ref> and set transformer <ref type="bibr" target="#b23">(Lee et al., 2019a)</ref>, may alleviate this issue but come at a price of O(n 2 ) time complexity. We instead design φ(d l t , x t ) to mimic the exact inference arg min θt D KL (q θt (w t ) p ψ,f (w t |d t )) by parameterizing the optimization process with respect to θ t . More specifically, consider the gradient descent on θ t with step size η:</p><formula xml:id="formula_7">θ k+1 t = θ k t − η ∇ θt D KL q θ k t (w) p ψ,f (w | d t ) .<label>(8)</label></formula><p>We would like to unroll this optimization dynamics up to the K-th step such that θ K t = φ(d l t , x t ) while make sure that θ K t is a good approximation to the optimum θ t , which consists of parameterizing</p><formula xml:id="formula_8">(a) the initialization θ 0 t and (b) the gradient ∇ θt D KL (q θt (w t ) p ψ,f (w t |d t )).</formula><p>By doing so, θ K t becomes a function of φ, ψ and x t 2 , we therefore realize q φ(d l t ,xt) as q θ K t . For (a), we opt to either let θ 0 t = λ to be a global data-independent initialization as in MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref> or let θ 0 t = λ(d l t ) with a few supervisions from the support set, where λ can be implemented by a permutation invariant network described in . In the second case, the features of the support set will be first averaged in terms of their labels and then scaled by a learnable vector of the same size.</p><p>For (b), the fundamental reason that we parameterize the gradient is because we do not have access to y t during the meta-testing phase, although we are able to follow (8) in meta-training to obtain</p><formula xml:id="formula_9">q θ t (w t ) ∝ p f (d t |w t )p ψ (w t ).</formula><p>To make a consistent parameterization in both meta-training and meta-testing, we thus do not touch y t when constructing the variational posterior. Recall that the true gradient decomposes as</p><formula xml:id="formula_10">∇ θt D KL q θt p ψ,f = E 1 n n i=1 ∂ t (ŷ t,i , y t,i ) ∂ŷ t,i ∂ŷ t,i ∂w t ∂w t (θ t , ) ∂θ t + ∇ θt D KL q θt p ψ (9)</formula><p>under a reparameterization w t = w t (θ t , ) with ∼ p( ), where all the terms can be computed without y t except for ∂ t ∂ŷt,i . Thus, we introduce a deep neural network ξ(ŷ t,i ) to synthesize it. The idea of synthetic gradients was originally proposed by <ref type="bibr" target="#b18">Jaderberg et al. (2017)</ref> to parallelize the back-propagation. Here, the purpose of ξ(ŷ t,i ) is to update θ t regardless of the groundtruth labels, which is slightly different from its original purpose. Besides, we do not introduce an additional loss between ξ(ŷ t,i ) and ∂ t ∂ŷt,i since ξ(ŷ t,i ) will be driven by the objective in <ref type="formula" target="#formula_6">(7)</ref>. As an intermediate computation, the synthetic gradient is not necessarily a good approximation to the true gradient.</p><p>To sum up, we have derived a particular implementation of φ(d l t , x t ) by parameterizing the exact inference update, namely (8), without using the labels of the query set, where the meta-model φ includes an initialization network λ and a synthetic gradient network ξ, such that φ(x t ) = θ K t , the K-th iterate of the following update:</p><formula xml:id="formula_11">θ k+1 t = θ k t − η E 1 n n i=1 ξ(ŷ t,i ) ∂ŷ t,i ∂w t ∂w t (θ k t , ) ∂θ t + ∇ θt D KL q θ k t p ψ .<label>(10)</label></formula><p>The overall algorithm is depicted in Algorithm 1. We also make a side-by-side comparison with MAML shown in <ref type="figure">Figure 1</ref>. Rather than viewing (10) as an optimization process, it may be more precise to think of it as a part of the computation graph created in the forward-propagation. The computation graph of the amortized inference is shown in <ref type="figure">Figure 2</ref>,</p><p>As an extension, if we were deciding to estimate the feature network f in a Bayesian manner, we would have to compute higher-order gradients as in the case of MAML. This is inpractical from a computational point of view and needs technical simplifications <ref type="bibr" target="#b30">(Nichol et al., 2018)</ref>. By introducing a series of synthetic gradient networks in a way similar to <ref type="bibr" target="#b18">Jaderberg et al. (2017)</ref>, the computation will be decoupled into computations within each layer, and thus becomes more feasible. We see this as a potential advantage of our method and leave this to our future work 3 . Sample a task t and the associated query set d t (plus optionally the support set d l t ).</p><formula xml:id="formula_12">Classifier forward SGD θ 0 SGD SGD θ 1 θ K x t f(x t ) f x l t f(x l t ) init y l t θ f(x) . detach()ŷ grad Classifier backward ∇ θ D KL ξ(ŷ ) ≈ ∂ℓ ∂ŷ ξ λ Classifier forward ŷ t Loss y t KL ℓ t D KL (q θ K∥p ψ ) + −ELBO</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic gradient module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Compute the initialization θ 0 t = λ or θ 0 t = λ(d l t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for k = 1, . . . , K do 7:</p><p>Compute θ k t via (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>end for 9:</p><formula xml:id="formula_13">Compute w t = w t (θ K t , ) with ∼ p( ). 10: Update ψ ← ψ − η ∇ ψ D KL (q θ K t (ψ) p ψ ). 11: Update φ ← φ − η ∇ φ D KL (q φ(xt) p f · p ψ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Optionally</p><formula xml:id="formula_14">, update f ← f + η ∇ f log p f (d t |w t ). 13: end while 4 GENERALIZATION ANALYSIS OF EMPIRICAL BAYES VIA THE CONNECTION TO INFORMATION BOTTLENECK</formula><p>The learning of empirical Bayes (EB) models follows the frequentist's approach, therefore, we can use frequentist's tool to analyze the model. In this section, we study the generalization ability of the empirical Bayes model through its connection to a variant of the information bottleneck principle <ref type="bibr" target="#b0">Achille &amp; Soatto (2017)</ref>; <ref type="bibr" target="#b44">Tishby et al. (2000)</ref>.</p><p>Abstract form of empirical Bayes From (3), we see that the empirical Bayes model implies a simpler joint distribution since</p><formula xml:id="formula_15">log p ψ,f (w 1 , . . . , w N , D) = N t=1 log p f (d t |w t ) + log p ψ (w t ),<label>(11)</label></formula><p>which is equal to the log-density of N iid samples drawn from the joint distribution</p><formula xml:id="formula_16">p(w, d, t) ≡ p ψ,f (w, d, t) = p f (d|w, t)p ψ,f (w)p(t) 4<label>(12)</label></formula><p>up to a constant if we introduce a random variable to represent the task and assume p(t) is an uniform distribution. We thus see that this joint distribution embodies the generative process of empirical Bayes. Correspondingly, there is another graphical model of the joint distribution characterizes the inference process of the empirical Bayes:</p><formula xml:id="formula_17">q(w, d, t) ≡ q φ (w, d, t) = q φ (w|d, t)q(d|t)q(t),<label>(13)</label></formula><p>where q φ (w|d, t) is the abstract form of the variational posterior with amortization, includes both the inductive form and the transductive form. The coupling between p(w, d, t) and q(w, d, t) is due to p(t) ≡ q(t) as we only have access to tasks through sampling.</p><p>We are interested in the case that the number of tasks N → ∞, such as the few-shot learning paradigm proposed by <ref type="bibr" target="#b45">Vinyals et al. (2016)</ref>, in which the objective of (7) can be rewritten in an abstract form of</p><formula xml:id="formula_18">E q(t) E q(d|t) E q(w|d,t) − log p(d|w, t) + D KL q(w|d, t) p(w) .<label>(14)</label></formula><p>In fact, optimizing this objective is the same as optimizing <ref type="formula" target="#formula_6">(7)</ref> from a stochastic gradient descent point of view.</p><p>The learning of empirical Bayes with amortized variational inference can be understood as a variational EM in the sense that the E-step amounts to aligning q(w|d, t) with p(w|d, t) while the M-step amounts to adjusting the likelihood p(d|w, t) and the prior p(w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connection to information bottleneck</head><p>The following theorem shows the connection between (14) and the information bottleneck principle. Theorem 1. Given distributions q(w|d, t), q(d|t), q(t), p(w) and p(d|w, t), we have</p><formula xml:id="formula_19">(14) ≥ I q (w; d|t) + H q (d|w, t),<label>(19)</label></formula><p>where I q (w; d|t) := D KL q(w, d|t) q(w|t)q(d|t) is the conditional mutual information and</p><formula xml:id="formula_20">H q (w|d, t) := E q(w,d,t) [− log q(w|d, t)]</formula><p>is the conditional entropy. The equality holds when ∀t : D KL (q(w|t) p(w)) = 0 and D KL (q(d|w, t) p(d|w, t)) = 0.</p><p>In fact, the lower bound on <ref type="formula" target="#formula_3">(14)</ref> is an extention of the information bottleneck principle <ref type="bibr" target="#b0">(Achille &amp; Soatto, 2017</ref>) under the multi-task setting, which, together with the synthetic gradient based variational posterior, inspire the name synthetic information bottleneck of our method. The tightness of the lower bound depends on both the parameterizations of p f (d|w, t) and p ψ (w) as well as the optimization of (14). It thus can be understood as how well we can align the inference process with the generative process. From an inference process point of view, for a given q(w|d, t), the optimal likelihood and prior have been determined. In theory, we only need to find the optimal q(w|d, t) using the information bottleneck in <ref type="bibr">(19)</ref>. However, in practice, minimizing <ref type="formula" target="#formula_3">(14)</ref> is more straightforward.</p><p>Generalization of empirical Bayes meta-learning The connection to information bottleneck suggests that we can eliminate p(d|w, t) and p(w) from the generalization analysis of empirical Bayes meta-learning and define the generalization error by q(w, d, t) only. To this end, we first identify the empirical risk for a single task t with respect to particular weights w and dataset d as</p><formula xml:id="formula_21">L t (w, d) := 1 n n i=1 t (ŷ i (f (x i ), w), y i ).<label>(15)</label></formula><p>The true risk for task t with respect to w is then the expected empirical risk E d∼q(d|t) L t (w, d). Now, we define the generalization error with respect to q(w, d, t) as the average of the difference between the true risk and the empirical risk over all possible t, d, w:</p><formula xml:id="formula_22">gen(q) := E q(t)q(d|t)q(w|d,t) E d∼q(d|t) L t (w, d) − L t (w, d) = E q(t)q(d|t)q(w|t) L t (w, d) − E q(t)q(d|t)q(w|d,t) L t (w, d),<label>(16)</label></formula><p>where q(w|t) is the aggregated posterior of task t.</p><p>Next, we extend the result from <ref type="bibr" target="#b47">Xu &amp; Raginsky (2017)</ref> and derive a data-dependent upper bound for gen(q) using mutual information. Theorem 2. Denote by z = (x, y). If t (ŷ i (f (x i ), w), y i ) ≡ t (w, z i ) is σ-subgaussian under q(w|t)q(z|t), then L t (w, d) is σ/ √ n-subgaussian under q(w|t)q(d|t) due to the iid assumption, and gen(q) ≤ 2σ 2 n I q (w; d|t).</p><p>Plugging this back to Theorem 1, we obtain a different interpretation for the empirical Bayes ELBO. Corollary 1. If t is chosen to be the negative log-likelihood, minimizing the population objective of empirical Bayes meta-learning amounts to minimizing both the expected generalization error and the expected empirical risk:</p><formula xml:id="formula_24">(14) ≥ n 2σ 2 gen(q) 2 + E q(t)q(d|t)q(w|d,t) L t (w, d).<label>(17)</label></formula><p>The Corollary 1 suggests that (14) amounts to minimizing a regularized empirical risk minimization. In general, there is a tradeoff between the generalization error and the empirical risk controlled by the coefficient n 2σ 2 , where n = |d| is the cardinality of d. If n is small, then we are in the overfitting regime. This is the case of the inductive inference with variational posterior q(w|d l , t), where the support set d l is fairly small by the definition of few-shot learning. On the other hand, if we were following the transductive setting, we expect to achieve a small generalization error since the implemented variational posterior is a better approximation to q(w|d, t). However, keeping increasing n will potentially over-regularize the model and thus yield negative effect. An empirical study on varying n can be found in <ref type="table">Table 5</ref> in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first validate our method on few-shot learning, and then on zero-shot learning (no support set and no class description are available). Note that many meta-learning methods cannot do zero-shot learning since they rely on the support set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FEW-SHOT CLASSIFICATION</head><p>We compare SIB with state-of-the-art methods on few-shot classification problems. Our code is available at https://github.com/amzn/xfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">SETUP</head><p>Datasets We choose standard benchmarks of few-shot classification for this experiment. Each benchmark is composed of disjoint training, validation and testing classes. MiniImageNet is proposed by <ref type="bibr" target="#b45">Vinyals et al. (2016)</ref>, which contains 100 classes, split into 64 training classes, 16 validation classes and 20 testing classes; each image is of size 84×84. CIFAR-FS is proposed by <ref type="bibr" target="#b4">Bertinetto et al. (2018)</ref>, which is created by dividing the original CIFAR-100 into 64 training classes, 16 validation classes and 20 testing classes; each image is of size 32×32.</p><p>Evaluation metrics In few-shot classification, a task (aka episode) t consists of a query set d t and a support set d l t . When we say the task t is k-way-n l -shot we mean that d l t is formed by first sampling k classes from a pool of classes; then, for each sampled class, n l examples are drawn and a new label taken from {0, . . . , k − 1} is assigned to these examples. By default, each query set contains 15k examples. The goal of this problem is to predict the labels of the query set, which are provided as ground truth during training. The evaluation is the average classification accuracy over tasks. ; <ref type="bibr" target="#b32">Qiao et al. (2018)</ref>; <ref type="bibr" target="#b13">Gidaris et al. (2019)</ref>, we implement f by a 4-layer convolutional network  ) or a WideResNet (WRN-28-10) <ref type="bibr" target="#b49">(Zagoruyko &amp; Komodakis, 2016)</ref>. We pretrain the feature network f (·) on the 64 training classes for a stardard 64-way classification. We reuse the feature averaging network proposed by  as our initialization network λ(·), which basically averages the feature vectors of all data points from the same class and then scales each feature dimension differently by a learned coefficient. For the synthetic gradient network ξ(·), we implement a three-layer MLP with hidden-layer size 8k. Finally, for the predictorŷ ij (·, w i ), we adopt the cosine-similarity based classifier advocated by <ref type="bibr" target="#b6">Chen et al. (2019)</ref> and .  Training details We run SGD with batch size 8 for 40000 steps, where the learning rate is fixed to 10 −3 . During training, we freeze the feature network. To select the best hyper-parameters, we sample 1000 tasks from the validation classes and reuse them at each training epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network architectures Following</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">COMPARISON TO STATE-OF-THE-ART META-LEARNING METHODS</head><p>In <ref type="table" target="#tab_2">Table 2</ref>, we show a comparison between the state-of-the-art approaches and several variants of our method (varying K or f (·)). Apart from SIB, TPN  and CTM <ref type="bibr" target="#b26">(Li et al., 2019)</ref> are also transductive methods.</p><p>First of all, comparing SIB (K = 3) to SIB (K = 0), we observe a clear improvement, which suggests that, by taking a few synthetic gradient steps, we do obtain a better variational posterior as promised. For 1-shot learning, SIB (when K = 3 or K = 5) significantly outperforms previous methods on both MiniImageNet and CIFAR-FS. For 5-shot learning, the results are also comparable. It should be noted that the performance boost is consistently observed with different feature networks, which suggests that SIB is a general method for few-shot learning.</p><p>However, we also observe a potential limitation of SIB: when the support set is relatively large, e.g., 5-shot, with a good feature network like WRN-28-10, the initialization θ 0 t may already be close to some local minimum, making the updates later less important.</p><p>For 5-shot learning, SIB is sligtly worse than CTM <ref type="bibr" target="#b26">(Li et al., 2019)</ref> and/or <ref type="bibr" target="#b13">Gidaris et al. (2019)</ref>. CMT <ref type="bibr" target="#b26">(Li et al., 2019)</ref> can be seen as an alternative way to incorporate transduction -it measures the similarity between a query example and the support set while making use of intra-and inter-class relationships. <ref type="bibr" target="#b13">Gidaris et al. (2019)</ref> uses in addition the self-supervision as an auxilary loss to learn a richer and more transferable feature model. Both ideas are complementary to SIB. We leave these extensions to our future work. GT of task 1 SIB predictions GT of task 2 SIB predictions GT of task 3 SIB predictions <ref type="figure">Figure 3</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ZERO-SHOT REGRESSION: SPINNING LINES</head><formula xml:id="formula_25">Left: the mean-square errors on D test , E t D KL (q θ K t (w t ) p(w t |d t )), D KL (p ψ (w) p(w)) and the estimate of I(w; d) ≈ E t D KL (q θ K t (w t ) p ψ (w t ))</formula><p>. Middle: the predicted y's by y = θ k t x for k = 0, . . . , 4. Right: the predictions of SIB.</p><p>Since our variational posterior relies only on x t , SIB is also applicable to zero-shot problems (i.e., no support set available). We first look at a toy multi-task problem, where I(w t ; d t ) is tractable.</p><p>Denote by D train := {d t } N t=1 the train set, which consists of datasets of size n: d = {(x i , y i )} n i=1 . We construct a dataset d by firstly sampling iid Gaussian random variables as inputs: x i ∼ N (µ, σ 2 ). Then, we generate the weight for each dataset by calculating the mean of the inputs and shifting with a Gaussian random variable w : w = 1</p><formula xml:id="formula_26">n i x i + w , w ∼ N (µ w , σ 2 w ). The output for x i is y i = w · x i .</formula><p>We decide ahead of time the hyperparameters µ, σ, µ w , σ w for generating x i and y i . Recall that a weighted sum of iid Gaussian random variables is still a Gaussian random variable.</p><formula xml:id="formula_27">Specifically, if w = i c i x i and x i ∼ N (µ i , σ 2 i ), then w ∼ N ( i c i µ i , i c 2 i σ 2 i )</formula><p>. Therefore, we have p(w) = N (µ + µ w , 1 n σ 2 + σ 2 w ). On the other hand, if we are given a dataset d of size n, the only uncertainty about w comes from w , that is, we should consider x i as a constant given d. Therefore, the posterior p(w|d) = N ( 1 n n i=1 x i + µ w , σ 2 w ). We use a simple implementation for SIB: The variational posterior is realized by q θ K Proof. Denote by q(w|t) := E q(d|t) q(w|d, t)q(d|t) the aggregated posterior of task t. (14) can be decomposed as</p><formula xml:id="formula_28">E q(t) E q(d|t) E q(w|d,t) − log p(d|w, t) + D KL q(w|d, t) p(w) (20) = E q(t) E q(d|t) E q(w|d,t) log q(w|d, t)q(w|t) p(d|w, t)p(w)q(w|t) (21) = E q(t) E q(d|t) E q(w|d,t) log q(w|d, t) q(w|t) + E q(t) E q(d|t) E q(w|d,t) − log p(d|w, t) + E q(t) E q(d|t) E q(w|d,t) log q(w|t) p(w) (22) = I q (w; d|t) + H q,p (d|w, t) + E q(t) D KL (q(w|t) p(w)) (23) ≥ I q (w; d|t) + H q,p (d|w, t).<label>(24)</label></formula><p>The inequality is because D KL (q(w|t) p(w)) ≥ 0 for all t's. Besides, we used the notation H q,p , which is the conditional cross entropy. Recall that D KL q(d|w, t) p(d|w, t) = −H q (d|w, t) + H q,p (d|w, t) ≥ 0. We attain the lower bound as desired if this inequality is applied to replace H q,p (d|w, t) by H q (d|w, t).</p><p>The following lemma and theorem show the connection between I q (w; d|t) and the generalization error. We first extend <ref type="bibr">Xu (2016, Lemma 4.2)</ref>.</p><formula xml:id="formula_29">Lemma 1. If, for all t, f t (X, Y ) is σ-subgaussain under P X ⊗ P Y , then E P (T ) E P (X,Y |T ) f T (X, Y ) − E P (X|T )P (Y |T ) f T (X, Y ) ≤ 2σ 2 I(X; Y |T ).<label>(25)</label></formula><p>Proof. The proof is adapted from the proof of <ref type="bibr">Xu (2016, Lemma 4.2)</ref>.</p><formula xml:id="formula_30">LHS ≤ E P (T ) E P (X,Y |T ) f T (X, Y ) − E P (X|T )P (Y |T ) f T (X, Y )<label>(26)</label></formula><formula xml:id="formula_31">≤ E P (T ) 2σ 2 D KL (P (X, Y |T ) P (X|T )P (Y |T )) (27) ≤ 2σ 2 E P (T ) D KL (P (X, Y |T ) P (X|T )P (Y |T ))<label>(28)</label></formula><p>= 2σ 2 I(X; Y |T ).</p><p>The second inequality was due to the Donsker-Varadhan variational representation of KL divergence and the definition of subgaussain random variable.</p><p>Theorem 2. Denote by z = (x, y). If t (ŷ i (f (x i ), w), y i ) ≡ t (w, z i ) is σ-subgaussian under q(w|t)q(z|t), then L t (w, d) is σ/ √ n-subgaussian under q(w|t)q(d|t) due to the iid assumption, and</p><formula xml:id="formula_33">gen(q) ≤ 2σ 2 n I q (w; d|t).<label>(30)</label></formula><p>Proof. First, if t (ŷ(f (x), w), y) is σ-subgaussian under q(w|t)q(z|t), by definition,  </p><formula xml:id="formula_34">E q(w|t)q(z|t) exp(λ t (w, z)) ≤ exp(λE q(w|t)q(z|t) t (w, z)) exp(λ 2 σ 2 /2) (31) It is straightforward to show L t (w, d) is σ/ √ n-subgaussian since E q(w|t)q(d|t) exp(λL t (w, d)) = n i=1 E w,zi exp( λ n t (w, z i )) (32) ≤ n i=1 exp λ n E w,zi t (w, z i ) + λ 2 σ 2 2n 2 (33) = exp λE w,z t (w, z) exp( λ 2 σ 2 2n ) (34) = exp λE q(w|t)q(d|t) L t (w, d) exp( λ 2 (σ/ √ n) 2 2 ).<label>(35)</label></formula><formula xml:id="formula_35">gen(q) = E q(t) E q(w|d,t)q(d|t) L t (w, d) − E q(w|t)q(d|t) L t (w, d) (36) ≤ 2σ 2 n I(w; d|t)<label>(37)</label></formula><p>as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ZERO-SHOT CLASSIFICATION: UNSUPERVISED MULTI-SOURCE DOMAIN ADAPTATION</head><p>A more interesting zero-shot multi-task problem is unsupervised domain adaptation. We consider the case where there exists multiple source domains and a unlabeled target domain. In this case, we treat each minibatch as a task. This makes sense because the difference in statistics between two minibatches are much larger than in the traditional supervised learning. The experimental setup is similar to few-shot classification described in Section 5.1, except that we do not have a support set and the class labels between two tasks are the same. Hence, it is possible to explore the relationship between class labels and self-supervised labels to implement the initialization network λ without resorting to support set. We reuse the same model implementation for SIB as described in Section 5.1. The only difference is the initialization network. Denote by z t := {z t,i } n i=1 the set of self-supervised labels of task t, the initialization network λ is implemented as follows:</p><formula xml:id="formula_36">θ 0 t = λ − η ∇ θ L t ẑ t ŷ t (f (x t ), w t (θ, )), f (x t ) , z t ,<label>(38)</label></formula><p>where λ 6 is a global initialization similar to the one used by MAML; L t is the self-supervised loss,ẑ t is the set of predictions of the self-supervised labels. One may argue that θ 0 t = λ would be a simpler solution. However, it is insufficient since the gap between two domains can be very large. The initial solution yielded by (38) is more dynamic in the sense that θ 0 t is adapted taking into account the information from x t .</p><p>In terms of experiments, we test SIB on the PACS dataset <ref type="bibr" target="#b25">(Li et al., 2017a)</ref>, which has 7 object categories and 4 domains (Photo, Art Paintings, Cartoon and Sketches), and compare with stateof-the-art algorithms for unsupervised domain adaptation. We follow the standard experimental setting <ref type="bibr" target="#b5">(Carlucci et al., 2019)</ref>, where the feature network is implemented by ResNet-18. We assign a self-supervised label z t,i to image i by rotating the image by a predicted degree. This idea was originally proposed by  for representation learning and adopted by <ref type="bibr" target="#b48">Xu et al. (2019)</ref> for domain adaptation. The training is done by running ADAM for 60 epochs with learning rate 10 −4 . We take each domain in turns as the target domain. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. It can be seen that SIB-Rot (K = 3) improves upon the baseline SIB-Rot (K = 0) for zero-shot classification, which also outperforms state-of-the-art methods when the baseline is comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IMPORTANCE OF SYNTHETIC GRADIENTS</head><p>To further verify the effectiveness of the synthetic gradient descent, we implement an inductive version of SIB inspired by MAML, where the initialization θ 0 t is generated exactly the same way as SIB using λ(d l t ), but it then follows the iterations in (6) as in MAML rather than follows the iterations in (10) as in standard SIB.</p><p>We conduct an experiment on CIFAR-FS using Conv-4-64 feature network. The results are shown in <ref type="table">Table 4</ref>. It can be seen that there is no improvement over SIB (K = 0) suggesting that the inductive approach is insufficient.  <ref type="table">Table 4</ref>: Average 5-way classification accuracies (with 95% confidence intervals) with Conv-4-64 on the test set of CIFAR-FS. For each test, we sample 5000 episodes containing 5 categories (5-way) and 15 queries in each category. We report the results with using different learning rate η as well as different number of updates K. Note that K = 0 is the performance only using the pre-trained feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D VARYING THE SIZE OF THE QUERY SET</head><p>We notice that changing the size of d t (i.e., n) during training does make a difference on testing. The results are shown in <ref type="table">Table 5</ref>.</p><p>n 5-way, 5-shot 5-way, 1-shot Validation Test Validation Test <ref type="table">Table 5</ref>: Average classification accuracies on the validation set and the test set of Mini-ImageNet with backbone Conv-4-128. We modify the number of query images, i.e., n, for each episode to study the effect on generalization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 2: The computation graph to compute the negative ELBO, where the input and output of the synthetic gradient module are highlighted in red. The detach() is used to stop the back-propagation down to the feature network. Note that we do not include every computation for simplicity.</figDesc><table><row><cell>Algorithm 1 Variational inference with synthetic gradients for empirical Bayes</cell></row></table><note>1: Input: the dataset D; the step size η; the number of inner iterations K; pretrained f .2: Initialize the meta-models ψ, and φ = (λ, ξ).3: while not converged do4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average classification accuracies (with 95% confidence intervals) on the test-set of Mini-ImageNet and CIFAR-FS. For evaluation, we sample 2000 and 5000 episodes respectively for MiniImageNet and CIFAR-FS and test three different architectures as the feature extractor: Conv-4-64, Conv-4-128 and WRN-28-10. We train SIB with learning rate 0.001 and try different numbers of synthetic gradient steps K.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Multi-source domain adaptation results on PACS with ResNet-18 features. Three domains are used as the source domains keeping the fourth one as target.</figDesc><table><row><cell>By Lemma 1, we have</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>7±0.5% 75.5±0.4% 59.2±0.5% 75.4±0.4% 59.2±0.5% 75.4±0.4% 1 1e-1 59.8±0.5% 71.2±0.4% 65.3±0.6% 75.8±0.4% 64.5±0.6% 77.3±0.4% 3 1e-1 59.6±0.5% 75.9±0.4% 65.0±0.6% 75.0±0.4% 64.0±0.6% 77.0±0.4% 5 1e-1 59.9±0.5% 74.9±0.4% 66.0±0.6% 76.3±0.4% 64.0±0.5% 76.8±0.4% 1 1e-2 59.7±0.5% 75.5±0.4% 67.8±0.6% 74.3±0.4% 63.6±0.6% 77.3±0.4% 3 1e-2 59.5±0.5% 75.8±0.4% 68.6±0.6% 77.4±0.4% 67.8±0.6% 78.5±0.4% 5 1e-2 59.8±0.5% 75.7±0.4% 67.4±0.6% 72.6±0.6% 67.7±0.7% 77.7±0.4% 1 1e-3 59.5±0.5% 75.6±0.4% 66.2±0.6% 75.7±0.4% 64.6±0.6% 78.1±0.4% 3 1e-3 59.9±0.5% 75.9±0.4% 68.7±0.6% 77.1±0.4% 66.8±0.6% 78.4±0.4% 5 1e-3 59.4±0.5% 75.7±0.4% 69.1±0.6% 76.7±0.4% 66.7±0.6% 78.5±0.4% 1 1e-4 58.8±0.5% 75.5±0.4% 59.0±0.5% 75.7±0.4% 59.3±0.5% 75.7±0.4% 3 1e-4 59.4±0.5% 75.9±0.4% 58.9±0.5% 75.6±0.4% 59.3±0.5% 75.9±0.4% 5 1e-4 59.3±0.5% 75.3±0.4% 60.1±0.5% 76.0±0.4% 60.5±0.5% 76.4±0.4%</figDesc><table><row><cell></cell><cell></cell><cell cols="2">inductive SIB</cell><cell></cell><cell>SIB</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Training on 1-shot</cell><cell cols="2">Training on 1-shot</cell><cell cols="2">Training on 5-shot</cell></row><row><cell></cell><cell></cell><cell cols="2">Testing on</cell><cell cols="2">Testing on</cell><cell cols="2">Testing on</cell></row><row><cell>K</cell><cell>η</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>0</cell><cell>-</cell><cell>59.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To distinguish from testing and training within a task, meta-testing and meta-training are referred to as testing and training over tasks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">In this paper, we propose to use synthetic gradient<ref type="bibr" target="#b18">(Jaderberg et al., 2017)</ref> to enable transduction, such that the variational posterior is implemented as a function of the labeled set d l t and the unlabeled set x t . The synthetic gradient is produced by chaining the output of a gradient network into autodifferentiation, which yields a surrogate of the inaccessible true gradient. The optimization process is similar to the inner gradient descent in MAML, but it iterates on the unlabeled x t rather than on the labeled d l t , since it does not rely on y t to compute the true gradient. The labeled set for generating the model for an unseen task is now optional, which is only used to compute the initialization of model weights in our case. In summary, our main contributions are the following:1. In section 2 and section 3, we develop a novel empirical Bayes formulation with transduction for meta-learning. To perform amortized variational inference, we propose a parameterization for the variational posterior based on synthetic gradient descent, which incoporates the contextual information from all the inputs of the query set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">θ K t is also dependent of f . We deliberately remove this dependency to simplify the update of f .3  We do not insist on Bayesian estimation of the feature network because most Bayesian versions of CNNs underperform their deterministic counterparts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Conv-4-64 consists of 4 convolutional blocks each implemented with a 3×3 convolutional layer followed by BatchNorm + ReLU + 2 × 2 max-pooling units. All blocks of Conv-4-64 have 64 feature channels. Conv-4-128 has 64 feature channels in the first two blocks and 128 feature channels in the last two blocks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t (w) = N (θ K t , σ w ), θ k+1 t = θ k t − 10 −3 n i=1x i ξ(θ k t x i ), and θ 0 t = λ ∈ R;(18)t is a mean squared error, implies that p(y|x, w) = N (wx, 1); p ψ (w) is a Gaussian distribution with parameters ψ ∈ R 2 ; The synthetic gradient network ξ is a three-layer MLP with hidden size 8.In the experiment, we sample 240 tasks respectively for both D train and D test . We learn SIB and BNN on D train for 150 epochs using the ADAM optimizer (Kingma &amp; Ba, 2014), with learning rate 10 −3 and batch size 8. Other hyperparameters are specified as follows: n = 32, K = 3, µ = 0, σ = 1, µ w = 1, σ w = 0.1. The results are shown inFigure 3. On the left, both D KL (q θ K t (w t ) p(w t |d t )) and D KL (p ψ (w) p(w)) are close to zero indicating the success of the learning. More interestingly, in the middle, we see that θ 0 t , θ 1 t , . . . , θ 4 t evolves gradually towards the ground truth, which suggests that the synthetic gradient network is able to identify the descent direction after meta-learning.6 CONCLUSIONWe have presented an empirical Bayesian framework for meta-learning. To enable an efficient variational inference, we followed the amortized inference paradigm, and proposed to use a transductive scheme for constructing the variational posterior. To implement the transductive inference, we make use of two neural networks: a synthetic gradient network and an initialization network, which together enables a synthetic gradient descent on the unlabeled data to generate the parameters of the amortized variational posterior dynamically. We have studied the theoretical properties of the proposed framework and shown that it yields performance boost on MiniImageNet and CIFAR-FS for few-shot classification.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">λ is overloaded to be both the network and its parameters.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOFS</head><p>Theorem 1. Given distributions q(w|d, t), q(d|t), q(t), p(w) and p(d|w, t), we have</p><p>where I q (w; d|t) := D KL q(w, d|t) q(w|t)q(d|t) is the conditional mutual information and H q (w|d, t) := E q <ref type="bibr">(w,d,t)</ref> [− log q(w|d, t)] is the conditional entropy. The equality holds when ∀t : D KL (q(w|t) p(w)) = 0 and D KL (q(d|w, t) p(d|w, t)) = 0.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01350</idno>
		<title level="m">Emergence of invariance and disentangling in deep representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meta-learning by adjusting priors based on extended pac-bayes theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cloutier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN-91-Seattle International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1991" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">969</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Statistical Decision Theory and Bayesian Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno>abs/1805.08136</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transferring knowledge across learning processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Flennerhag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Danilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01622</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Neural processes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05186</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Some history of the hierarchical bayesian methodology. Trabajos de estadística y de investigación operativa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irving John</forename><surname>Good</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">489</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Recasting gradientbased meta-learning as hierarchical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08930</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decoupled neural interfaces using synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1627" to="1635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05761</idno>
		<title level="m">Oriol Vinyals, and Yee Whye Teh. Attentive neural processes</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.0292</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Population empirical bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Finding Task-Relevant Features for Few-Shot Learning by Category Traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Discriminative models, not discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<idno>MSR-TR-2005-144</idno>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodríguez López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Amortized bayesian meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beatson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An empirical bayes approach to statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Herbert Robbins Selected Papers</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985" />
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJgklhAcK7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruna</surname></persName>
		</author>
		<idno>abs/1711.04043</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning. on learning now to learn: The meta-meta-meta</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://www.idsia.ch/~juergen/diploma.html" />
		<imprint>
			<date type="published" when="1987" />
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technische Universitat Munchen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Phd thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bialek</surname></persName>
		</author>
		<title level="m">The information bottleneck method. arXiv preprint physics/0004057</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Information-theoretic limitations of distributed information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aolin</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Information-theoretic analysis of generalization capability of learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aolin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Raginsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2524" to="2533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-supervised domain adaptation for computer vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="156694" to="156706" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Lafferty</surname></persName>
		</author>
		<idno>2003. 3 77.97 ± 0.34% 75.91 ± 0.66% 63.60 ± 0.52% 61.32 ± 1.02% 5 78.14 ± 0.35% 76.01 ± 0.66% 64.67 ± 0.55% 62.50 ± 1.02%</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
