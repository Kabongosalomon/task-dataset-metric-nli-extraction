<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<email>yanjunjie@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BSN++: Complementary Boundary Regressor with Scale-Balanced Relation Modeling for Temporal Action Proposal Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the Ushaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1 st place in the CVPR19 -ActivityNet challenge leaderboard on temporal action localization task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Temporal action detection task has received much attention from many researchers in recent years, which requires not only categorizing the real-world untrimmed videos but also locating the temporal boundaries of action instances. Akin to object proposals for object detection in images, temporal action proposal indicates the temporal intervals containing the actions and plays an important role in temporal action detection. It has been commonly recognized that high-quality proposals usually have two crucial properties: (1) the generated proposals should cover the action instances temporally with both high recall and temporal overlapping; (2) the quality of proposals should be evaluated accurately, thus providing a overall confidence for later retrieving step.</p><p>Figure 1: (a) Given an untrimmed video containing several action instances of small scale, (b) IoU-balanced sampling is widely used to train the proposal confidence regressor, which still suffers from inferior quality owing to the imbalanced distribution of the temporal durations, resulting in the long-tailed proposal dataset. (c) BSN++ aims at generating high-quality proposal boundaries as well as reliable confidence scores with complementary boundary regressor and scale-balanced proposal relation block.</p><p>To cater for these two conditions and achieve high quality proposals, there are two main categories in the existing proposal generation methods <ref type="bibr" target="#b2">(Buch et al. 2017b;</ref><ref type="bibr" target="#b20">Lin, Zhao, and Shou 2017;</ref><ref type="bibr" target="#b27">Shou, Wang, and Chang 2016)</ref>. The first type adopts the top-down fashion, where proposals are generated based on sliding windows <ref type="bibr" target="#b27">(Shou, Wang, and Chang 2016)</ref> or uniform-distributed anchors <ref type="bibr" target="#b20">(Lin, Zhao, and Shou 2017)</ref>, then a binary classifier is employed to evaluate confidence for the proposals. However, the proposals generated in this way are doomed to have imprecise boundaries though with regression. Under this circumstance, the other type of methods <ref type="bibr" target="#b42">Xiong et al. 2017;</ref><ref type="bibr" target="#b19">Lin et al. 2019</ref>) attract many researchers recently arXiv:2009.07641v5 [cs.CV] 1 Mar 2021 which tackle this problem in a bottom-up fashion, where the input video is evaluated in a finer-level. ) is a typical method in this type which proposes the Boundary-Sensitive Network (BSN) to generate proposals with flexible durations and reliable confidence scores. Though BSN achieves convincing performance, it still suffers from three main drawbacks: (1) BSN only employs the local details around the boundaries to predict boundaries, without taking advantage of the rich temporal contexts through the whole video sequence; (2) BSN fails to consider the proposalproposal relations for confidence evaluation; (3) the imbalance data distribution between positive/negative proposals and temporal durations is also neglected.</p><p>To relieve these issues, we propose BSN++, for temporal proposal generation. (i) To exploit the rich contexts for boundary prediction, we adopt the U-shaped architecture with nested skip connections. Meanwhile, the two optimized boundary classifiers share the same goals especially in detecting the sudden change from background to actions or learning the discriminativeness from actions to background, thus are complementary with each other. Under this circumstance, we propose the complementary boundary regressor, where the starting classifier can also be used to predict the ending locations when the input videos are processed in a reversed direction, and vice versa. In this way, we can achieve high precision without adding extra parameters. (ii) In order to predict the confidence scores of denselydistributed proposals, we design a proposal relation block aiming at leveraging both channel-wise and position-wise global dependencies for proposal-proposal relation modeling. (iii) To relieve the imbalance scale-distribution among the sampling positives as well as the negatives (see <ref type="figure">Fig. 1</ref>), we implement a two-stage re-sampling scheme consisting of the IoU-balanced (positive-negative) sampling and the scale-balanced re-sampling. The boundary map and the confidence map are generated simultaneously and jointly trained in a unified framework. In summary, the main contributions of our work are listed below in three-folds:</p><p>• We revisit the boundary prediction problem and propose a complementary boundary generator to exploit both "local and global", "past and future" contexts for accurate temporal boundary prediction.</p><p>• We propose a proposal relation block for proposal confidence evaluation, where two self-attention modules are adopted to model the proposal relations from two complementary aspects. Besides, we devise a two-stage resampling scheme for equivalent balancing.</p><p>• Thorough experiments are conducted to reveal the effectiveness of our method. Further combining with the existing action classifiers, our method can achieve the state-ofthe-art temporal action detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Action Recognition</head><p>Action recognition is an essential branch which has been extensively explored in recent years. Earlier methods such as improved Dense Trajectory (iDT) <ref type="bibr" target="#b35">(Wang et al. 2011;</ref><ref type="bibr" target="#b36">Wang and Schmid 2013)</ref> mainly adopt the hand-crafted features including HOG, MBH and HOF. Current deep learning based methods <ref type="bibr" target="#b8">(Feichtenhofer, Pinz, and Zisserman 2016;</ref><ref type="bibr" target="#b29">Simonyan and Zisserman 2014;</ref><ref type="bibr" target="#b34">Tran et al. 2015;</ref><ref type="bibr" target="#b38">Wang et al. 2016;</ref><ref type="bibr" target="#b31">Su et al. 2020a</ref>) typically contain two main categories: the two-stream networks <ref type="bibr" target="#b8">(Feichtenhofer, Pinz, and Zisserman 2016;</ref><ref type="bibr" target="#b29">Simonyan and Zisserman 2014)</ref> capture the appearance and motion information from RGB image and stacked optical flow respectively; 3D networks <ref type="bibr" target="#b34">(Tran et al. 2015;</ref><ref type="bibr" target="#b25">Qiu, Yao, and Tao 2017)</ref> exploit 3D convolutions to capture the spatial and temporal information directly from the raw videos. Action recognition networks are usually adopted to extract visual feature sequence from untrimmed videos for the temporal action proposals and detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imbalanced Distribution Training</head><p>Imbalanced data distribution naturally exists in many largescale datasets <ref type="bibr" target="#b4">(Cordt et al. 2018;</ref><ref type="bibr" target="#b5">Cordts et al. 2016;</ref><ref type="bibr" target="#b9">Feng et al. 2018)</ref>. Current literature can be mainly divided into three categories: (1) re-sampling, includes oversampling the minority classes <ref type="bibr" target="#b7">(Estabrooks, Jo, and Japkowicz 2004;</ref><ref type="bibr" target="#b17">Ji et al. 2020)</ref> or downsampling the majority classes <ref type="bibr" target="#b40">(Weiss, McCarthy, and Zabar 2007;</ref><ref type="bibr" target="#b15">Hu et al. 2020)</ref>; (2) reweighting, namely cost sensitive learning <ref type="bibr" target="#b24">(McCarthy, Zabar, and Weiss 2005;</ref><ref type="bibr" target="#b6">Cui et al. 2019)</ref>, which aims to dynamically adjust the weight of samples or different classes during training process.</p><p>(3) In object detection task, the imbalanced issue is more serious between background and foreground for one-stage detector. Some methods such as Focal loss ) and online hard negative mining <ref type="bibr" target="#b28">(Shrivastava, Gupta, and Girshick 2016)</ref> are designed for two-stage detector. In this paper, we implement the scale-balanced resampling upon the IoU-balanced sampling for proposal confidence evaluation, motivated by the mini-batch imbalanced loss distribution against proposal durations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Action Detection and Proposals</head><p>Akin to object detection in images, temporal action detection also can be divided into proposal and classification stages. Current methods train these two stages separately <ref type="bibr" target="#b30">(Singh and Cuzzolin 2016)</ref> or jointly <ref type="bibr" target="#b1">(Buch et al. 2017a;</ref><ref type="bibr" target="#b20">Lin, Zhao, and Shou 2017)</ref>. Top-down methods <ref type="bibr" target="#b20">(Lin, Zhao, and Shou 2017)</ref> are mainly based on sliding windows or pre-defined anchors, while bottom-up methods <ref type="bibr" target="#b21">Lin et al. 2018</ref><ref type="bibr" target="#b19">Lin et al. , 2019</ref> first evaluate the actionness or boundary probabilities of each temporal location in a finer level. However, proposals generated in a local fashion of ) cannot be further retrieved without confidence scores evaluated from a global view. And probabilities sequence generated in <ref type="bibr" target="#b23">Liu et al. 2019</ref>) is sensitive to noises, causing many false alarms. Besides, proposalproposal relations fail to be considered for confidence evaluation. Meanwhile, the imbalanced distribution among the proposals remains to be settled. To address these issues, we propose BSN++, which is unique to previous works in three main aspects: (1) we revisit the boundary prediction task and propose to exploit rich contexts together with bi-directional matching strategy for accurate boundary prediction;</p><p>(2) we <ref type="figure">Figure 2</ref>: The framework of BSN++. Given an untrimmed video, two-stream network is adopted to extract visual features. Then BSN++ can densely evaluate all proposals by producing the boundary map with a complementary boundary generator and the confidence map with a proposal relation block simultaneously.</p><p>devise a proposal relation block for proposal-proposal relations modeling;</p><p>(3) two-stage re-sampling scheme is designed for equivalent balancing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Approach Problem Definition</head><p>Denote an untrimmed video sequence as U = {u t } lv t=1 , where u t indicates the t-th frame in the video of length l v . A set of action instances Ψ g = {ϕ n = (t s n , t e n )} Ng n=1 are temporally annotated in the video S v , where N g is the number of ground truth action instances, and t s n , t e n are the starting time and ending time of the action instance ϕ n respectively. During training phase, the Ψ g is provided. While in the testing phase, the predicted proposal set Ψ p should cover the Ψ g with high recall and high temporal overlapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Feature Encoding</head><p>Before applying our algorithm, we adopt the two-stream network <ref type="bibr" target="#b29">(Simonyan and Zisserman 2014)</ref> in advance to encode the visual features from raw video as many previous works <ref type="bibr" target="#b10">Gao, Kan, and Nevatia 2018;</ref><ref type="bibr" target="#b32">Su, Zhao, and Lin 2018;</ref><ref type="bibr" target="#b33">Su et al. 2020b</ref>). This kind of architecture has been widely used in many video analysis tasks <ref type="bibr" target="#b20">(Lin, Zhao, and Shou 2017;</ref><ref type="bibr" target="#b46">Zhao et al. 2017;</ref>. Concretely, given an untrimmed video S v which contains l v frames, we process the input video in a regular interval σ for reducing the computational cost. We concatenate the output of the last FC-layer in the two-stream network to form the feature sequence F = {f i } ls i=1 , where l s = l v /σ. Final, the feature sequence F is used as the input of our BSN++. <ref type="figure">Figure 3</ref>: Illustration of the complementary boundary generator. U-shaped encoder-decoder with dense skip connections are utilized for accurate boundary prediction. Consistent regularization is performed on the intermediate features during the training process. In inference stage, starting/ending classifiers are also utilized to predict the ending/starting locations in a backward order. The two siamese backbones share the weights. Finally the boundary map is constructed through matching boundary locations into pairs based on the two-passes boundary probabilities sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Network Architecture: BSN++</head><p>In contrast to the previous BSN , which consists of multiple stages, BSN++ is designed to generate the proposal map directly in a unified network. To obtain the proposal map, BSN++ first generates the boundary map which represents the boundary information and confidence map which represents the confidence scores of densely distributed proposals. As shown in <ref type="figure">Fig. 2</ref>, BSN++ model mainly contains three main modules: Base Module handles the input video features to perform temporal information modeling, then the output features are shared by the two following modules. Complementary Boundary Generator processes the input video features to evaluate the boundary probabilities sequence, using a nested U-shaped encoder-decoder; Proposal Relation Block aims to model the proposal-proposal relations with two self-attention modules responsible for two complementary dependencies. Base Module. The goal of this module is to handle the extracted features for temporal relationship modeling, which serves as the base module of the following two branches. It mainly includes two 1D convolutional layers, with 256 filters, kernel size 3 and stride 1, followed by a ReLU activation layer. Since the length of videos is uncertain, we truncate the video sequence into a series of sliding windows. The detailed of data construction is illustrated in Section 4.1. Complementary Boundary Generator. Inspired by the success of U-Net <ref type="bibr" target="#b26">(Ronneberger, Fischer, and Brox 2015;</ref><ref type="bibr" target="#b47">Zhou et al. 2018</ref>) used in image segmentation, we design our boundary generator as Encoder-Decoder networks because this kind of architecture is able to capture both highlevel global context and low-level local details at the same time. As shown in <ref type="figure">Fig. 3</ref>, each circle represents a 1D convolutional layer with 512 filters and kernel size 3, stride 1, together with a batch normalization layer and a ReLU layer except the prediction layer. To reduce over-fitting, we just add two down-sampling layers to expand the receptive fields and the same number of up-sampling layers are followed to recover the original temporal resolutions. Besides, deep supervision (shown red) is also performed for fast convergent speed and nested skip connections are employed for bridging the semantic gap between feature maps of the encoder and decoder prior to fusion.</p><p>We observe that the starting classifier learns to detect the sudden change from background to actions and vice versa. Hence, the starting classifier can be regarded as a pseudo ending classifier when processes the input video in a reversed direction, thus the bi-directional prediction results are complementary. With this observation, bi-directional encoder-decoder networks are optimized in parallel, and the consistent constraint is performed upon the intermediate fea-</p><formula xml:id="formula_0">tures (i.e. f (x f i ) and f (x b i )</formula><p>) on both sides before the prediction layer as shown in <ref type="figure">Fig. 3</ref>. During the inference stage, the aforementioned encoder-decoder network is adopted to predict the the starting heatmap</p><formula xml:id="formula_1">− → H s = { − → h s i } ls i=1 and ending heatmap − → H e = { − → h e i } ls i=1</formula><p>respectively, where h s i and h e i indicate the starting and ending probabilities of the i-th snippet respectively. Meanwhile, we feed the input feature sequence in a reversed order to the identical backbone. Similarly, we can obtain the starting heatmap ← − H s and ending heatmap ← − H e . After the two-passes, in order to select the boundaries of high scores, we fuse the two pairs of heatmaps to yield the final heatmaps:</p><formula xml:id="formula_2">H s = { − → h s i × ← − h s i } ls i=1 , H e = { − → h e i × ← − h e i } ls i=1 ,</formula><p>(1) With these two boundary points heatmaps, we can further construct the boundary map M b ∈ R 1×D×T which can represent the boundary information of all densely distributed proposals, where T and D are the length of the feature sequence and maximum duration of proposals separately:</p><formula xml:id="formula_3">M b j,i = {{h s i × h e i+j } T i=1 } D j=1 , i + j &lt; T,<label>(2)</label></formula><p>Proposal Relation Block. The goal of this block is to evaluate the confidence scores of dense proposals. Before performing proposal-proposal relations, we follow the previous work BMN  to generate the proposal feature maps as F p ∈ R D×T ×128×N . N is set to 32. Then the proposal feature maps are fed to a 3D convolutional layer with kernel size 1×1×32 and 512 filters, followed by a ReLU activation layer. Thus the reduced proposal features maps are F p ∈ R D×T ×512 . The proposal relation block consists of two self-attention modules as follows.</p><p>Position-aware attention module. As illustrated in <ref type="figure">Fig.  4</ref>, given the proposal features F p , we adopt the similar selfattention mechanism as , where the proposal feature maps are fed into a convolutional layer separately to generate two new feature maps A and B for spatial matrix multiplication with reshape and transpose operations. And then a Softmax layer is applied to calculate the positionaware attention P A ∈ R L×L , where L = D × T :</p><formula xml:id="formula_4">P A j,i = exp(A i · B j ) L i=1 exp(A i · B j ) ,<label>(3)</label></formula><p>Figure 4: Illustration of the proposal relation block. After generating the proposal feature maps, two complementary branches are followed to model the proposal relation separately. In the upper branch, the position-aware attention module aims to leverage global dependencies. While in the lower branch, the channel-aware attention module aims to attend to the discriminative features by channel matrix calculation. Finally, we aggregate the outputs from the three branches for pixel-level confidence prediction.</p><p>where P A j,i indicates the attention of the i th position on the j th position. Finally, the attended features are further weighted summed with the proposal features and fed to the convolutional layers for confidence prediction.</p><p>Channel-aware attention module. In contrast to the position-aware attention module, this module directly performs channel-wise matrix multiplication in order to exploit the inter-dependencies among different channels, which can help enhance the proposal feature representations for confidence prediction. The process of attention calculation is the same as the former module except for the attended dimension. Similarly, the attended features after weighted summed with the proposal features are further captured by a 2D convolutional layer to generate the confidence map M c ∈ R D×T . We also aggregate the outputs of the two attention modules for proposal confidence prediction, and finally we fuse the predicted confidence maps from the three branches for a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-sampling</head><p>Imbalanced data distribution can affect the model training especially in the long-tailed dataset. In this paper, we revisit the positive/negative samples distribution for improving the quality of proposal confidence prediction and design a proposal-level re-sampling method to improve the performance of training on the long-tailed dataset. Our resampling scheme consists of two stages aiming at not only balancing the positives and negatives proposals, but also balancing the temporal duration of the proposals. <ref type="figure">Fig. 1</ref>, we can see from the mini-batch loss distribution that the number of positives and negatives differs greatly which dooms to bias the training model without effective measures. Previous works usually design a positive-negative sampler (i.e. IoUbalanced sampler) to balance the data distribution for each mini-batch, thus ensuring the ratio of positive and negative samples is nearly 1:1. However, we can also conclude from the <ref type="figure">Fig. 1</ref> that the scale of positives or negatives fails to conform the uniform distribution. Under this circumstance, we should consider how to balance the scales of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IoU-balanced sampling As shown in</head><p>Scale-balanced re-sampling To relieve the issue among long-tailed scales, we propose a second-stage positive/negative re-sampling method, which is upon the principle of IoU-balanced sampling. Specifically, define P i as the number of positive proposals with the scale s i , then r i is the positive ratio of s i :</p><formula xml:id="formula_5">r i = P i Ns j=1 P j , r i =    λ * exp ( r i λ −1) (0 &lt; r i ≤ λ), r i (λ &lt; r i ≤ 1),<label>(4)</label></formula><p>where N s is the number of pre-defined normalized scale regions (i.e. [0 − 0.3, 0.3 − 0.7, 0.7 − 1.0]). Then we design a positive ratio sampling function, the resulting ratio r i is bigger than r i for proposal scale with a frequency lower than λ, where λ is a hyper-parameter which we set to 0.15 empirically. Hence, we use the re-normalized r i as the sampling probability of the specific proposal scale region s i to construct the mini-batch data. As for the negative proposals, the same process is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Inference of BSN++ Training</head><p>Overall Objective Function. As described above, BSN++ consists of three main sub-modules. The multi-task objective function is defined as:</p><formula xml:id="formula_6">L BSN ++ = L CBG + β · L P RB + γ · L 2 (Θ),<label>(5)</label></formula><p>where L CBG and L P RB are the objective functions of the complementary boundary generator and the proposal relation block respectively, while L 2 (Θ) is a regularization term. β and γ are set to 10 and 0.0001 separately to trade off the training process of two modules and reduce over-fitting. Training Data Construction. Given the extracted feature F with length l s , we truncate F into sliding windows of length l w with 75% temporal overlapping. Then we construct the training dataset as Φ = {F w n } Nw n=1 , where N w is the number of retained windows containing at least one ground-truth. Label Assignment. For the Complementary Boundary Generator (CBG), in order to predict the boundary probabilities sequence, we need to generate the corresponding label sequence G w s and G w e as in . Specifically, for each action instance ϕ g in the annotation set Ψ w g , we denote it's starting and ending regions as [t s g − d ϕ /10, t s g + d ϕ /10] and [t e g −d ϕ /10, t e g +d ϕ /10] respectively, where d ϕ = t e g −t s g is the duration of ϕ g . Then for each temporal location, if it lies in the starting or ending regions of any action instances, the corresponding label g s or g e will be set to 1. Hence the label sequence of starting and ending used in CBG are</p><formula xml:id="formula_7">G w s = {g s i } lw i=1 , G w e = {g e i } lw i=1</formula><p>respectively. For the Proposal Relation Block (PRB), we predict the confidence map M c ∈ R D×lw of all densely distributed proposals, where the point g c j,i in the label confidence map M c g = {{g c j,i } lw i=1 } D j=1 represents the maximum IoU (Intersection-over-Union) values of proposal ϕ j,i = [t s = i, t e = i + j] with all ϕ g in Ψ w g . Objective of CBG. We follow  to adopt the weighted binary logistic regression loss L bl as the objective between the output probability and the corresponding label sequence. The objective is:</p><formula xml:id="formula_8">L CBG = − → L s bl + − → L e bl f orward + ← − L s bl + ← − L e bl backward +||f (x f ) − f (x b )|| 2 ,<label>(6)</label></formula><p>where − → L s bl and − → L e bl represent the L bl between − → H s and G w s , − → H e and G w E respectively in the forward pass. Mean-Squared Loss is also performed on two-passes intermediate features.</p><p>Objective of PRB. Taking the constructed proposal feature maps F p as input, our PRB will generate two types of confidence maps M cr and M cc for all densely distributed proposals as . The training objective is defined as the regression loss L reg and the binary classification loss L cls respectively:</p><formula xml:id="formula_9">L P RB = L reg + L cls ,<label>(7)</label></formula><p>where the smooth-L 1 loss <ref type="bibr" target="#b13">(Girshick 2015</ref>) is adopted as L reg , and the points g c i,j with value large than 0.7 or lower than 0.3 are regarded as positives and negatives respectively. And we ensure the scale and number ratio between positives and negatives to be near 1:1 by the two-stage sampling scheme described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference</head><p>During inference stage, our BSN++ can generate the boundary map M b based on the bidirectional boundary probabilities (H s and H e ) and confidence map (M cc and M cr ). We form the proposal map M p directly by fusing the M b and M c with dot multiplication. Then we can filter the points with high scores in the proposal map M p as candidate proposals used for post-processing. Score Fusion. As described above, the final scores of proposals in M p involve the local boundary information and global confidence scores. Take the proposal ϕ = [t s , t e ] for example, the combination of final score p ϕ can be shown as:</p><formula xml:id="formula_10">p ϕ = M b</formula><p>te−ts,ts · M cc te−ts,ts · M cr te−ts,ts ,</p><p>Redundant Proposals Suppression. BSN++ can generate the proposal candidates set as Ψ p = {ϕ n = (t s , t e , p ϕ )} Np n=1 , where N p is the number of proposals. Since the generated proposals may overlap with each other, we conduct Soft-NMS <ref type="bibr" target="#b0">(Bodla et al. 2017</ref>) algorithm to suppress the confidence scores of redundant proposals. Final, the proposals set is Ψ p = {ϕ n = (t s , t e , p ϕ )} Np n=1 , where p ϕ is the decayed score of proposal ϕ n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets and Setup</head><p>Datasets. ActivityNet-1.3 <ref type="figure" target="#fig_0">(Caba Heilbron et al. 2015)</ref> is a large-scale video dataset for action recognition and temporal action detection tasks used in the ActivityNet Challenge from 2016 to 2020. It contains 19, 994 videos with 200 action classes temporally annotated, and the ratio of training, validation and testing sets is 1:1:2. THUMOS-14 <ref type="bibr" target="#b18">(Jiang et al. 2014</ref>) contains 200 and 213 untrimmed videos with temporal annotations of 20 action classes in validation and testing sets respectively. Implementation details. For feature encoding, we adopt the two-stream network <ref type="bibr" target="#b29">(Simonyan and Zisserman 2014)</ref>, where ResNet network <ref type="bibr" target="#b14">(He et al. 2016</ref>) and BN-Inception network <ref type="bibr" target="#b16">(Ioffe and Szegedy 2015)</ref> are used as the spatial and temporal networks respectively. During feature extraction, the interval σ is set to 16 and 5 on ActivityNet-1.3 and THUMOS14 respectively. On ActivityNet-1.3, we rescale the feature sequence of input videos to l w = 100 by linear interpolation following , and the maximum duration D is also set to 100 to cover all action instances. While on THUMOS14, the length l w of sliding windows is set to 128 while the maximum duration D is set to 64, which can cover almost 98% action instances. On both datasets, we train our BSN++ from scratch using the Adam optimizer and the batch size is set to 16. And the initial learning rate is set to 0.001 for 7 epochs, then 0.0001 for another 3 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Proposal Generation</head><p>Evaluation metrics. Following the conventions, Average Recall (AR) is calculated under different tIoU thresholds which are set to [0.5:0.05:0.95] on ActivityNet-1.3, and [0.5:0.05:1.0] on THUMOS14. We measure the relation between AR and Average Number (AN) of proposals, denoted as AR@AN. And we also calculate the area (AUC) under the AR vs. AN curve as another evaluation metric on ActivityNet-1.3 dataset, where AN ranges from 0 to 100. Comparison to the state-of-the-arts. <ref type="table" target="#tab_0">Table 1</ref> illustrates the comparison results on ActivityNet-1.3. It can be observed that our BSN++ outperforms other state-of-the-art proposal generation methods with a big margin in terms of AR@AN and AUC on validation set of ActivityNet-1.3. For a direct comparison to BSN, our BSN++ improves AUC from 66.17% to 68.26% on validation set. Particularly, when the AN is 100, our method significantly improves AR from 74.16% to 76.52% by 2.36%. And when the AN is 1, the AR which our BSN++ can obtain is 34.30%. <ref type="table" target="#tab_1">Table 2</ref> illustrates the comparison results on THUMOS14 dataset. For fair comparisons, we use the features when compared with other methods, which mainly includes twostream features and C3D features <ref type="bibr" target="#b34">(Tran et al. 2015)</ref>. Results shown in <ref type="table" target="#tab_1">Table 2</ref> clearly demonstrate that: (1) the performance of our BSN++ obviously outperforms other state-ofthe-methods in terms of AR@AN with AN varying from 50 to 1000, no matter what kind of features is served as input;</p><p>(2) when post-processed with Soft-NMS, the higher AR can be obtained with fewer proposals. Qualitative examples on THUMOS14 and ActivityNet-1.3 are shown in <ref type="figure" target="#fig_0">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Experiments</head><p>In this section, we comprehensively evaluate our proposed BSN++ on the validation set of ActivityNet-1.3. Effectiveness and efficiency of modules in BSN++. We perform the ablation studies with different architecture set-  tings to verify the effectiveness and efficiency of each module proposed in BSN++. The evaluation results shown in Table 3 demonstrate that: (1) the Encoder-Decoder architecture can effectively learn "local and global" contexts for accurate boundary prediction compared to the previous works which only explore the local details;</p><p>(2) the bidirectional matching mechanism further validates the importance of future context in assisting the boundary judgement; (3) unlike the previous works which treat the proposals separately, the proposal relation block can provide more comprehensive features for accurate and discriminative proposals scoring; (4) besides, with scale-balanced sampling, the model can obtain equivalent balancing; (5) final, integrating all the separated modules into an end-to-end network, we can obtain the competing performance improvement; (6) BSN++ achieves the great overall efficiency than previous methods. Ablation comparison with BSN. We conduct a direct comparison to BSN  to confirm the effectiveness and superiority of our BSN++. As shown in <ref type="table" target="#tab_2">Table 3</ref>, the TEM of BSN which only considers the local details for boundary probabilities sequence generation is inferior with limited receptive fields. Meanwhile, without the full usage of temporal context, it is also not robust in complicated scenarios. Besides, BSN fails to model the proposal relations for confidence regression, as well as neglect the imbalance data distribution against proposal duration. However, our BSN++ handles these issues accordingly and effectively.  Generalizability of proposals. Another key property of the proposal generation method is the generalizability. To evaluate this property, two un-overlapped action subsets: "Sports, Exercise, and Recreation" and "Socializing, Relaxing, and Leisure" of ActivityNet-1.3 are chosen as seen and unseen subsets separately. There are 87 and 38 action categories, 4455 and 1903 training videos, 2198 and 896 validation videos on seen and unseen subsets separately. We adopt C3D network pre-trained on Sports-1M dataset for feature extraction. Then we train BSN++ with seen and seen+unseen training videos separately, and evaluate both models on seen and unseen validation videos separately. Results in <ref type="table" target="#tab_3">Table 4</ref> reveal that there is only slight performance drop on unseen categories, suggesting that BSN++ achieves great generalizability to generate high quality proposals for unseen actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Detection with Our Proposals</head><p>Evaluation metrics. For temporal action detection task, mean Average Precision (mAP) is a conventional evaluation metric, where Average Precision (AP) is calculated for each action category respectively. On ActivityNet-1.3, the mAP with tIoU thresholds set {0.5, 0.75, 0.95} and the average mAP with tIoU thresholds [0.5:0.05:0.95] are reported. On THUMOS14, mAP with tIoU thresholds set {0.3, 0.4, 0.5, 0.6, 0.7} is used.</p><p>Comparison to the state-of-the-arts. To further examine  <ref type="bibr" target="#b20">Lin, Zhao, and Shou 2017;</ref><ref type="bibr" target="#b21">Lin et al. 2018</ref><ref type="bibr" target="#b19">Lin et al. , 2019</ref><ref type="bibr" target="#b45">Zeng et al. 2019;</ref><ref type="bibr">Xu et al. 2020)</ref> on validation set of ActivityNet-1.3, where our proposals are combined with video-level classification results generated by ).  the quality of proposals generated by BSN++, following BSN , we feed them to the state-of-the-art action classifiers to obtain the categories for action detection in a "detection by classification" framework. On ActivityNet-1.3, we use the top-1 video-level classification results generated by ) for all the generated proposals. And on THUMOS14, we use the top-2 video-level classification results generated by UntrimmedNet . Comparison results are illustrated in <ref type="table" target="#tab_4">Table 5</ref> and <ref type="table" target="#tab_6">Table 6</ref> respectively. We can observe that with the same classifiers, the detection performance of our method can be boosted greatly, which can further demonstrate the effectiveness and superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We propose BSN++ for temporal action proposal generation. The complementary boundary generator takes the advantage of U-shaped architecture and bi-directional boundary matching mechanism to learn rich contexts for boundary prediction. To model the proposal-proposal relations for confidence evaluation, we devise the proposal relation block which employs two self-attention modules to perform global and inter-dependencies modeling. Meanwhile, we are the first to consider the imbalanced data distribution of proposal durations. Both the boundary map and confidence map can be generated simultaneously in a unified network. Extensive experiments conducted on ActivityNet-1.3 and THUMOS14 datasets demonstrate the effectiveness of our method in both temporal action proposal and detection performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative examples of proposals generated by BSN++ on THUMOS14 (top) and ActivityNet-1.3 (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons with other state-ofthe-art proposal generation methods on validation set of ActivityNet-1.3 in terms of AUC and AR@AN.</figDesc><table><row><cell>Method</cell><cell cols="4">SSAD-prop CTAP BSN MGG BMN BSN++</cell></row><row><cell>AR@1 (val)</cell><cell>-</cell><cell>-32.17 -</cell><cell>-</cell><cell>34.30</cell></row><row><cell cols="2">AR@100 (val) 73.01</cell><cell cols="3">73.17 74.16 74.54 75.01 76.52</cell></row><row><cell>AUC (val)</cell><cell>64.40</cell><cell cols="3">65.72 66.17 66.43 67.10 68.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Comparisons with other state-of-the-art proposal</cell></row><row><cell cols="4">generation methods SCNN-prop(Shou, Wang, and Chang</cell></row><row><cell cols="4">2016), SST(Buch et al. 2017b), TURN(Gao et al. 2017),</cell></row><row><cell cols="4">MGG(Liu et al. 2019), BSN(Lin et al. 2018), BMN(Lin et al.</cell></row><row><cell cols="4">2019) on THUMOS14 in terms of AR@AN, where SNMS</cell></row><row><cell cols="2">stands for Soft-NMS.</cell><cell></cell></row><row><cell>Feature</cell><cell>Method</cell><cell cols="2">@50 @100 @200 @500 @1000</cell></row><row><cell>C3D</cell><cell>TURN</cell><cell cols="2">19.63 27.96 38.34 53.52 60.75</cell></row><row><cell>C3D</cell><cell>MGG</cell><cell cols="2">29.11 36.31 44.32 54.95 60.98</cell></row><row><cell>C3D</cell><cell>BSN(SNMS)</cell><cell cols="2">29.58 37.38 45.55 54.67 59.48</cell></row><row><cell>C3D</cell><cell>BMN(SNMS)</cell><cell cols="2">32.73 40.68 47.86 56.42 60.44</cell></row><row><cell>C3D</cell><cell cols="3">BSN++(SNMS) 34.88 43.72 50.12 58.88 61.39</cell></row><row><cell cols="2">2-Stream CTAP</cell><cell>32.49 42.61 51.97 -</cell><cell>-</cell></row><row><cell cols="2">2-Stream MGG</cell><cell cols="2">39.93 47.75 54.65 61.36 64.06</cell></row><row><cell cols="2">2-Stream BSN(SNMS)</cell><cell cols="2">37.46 46.06 53.21 60.64 64.52</cell></row><row><cell cols="2">2-Stream BMN(SNMS)</cell><cell cols="2">39.36 47.72 54.70 62.07 65.49</cell></row><row><cell cols="2">2-Stream Ours(SNMS)</cell><cell cols="2">42.44 49.84 57.61 65.17 66.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments in the validation set of ActivityNet-1.3. Complementary boundary generator is abbreviated as CBG and BBM denotes bi-directional matching. PRB is the proposal relation block and SBS is the scalebalanced sampling. PAM and CAM indicate the two selfattention modules. Inference speed here is the seconds (s) cost T cost for processing a 3-minute videos using a Nvidia 1080-Ti card. e2e denotes the joint training manner.</figDesc><table><row><cell>Model</cell><cell>Module</cell><cell cols="2">e2e AUC T cost</cell></row><row><cell>BSN</cell><cell>TEM</cell><cell>-</cell><cell>64.80 0.036</cell></row><row><cell>BSN</cell><cell>TEM+PEM</cell><cell>×</cell><cell>66.17 0.629</cell></row><row><cell>BMN</cell><cell>TEM</cell><cell>-</cell><cell>65.17 0.035</cell></row><row><cell>BMN</cell><cell>TEM+PEM</cell><cell></cell><cell>67.10 0.052</cell></row><row><cell>BSN++</cell><cell>CBG(w/o BBM)</cell><cell>-</cell><cell>66.02 0.019</cell></row><row><cell>BSN++</cell><cell>CBG(w/ BBM)</cell><cell>-</cell><cell>66.43 0.025</cell></row><row><cell cols="2">BSN++ CBG+PRB (w/o SBS)</cell><cell>×</cell><cell>67.34 0.054</cell></row><row><cell cols="2">BSN++ CBG+PRB (w/o SBS)</cell><cell></cell><cell>67.77 0.039</cell></row><row><cell cols="2">BSN++ CBG+PRB (w/o PAM)</cell><cell></cell><cell>67.99 0.034</cell></row><row><cell cols="2">BSN++ CBG+PRB (w/o CAM)</cell><cell></cell><cell>68.01 0.035</cell></row><row><cell>BSN++</cell><cell>CBG+PRB</cell><cell></cell><cell>68.26 0.039</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Generalizability evaluation on ActivityNet-1.3. /74.56 65.02/66.34 72.68/74.32 65.06/66.37 Seen 72.47/74.03 64.37/65.87 72.46/73.82 64.47/65.89</figDesc><table><row><cell>BMN/BSN++</cell><cell cols="2">Seen(validation)</cell><cell cols="2">Unseen(validation)</cell></row><row><cell>Train Data</cell><cell>AR@100</cell><cell>AUC</cell><cell>AR@100</cell><cell>AUC</cell></row><row><cell cols="2">Seen+Unseen 72.96</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Detection results compared with</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Detection results compared with<ref type="bibr" target="#b21">Lin et al. 2018;</ref><ref type="bibr" target="#b23">Liu et al. 2019;</ref><ref type="bibr" target="#b19">Lin et al. 2019</ref>) on testing set of THUMOS14, where video-level classifier is combined with proposals generated by BSN++.</figDesc><table><row><cell></cell><cell cols="2">THUMOS14 (testing), mAP@tIoU</cell></row><row><cell>Method</cell><cell cols="2">Classifier 0.7 0.6 0.5 0.4 0.3</cell></row><row><cell>TURN</cell><cell>UNet</cell><cell>6.3 14.1 24.5 35.3 46.3</cell></row><row><cell>BSN</cell><cell>UNet</cell><cell>20.0 28.4 36.9 45.0 53.5</cell></row><row><cell>MGG</cell><cell>UNet</cell><cell>21.3 29.5 37.4 46.8 53.9</cell></row><row><cell>BMN</cell><cell>UNet</cell><cell>20.5 29.7 38.8 47.4 56.0</cell></row><row><cell>Ours</cell><cell>UNet</cell><cell>22.8 31.9 41.3 49.5 59.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improving Object Detection With One Line of Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1704.04503</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-End, Single-Stream Temporal Action Detection in Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detec-tion, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multiple resampling method for learning from imbalanced data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Estabrooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Challenges on Large Scale Surveillance Video Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04821</idno>
		<title level="m">CTAP: Complementary Temporal Action Proposal Generation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01180</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3648" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Class-wise Dynamic Graph Convolution for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09690</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Context-Aware Graph Convolution Network for Target Reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04298</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">BMN: Boundary-Matching Network for Temporal Action Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<idno>abs/1907.09702</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02964</idno>
		<title level="m">BSN: Boundary Sensitive Network for Temporal Action Proposal Generation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-granularity Generator for Temporal Action Proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Does costsensitive learning beat sampling for classifying rare classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ofthe 1st international workshop on Utilitybased data mining</title>
		<meeting>the 1st international workshop on Utilitybased data mining</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Training region-based object detectors with online hard ex-ample mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Untrimmed video classification for activity detection: submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01979</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06902</idno>
		<title level="m">Collaborative Distillation in the Parameter and Spectrum Domains for Video Action Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cascaded Pyramid Mining Network for Weakly Supervised Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transferable Knowledge-Based Multi-Granularity Fusion Network for Weakly Supervised Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Nonlocal Neural Networks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Costsensitive learning vs. sampling: Which is best for handling unbalanced classes with unequal error costs? Dmin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zabar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">CUHK &amp; ETHZ &amp; SIAT submission to ActivityNet challenge 2016. CVPR ActivityNet Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02716</idno>
		<title level="m">A pursuit of temporal accuracy in general activity detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sub-Graph Localization for Temporal Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G-Tad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">UNet++: A Nested U-Net Architecture for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
