<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Symmetry Consistent Deep CNNs for Face Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<email>wmzuo@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<addrLine>3 Anyvision</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Symmetry Consistent Deep CNNs for Face Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(a) Input (b) Iizuka et al. <ref type="bibr" target="#b10">[11]</ref> (c) Li et al. <ref type="bibr" target="#b18">[19]</ref> (d) Yu et al. <ref type="bibr" target="#b32">[33]</ref> (e) Liu et al. <ref type="bibr" target="#b19">[20]</ref> (f) Ours </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Deep convolutional networks (CNNs) have achieved great success in face completion to generate plausible facial structures. These methods, however, are limited in maintaining global consistency among face components and recovering fine facial details. On the other hand, reflectional symmetry is a prominent property of face image and benefits face recognition and consistency modeling, yet remaining uninvestigated in deep face completion. In this work, we leverage two kinds of symmetry-enforcing subnets to form a symmetry-consistent CNN model (i.e., SymmFCNet) for effective face completion. For missing pixels on only one of the half-faces, an illumination-reweighted warping subnet is developed to guide the warping and illumination reweighting of the other half-face. As for missing pixels on both of half-faces, we present a generative reconstruction subnet together with a perceptual symmetry loss to enforce symmetry consistency of recovered structures. The Symm-FCNet is constructed by stacking generative reconstruction subnet upon illumination-reweighted warping subnet, and can be end-to-end learned from training set of unaligned face images. Experiments show that SymmFCNet can generate high quality results on images with synthetic and real occlusion, and performs favorably against state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of face completion is to fill in missing facial pixels with visually plausible hypothesis <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref>. The generated solutions for missing parts aim to restore semantic facial structures and realistic fine details, but are not required to exactly approximate the unique ground-truth. Unlike images of natural scene, face images usually contain little repetitive structures <ref type="bibr" target="#b32">[33]</ref>, which further increases the difficulties of face completion. Moreover, face completion can also be used in many real world face-related applications such as unwanted content removal (e.g., glasses, scarf, and HMD in interactive AR/VR), interactive face editing, and occluded face recognition.</p><p>Recently, along with the development of deep learning, significant progress has been made in image inpainting <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> and face completion <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>. The existing methods generally adopt the generative adversarial network (GAN) <ref type="bibr" target="#b6">[7]</ref> framework which involves a generator and a discriminator. On one hand, contextual attention <ref type="bibr" target="#b32">[33]</ref> and shift-connection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> have been introduced into the baseline generator (i.e., context-encoder) <ref type="bibr" target="#b23">[24]</ref> to exploit surrounding repetitive structures for generating visually plausible content with fine details. On the other hand, global and local discriminators are incorporated to obtain globally consistent result with locally realistic details <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref>, and semantic parsing loss is also adopted to enhance the consistency of face completion result <ref type="bibr" target="#b18">[19]</ref>.</p><p>However, face completion is not a simple application of image inpainting, and remains not well solved. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the results by state-of-the-art CNN-based methods, including Iizuka et al. <ref type="bibr" target="#b10">[11]</ref>, Li et al. <ref type="bibr" target="#b18">[19]</ref>, Yu et al. <ref type="bibr" target="#b32">[33]</ref>, and Liu et al. <ref type="bibr" target="#b19">[20]</ref>. Because face images are generally of nonrepetitive structures, blurry results remain inevitable in the methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> based on auto-encoder and contextual attention. Furthermore, from the top image in <ref type="figure" target="#fig_0">Fig. 1(e)</ref>, although the generated right eye by <ref type="bibr" target="#b19">[20]</ref> is locally satisfying, it is globally inconsistent with the left eye.</p><p>In this work, we present a deep symmetry-consistent face completion network (SymmFCNet), which leverages face symmetry to improve the global consistency and local fine details of face completion result. The reflectional symmetry of face images, which has been widely adopted to face recognition and consistency modeling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27]</ref>, remains a non-trivial issue to exploit the symmetry property in face completion due to the effect of illumination and pose. Nonsymmetric lighting causes the pixel value not equal to the corresponding pixel value in the other half-face. The deviation from frontal face further breaks the reflectional symmetry and makes the pixel correspondence between two halffaces much more complicated.</p><p>As shown in <ref type="figure">Fig. 2</ref>, the correspondence between two half-faces can be divided into three types: (1) The missing pixels in input image correspond to non-occluded pixels in flip image (red lines in <ref type="figure">Fig. 2(a)</ref>), which indicates that these missing pixels can be filled by their symmetric ones. <ref type="bibr" target="#b1">(2)</ref> The missing pixels in input image correspond to missing pixels in flip image (green lines in <ref type="figure">Fig. 2(a)</ref>), which indicates that these missing pixels can only be filled by generation. (3) The remaining pixels in input image correspond to other pixels in flip image (blue lines in <ref type="figure">Fig. 2(a)</ref>). Based on this, we present two mechanisms to leverage symmetric consistency for filling in two types of missing pixels.</p><p>For missing pixels happened on only one of the halffaces (see <ref type="figure">Fig. 2</ref>(a)), it is natural to fill them by reweighting the illumination of the corresponding pixels in the other half-face (the red correspondence in <ref type="figure">Fig. 2(a)</ref>). To cope with pose and illumination variation between half-faces, we suggest an illumination-reweighted warping subnet of two parts: (i) a FlowNet to establish the correspondence map between two half-faces, and (ii) a LightNet to indicate the ratio of illumination between two half-faces. For missing pixels happened on both of the half-faces (see <ref type="figure">Fig. 2(b)</ref>), perceptual symmetry loss is incorporated with a generative reconstruction subnet (RecNet) for symmetry-consistent completion. Based on the correspondence map established by FlowNet, the perceptual symmetry loss is defined on the decoder feature layer to alleviate the effect of illumination inconsistency between two half-faces. To sum up, our full SymmFCNet can be constructed by stacking generative reconstruction subnet upon illumination-reweighted warping subnet. Perceptual symmetry, reconstruction and adversarial losses are deployed on RecNet to end-to-end train the full SymmFCNet. While illumination consistency loss, landmark loss and total variation (TV) regularization are employed to illumination-reweighted warping subnet for improving the training stability of FlowNet and LightNet.</p><p>Experiments show that illumination-reweighted warping is effective in filling in missing pixels happened on only one of the half-faces. In contrast, the RecNet can not only generate symmetry-consistent result for missing pixels happened on both of the half-faces, but also benefit the refinement of the result by illumination-reweighted warping. In terms of quantitative metrics and visual quality, our SymmFCNet performs favorably against state-of-the-arts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>, and achieves high quality results on face images with real occlusions. The contributions of this work include:</p><p>• An illumination-reweighted warping network for filling in missing pixels on only one of the half-faces.</p><p>• A generative reconstruction network equipped with perceptual symmetry loss for the inpainting of missing pixels on both of the half-faces.</p><p>• Our full SymmFCNet for high quality symmetryconsistent face completion with either rectangular or irregular missing regions.</p><p>• Favorable performance of SymmFCNet in comparison to state-of-the-arts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review the relevant work of three sub-fields: deep image inpainting, deep face completion and the applications of symmetry in face analysis.</p><p>Deep Image Inpainting. Image inpainting aims to fill in missing pixels in a seamless manner <ref type="bibr" target="#b1">[2]</ref>, which has wide applications, such as restoration of damaged image and unwanted content removal. Recently, motivated by the unprecedented success of GAN in many vision tasks like style transfer <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, image-to-image translation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39]</ref>, image super-resolution <ref type="bibr" target="#b14">[15]</ref> and face attribute manipulation <ref type="bibr" target="#b16">[17]</ref>, deep CNNs have also greatly facilitated the development of image inpainting. Originally, Pathak et al. <ref type="bibr" target="#b23">[24]</ref> present an encoder-decoder (i.e., context encoder) network to learn the image semantic structure for the recovery of the missing pixels, and an adversarial loss is deployed to enhance the visual quality of the inpainting result. Subsequently, global and local discriminators <ref type="bibr" target="#b10">[11]</ref> are adopted for better discrimination between real images and inpainting results. In addition, dilated convolution <ref type="bibr" target="#b10">[11]</ref> and partial convolution <ref type="bibr" target="#b19">[20]</ref> are introduced to improve the training of generator. To exploit the repetitive structures in surrounding contexts, multiscale neural patch synthesis (MNPS) <ref type="bibr" target="#b28">[29]</ref> is suggested, and contextual attention <ref type="bibr" target="#b32">[33]</ref> and shift-connection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> are further presented to overcome the inefficiency of MNPS. Unlike natural images, face images generally exhibit nonrepetitive structures and are more sensitive to semantic consistency and visual artifacts, making it difficult to directly apply general-purposed inpainting models.</p><p>Deep Face Completion. Apart from the aforementioned image inpainting methods, Yeh et al. <ref type="bibr" target="#b30">[31]</ref> develop a semantic face completion method, which exploits the trained GAN to find the closest encoding and then fill the missing pixels by considering both context discriminator and corrupted input image. Li et al. <ref type="bibr" target="#b18">[19]</ref> learn a generative model to recover missing pixels by minimizing the combination of reconstruction loss, local and global adversarial losses as well as semantic parsing loss. For better recovery of facial details, Zhao et al. <ref type="bibr" target="#b36">[37]</ref> suggest a guidance image from the extra non-occluded face image to facilitate identity-aware completion. However, the introduction of guidance image certainly limits its wide applications, and its performance degrades remarkably when the guidance and occluded images are of different poses. Instead of guidance image, we leverage the symmetry of face images to establish the correspondence between two half-faces, which is then used to guide the generation of high quality completion result.</p><p>Face Symmetry. Symmetry is closely related to the human perception, understanding and discovery of images, and also has received upsurging interests in computer vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. In computational symmetry, numerous methods have been proposed to detect reflection, rotation, translation and medial-axis-like symmetries from images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. Reflectional symmetry is also an important characteristic of face images, which has been used to assist 3D face reconstruction <ref type="bibr" target="#b3">[4]</ref>, 3D face alignment <ref type="bibr" target="#b22">[23]</ref> and face recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. In addition, Huang et al. <ref type="bibr" target="#b8">[9]</ref> adopt symmetry loss on pixel and Laplacian space for identitypreserving face frontalization. Unlike <ref type="bibr" target="#b8">[9]</ref>, we present a more general scheme for modeling face symmetry for face completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Face completion aims at learning a mapping from occluded face I o as well as its binary indicator mask M to a desired completion resultÎ (i.e., an estimation of the ground-truth I). Here, the images I o , M , andÎ are of the same size h×w, and M (i, j) = 0 indicates the pixel at (i, j) is missing. To exploit face symmetry, we present our twostage SymmFCNet to generate symmetry-consistent completion result. In the first stage, an illumination-reweighted warping subnet is deployed to fill in missing pixels happened on only one of the half-faces (see <ref type="figure">Fig. 2</ref>(a)), where a FlowNet is included to establish the correspondence between two half-faces. In the second stage, a generative reconstruction subnet is used to handle missing pixels happened on both of the half-faces and further refine the inpainting result (see <ref type="figure">Fig. 2(b)</ref>). Using the output of FlowNet, we define a perceptual symmetry loss on the decoder feature layer to enforce symmetry consistent completion. In this section, we first detail the architecture of SymmFCNet and then define the learning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Illumination-reweighted warping</head><p>Unlike general-purposed image inpainting, face is a highly structured object with prominent reflectional symmetric characteristic. Thus, when the missing pixels are within only half of the face, it is reasonable to fill them based on the corresponding pixels in the other half-face.</p><p>To this end, we should solve the illumination inconsistence and create correspondence between the pixels from two half-faces. For example, given a missing pixel (i, j), if its corresponding pixel (i , j ) in the other half-face and their illumination ratio R(i, j) = I(i,j) I(i ,j ) is known, the valuê I(i, j) can then be computed by I o (i , j )R(i, j) (Note that I o (i , j ) = I(i , j )). In the following, we introduce a FlowNet and a LightNet for computing pixel correspondence and illumination ratio, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">FlowNet</head><p>One may establish the correspondence between the pixels from two half-faces by direct matching. However such approach is computational costly and the annotation of dense correspondence is also practically infeasible. Instead, we introduce the flip image </p><formula xml:id="formula_0">Φ = (Φ x , Φ y ), Φ = F w (I o , I o ; Θ w ),<label>(1)</label></formula><p>where Θ w denotes the FlowNet model parameters. Given</p><formula xml:id="formula_1">a pixel (i, j) in I o , (Φ x i,j , Φ y i,j ) indicates the position of its corresponding pixel in I o . Note that I o is the flip image of I o . Thus, I o (i, j) and I o (Φ x i,j , Φ y i,j )</formula><p>are a pair of corresponding pixels from different half-faces, and the correspondence between two half-faces is then constructed.</p><p>With Φ, the pixel value at (i, j) of the warped image I w is defined as the pixel value at</p><formula xml:id="formula_2">(Φ x i,j , Φ y i,j ) of the flipped image I o . Since Φ x i,j and Φ y i,j are real numbers, then I o (Φ x i,j ,Φ y i,j )</formula><p>can be bilinear interpolated by its 4 surrounding neighboring pixels. Thus, the warped image I w i,j can be computed as the interpolation result:</p><formula xml:id="formula_3">I w i,j = (h,w)∈N I o h,w max(0, 1−|Φ y i,j −h|) max(0, 1−|Φ x i,j −w|),<label>(2)</label></formula><p>where N denotes the 4-pixel neighbors of (Φ x i,j , Φ y i,j ). Analogously, the warped mask image M w of M can be given by:</p><formula xml:id="formula_4">M w i,j = (h,w)∈N M h,w max(0, 1−|Φ y i,j −h|) max(0, 1−|Φ x i,j −w|).<label>(3)</label></formula><p>By defining M s1 = M w (1−M ), we can then identify the missing pixels (i, j) within only half of the face as M s1 i,j = 1. Here, represents the element-wise product operation.</p><p>The FlowNet adopts the encoder-decoder architecture which is the same as pix2pix <ref type="bibr" target="#b11">[12]</ref> except that the inputs contain 6 channels rather than 3 ones. As for the last activation function, we employ tanh to normalize the two channels coordinates to the range [−1, 1]. Please refer to the appendix for more details of FlowNet.</p><p>Because it is unpractical to annotate the dense correspondence between left and right half-faces, alternative losses are required to train FlowNet. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38]</ref>, the losses are enforced on the warped images. For face completion, however, the ground-truth of warped image is unknown, and the two half-faces may be of different illumination, making it unsuitable to use I o as the ground-truth.</p><p>Following <ref type="bibr" target="#b17">[18]</ref>, we train FlowNet in a semi-supervised manner by incorporating landmark loss with a TV regularizer. Given the ground-truth image I, we detect its 68 facial landmarks (x g i , y g i ) 68</p><p>i=1 through <ref type="bibr" target="#b35">[36]</ref>. Denote by I the horizontal flip of I. Landmarks of I , denoted by (x g i , y g i ) 68 i=1 , can be obtained by horizontal flip of (x g , y g ). In order to align I o to the pose of I o , it is natural to require (Φ x</p><formula xml:id="formula_5">x g i ,y g i , Φ y x g i ,y g i</formula><p>) be close to (x g i , y g i ), and we thus define the landmark loss as:</p><formula xml:id="formula_6">lm = 68 i=1 (Φ x x g i ,y g i − x g i ) 2 + (Φ y x g i ,y g i − y g i ) 2 .<label>(4)</label></formula><p>Furthermore, TV regularization is deployed to constrain the spatial smoothness of flow field Φ. Given the 2D dense flow field (Φ x , Φ y ), the TV regularization is defined as:</p><formula xml:id="formula_7">T V = ∇ x Φ x 2 + ∇ y Φ x 2 + ∇ x Φ y 2 + ∇ y Φ y 2 ,<label>(5)</label></formula><p>where ∇ x and ∇ y denote the gradient operators along x and y coordinates, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">LightNet</head><p>Generally, the left and right half-faces are lighting inconsistent, therefore, we cannot fill in missing pixels directly by I w . In order to compensate the illumination variation, we add the light adjustment module (LightNet) to make the completion result more harmonious. LightNet takes I o and I o as inputs, and adopts the same network architecture of FlowNet but it predicts the illumination ratio R as shown in the start of Sec. 3.1 as follows:</p><formula xml:id="formula_8">R = F l (I o , I o ; Θ l ),<label>(6)</label></formula><p>where Θ l denotes the LightNet model parameters.</p><p>Given the illumination ratio R, warped image I w , the inpainting result for missing pixels within only one of the half-face can be given by M s1 I w R. Taking the surrounding context into account, the completion result in the first stage can be obtained by:</p><formula xml:id="formula_9">I 1 = M s1 I w R + I o (1 − M s1 ).<label>(7)</label></formula><p>Here, represents the element-wise product operation. We note that illumination reweighted warping cannot handle missing pixels happened on both of the half-faces, which will be addressed in the second stage.</p><p>For training LightNet, we introduce an illumination consistency loss. Denote by I w the warped version of the flip ground-truth I . Then, the illumination reweighted I w is required to approximate the original ground-truth I. And we thus define the illumination consistency loss as:</p><formula xml:id="formula_10">L l = I w R − I 2 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generative reconstruction</head><p>We further present a generative reconstruction subnet for the inpainting of missing pixels happened on both of the half-faces. Let M s2 = 1 − M − M s1 . When M s2 i,j = 1, it indicates that pixel at location (i, j) is missing. Thus, generative reconstruction subnet (RecNet) takesÎ 1 (the completion result in the first stage) and M s2 as input to generate the final completion result.</p><formula xml:id="formula_11">I = F r (Î 1 , M s2 ; Θ r ),<label>(9)</label></formula><p>where Θ r represents the RecNet model parameters. For RecNet, we adopt the U-Net architecture <ref type="bibr" target="#b24">[25]</ref> which has the same structure with FlowNet. Moreover, skip connections are included to concatenate each l-th layer to the (L − l)-th layer, where L is the network depth. The flow field Φ is further utilized to enforce the symmetry consistency on the completion results of missing pixels on both of the half-faces. RecNet also takes the flip versions ofÎ 1 and M s2 as input to generateÎ . We define Ω l (Ω l ) as the (L − l)-th layer of decoder feature map forÎ 1 and M s2 (their flip versions). By downsampling Φ (M s2 ) to Φ ↓ (M s2 ↓ ) which has the same size with Ω l , the perceptual symmetry loss can then be defined as:</p><formula xml:id="formula_12">L s = 1 C l i,j ((Ω l (i, j)−Ω l (Φ x ↓,i,j , Φ y ↓,i,j )) M s2 ↓ (i, j)) 2 ,<label>(10)</label></formula><p>where C l denotes the channel number of the feature map Ω l . In our implementation, we set l = 1 with feature size 128 × 128. Benefited from L s , we can maintain symmetric consistency even for filling in the missing pixels on both of the half-faces.</p><p>Reconstruction loss is introduced to require the final completion resultÎ be close to the ground-truth I, which involves two terms. The first one, 2 loss, is defined as the squared Euclidean distance betweenÎ and I,</p><formula xml:id="formula_13">2 = Î − I 2 .<label>(11)</label></formula><p>Inspired by <ref type="bibr" target="#b12">[13]</ref>, the second term adopts the perceptual loss defined on pre-trained VGG-Face <ref type="bibr" target="#b21">[22]</ref>. Denote by Ψ the VGG-Face model, and Ψ k the k-th layer (i.e., k = 5) of feature map. The perceptual loss is then defined as,</p><formula xml:id="formula_14">perceptual = 1 C k H k W k Ψ k (Î) − Ψ k (I) 2 ,<label>(12)</label></formula><p>where the C k , H k and W k denote the channel number, height and width of feature maps, respectively. Then, the reconstruction loss is defined as,</p><formula xml:id="formula_15">L r = λ r,2 2 + λ r,p perceptual .<label>(13)</label></formula><p>where λ r,2 and λ r,p are the tradeoff parameters. Finally, adversarial loss is deployed to generate photorealistic completion result. In <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref>, global and local discriminators are exploited, where local discriminator is defined on the inpainting result of a hole. Considering that the hole may be irregular, it is inconvenient to define and learn local discriminator. Instead, we apply local discriminators to four specific facial parts, i.e., left/right eye, nose and mouth. Thus, local discriminators are consistent for any images with any missing masks, and facilitate the learning process of SymmFCNet. For each part, we define its local adversarial loss as,</p><formula xml:id="formula_16">a,pi = min Θ max Dp i E Ip i ∼p data (Ip i ) [log D pi (I pi )]+ EÎ p i ∼prec(Îp i ) [log(1 − D pi (Î pi ))],<label>(14)</label></formula><p>where p data (I pi ) and p rec (Î pi ) stands for the distributions of the i-th part from I andÎ, respectively. D pi denotes the i-th part discriminator. To sum up, the overall adversarial loss is defined as, L a = λ a,g a,g + i λ a,pi a,pi ,</p><p>where a,g represents the global adversarial loss <ref type="bibr" target="#b6">[7]</ref> working on the whole image rather than parts, λ a,g and λ a,pi are the tradeoff parameters for the global and local adversarial losses, respectively. Here, left eye, right eye, nose, and mouth denote the first, second, third and fourth parts, respectively. For each part cropped from face images, we employ bi-linear interpolation to resize it to 128 × 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning Objective</head><p>Taking all the losses on FlowNet, LightNet and RecNet into account, the overall objective of SymmFCNet can be defined as,</p><formula xml:id="formula_18">L = L r + L a + λ s L s + λ l L l + λ lm L lm + λ T V L T V ,<label>(16)</label></formula><p>where λ s , λ l , λ lm and λ T V are the tradeoff parameters for symmetry consistency loss, illumination consistency loss, landmark loss and TV regularization, respectively. Note that our SymmFCNet is constructed by stacking generative reconstruction subnet upon illumination reweighted warping subnet and can be trained in an end-to-end manner. Thus, FlowNet and LightNet can also be learned by minimizing L r , L a and L s even they are defined on RecNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, experiments are conducted to assess our SymmFCNet and compare it with the state-of-the-art image inpainting and face completion methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>. For comprehensive evaluation, quantitative and qualitative results as well as user study are reported. In addition, we test the completion performance on both images with synthetic missing pixels and images with real occlusion. Testing code is available at: https://github.com/ csxmli2016/SymmFCNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Setting</head><p>The VGGFace2 dataset <ref type="bibr" target="#b2">[3]</ref> is used to train our Symm-FCNet. The dataset contains 9,131 identities and each has an average of 362 images, from which we manually select 19,000 images to constitute our training set by excluding images with low quality and large occlusions. A validation set is also built by selecting another 400 images from VGGFace2 for guiding the settings of model and learning parameters. We adopt two test sets to assess SymmFC-Net. The first one involves 1,200 images from VGGFace2, and the other includes 1,200 images from WebFace <ref type="bibr" target="#b31">[32]</ref> to verify generalization performance across datasets. The identities of face images from training, validation and test sets are non-overlapped. Using bounding box detected by MTCNN <ref type="bibr" target="#b33">[34]</ref>, each face image is cropped and resized to 256 × 256.</p><p>The model parameters for SymmFCNet are set as follows: λ r,2 = 300, λ r,p = 0.01, λ a,g = 100, λ a,p1 = λ a,p2 = 100, λ a,p3 = λ a,p4 = 80, λ s = 50, λ lm = 10, λ T V = 1, λ l = 100. The pixel-missing masks are generated by randomly selecting the location and mask size. Data augmentation such as flipping and random cropping are also adopted. The training of SymmFCNet includes three stages. (i) We first pre-train illumination reweighted warping subnet for 10 epochs. (ii) Fixed FlowNet and LightNet, we pre-train RecNet for 20 epochs. (iii) Finally, the full Symm-FCNet is end-to-end trained by minimizing the learning objective L. To train SymmFCNet, we use the ADAM algorithm <ref type="bibr" target="#b13">[14]</ref> with the learning rate of 2 × 10 −4 , 2 × 10 −5 , 2 × 10 −6 and β 1 = 0.5, where a smaller learning rate is adopted until the reconstruction loss L r on validation set becomes non-decreasing. To improve the perception quality, the tradeoff parameters of adversarial losses are gradually increased according to L r on validation set. The batch size is 1 and the training is stopped after 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Images with Synthetic Missing Pixels</head><p>Quantitative and qualitative results are reported on our SymmFCNet and four state-of-the-art methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>. Among them, Li et al. <ref type="bibr" target="#b18">[19]</ref> and Iizuka et al. <ref type="bibr" target="#b10">[11]</ref> can only handle 128 × 128 images, and we use bicubic interpolation to upsample the output to the size of 256 × 256. For</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>VggFace2 <ref type="bibr" target="#b2">[3]</ref> WebFace <ref type="bibr">[</ref> Iizuka et al. <ref type="bibr" target="#b10">[11]</ref>, we exploit the alignment tool suggested by the authors to pre-process the input image. For Liu et al. <ref type="bibr" target="#b19">[20]</ref>, it upsamples the input to 512 × 512 and we downsample the output to 256×256. Online manual specification of missing masks is required to obtain the results by Liu et al. <ref type="bibr" target="#b19">[20]</ref>, and we thus do not report its quantitative metrics (e.g., PSNR) because it is exhausted to manually edit the masks for thousands of images. <ref type="table">Table 1</ref> lists the PSNR, SSIM, identity distance (Dis.) by OpenFace toolbox <ref type="bibr" target="#b0">[1]</ref>, and perceptual similarity (LPIPS) <ref type="bibr" target="#b34">[35]</ref> on the the two test sets (i.e., VGGFace and WebFace). In comparison with the competing methods, notable PSNR gain (i.e., &gt;1 dB) is achieved by our Symm-FCNet. In terms of SSIM, our SymmFCNet also performs favorably. LPIPS <ref type="bibr" target="#b34">[35]</ref> is a recently proposed perceptual similarity which is more consistent with human perception. Again our SymmFCNet achieves the best LPIPS performance in comparison to the competing methods. In addition, identity distance measures whether the result and ground-truth have the same identity, and thus can be used to assess the coherence of the completion result with surrounding context. From <ref type="table">Table 1</ref>, it can be seen that SymmFCNet exhibits better identity-preserving ability than the competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative Results</head><p>The solutions for face completion are neither unique nor required to exactly approximate the ground-truth. Thus, qualitative comparison is conducted to show the effectivenss of our methods. <ref type="figure" target="#fig_2">Figs. 3 and 4</ref> show the completion results on rectangular and irregular holes, respectively. The results by Liu et al. <ref type="bibr" target="#b19">[20]</ref> are also included for comparison. Benefited from the joint effectiveness of illumination reweighted warping and perceptual symmetric loss, our SymmFCNet can achieve very promising inpainting results which preserve visually pleasing symmetry consistent details for missing pixels within only one half-faces and both half-faces. In comparison, the methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> fail to re-    <ref type="bibr" target="#b19">[20]</ref> is still limited in maintaining global symmetry consistency and sometimes fails in generating plausible results with large occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">User Study</head><p>User study is conducted on a crowdsourcing platform for three types of missing pixels, i.e., regular mask, irregular mask and real occlusions, which contain 50, 25 and 25 images, respectively. For each image, we display the results by our SymmFCNet and the methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref> in random order to 50 workers who are required to choose the one with the best global consistency and perception quality. We use the percent of the votes of one particular algorithm against all votes to evaluate the performance of the algorithm in <ref type="table">Ta-ble.</ref> 2. The result by SymmFCNet has 84.68% probability on average to be selected as the best one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Running Time</head><p>All the experiments are conducted on a computer equipped with Intel Xeon E3 CPU and NVIDIA GeForce GTX 1080Ti GPU. And the model is trained and tested with Torch. Our SymmFCNet takes 36.29 ms on average for completing a 256 × 256 image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Images with Real Occlusions</head><p>By manually specifying the missing masks, <ref type="figure">Fig. 5</ref> shows the completion results on two face images with real occlusions. For the first image, even the occlusion is large and nearly symmetric, SymmFCNet still performs favorably, validating the effectiveness of perceptual symmetry loss. As for the second image, the occlusion is mainly in one half-face, and the result by SymmFCNet is globally more symmetry consistent in comparison to Liu et al. <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Two groups of experiments are conducted to assess the contributions of main components in our SymmFCNet. In the first group of experiments, <ref type="figure">Fig. 6</ref> shows the intermediate results of SymmFCNet, including warped images, and the completion results after FlowNet, LightNet, and Rec-Net. From <ref type="figure">Fig. 6</ref>, we have the following observations: (i) FlowNet can correctly align the flip image with the original one, and construct the correspondence between left and right half-faces. (ii) Although the correspondence can be used to fill in missing pixels within only one half-face, the result suffers from illumination inconsistency, and can be improved via the introduction of LightNet. (iii) RecNet not only can fill in missing pixels on both of the half-faces, but also is effective in further refining the result of illuminationreweighted warping.</p><p>In the second group of experiments, we further assess the effect of perceptual symmetry loss, FlowNet, and Light-Net. To this end, we consider five variants of SymmFCNet:  <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_6">Fig. 7</ref> report the quantitative and qualitative results of these variants. By removing FlowNet, LightNet and perceptual symmetry loss, plain RecNet only performs on par with Li et al. <ref type="bibr" target="#b18">[19]</ref>  <ref type="table">(Table 1)</ref> and is prone to symmetry-inconsistent completion results ( <ref type="figure" target="#fig_6">Fig. 7 (b)</ref>).</p><p>FlowNet. The flow field by FlowNet can be exploited for (i) guiding the completion of missing pixels within only one half-face and (ii) incorporating with L s to train Rec-Net. Here we only focus on (i) and compare SymmFC-Net (-L) and SymmFCNet (-GL0). By using FlowNet to complete missing pixels within only one half-face, notable gains on PSNR, LPIPS and identity distance can be attained by SymmFCNet (-L) (see <ref type="table">Table 1</ref>). From <ref type="figure" target="#fig_6">Fig. 7</ref>(c) and <ref type="figure" target="#fig_6">Fig. 7(d)</ref>, SymmFCNet (-GL0) is still limited in preserving the symmetry consistency of result, while it can be well addressed by SymmFCNet (-L).</p><p>Perceptual symmetry loss. The contribution of perceptual symmetry loss can be assessed by both SymmFCNet (-GL0) vs RecNet and SymmFCNet (Full) vs SymmFCNet (-S). In comparison to plain RecNet, SymmFCNet (-GL0) can achieve moderate gains on quantitative metrics (see <ref type="table">Table 1</ref>) and more symmetry consistent results (see <ref type="figure" target="#fig_6">Fig. 7</ref>(b)(c)). It is worth to note that, compared with SymmFCNet (-S), much more gains (e.g., 1.1 dB by PSNR) can be obtained by SymmFCNet (Full). From <ref type="figure" target="#fig_6">Fig. 7</ref> (e) and (f), SymmFC-Net (Full) is also able to correct the artifacts and illumination inconsistency produced in the first stage. Thus, RecNet with perceptual symmetry loss is helpful in filling missing pixels on both of the half-faces and refining the result of illumination-reweighted warping. LightNet. We further compare SymmFCNet (Full) with SymmFCNet (-L) to assess the contribution of LightNet. It can be seen that the introduction of LightNet can further improve the quantitative performance (see <ref type="table">Table 1</ref>) and generate illumination consistent results (see <ref type="figure" target="#fig_6">Fig. 7</ref>(d)(f)). We also note that, RecNet also benefits the correction of illumination inconsistency, and SymmFCNet (-L) attains the second best quantitative performance among the five SymmFCNet varaints. Even though, from the top image in <ref type="figure" target="#fig_6">Fig. 7(d)</ref>, illumination inconsistency remains obvious for the result by SymmFCNet (-L), indicating that LightNet is still required and cannot be totally replaced by RecNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This work presents a symmetry consistent CNN model, i.e., SymmFCNet, for effective face completion. In the proposed method, a FlowNet is adopted to construct the correspondence between two half-faces. The correspondence is then combined with a LightNet for filling in missing pixels within only one half-face, and incorporated with RecNet in the form of perceptual symmetry loss for recovering missing pixels in both of half-faces. Extensive experiments show the the effectiveness of SymmFCNet on generating photorealistic results with fine details for inpainting rectangular and irregular holes and even real occlusions. In terms of quantitative metrics, perception quality and user study, our SymmFCNet performs favorably against state-of-the-arts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Completion results of face images with synthetic and real occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>I o (M ) of occluded face (mask) I o (M ), and adopt a FlowNet which takes both I o and I o to predict the flow field</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Iizuka et al.<ref type="bibr" target="#b10">[11]</ref> (c) Li et al.<ref type="bibr" target="#b18">[19]</ref> (d) Yu et al.<ref type="bibr" target="#b32">[33]</ref> (e) Liu et al.<ref type="bibr" target="#b19">[20]</ref> (f) Ours (g) Ground-truth Completion results on regular holes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Iizuka et al.<ref type="bibr" target="#b10">[11]</ref> (c) Li et al.<ref type="bibr" target="#b18">[19]</ref> (d) Yu et al.<ref type="bibr" target="#b32">[33]</ref> (e) Liu et al.<ref type="bibr" target="#b19">[20]</ref> (f) Ours (g) Ground-truth Completion results on irregular holes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(i) SymmFCNet (Full), (ii) SymmFCNet (-S): removing perceptual symmetry loss, (iii) SymmFCNet (-L): removing LightNet, (iv) SymmFCNet (-GL0): removing Light-Net and applying the predicted flow field only in perceptual symmetry loss, (v) plain RecNet: removing FlowNet, LightNet and perceptual symmetry loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Iizuka et al.<ref type="bibr" target="#b10">[11]</ref> (c) Li et al.<ref type="bibr" target="#b18">[19]</ref> (d) Yu et al.[33] (e) Liu et al. [20] (f) Ours Completion results on images with real occlusion. Intermediate results of SymmFCNet, (a) occluded face I o , (b) warped image I w by FlowNet, (c) completion result in the first stage without illumination correction, (d) completion result in the first stage after illumination correction, (e) final completion resultÎ from RecNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Results of our SymmFCNet variants. From left to right: (a) input, (b) RecNet, (c) SymmFCNet (-GL0), (d) Symm-FCNet (-L), (e) SymmFCNet (-S), (f) SymmFCNet (Full), (g) ground-truth. Best viewed by zooming in the screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Iizuka et al.<ref type="bibr" target="#b10">[11]</ref> 18.62 .688 .513 1.325 19.04 .683 .504 1.462 Li et al. [19] 25.05 .932 .397 0.932 25.65 .959 .371 1.116 Yu et al. [33] 25.53 .963 .292 0.788 25.96 .965 .270 0.965 Ablation study Plain RecNet 24.99 .957 .279 0.864 25.81 .959 .310 1.035 SymmFcNet (-GL0) 25.54 .959 .260 0.830 25.94 .963 .294 0.987</figDesc><table><row><cell></cell><cell>32]</cell></row><row><cell></cell><cell>PSNR↑SSIM↑LPIPS↓ Dis.↓ PSNR↑SSIM↑LPIPS↓ Dis.↓</cell></row><row><cell>State-of-</cell><cell></cell></row><row><cell>the-arts</cell><cell></cell></row><row><cell cols="2">SymmFcNet (-L) 26.43 .967 .226 0.622 26.43 .968 .258 0.852</cell></row><row><cell cols="2">SymmFCNet (-S) 26.33 .962 .232 0.714 26.17 .961 .266 0.945</cell></row><row><cell>SymmFCNet (Full)</cell><cell>27.81 .970 .219 0.617 27.22 .969 .252 0.849</cell></row><row><cell cols="2">Table 1: Quantitative results. Here, ↑ (↓) indicates higher</cell></row><row><cell>(lower) is better.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Voting results for three types of missing pixels. cover rich details and even semantic facial structures, while Liu et al.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Openface: A general-purpose face recognition library with mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ludwiczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>CMU School of Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 27th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical symmetric shape from shading for 3d structure recovery of faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dovgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2017 iccv challenge: Detecting symmetry in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepwarp: Photorealistic image resynthesis for gaze manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sungatullina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Is there a connection between face symmetry and face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harguess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in realtime with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for realtime style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep identity-aware transfer of facial attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05586</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning warped guidance for blind face restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative face completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Computational symmetry in computer vision and computer graphics. Foundations and Trends R in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hel-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Using facial symmetry to handle pose variations in realworld 3d face recognition. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contextual-based image inpainting: Infer, match, and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Face recognition robust to left/right shadows; facial symmetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U.-D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shift-net: Image inpainting via deep feature rearrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semantic facial expression editing using autoencoded flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09961</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Identity preserving face completion for large ocular region occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bessinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conv (d, k, s) and TransConv (d, k, s) denote the convolutional layer and transposed convolutional layer, respectively. d, k and s represent output dimension, kernel size and stride, respectively. BN is batch normalization and Concat indicates the concatenation from the i-th layer to the (L − i)-th layer via skip connetion (L is the depth of Rec-Net). LReLU and DropOut are equipped with parameters of 0.2 and 0.5, respectively. Besides, as for global and part discriminators, the architectures are demonstrated in Table B</title>
	</analytic>
	<monogr>
		<title level="m">Our SymmFCNet consists of three sub-networks, i.e., FlowNet, LightNet and RecNet. FlowNet and LightNet have the same structure except the channel number of the output. Architecture details are shown in Table A. Here</title>
		<imprint/>
	</monogr>
	<note>PatchGAN is adopted to classify if each N × N patch in an image is real or fake [12</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">FlowNet LightNet</title>
		<imprint/>
	</monogr>
	<note>RecNet Input (6 × 256 × 256) Input(6 × 256 × 256</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Transconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dropout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Concat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Transconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Transconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dropout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Concat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Transconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Concat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Transconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Transconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Concat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Transconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Transconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Concat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Transconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Transconv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Concat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LReLU Conv (128</title>
		<imprint>
			<date type="published" when="1024" />
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note>LReLU TransConv. ReLU TransConv (1024, 4, 2), BN, DropOut, Concat, LReLU TransConv (512, 4, 2), BN, ReLU TransConv (512, 4, 2. Tanh TransConv (3, 4, 2. ReLU TransConv (3, 4, 2. Sigmoid Output(2 × 256 × 256) Output(3 × 256 × 256) Output(3 × 256 × 256</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<title level="m">Table A: Network architecture of SymmFCNet. Global Discriminator Part Discriminator Input(3 × 256 × 256)</title>
		<imprint/>
	</monogr>
	<note>Input(3 × 128 × 128</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<title level="m">LReLU Conv</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lrelu</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>Sigmoid Conv(1, 4, 1. Sigmoid Output(1 × 30 × 30</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Table B: Network architecture of global and part discriminators</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
