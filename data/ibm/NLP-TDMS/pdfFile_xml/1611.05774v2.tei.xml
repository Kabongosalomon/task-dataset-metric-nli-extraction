<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Do Recurrent Neural Network Grammars Learn About Syntax?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
							<email>akuncoro@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
							<email>miguel.ballesteros@ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
							<email>lingpenk@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<email>gneubig@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">What Do Recurrent Neural Network Grammars Learn About Syntax?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural network grammars (RNNG) are a recently proposed probabilistic generative modeling family for natural language. They show state-ofthe-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model's latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we focus on a recently proposed class of probability distributions, recurrent neural network grammars (RNNGs; <ref type="bibr" target="#b10">Dyer et al., 2016</ref>), designed to model syntactic derivations of sentences. We focus on RNNGs as generative probabilistic models over trees, as summarized in §2.</p><p>Fitting a probabilistic model to data has often been understood as a way to test or confirm some aspect of a theory. We talk about a model's assumptions and sometimes explore its parameters or posteriors over its latent variables in order to gain understanding of what it "discovers" from the data. In some sense, such models can be thought of as mini-scientists.</p><p>Neural networks, including RNNGs, are capable of representing larger classes of hypotheses than traditional probabilistic models, giving them more freedom to explore. Unfortunately, they tend to be bad mini-scientists, because their parameters are difficult for human scientists to interpret.</p><p>RNNGs are striking because they obtain stateof-the-art parsing and language modeling performance. Their relative lack of independence assumptions, while still incorporating a degree of linguistically-motivated prior knowledge, affords the model considerable freedom to derive its own insights about syntax. If they are mini-scientists, the discoveries they make should be of particular interest as propositions about syntax (at least for the particular genre and dialect of the data).</p><p>This paper manipulates the inductive bias of RNNGs to test linguistic hypotheses. <ref type="bibr">1</ref> We begin with an ablation study to discover the importance of the composition function in §3. Based on the findings, we augment the RNNG composition function with a novel gated attention mechanism (leading to the GA-RNNG) to incorporate more interpretability into the model in §4. Using the GA-RNNG, we proceed by investigating the role that individual heads play in phrasal representation ( §5) and the role that nonterminal category labels play ( §6). Our key findings are that lexical heads play an important role in representing most phrase types (although compositions of multiple salient heads are not infrequent, especially 1 RNNGs have less inductive bias relative to traditional unlexicalized probabilistic context-free grammars, but more than models that parse by transducing word sequences to linearized parse trees represented as strings <ref type="bibr" target="#b31">(Vinyals et al., 2015)</ref>. Inductive bias is necessary for learning <ref type="bibr" target="#b25">(Mitchell, 1980)</ref>; we believe the important question is not "how little can a model get away with?" but rather the benefit of different forms of inductive bias as data vary.</p><p>for conjunctions) and that nonterminal labels provide little additional information. As a by-product of our investigation, a variant of the RNNG without ensembling achieved the best reported supervised phrase-structure parsing (93.6 F 1 ; English PTB) and, through conversion, dependency parsing <ref type="bibr">(95.8 UAS,</ref><ref type="bibr">94.6 LAS;</ref><ref type="bibr">PTB SD)</ref>. The code and pretrained models to replicate our results are publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recurrent Neural Network Grammars</head><p>An RNNG defines a joint probability distribution over string terminals and phrase-structure nonterminals. 3 Formally, the RNNG is defined by a triple N, Σ, Θ , where N denotes the set of nonterminal symbols (NP, VP, etc.), Σ the set of all terminal symbols (we assume that N ∩ Σ = ∅), and Θ the set of all model parameters. Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations <ref type="bibr" target="#b6">(Collins, 1997;</ref><ref type="bibr" target="#b21">Klein and Manning, 2003)</ref>, the RNNG implicitly parameterizes the information passed through compositions of phrases (in Θ and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars.</p><p>The RNNG is based on an abstract state machine like those used in transition-based parsing, with its algorithmic state consisting of a stack of partially completed constituents, a buffer of already-generated terminal symbols, and a list of past actions. To generate a sentence x and its phrase-structure tree y, the RNNG samples a sequence of actions to construct y top-down. Given y, there is one such sequence (easily identified), which we call the oracle, a = a 1 , . . . , a n used during supervised training.</p><p>The RNNG uses three different actions:</p><formula xml:id="formula_0">• NT(X), where X ∈ N ,</formula><p>introduces an open nonterminal symbol onto the stack, e.g., "(NP";  <ref type="figure">Figure 1</ref>: The RNNG consists of a stack, buffer of generated words, and list of past actions that lead to the current configuration. Each component is embedded with LSTMs, and the parser state summary u t is used as top-layer features to predict a softmax over all feasible actions. This figure is due to <ref type="bibr" target="#b10">Dyer et al. (2016)</ref>.</p><formula xml:id="formula_1">• GEN(x), where x ∈ Σ,</formula><p>open nonterminal) are popped, a composition function is executed, yielding a composed representation that is pushed onto the stack.</p><p>At each timestep, the model encodes the stack, buffer, and past actions, with a separate LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> for each component as features to define a distribution over the next action to take (conditioned on the full algorithmic state). The overall architecture is illustrated in <ref type="figure">Figure 1</ref>; examples of full action sequences can be found in <ref type="bibr" target="#b10">Dyer et al. (2016)</ref>.</p><p>A key element of the RNNG is the composition function, which reduces a completed constituent into a single element on the stack. This function computes a vector representation of the new constituent; it also uses an LSTM (here a bidirectional one). This composition function, which we consider in greater depth in §3, is illustrated in <ref type="figure">Fig. 2</ref>. <ref type="figure">Figure 2</ref>: RNNG composition function on each REDUCE operation; the network on the right models the structure on the left <ref type="bibr" target="#b10">(Dyer et al., 2016)</ref>.</p><formula xml:id="formula_2">NP u v w NP u v w NP x x</formula><p>Since the RNNG is a generative model, it attempts to maximize p(x, y), the joint distribution of strings and trees, defined as p(x, y) = p(a) = n t=1 p(a t | a 1 , . . . , a t−1 ).</p><p>In other words, p(x, y) is defined as a product of local probabilities, conditioned on all past actions. The joint probability estimate p(x, y) can be used for both phrase-structure parsing (finding arg max y p(y | x)) and language modeling (finding p(x) by marginalizing over the set of possible parses for x). Both inference problems can be solved using an importance sampling procedure. <ref type="bibr">4</ref> We report all RNNG performance based on the corrigendum to <ref type="bibr" target="#b10">Dyer et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Composition is Key</head><p>Given the same data, under both the discriminative and generative settings RNNGs were found to parse with significantly higher accuracy than (respectively) the models of <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref> and Choe and Charniak (2016) that represent y as a "linearized" sequence of symbols and parentheses without explicitly capturing the tree structure, or even constraining the y to be a well-formed tree (see <ref type="table">Table 1</ref>). <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref> directly predict the sequence of nonterminals, "shifts" (which consume a terminal symbol), and parentheses from left to right, conditional on the input terminal sequence x, while Choe and Charniak (2016) used a sequential LSTM language model on the same linearized trees to create a generative variant of the <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref> model. The generative model is used to re-rank parse candidates. The results in <ref type="table">Table 1</ref> suggest that the RNNG's explicit composition function <ref type="figure">(Fig. 2)</ref>, which <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref> and Choe and Charniak (2016) must learn implicitly, plays a crucial role in the RNNG's generalization success. Beyond this, Choe and Charniak's generative variant of <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref> is another instance where generative models trained on the PTB outperform discriminative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ablated RNNGs</head><p>On close inspection, it is clear that the RNNG's three data structures-stack, buffer, and action history-are redundant. For example, the action history and buffer contents completely determine the structure of the stack at every timestep. Every generated word goes onto the stack, too; and some past words will be composed into larger structures, but through the composition function, they are all still "available" to the network that predicts the next action. Similarly, the past actions are redundant with the stack. Despite this redundancy, only the stack incorporates the composition function. Since each of the ablated models is sufficient to encode all necessary partial tree information, the primary difference is that ablations with the stack use explicit composition, to which we can therefore attribute most of the performance difference.</p><p>We conjecture that the stack-the component that makes use of the composition function-is critical to the RNNG's performance, and that the buffer and action history are not. In transitionbased parsers built on expert-crafted features, the most recent words and actions are useful if they are salient, although neural representation learners can automatically learn what information should be salient.</p><p>To test this conjecture, we train ablated RN-NGs that lack each of the three data structures (action history, buffer, stack), as well as one that lacks both the action history and buffer. 5 If our conjecture is correct, performance should degrade most without the stack, and the stack alone should perform competitively.</p><p>Experimental settings. We perform our experiments on the English PTB corpus, with §02-21 for training, §24 for validation, and §23 for test; no additional data were used for training. We fol-low the same hyperparameters as the generative model proposed in <ref type="bibr" target="#b10">Dyer et al. (2016)</ref>. <ref type="bibr">6</ref> The generative model did not use any pretrained word embeddings or POS tags; a discriminative variant of the standard RNNG was used to obtain tree samples for the generative model. All further experiments use the same settings and hyperparameters unless otherwise noted.</p><p>Experimental results. We trained each ablation from scratch, and compared these models on three tasks: English phrase-structure parsing (labeled F 1 ), <ref type="table">Table 2</ref>; dependency parsing, <ref type="table" target="#tab_4">Table 3</ref>, by converting parse output to Stanford dependencies (De <ref type="bibr" target="#b8">Marneffe et al., 2006)</ref> using the tool by <ref type="bibr" target="#b22">Kong and Smith (2014)</ref>; and language modeling, GA-RNNG 93.5 <ref type="table">Table 2</ref>: Phrase-structure parsing performance on PTB §23. † indicates systems that use additional unparsed data (semisupervised). The GA-RNNG results will be discussed in §4.</p><p>Discussion. The RNNG with only a stack is the strongest of the ablations, and it even outperforms the "full" RNNG with all three data structures. Ablating the stack gives the worst among the new results. This strongly supports the importance of the composition function: a proper REDUCE operation that transforms a constituent's parts and nonterminal label into a single explicit (vector) representation is helpful to performance.</p><p>It is noteworthy that the stack alone is stronger than the original RNNG, which-in principlecan learn to disregard the buffer and action his-  tory. Since the stack maintains syntactically "recent" information near its top, we conjecture that the learner is overfitting to spurious predictors in the buffer and action history that explain the training data but do not generalize well.</p><p>A similar performance degradation is seen in language modeling <ref type="table" target="#tab_2">(Table 4)</ref>: the stack-only RNNG achieves the best performance, and ablating the stack is most harmful. Indeed, modeling syntax without explicit composition (the stackablated RNNG) provides little benefit over a sequential LSTM language model.  We remark that the stack-only results are the best published PTB results for both phrasestructure and dependency parsing among supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Gated Attention RNNG</head><p>Having established that the composition function is key to RNNG performance ( §3), we now seek to understand the nature of the composed phrasal representations that are learned. Like most neural networks, interpreting the composition function's behavior is challenging. Fortunately, linguistic theories offer a number of hypotheses about the nature of representations of phrases that can provide a conceptual scaffolding to understand them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Linguistic Hypotheses</head><p>We consider two theories about phrasal representation. The first is that phrasal representations are strongly determined by a privileged lexical head. Augmenting grammars with lexical head information has a long history in parsing, starting with the models of <ref type="bibr" target="#b6">Collins (1997)</ref>, and theories of syntax such as the "bare phrase structure" hypothesis of the Minimalist Program <ref type="bibr" target="#b5">(Chomsky, 1993)</ref> posit that phrases are represented purely by single lexical heads. Proposals for multiple headed phrases (to deal with tricky cases like conjunction) likewise exist <ref type="bibr" target="#b12">(Jackendoff, 1977;</ref><ref type="bibr" target="#b18">Keenan, 1987)</ref>. Do the phrasal representations learned by RN-NGs depend on individual lexical heads or multiple heads? Or do the representations combine all children without any salient head?</p><p>Related to the question about the role of heads in phrasal representation is the question of whether phrase-internal material wholly determines the representation of a phrase (an endocentric representation) or whether nonterminal relabeling of a constitutent introduces new information (exocentric representations). To illustrate the contrast, an endocentric representation is representing a noun phrase with a noun category, whereas S → NP VP exocentrically introduces a new syntactic category that is neither NP nor VP <ref type="bibr" target="#b4">(Chomsky, 1970)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Gated Attention Composition</head><p>To investigate what the stack-only RNNG learns about headedness (and later endocentricity), we propose a variant of the composition function that makes use of an explicit attention mechanism <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref> and a sigmoid gate with multiplicative interactions, henceforth called GA-RNNG.</p><p>At every REDUCE operation, the GA-RNNG assigns an "attention weight" to each of its children (between 0 and 1 such that the total weight off all children sums to 1), and the parent phrase is represented by the combination of a sum of each child's representation scaled by its attention weight and its nonterminal type. Our weighted sum is more expressive than traditional head rules, however, because it allows attention to be divided among multiple constituents. Head rules, con-versely, are analogous to giving all attention to one constituent, the one containing the lexical head.</p><p>We now formally define the GA-RNNG's composition function. Recall that u t is the concatenation of the vector representations of the RNNG's data structures, used to assign probabilities to each of the actions available at timestep t (see <ref type="figure">Fig. 1</ref>, the layer before the softmax at the top). For simplicity, we drop the timestep index here. Let o nt denote the vector embedding (learned) of the nonterminal being constructed, for the purpose of computing attention weights.</p><p>Now let c 1 , c 2 , . . . denote the sequence of vector embeddings for the constituents of the new phrase. The length of these vectors is defined by the dimensionality of the bidirectional LSTM used in the original composition function <ref type="figure">(Fig. 2)</ref>. We use semicolon (;) to denote vector concatenation operations.</p><p>The attention vector is given by:</p><formula xml:id="formula_3">a = softmax [c 1 c 2 · · · ] V [u; o nt ]<label>(1)</label></formula><p>Note that the length of a is the same as the number of constituents, and that this vector sums to one due to the softmax. It divides a single unit of attention among the constituents. Next, note that the constituent source vector m = [c 1 ; c 2 ; · · · ]a is a convex combination of the child-constituents, weighted by attention. We will combine this with another embedding of the nonterminal denoted as t nt (separate from o nt ) using a sigmoid gating mechanism:</p><formula xml:id="formula_4">g = σ (W 1 t nt + W 2 m + b)<label>(2)</label></formula><p>Note that the value of the gate is bounded between [0, 1] in each dimension.</p><p>The new phrase's final representation uses element-wise multiplication ( ) with respect to both t nt and m, a process reminiscent of the LSTM "forget" gate:</p><formula xml:id="formula_5">c = g t nt + (1 − g) m.</formula><p>(</p><p>The intuition is that the composed representation should incorporate both nonterminal information and information about the constituents (through weighted sum and attention mechanism). The gate g modulates the interaction between them to account for varying importance between the two in different contexts.</p><p>Experimental results. We include this model's performance in Tables 2-4 (last row in all tables). It is clear that the model outperforms the baseline RNNG with all three structures present and achieves competitive performance with the strongest, stack-only, RNNG variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Headedness in Phrases</head><p>We now exploit the attention mechanism to probe what the RNNG learns about headedness on the WSJ §23 test set (unseen before by the model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Heads that GA-RNNG Learns</head><p>The attention weight vectors tell us which constituents are most important to a phrase's vector representation in the stack. Here, we inspect the attention vectors to investigate whether the model learns to center its attention around a single, or by extension a few, salient elements, which would confirm the presence of headedness in GA-RNNG.</p><p>First, we consider several major nonterminal categories, and estimate the average perplexity of the attention vectors. The average perplexity can be interpreted as the average number of "choices" for each nonterminal category; this value is only computed for the cases where the number of components in the composed phrase is at least two (otherwise the attention weight would be trivially 1). The minimum possible value for the perplexity is 1, indicating a full attention weight around one component and zero everywhere else. <ref type="figure" target="#fig_0">Figure 3</ref> (in blue) shows much less than 2 average "choices" across nonterminal categories, which also holds true for all other categories not shown. For comparison we also report the average perplexity of the uniform distribution for the same nonterminal categories ( <ref type="figure" target="#fig_0">Fig. 3 in red)</ref>; this represents the highest entropy cases where there is no headedness at all by assigning the same attention weight to each constituent (e.g. attention weights of 0.25 each for phrases with four constituents). It is clear that the learned attention weights have much lower perplexity than the uniform distribution baseline, indicating that the learned attention weights are quite peaked around certain components. This implies that phrases' vectors tend to resemble the vector of one salient constituent, but not exclusively, as the perplexity for most categories is still not close to one.</p><p>Next, we consider the how attention is distributed for the major nonterminal categories in <ref type="table" target="#tab_7">Table 5</ref>, where the first five rows of each category represent compositions with highest entropy, and the next five rows are qualitatively analyzed. The high-entropy cases where the attention is most divided represent more complex phrases with conjunctions or more than one plausible head.</p><p>NPs. In most simple noun phrases (representative samples in rows 6-7 of Table 5), the model pays the most attention to the rightmost noun and assigns near-zero attention on determiners and possessive determiners, while also paying nontrivial attention weights to the adjectives. This finding matches existing head rules and our intuition that nouns head noun phrases, and that adjectives are more important than determiners.</p><p>We analyze the case where the noun phrase contains a conjunction in the last three rows of <ref type="table" target="#tab_7">Table  5</ref>. The syntax of conjunction is a long-standing source of controversy in syntactic analysis <ref type="bibr">(Johannessen, 1998, inter alia)</ref>. Our model suggests that several representational strategies are used, when coordinating single nouns, both the first noun (8) and the last noun (9) may be selected. However, in the case of conjunctions of multiple noun phrases (as opposed to multiple single-word nouns), the model consistently picks the conjunction as the head. All of these representational strategies have been argued for individually on linguistic grounds, and since we see all of them present, RNNGs face the same confusion that linguists do.</p><p>VPs. The attention weights on simple verb phrases (e.g., "VP → V NP", 9) are peaked around the noun phrase instead of the verb. This implies that the verb phrase would look most similar to the noun under it and contradicts existing head rules that unanimously put the verb as the head of the verb phrase. Another interesting finding is that the model pays attention to polarity information, where negations are almost always assigned nontrivial attention weights. 7 Furthermore, we find that the model attends to the conjunction terminal in conjunctions of verb phrases (e.g., "VP → VP and VP", 10), reinforcing the similar finding for conjunction of noun phrases.</p><p>PPs. In almost all cases, the model attends to the preposition terminal instead of the noun phrases or complete clauses under it, regardless of the type of preposition. Even when the preposi-tional phrase is only used to make a connection between two noun phrases (e.g., "PP → NP after NP", 10), the prepositional connector is still considered the most salient element. This is less consistent with the Collins and Stanford head rules, where prepositions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule <ref type="bibr" target="#b15">(Johansson and Nugues, 2007)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison to Existing Head Rules</head><p>To better measure the overlap between the attention vectors and existing head rules, we converted the trees in PTB §23 into a dependency representation using the attention weights. In this case, the attention weight functions as a "dynamic" head rule, where all other constituents within the same composed phrase are considered to modify the constituent with the highest attention weight, repeated recursively. The head of the composed representation for "S" at the top of the tree is attached to a special root symbol and functions as the head of the sentence. We measure the overlap between the resulting tree and conversion results of the same trees using the <ref type="bibr" target="#b6">Collins (1997)</ref> and Stanford dependencies <ref type="bibr" target="#b8">(De Marneffe et al., 2006)</ref> head rules. Results are evaluated using the standard evaluation script (excluding punctuation) in terms of UAS, since the attention weights do not provide labels.</p><p>Results. The model has a higher overlap with the conversion using Collins head rules (49.8 UAS) rather than the Stanford head rules (40.4 UAS). We attribute this large gap to the fact that the Stanford head rules incorporate more semantic considerations, while the RNNG is a purely syntactic model. In general, the attention-based tree output has a high error rate (≈ 90%) when the dependent is a verb, since the constituent with the highest attention weight in a verb phrase is often the noun phrase instead of the verb, as discussed above. The conversion accuracy is better for nouns (≈ 50% error), and much better for determiners (30%) and particles (6%) with respect to the Collins head rules.</p><p>Discussion. GA-RNNG has the power to infer head rules, and to a large extent, it does. It follows some conventions that are established in one or more previous head rule sets (e.g., prepositions head prepositional phrases, nouns head noun phrases), but attends more to a verb phrase's nominal constituents than the verb. It is important to note that this is not the by-product of learning a specific model for parsing; the training objective is joint likelihood, which is not a proxy loss for parsing performance. These decisions were selected because they make the data maximally likely (though admittedly only locally maximally likely). We leave deeper consideration of this noun-centered verb phrase hypothesis to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The Role of Nonterminal Labels</head><p>Emboldened by our finding that GA-RNNGs learn a notion of headedness, we next explore whether heads are sufficient to create representations of phrases (in line with an endocentric theory of phrasal representation) or whether extra nonterminal information is necessary. If the endocentric hypothesis is true (that is, the representation of a phrase is built from within depending on its components but independent of explicit category labels), then the nonterminal types should be easily inferred given the endocentrically-composed representation, and that ablating the nonterminal information would not make much difference in performance. Specifically, we train a GA-RNNG on unlabeled trees (only bracketings without nonterminal types), denoted U-GA-RNNG.</p><p>This idea has been explored in research on methods for learning syntax with less complete annotation <ref type="bibr" target="#b26">(Pereira and Schabes, 1992)</ref>. A key finding from <ref type="bibr" target="#b20">Klein and Manning (2002)</ref> was that,  given bracketing structure, simple dimensionality reduction techniques could reveal conventional nonterminal categories with high accuracy; <ref type="bibr" target="#b27">Petrov et al. (2006)</ref> also showed that latent variables can be used to recover fine-grained nonterminal categories. We therefore expect that the vector embeddings of the constituents that the U-GA-RNNG correctly recovers (on test data) will capture categories similar to those in the Penn Treebank.</p><p>Experiments. Using the same hyperparameters and the PTB dataset, we first consider unlabeled F 1 parsing accuracy. On test data (with the usual split), the GA-RNNG achieves 94.2%, while the U-GA-RNNG achieves 93.5%. This result suggests that nonterminal category labels add a relatively small amount of information compared to purely endocentric representations.</p><p>Visualization. If endocentricity is largely sufficient to account for the behavior of phrases, where do our robust intuitions for syntactic category types come from? We use t-SNE (van der <ref type="bibr" target="#b30">Maaten and Hinton, 2008)</ref> to visualize composed phrase vectors from the U-GA-RNNG model applied to the unseen test data. <ref type="figure" target="#fig_2">Fig. 4</ref> shows that the U-GA-RNNG tends to recover nonterminal categories as encoded in the PTB, even when trained without them. 8 These results suggest nonterminal types can be inferred from the purely endocentric compositional process to a certain extent, and that the phrase clusters found by the U-GA-RNNG largely overlap with nonterminal categories.</p><p>Analysis of PP and SBAR. <ref type="figure" target="#fig_2">Figure 4</ref> indicates a certain degree of overlap between SBAR (red) and PP (yellow). As both categories are interesting from the linguistic perspective and quite similar, we visualize the learned phrase vectors of 40 randomly selected SBARs and PPs from the test set (using U-GA-RNNG), illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>. First, we can see that phrase representations for PPs and SBARs depend less on the nonterminal categories 9 and more on the connector. For instance, the model learns to cluster phrases that start with words that can be either prepositions or complementizers (e.g., for, at, to, under, by), regardless of whether the true nonterminal labels are PPs or SBARs. This suggests that SBARs that start with "prepositional" words are similar to PPs from the model's perspective. Second, the model learns to disregard the word that, as "SBAR → that S" and "SBAR → S" are close together. This finding is intuitive, as complementizer that is often optional (Jaeger, 2010), unlike prepositional words that might describe relative time and location. Third, certain categories of PPs and SBARs form their own separate clusters, such as those that involve the words because and of. We attribute these distinctions to the fact that these words convey different meanings than many prepositional words; the word of indicates possession while because indicates cause-and-effect relationship. These examples show that, to a certain extent, the GA-RNNG is able to learn non-trivial semantic information, even when trained on a fairly small amount of syntactic data. <ref type="bibr">SBAR</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The problem of understanding neural network models in NLP has been previously studied for sequential RNNs <ref type="bibr" target="#b17">(Karpathy et al., 2015;</ref><ref type="bibr" target="#b24">Li et al., 2016)</ref>. <ref type="bibr" target="#b29">Shi et al. (2016)</ref> showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by <ref type="bibr" target="#b31">Vinyals et al. (2015)</ref> and <ref type="bibr" target="#b32">Wiseman and Rush (2016)</ref>, who achieved competitive parsing accuracy without explicit composition. In another work,  investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sentiment and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG composition function is crucial to obtaining state-ofthe-art parsing performance.</p><p>Extensive prior work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized <ref type="bibr" target="#b6">(Collins, 1997)</ref> and nonterminal <ref type="bibr" target="#b16">(Johnson, 1998;</ref><ref type="bibr" target="#b21">Klein and Manning, 2003)</ref> augmentations. The conjecture that fine-grained nonterminal rules and labels can be discovered given weaker bracketing structures was based on several studies <ref type="bibr" target="#b2">(Chiang and Bikel, 2002;</ref><ref type="bibr" target="#b20">Klein and Manning, 2002;</ref><ref type="bibr" target="#b27">Petrov et al., 2006)</ref>.</p><p>In a similar work, Sangati and Zuidema (2009) proposed entropy minimization and greedy familiarity maximization techniques to obtain lexical heads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the "head rules" in the GA-RNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GA-RNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We probe what recurrent neural network grammars learn about syntax, through ablation scenarios and a novel variant with a gated attention mechanism on the composition function. The composition function, a key differentiator between the RNNG and other neural models of syntax, is crucial for good performance. Using the attention vectors we discover that the model is learning something similar to heads, although the attention vectors are not completely peaked around a single component. We show some cases where the attention vector is divided and measure the relationship with existing head rules. RNNGs without access to nonterminal information during training are used to support the hypothesis that phrasal representations are largely endocentric, and a visualization of representations shows that traditional nonterminal categories fall out naturally from the composed phrase vectors. This confirms previous conjectures that bracketing annotation does most of the work of syntax, with nonterminal categories easily discoverable given bracketing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Average perplexity of the learned attention vectors on the test set (blue), as opposed to the average perplexity of the uniform distribution (red), computed for each major phrase type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>09) Auto (0.31) Workers (0.2) union (0.22) president (0.18) buying (0.31) and (0.25) selling (0.21) NP (0.23) ADVP (0.14) on (0.72) NP (0.14) 2 no (0.29) major (0.05) Eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22) ADVP (0.27) show (0.29) PRT (0.23) PP (0.21) ADVP (0.05) for (0.54) NP (0.40) 3 Saatchi (0.12) client (0.14) Philips (0.21) Lighting (0.24) Co. (0.29) pleaded (0.48) ADJP (0.23) PP (0.15) PP (0.08) PP (0.06) ADVP (0.02) because (0.73) of (0.18) NP (0.07) 4 nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25) received (0.33) PP (0.18) NP (0.32) PP (0.17) such (0.31) as (0.65) NP (0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>t-SNE on composed vectors when training without nonterminal categories. Vectors in dark blue are VPs, red are SBARs, yellow are PPs, light blue are NPs, and green are Ss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Sample of PP and SBAR phrase representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The last row of each table reports the performance of a novel variant of the (stack-only) RNNG with attention, to be presented in §4.</figDesc><table><row><cell>Model</cell><cell>F 1</cell></row><row><cell>Vinyals et al. (2015)  †</cell><cell>92.1</cell></row><row><cell>Choe and Charniak (2016)</cell><cell>92.6</cell></row><row><cell cols="2">Choe and Charniak (2016)  † 93.8</cell></row><row><cell>Baseline RNNG</cell><cell>93.3</cell></row><row><cell cols="2">Ablated RNNG (no history) 93.2</cell></row><row><cell>Ablated RNNG (no buffer)</cell><cell>93.3</cell></row><row><cell>Ablated RNNG (no stack)</cell><cell>92.5</cell></row><row><cell>Stack-only RNNG</cell><cell>93.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Dependency parsing performance on PTB §23 with Stanford Dependencies (De Marneffe and Manning, 2008). † indicates systems that use additional unparsed data (semisupervised).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Language modeling: perplexity. IKN refers to Kneser-Ney 5-gram LM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Attention weight vectors for some representative samples for NPs, VPs, and PPs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Importance sampling works by using a proposal distribution q(y | x) that is easy to sample from. In<ref type="bibr" target="#b10">Dyer et al. (2016)</ref> and this paper, the proposal distribution is the discriminative variant of the RNNG; see<ref type="bibr" target="#b10">Dyer et al. (2016)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Note that the ablated RNNG without a stack is quite similar to<ref type="bibr" target="#b31">Vinyals et al. (2015)</ref>, who encoded a (partial) phrasestructure tree as a sequence of open and close parentheses, terminals, and nonterminal symbols; our action history is quite close to this, with each NT(X) capturing a left parenthesis and X nonterminal, and each REDUCE capturing a right parenthesis.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The model is trained using stochastic gradient descent, with a learning rate of 0.1 and a per-epoch decay of 0.08. All experiments with the generative RNNG used 100 tree samples for each sentence, obtained by sampling from the local softmax distribution of the discriminative RNNG.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Cf.<ref type="bibr" target="#b24">Li et al. (2016)</ref>, where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntax and language modeling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We see a similar result for the non-ablated GA-RNNG model, not shown for brevity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Recall that U-GA-RNNG is trained without access to the nonterminal labels; training the model with nonterminal information would likely change the findings.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was sponsored in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114; it was also supported in part by Contract No. W911NF-15-1-0543 with DARPA and the Army Research Office (ARO). Approved for public release, distribution unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Slav Petrov, and Michael Collins</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recovering latent information in treebanks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Bikel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COL-ING</title>
		<meeting>of COL-ING</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Remarks on nominalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in English Transformational Grammar</title>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A Minimalist Program for Linguistic Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three generative, lexicalised models for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Stanford typed dependencies manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<title level="m">Deep biaffine attention for neural dependency parsing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Jackendoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Redundancy and reduction: Speakers manage syntactic information density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Bondi Johannessen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extended constituent-to-dependency conversion for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NODALIDA</title>
		<meeting>of NODALIDA</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PCFG models of linguistic tree representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiply-headed noun phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">L</forename><surname>Keenan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic Inquiry</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="481" to="490" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simple and accurate dependency parsing using bidirectional LSTM feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A generative constituent-context model for improved grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An empirical comparison of parsing methods for stanford dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.4314</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">When are tree structures necessary for deep learning of representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The need for biases in learning generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Insideoutside reestimation from partially bracketed corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Schabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING-ACL</title>
		<meeting>of COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised methods for head assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Sangati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Does string-based neural MT learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using tsne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
