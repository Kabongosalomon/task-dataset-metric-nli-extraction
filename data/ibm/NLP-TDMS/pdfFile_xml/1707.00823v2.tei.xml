<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Human Pose Models from Synthesized Data for Robust RGB-D Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
							<email>jian.liu@research.uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">CSSE</orgName>
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling High-way</addrLine>
									<postCode>6009</postCode>
									<settlement>Crawley</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Rahmani</surname></persName>
							<email>hossein.rahmani@uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">CSSE</orgName>
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling High-way</addrLine>
									<postCode>6009</postCode>
									<settlement>Crawley</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
							<email>naveed.akhtar@uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">CSSE</orgName>
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling High-way</addrLine>
									<postCode>6009</postCode>
									<settlement>Crawley</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
							<email>ajmal.mian@uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">CSSE</orgName>
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling High-way</addrLine>
									<postCode>6009</postCode>
									<settlement>Crawley</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CSSE</orgName>
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling High-way</addrLine>
									<postCode>6009</postCode>
									<settlement>Crawley</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CSSE</orgName>
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling High-way</addrLine>
									<postCode>6009</postCode>
									<settlement>Crawley</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CSSE</orgName>
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling High-way</addrLine>
									<postCode>6009</postCode>
									<settlement>Crawley</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CSSE</orgName>
								<orgName type="institution">The University of Western Australia</orgName>
								<address>
									<addrLine>35 Stirling High-way</addrLine>
									<postCode>6009</postCode>
									<settlement>Crawley</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Human Pose Models from Synthesized Data for Robust RGB-D Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human Action Recognition · Cross-view · Cross-subject · Depth Sensor · CNN · GAN</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Human Pose Models that represent RGB and depth images of human poses independent of clothing textures, backgrounds, lighting conditions, body shapes and camera viewpoints. Learning such universal models requires training images where all factors are varied for every human pose. Capturing such data is prohibitively expensive. Therefore, we develop a framework for synthesizing the training data. First, we learn representative human poses from a large corpus of real motion captured human skeleton data. Next, we fit synthetic 3D humans with different body shapes to each pose and render each from 180 camera viewpoints while randomly varying the clothing textures, background and lighting. Generative Adversarial Networks are employed to minimize the gap between synthetic and real image distributions. CNN models are then learned that transfer human poses to a shared high-level invariant space. The learned CNN models are then used as invariant feature extractors from real RGB and depth frames of human action videos and the temporal variations are modelled by Fourier Temporal Pyramid. Finally, linear SVM is used for classification. Experiments on three benchmark cross-view human ac-tion datasets show that our algorithm outperforms existing methods by significant margins for RGB only and RGB-D action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human action recognition has many applications in security, surveillance, sports analysis, human computer interaction and video retrieval. However, automatic human action recognition algorithms are still challenged by noises due to action irrelevant factors such as changing camera viewpoints, clothing textures, body shapes, backgrounds and illumination conditions. In this paper, we address these challenges to perform robust human action recognition in conventional RGB videos and RGB-D videos obtained from range sensors.</p><p>A human action can be defined as a collection of sequentially organized human poses where the action is encoded in the way the human pose transitions from one pose to the other. However, for action classification, a human pose must be represented in a way that is invariant to the above conditions. Since some human poses are common between multiple actions and the space of possible human poses is much smaller compared to that of possible human actions, we first model the human pose independently and then model the actions as the temporal variations between human poses.</p><p>To suppress action irrelevant information in videos, many techniques use dense trajectories <ref type="bibr" target="#b12">(Gupta et al, 2014;</ref><ref type="bibr" target="#b53">Wang et al, 2011</ref><ref type="bibr" target="#b54">Wang et al, , 2013a</ref><ref type="bibr" target="#b52">Wang and Schmid, 2013)</ref> or Hanklets  which encode only the temporal cues that are essential for action classification. Such methods have shown good performance for human action recognition in conventional videos. How- <ref type="figure">Fig. 1</ref> Block diagram of the proposed synthetic RGB data generation. Representative human poses are learned from CMU MoCap skeleton database and a 3D human model is fitted to each skeleton. Four different 3D human body shapes are used. Clothing textures are randomly selected from a choice of 262 textures for shirts and 183 for trousers. Each model is placed in a random background, illuminated with three random intensity lamps and rendered from 180 camera viewpoints to generate RGB training images with known pose labels ever, they are still sensitive to viewpoint variations and do not fully exploit the appearance (human pose) information. Dense trajectories are also noisy and contain self occlusion artefacts.</p><p>While appearance is an important cue for action recognition, human poses appear very differently from different camera viewpoints. Research efforts have been made to model these variations. For example, synthetic 2D human poses from many viewpoints and their transitions were used for action recognition in <ref type="bibr">(Lv and Nevatia, 2007)</ref>. However, 3D viewpoint variations cannot be modelled accurately using 2D human poses. Spatiotemporal 3D occupancy grids built from multiple viewpoints were used in <ref type="bibr" target="#b62">(Weinland et al, 2007)</ref> to achieve view-invariant action recognition. However, occupancy grids rely on silhouettes which are noisy in real videos. In this paper, we use full 3D human models to learn a representation of the human pose that is not only invariant to viewpoint but also to other action irrelevant factors such as background, clothing and illumination.</p><p>Our contributions can be summarized as follows. Firstly, we propose a method for generating RGB and Depth images of human poses using Computer Graphics and Generative Adversarial Network (GAN) training. We learn representative human poses by clustering real human joint/skeleton data obtained with motion capture technology (CMU MoCap database 1 ). Each representative pose skeleton is fitted with synthetic 3D human models and then placed in random scenes, given different clothes, illuminated from multiple directions and rendered from 180 camera viewpoints to generate RGB and depth images of the human poses with known labels. <ref type="figure">Figure 1</ref> illustrates the proposed RGB data generation pipeline. Depth images are generated in a similar way except that they are devoid of texture and background. We employ GANs to minimize the gap between the distributions of synthetic and real images. Although used as an essential component of network 1 http://mocap.cs.cmu.edu training in this work, the proposed synthetic data generation technique is generic and can be used to produce large amount of synthetic human poses for deep learning in general.</p><p>Secondly, we propose Human Pose Models (HPM) that are Convolutional Neural Networks and transfer human poses to a shared high level invariant space. The HPMs are trained with the images that are refined with GAN and learn to map input (RGB or Depth) images to one of the representative human poses irrespective of the camera viewpoint clothing, human body size, background and lighting conditions. The layers prior to the Softmax label in the CNNs serve as high-level invariant human pose representations. Lastly, we propose to temporally model the invariant human pose features with the Fourier Temporal Pyramid and use SVM for classification. The proposed methods work together to achieve robust RGB-D human action recognition under the modeled variations.</p><p>Experiments on three benchmark cross-view human action datasets show that our method outperforms existing state-of-the-art for action recognition in conventional RGB videos as well as RGB-D videos obtained from depth sensors. The proposed method improves RGB-D human action recognition accuracies by 15.4%, 11.9% and 8.4% on the UWA3D-II , NUCLA <ref type="bibr" target="#b56">(Wang et al, 2014)</ref> and NTU <ref type="bibr" target="#b40">(Shahroudy et al, 2016a)</ref> datasets respectively. Our method also improves RGB human action recognition accuracy by 9%.</p><p>This work is an extension of  where only depth image based human pose model and action recognition results were presented. However, depth images are almost always accompanied with RGB images. Therefore, we present the following extensions: (1) We present a method 2 for synthesizing realistic RGB human pose images containing variations in clothing textures, background, lighting conditions and more variations in body shapes. This method can also synthesize depth images more efficiently compared to . <ref type="formula">(2)</ref> We adopt Generative Adversarial Networks (GANs) to refine the synthetic RGB images and depth images so as to reduce their distribution gaps from real images and achieve improved accuracy. (3) We present a Human Pose Model HPM RGB for human action recognition in conventional RGB videos which has wider applications. The proposed HPM RGB achieves state-of-the-art human action recognition accuracy in conventional RGB videos. (4) We re-train the depth model HPM 3D using GoogleNet <ref type="bibr" target="#b47">(Szegedy et al, 2015)</ref> architecture which performs similar to the AlexNet <ref type="bibr" target="#b23">(Krizhevsky et al, 2012)</ref> model in  but with four times smaller feature dimensionality. (5) We perform additional experiments on the largest RGB-D human action dataset (NTU <ref type="bibr" target="#b40">(Shahroudy et al, 2016a)</ref>) and report state-ofthe-art results for action recognition in RGB videos and RGB-D videos in the cross-view and cross-subject settings. From here on, we refer to the Human Pose Models as HPM RGB and HPM 3D for RGB and depth modalities respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ralated Work</head><p>The closest work to our method is the key pose matching technique proposed by <ref type="bibr">Lv and Nevatia (2007)</ref>. In their approach, actions are modelled as series of synthetic 2D human poses rendered from many viewpoints and the transition between the synthetic poses is represented by an Action Net graph. However, the rendered images are not realistic as they do not model variations in clothing, background and lighting as in our case. Moreover, our method directly learns features from the rendered images rather than hand crafting features.</p><p>Another closely related work to our method is the 3D exemplars for action recognition proposed by <ref type="bibr" target="#b62">Weinland et al (2007)</ref>. In their framework, actions are modelled with 3D occupancy grids built from multiple viewpoints. The learned 3D exemplars are then used to produce 2D images that are compared to the observations during recognition. This method essentially relies on silhouettes which may not be reliably extracted from the test videos especially under challenging background/lighting conditions.  proposed Hankelet which is a viewpoint invariant representation that captures the dynamic properties of short tracklets. Hanklets do not carry any spatial information and their viewpoint invariant properties are limited. Very early attempts for view invariant human action recognition include the following methods. <ref type="bibr" target="#b64">Yilmaz and Shah (2005)</ref> proposed action sketch, an action representation that is a sequence of the 2D contours of an action in the x, y, t space-time. Such a representation is not completely viewpoint invariant. <ref type="bibr" target="#b33">Parameswaran and Chellappa (2006)</ref> used 2D projections of 3D human motion capture data as well on manually segmented real image sequences to perform viewpoint robust action recognition. <ref type="bibr" target="#b39">Rao et al (2002)</ref> used the spatio-temporal 2D trajectory curvatures as a compact representation for view-invariant action recognition. However, the same action can result in very different 2D trajectories when observed from different viewpoints. <ref type="bibr" target="#b61">Weinland et al (2006)</ref> proposed Motion History Volumes (MHV) as a viewpoint invariant representation for human actions. MHVs are aligned and matched using Fourier Transform. This method requires multiple calibrated and background-subtracted video cameras which is only possible in controlled environments.</p><p>View knowledge transfer methods transfer features of different viewpoints to a space where they can be directly matched to achieve viewpoint invariant action recognition. Early methods in this category learned similar features between different viewpoints. For example, <ref type="bibr" target="#b6">Farhadi and Tabrizi (2008)</ref> represented actions with histograms of silhouettes and optical flow and learned features with maximum margin clustering that are similar in different views. Source views are then transferred to the target view before matching. Given sufficient multiview training instances, it was shown later that a hash code with shared values can be learned <ref type="bibr" target="#b7">(Farhadi et al, 2009</ref>). <ref type="bibr" target="#b11">Gopalan et al (2011)</ref> used domain adaptation for view transfer. <ref type="bibr" target="#b27">Liu et al (2011)</ref> used a bipartite graph to model two view-dependent vocabularies and applied bipartite graph partitioning to co-cluster two vocabularies into visual-word clusters called bilingual-words that bridge the semantic gap across view-dependent vocabularies. More recently, <ref type="bibr" target="#b25">Li and Zickler (2012)</ref> proposed the idea of virtual views that connect action descriptors from one view to those extracted from another view. Virtual views are learned through linear transformations of the action descriptors. <ref type="bibr" target="#b68">Zhang et al (2013)</ref> proposed the idea of continuous virtual path that connects actions from two different views. Points on the virtual path are virtual views obtained by linear transformations of the action descriptors. They proposed a virtual view kernel to compute similarity between two infinite-dimensional features that are concatenations of the virtual view descriptors leading to kernelized classifiers. <ref type="bibr" target="#b69">Zheng and Jiang (2013)</ref> learned a view-invariant sparse representation for cross-view action recognition. <ref type="bibr" target="#b35">Rahmani and Mian (2015)</ref> proposed a non-linear knowledge transfer model that mapped dense trajectory action descriptors to canonical views. However, this method does not exploit the appearance/shape features.</p><p>Deep learning has also been used for action recognition. <ref type="bibr" target="#b44">Simonyan and Zisserman (2014)</ref> and <ref type="bibr" target="#b8">Feichtenhofer et al (2016)</ref> proposed two stream CNN architectures using appearance and optical flow to perform action recognition. <ref type="bibr" target="#b57">Wang et al (2015)</ref> proposed a two stream structure to combine hand-crafted features and deep learned features. They used trajectory pooling for one stream and deep learning for the second and combined the features from the two streams to form trajectorypooled deep-convolutional descriptors. Nevertheless, the method did not explicitly address viewpoint variations. <ref type="bibr" target="#b48">Toshev and Szegedy (2014)</ref> proposed DeepPose for human pose estimation based on Deep Neural Networks which treated pose estimation as a regression problem and represented the human pose body joint locations. This method is able to capture the context and reasoning about the pose in a holistic manner however, the scale of the dataset used for training was limited and the method does not address viewpoint variations. <ref type="bibr" target="#b34">Pfister et al (2015)</ref> proposed a CNN architecture to estimate human poses. Their architecture directly regresses pose heat maps and combines them with optical flow. This architecture relies on neighbouring frames for pose estimations. <ref type="bibr" target="#b17">Ji et al (2013)</ref> proposed a 3D Convolutional Neural Network (C3D) for human action recognition. They used a set of hard-wired kernels to generate multiple information channels corresponding to the gray pixel values, (x, y) gradients, and (x, y) optical flow from seven input frames. This was followed by three convolution layers whose parameters were learned through back propagation. The C3D model did not explicitly address invariance to viewpoint or other factors. <ref type="bibr" target="#b59">Wang et al (2016b)</ref> proposed joint trajectory maps, projections of 3D skeleton sequences to multiple 2D images, for human action recognition. <ref type="bibr" target="#b20">Karpathy et al (2014)</ref> suggested a multi-resolution foveated architecture for speeding up CNN training for action recognition in large scale videos. <ref type="bibr" target="#b50">Varol et al (2017a)</ref> proposed long-term temporal convolutions (LTC) and showed that LTC-CNN models with increased temporal extents improve action recognition accuracy. <ref type="bibr" target="#b49">Tran et al (2015)</ref> treated videos as cubes and performed convolutions and pooling with 3D kernels. Recent methods <ref type="bibr" target="#b26">(Li et al, 2016;</ref><ref type="bibr" target="#b70">Zhu et al, 2016;</ref><ref type="bibr" target="#b60">Wang and Hoai, 2016;</ref><ref type="bibr" target="#b67">Zhang et al, 2016;</ref><ref type="bibr" target="#b46">Su et al, 2016;</ref><ref type="bibr" target="#b58">Wang et al, 2016a</ref>) emphasize on action recognition in large scale videos where the background context is also taken into account. <ref type="bibr" target="#b41">Shahroudy et al (2016b)</ref> divided the actions into body parts and proposed a multimodal-multipart learning method to represent their dynamics and appearances. They selected the discriminative body parts by integrating a part selection process into the learning and proposed a hierarchical mixed norm to apply sparsity between the parts, for group feature selection. This method is based on depth and skeleton data and uses LOP (local occupancy patterns) and HON4D (histogram of oriented 4D normals) as features. <ref type="bibr" target="#b66">Yu et al (2016)</ref> proposed a Structure Preserving Projection (SPP) to represent RGB-D video data fusion. They described the gradient fields of RGB and depth data with a new Local Flux Feature (LFF), and then fused the LFFs from RGB and depth channels. With structure-preserving projection, the pairwise structure and bipartite graph structure are preserved when fusing RGB and depth information into a Hamming space, which benefits the general action recognition. <ref type="bibr" target="#b16">Huang et al (2016)</ref> incorporated the Lie group structure into deep learning, to transform high-dimensional Lie group trajectory into temporally aligned Lie group features for skeleton-based action recognition. The incorporated learning structure generalizes the traditional neural network model to non-Euclidean Lie groups. <ref type="bibr" target="#b29">Luo et al (2017)</ref> proposed to use Recurrent Neural Network based Encoder-Decoder framework to learn video representation in capturing motion dependencies. The learning process is unsupervised and it focuses on encoding the sequence of atomic 3D flows in consecutive frames. <ref type="bibr" target="#b18">Jia et al (2014a)</ref> proposed a latent tensor transfer learning method to transfer knowledge from the source RGB-D dataset to the target RGB only dataset such that the missing depth information in the target dataset can be compensated. The learned 3D geometric information is then coupled with RGB data in a cross-modality regularization framework to align them. However, to learn the latent depth information for RGB data, a RGB-D source dataset is required to perform the transfer learning, and for different source datasets, the learned information may not be consistent which could affect the final performance. <ref type="bibr" target="#b22">Kong and Fu (2017)</ref> proposed max-margin heterogeneous information machine (MMHIM) to fuse RGB and depth features. The histograms of oriented gradients (HOG) and histograms of optical flow (HOF) descriptors are projected into independent shared and private feature spaces, and the features are represented in matrix forms to build a low-rank bilinear model for the classification. This method utilizes the cross-modality and private information, which are also de-noised before the final classification. <ref type="bibr" target="#b21">Kerola et al (2017)</ref> used spatio-temporal key points (STKP) and skeletons to represent an action as a temporal sequence of graphs, and then applied the spectral graph wavelet transform to create the action descriptors. <ref type="bibr" target="#b51">Varol et al (2017b)</ref> recently proposed SUR- REAL to synthesize human pose images for the task of body segmentation and depth estimation. The generated dataset includes RGB images, together with depth maps and body parts segmentation information. They then learned a CNN model from the synthetic dataset, and then conduct pixel-wise classification for the real RGB pose images. This method made efforts in diversifying the synthetic data, however, it didn't address the distribution gap between synthetic and real images. Moreover, this method only performs human body segmentation and depth estimation.</p><p>Our survey shows that none of the existing techniques explicitly learn invariant features through a training dataset that varies all irrelevant factors for the same human pose. This is partly because such training data is very difficult and expensive to generate. We resolve this problem by developing a method to generate such data synthetically. The proposed data generation technique is a major contribution of this work that can synthesize large amount of data for training data-hungry deep network models. By easily introducing a variety of action irrelevant variations in the synthetic data, it is possible to learn effective models that can extract invariant information. In this work, we propose Human Pose Models for extracting such information from both RGB and depth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generating Synthetic Training Data</head><p>The proposed synthetic data generation steps are explained in the following subsections. Unless specified, the steps are shared by RGB and depth data generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Representative Human Poses</head><p>Since the space of possible human poses is extremely large, we learn a finite number of representative human poses in a way that is not biased by irrelevant factors such as body shapes, appearances, camera viewpoints, illumination and backgrounds. Therefore, we learn the representative poses from 3D human joints (skeletons), because each skeleton can be fitted with 3D human models of any size/shape. The CMU MoCap database is ideal for this purpose because it contains joint locations of real humans performing different actions resulting in a large number of different poses. This data consists of over 2500 motion sequences and over 200,000 human poses. We randomly sample 50,000 frames as the pose candidates and cluster them with HDBSCAN algorithm (McInnes et al, 2017) using the skeletal distance function (Shakhnarovich, 2005)</p><formula xml:id="formula_0">D(θ 1 , θ 2 ) = max i≤j≤L d∈x,y,z | θ i d,1 − θ j d,2 |<label>(1)</label></formula><p>where θ 1 and θ 2 are the x, y, z joint locations of two skeletons. By setting the minimum cluster size to 20, the HDBSCAN algorithm outputs 339 clusters and we choose the pose with highest HDBSCAN score in each cluster to form the representative human poses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generating 3D Human Models</head><p>The 339 representative human pose skeletons are fitted with full 3D human models. We use the open source MakeHuman 3 software to generate 3D human models because it has three attractive properties. Firstly, the 3D human models created by MakeHuman contain information for fitting the model to the MoCap skeletons to adopt that pose. Secondly, it is possible to vary the body shape, proportion and gender properties to model different shape humans. <ref type="figure" target="#fig_1">Figure 3</ref> shows the four human body shapes we used in our experiments. Thirdly, MakeHuman allows for selecting some common clothing types as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Although, the MakeHuman offers limited textures for the clothing, we write a Python script to apply many different types of clothing textures obtained from the Internet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fitting 3D Human Models to MoCap Skeletons</head><p>MakeHuman generates 3D human models in the same canonical pose as shown in <ref type="figure" target="#fig_1">Figure 3</ref> and 4. We use the open source Blender 4 software to fit the 3D human models to the 339 representative human pose skeletons. Blender loads the 3D human model and re-targets its rigs to the selected MoCap skeleton. As a result, the 3D human model adopts the pose of the skeleton and we get the representative human poses as full 3D human models with different clothing types and body shapes. Clothing textures are varied later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multiview Rendering to Generate RGB Images</head><p>We place each 3D human model (with a representative pose) in different backgrounds and lighting conditions using Blender. In the following, we explain how different types of variations were introduced in the rendered images. Camera Viewpoint: We place 180 virtual cameras on a hemisphere over the 3D human model to render RGB images. These cameras are 12 degrees apart along the latitude and longitude and each camera points to the center of the 3D human model. <ref type="figure">Figure 5</ref> illustrates the virtual cameras positioned around a 3D human model where no background has been added yet to make the cameras obvious. <ref type="figure" target="#fig_4">Figure 6</ref> shows a few images rendered from multiple viewpoints after adding the background and lighting.</p><p>Background and Lighting: We incorporate additional rich appearance variations in the background and lighting conditions to synthesize images that are as realistic as possible. Background variation is performed in two modes. One, we download thousands of 2D indoor scenes from Google Images and randomly select one as Blender background during image rendering. Two, we download 360 o spherical High Dynamic Range Images (HDRI) from Google Images and use them as the environmental background in Blender. In the latter case, when rendering images from different viewpoints, the background changes accordingly. <ref type="figure" target="#fig_4">Figure 6</ref> shows some illustrations. In total, we use 2000 different backgrounds that are mostly indoor scenes, building lobbies with natural lighting and a few outdoor natural scenes. We place three lamps at different locations in the scene and randomly change their energy to achieve lighting variations.</p><p>Clothing Texture: Clothing texture is varied by assigning different textures, downloaded from Google Images, to the clothing of the 3D human models. In total, we used 262 different textures of shirts and 183 textures for trousers/shorts to generate our training data. <ref type="figure" target="#fig_5">Figure 7</ref> shows some of the clothing textures we used and <ref type="figure">Figure 8</ref> shows some rendered images containing all types of variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multiview Rendering to Generate Depth Images</head><p>Depth images simply record the distance of the human from the camera without any background, texture or lighting variation. Therefore, we only vary the cam-  <ref type="figure" target="#fig_6">Figure 9</ref> shows some depth images rendered from different viewpoints. In the Blender rendering environment, the bounding box of the human model is recorded as a group of vertices, which can be converted to an xy bounding box around the human in the rendered image coordinates. This bounding box is used to crop the human in the rendered depth images as well as RGB images. The cropped images are used to learn the Human Pose Models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Efficiency of Synthetic Data Generation</head><p>To automate the data generation, we implemented a Python script 5 in Blender on a 3.4GHz machine with 32GB RAM. The script runs six separate processing threads and on the average, generates six synthetic pose images per second. Note that this is an off-line process and can be further parallelized since each image is rendered independently. Moreover, the script can be implemented on the cloud for efficiency without the need to upload any training data. The training data is only required for learning the model and can be deleted afterwards. For each of the 339 representative poses, we generate images from all 180 viewpoints while applying a random set of other variations (clothing, background, body shape and lighting). In total, about 700,000 synthetic RGB and depth images are generated to train the proposed HPM RGB and HPM 3D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Synthetic Data Refinement with GANs</head><p>The synthetic images are labelled with 339 different human poses and cover common variations that occur in real data. They can be used to learn Human Pose Models that transfer human poses to a high level viewinvariant space. However, it is likely that the synthetic images are sampled from a distribution which is different from the distribution of real images. Given this distribution gap, the Human Pose Models learned from synthetic images may not generalize well to real images. Therefore, before learning the models, we minimize the gap between the distributions. For this purpose, we adopt the simulated and unsupervised learning framework (SimGAN) <ref type="bibr" target="#b43">(Shrivastava et al, 2016)</ref> proposed by Shrivastava et al.. This framework uses an adversarial network structure similar to the Generative Adversarial Network (GAN) <ref type="bibr" target="#b10">(Goodfellow et al, 2014)</ref>, but the learning process is based on synthetic images, instead of random noises as in the original GAN method. The Sim-GAN framework learns two competing networks, refiner</p><formula xml:id="formula_1">R θ (x) and discriminator D φ (x, y), where x is synthetic image, y is unlabelled real image,x = R θ (x) is refined image.</formula><p>The loss function of these two networks are defined as L R (θ) and L D (φ) <ref type="bibr" target="#b43">(Shrivastava et al, 2016</ref>)</p><formula xml:id="formula_2">L R (θ)= − i log(1−D φ (R θ (x i )))+λ R θ (x i )−x i 1 , (2) L D (φ) = − i log(D φ (x i )) − j log(1 − D φ (y j )), (3) where x i is the i th synthetic image,x i is its correspond- ing refined image, y j is the j th real image, . 1 is 1 norm, and λ ∈ [0, 1] is the regularization factor.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We modify the Tensorflow implementation 6 of SimGAN to make it suitable for our synthetic RGB images. For the refiner network, we extend the input data channel from 1 to 3. The input images are first convolved with 7 × 7 filters and then converted into 64 feature maps. The 64-channel feature maps are passed through multiple ResNet blocks. The setting of ResNet blocks and the structure of discriminator network are the same as <ref type="bibr" target="#b43">(Shrivastava et al, 2016)</ref>.</p><p>To get benchmark distribution for the synthetic images, we randomly select 100,000 unlabelled real images from the NTU RGB+D Human Activity Dataset <ref type="bibr" target="#b40">(Shahroudy et al, 2016a)</ref>. Each image is cropped to get the human body as the region of interest and then resized to 224 × 224. Through adversarial learning, Sim-GAN framework <ref type="bibr" target="#b43">(Shrivastava et al, 2016)</ref> will force the distribution of synthetic images to approach this benchmark distribution. Although, we use samples from the NTU RGB+D Human Activity Dataset as benchmark to train the SimGAN network, this is not mandatory as any other dataset containing real human images can be used. This is because the SimGAN learning is an unsupervised process, which means no action labels are required. Our experiments in later sections also illustrate that the performance improvement gained from GAN-refinement has no dependence on the type of real images used for SimGAN learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Qualitative Analysis of GAN Refined Images</head><p>We compare the real images, raw synthetic images, and GAN-refined synthetic images, to analyse the effect of GAN refinement on our synthetic RGB and depth human pose datasets. <ref type="figure" target="#fig_7">Figure 10</ref> compares the real, raw synthetic and GANrefined synthetic RGB images. One obvious difference between real and synthetic RGB images is that the synthetic ones are sharper and have more detail than the real images. This is because the synthetic images are generated under ideal conditions and the absence of realistic image noises makes them different from real images. However, with GAN learning, the refined RGB images lose some of the details (i.e. they are not as sharp) and become more realistic. The last row of <ref type="figure" target="#fig_7">Figure 10</ref> shows the difference between the raw and refined synthetic images. Notice that the major differences (bright pixels) are at the locations of the humans and especially at their boundaries whereas the backgrounds have minimal differences (dark pixels). The reason for this is that the synthetic images are created using synthetic 3D human models but real background images i.e. rather than building Blender scene models (walls, floors, furnitures, etc.) from scratch, we used real background images for efficiency and diversity of data. Moreover, the Blender lighting function causes shading variation on the human models only, and the shading effects of backgrounds always remain the same. All these factors make the human model stand out of the background. On the other hand, the human subjects are perfectly blended with the background in the real images. The <ref type="figure">Fig. 11</ref> Comparing real, raw synthetic and GAN-refined synthetic depth images GAN refinement removes such differences in synthetic images and makes the human models blend into the background. Especially, the bright human boundaries (last row) shows that the GAN refinement process is able to sense and remove the difference between human model and background images. <ref type="figure">Figure 11</ref> shows a similar comparison for depth images. The most obvious difference between real and synthetic depth images is the noise along the boundary. The edges in the real depth images are not smooth whereas they are very smooth in the synthetic depth images. Other differences are not so obvious to the naked eye but nevertheless, these differences might limit the generalization ability of the Human Pose Model learned from synthetic images. The third row of <ref type="figure">Fig. 11</ref> shows the refined synthetic depth images and the last column shows the difference between the images in more detail. Thus the GAN refinement successfully learns to model boundary noise and other non-obvious differences of real images and applies them to the synthetic depth maps narrowing down their distribution gap.</p><p>In the experiments, we will show quantitative results indicating that the Human Pose Models learned from the GAN refined synthetic images outperform those that are learned from raw synthetic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning the Human Pose Models</head><p>Every image in our synthetic data has a label corresponding to one of the 339 representative human poses. For a given human pose, the label remains the same irrespective of the camera viewpoint, clothing texture, body shape, background and lighting conditions. We learn CNN models that map the rendered images to their respective human pose labels. We learn HPM RGB and HPM 3D for RGB and depth images independently and test three popular CNN architectures, i.e. AlexNet <ref type="bibr" target="#b23">(Krizhevsky et al, 2012)</ref>, GoogLeNet <ref type="bibr" target="#b47">(Szegedy et al, 2015)</ref>, and ResNet-50 <ref type="bibr" target="#b13">(He et al, 2016a)</ref>, to find the most optimal architecture through controlled experiments. These CNN architectures performed well in the Ima-geNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, 2014, and 2015 respectively, and come with increasing number of layers. We fine tune the ILSVRC pre-trained models using our synthetic data and compare their performance on human action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Learning</head><p>The three pre-trained models have a last InnerProduct layer with 1000 neurons. For fine tuning, we replace the last layer with a 339 neuron layer representing the number of classes in our synthetic human pose dataset. All synthetic images are cropped to include only the human body and then resized to 256×256 pixels. During training, these images are re-cropped to the required input dimension for the specific network with default data augmentation, and are also mirrored with a probability of 0.5. We use the synthetic pose images from 162 randomly selected camera viewpoints for training, and the images from the remaining 18 cameras for validation.</p><p>The Caffe library <ref type="bibr" target="#b19">(Jia et al, 2014b</ref>) is used to learn the proposed HPM RGB and HPM 3D models. The initial learning rate of the model was set to 0.01 for the last fully-connected layers and 0.001 for all other layers. We used a batch size of 100 and trained the model for 3 epochs. We decreased the learning rate by a factor of 10 after every epoch. Training was done using a single NVIDIA Tesla K-40 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Extracting Features from Real Videos</head><p>To extract features from real videos, the region containing the human is first cropped from each frame and then the cropped region is resized to match the input of the network. The cropped-resized regions from each frame are passed individually through the learned Human Pose Model (HPM RGB for RGB frames, and HPM 3D for depth frames), and a layer prior to the labels is used as invariant representation of the human pose. Specifically, we use fc7 layer for AlexNet, pool5/7x7 s1 layer for GoogLeNet and pool5 for ResNet-50. This representation is unique, compact, invariant to the irrelevant factors and has the added advantage that it aligns the features between multiple images. While the pixels of one image may not correspond to the pixels of another image, the individual variables of the CNN features are aligned. Therefore, we can perform temporal analysis along the individual variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Temporal Representation and Classification</head><p>For temporal representation, we use the Fourier Temporal Pyramid (FTP) <ref type="bibr" target="#b55">(Wang et al, 2013b)</ref> on the features extracted from the video frames. Temporal representation for HPM 3D and HPM RGB features is done in a similar way and explained in general in the next paragraph.</p><p>Let V i t denote the t-th frame of i-th video, t = 1, 2, . . . , f where f is the total number of frames. Take HPM with GoogleNet structure as an example, denote the pool5/7x7 s1 layer activations of frame V i t as A i t ∈ R 1024×1 and the frame-wise pose features of the i-th video as</p><formula xml:id="formula_3">A i = [A i 1 , A i 2 , . . . , A i f ] T .</formula><p>FTP is applied on A i for temporal encoding using a pyramid of three levels where A i is divided in half at each level giving 1+2+4 = 7 feature groups. Short Fourier Transform is applied to each feature group, and the first four low-frequency coefficients (i.e. 4 × 7 = 28) are used to form a spatiotemporal action descriptor B i ∈ R 1024×28 . Finally, B i is stretched to D i ∈ R 1×28672 to get the final spatiotemporal representation of the i-th video. When the dimension of frame-wise pose feature changes, the dimension of spatio-temporal descriptor changes accordingly, for example, B i ∈ R 4096×28 for AlexNet, and B i ∈ R 2048×28 for ResNet-50.</p><p>Note that the FTP encodes the temporal variations of the RGB action videos in the HPM RGB feature space. The video frames are first aligned in the A i t HPM RGB feature space which makes it possible to preserve the spatial location of the features while temporal encoding with FTP. On the other hand, dense trajectories model temporal variations in the pixel space (of RGB videos) where pixels corresponding to the human body pose are not aligned. This is the main reason why dense trajectory features are encoded with Bag of Visual Words (BoVW) which facilitates direct matching of dense trajectory features from two videos. However, this process discards the spacial locations of the trajectories. Thus, similar trajectories from different locations in the frame will vote to the same bin in BoVW feature.</p><p>An advantage of performing temporal encoding in different feature spaces is that the features are nonredundant. Thus our HPM RGB and dense trajectories capture complementary information. Although dense trajectories cannot capture the appearance information, they are somewhat robust to viewpoint changes as shown in <ref type="bibr" target="#b38">(Rahmani et al, 2017)</ref>. Therefore, we augment our HPM RGB features with dense trajectory features before performing classification. We use the improved dense trajectories (iDT) <ref type="bibr" target="#b52">(Wang and Schmid, 2013)</ref> implementation which provides additional features such as HOG <ref type="bibr" target="#b0">(Dalal and Triggs, 2005)</ref>, HOF and MBH <ref type="bibr">(Dalal et al,</ref><ref type="bibr">Fig. 12</ref> Sample frames from the UWA3D Multiview Activity II dataset  2006). However, we only use the trajectory part and discard HOG/HOF/MBH features for two reasons. Firstly, unlike our HPMs, HOG/HOF/MBH features are not view-invariant. Secondly, our HPMs already encode the appearance information. We use the NKTM <ref type="bibr" target="#b35">(Rahmani and Mian, 2015)</ref> codebook to encode the trajectory features and denote the encoded BoVW as D i ∈ R 2000 for video V i .</p><p>We use SVM <ref type="bibr" target="#b5">(Fan et al, 2008)</ref> for classification and report results in three settings i.e. RGB, depth and RGB-D. In the RGB setting, we represent the HPM RGB features temporally encoded with FTP and then combine them with the trajectory BoVW features since both types of features can be extracted from RGB videos. In the depth setting, we represent the HPM 3D features with FTP but do not combine trajectory features because trajectories cannot be reliably extracted from the depth videos. In the RGB-D setting, we combine the FTP features from both HPM models with the trajectory BoVW features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Datasets</head><p>Experiments are performed in the following three benchmark datasets for cross-view human action recognition. <ref type="figure" target="#fig_0">Figure 12</ref> shows sample frames from this dataset. The dataset  consists of 30 human actions performed by 10 subjects and recorded from 4 different viewpoints at different times using the Kinect v1 sensor. The 30 actions are: (1) one hand waving, (2) one hand punching, (3) two hands waving, (4) two hands punching, (5) sitting down, (6) standing up, (7) vibrating, (8) falling down, (9) holding chest, (10) holding head, (11) holding back, <ref type="formula" target="#formula_0">(12)</ref>   This dataset is challenging because of the large number of action classes and because the actions are not recorded simultaneously leading to intra-action differences besides viewpoint variations. The dataset also contains self-occlusions and human-object interactions in some videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">UWA3D Multiview Activity-II Dataset</head><p>We follow the protocol of  where videos from two views are used for training and the videos from the remaining views are individually used for testing leading to 12 different cross-view combinations in this evaluation protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Northwestern-UCLA Multiview Dataset</head><p>This dataset <ref type="bibr" target="#b56">(Wang et al, 2014)</ref> contains RGB-D videos captured simultaneously from three different viewpoints with the Kinect v1 sensor. <ref type="figure" target="#fig_1">Figure 13</ref> shows sample frames of this dataset from the three viewpoints. The dataset contains RGB-D videos of 10 subjects performing 10 actions: (1) pick up with one hand, (2) pick up with two hands, (3) drop trash, (4) walk around, (5) sit down, (6) stand up, (7) donning, (8) doffing, (9) throw, and (10) carry. The three viewpoints are: (a) left, (b) front, and (c) right. This dataset is very challenging because many actions share the same "walking" pattern before and after the actual action is performed. Moreover, some actions such as "pick up with on hand" and "pick up with two hands" are hard to distinguish from different viewpoints.</p><p>We use videos captured from two views for training and the third view for testing making three possible cross-view combinations.  <ref type="bibr" target="#b40">(Shahroudy et al, 2016a)</ref>. Three sensors C-1, C-2 and C-3 are used to record this dataset. The left group of images in this figure shows the actions recorded with the performer facing the sensor C-3, and the right group of images are recorded when the action performer faces the sensor C-2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">NTU RGB+D Human Activity Dataset</head><p>The NTU RGB+D Human Activity Dataset <ref type="bibr" target="#b40">(Shahroudy et al, 2016a</ref>) is a large-scale RGB+D dataset for human activity analysis. This dataset was collected with the Kinect v2 sensor and includes 56,880 action samples each for RGB videos, depth videos, skeleton sequences and infra-red videos. We only use the RGB and depth parts of the dataset. There are 40 human subjects performing 60 types of actions including 50 single person actions and 10 two-person interactions. Three sensors were used to capture data simultaneously from three horizontal angles: −45 • , 0 • , 45 • , and every action performer performed the action twice, facing the left or right sensor respectively. Moreover, the height of sensors and their distance to the action performer were further adjusted to get more viewpoint variations. The NTU RGB+D dataset is the largest and most complex cross-view action dataset of its kind to date. <ref type="figure" target="#fig_3">Figure 14</ref> shows RGB and depth sample frames in NTU RGB+D dataset.</p><p>We follow the standard evaluation protocol proposed in <ref type="bibr" target="#b40">(Shahroudy et al, 2016a)</ref>, which includes cross-subject and cross-view evaluations. For cross-subject protocol, 40 subjects are split into training and testing groups, and each group consists of 20 subjects. For cross-view protocol, the videos captured by sensor C-2 and C-3 are used as training samples, and the videos captured by sensor C-1 are used as testing samples. We first use raw synthetic images to train HPM RGB and HPM 3D for the three different CNN architectures (AlexNet, GoogLeNet, and ResNet-50), and compare their performance on the UWA and NUCLA datasets. The best performing architecture is then selected and re-trained on the GAN-refined synthetic images. Next, we compare the HPM models trained on raw synthetic images to those trained on the GAN refined synthetic images. Finally, we perform comprehensive experiments to compare our proposed models trained on GAN refined synthetic images to existing methods on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">HPM Performance with Different Architectures</head><p>We determine the best CNN architecture that maximizes generalization power of HPM RGB and HPM 3D for RGB and depth images respectively. We use the raw synthetic pose images to fine tune AlexNet, GoogLeNet, and ResNet-50, and then test them on the UWA and NUCLA datasets. Since the trained model is to be used as a frame-wise feature extractor for action recognition, we take recognition accuracy and feature dimensionality both into account. <ref type="table">Table 1</ref> compares the average results on all possible cross-view combinations for the two datasets. The results show that for RGB videos, GoogLeNet and ResNet-50 perform much better than AlexNet. GoogLeNet also performs the best for depth videos and has the smallest feature dimensionality. Therefore, we select GoogLeNet as the network architecture for both HPM RGB and HPM 3D in the remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Quantitative Analysis of GAN Refinement</head><p>We quantitatively compare the effect of GAN-refinement using the UWA and NUCLA datasets by comparing the action recognition accuracies when the HPM RGB and HPM 3D are fine tuned once on raw synthetic images and once on GAN-refined synthetic images. <ref type="table">Table 2</ref> shows the average accuracies for all crossview combinations on the respective datasets. We can see that the HPM RGB fine tuned on the GAN-refined synthetic RGB images achieves 3.3% and 1.4% improvement over the one fine tuned with raw synthetic images on the UWA and NUCLA datasets respectively. For HPM 3D , GAN-refined synthetic data also improves the recognition accuracy for the two datasets by 1% and 1.3% respectively. The improvements are achieved because the distribution gap between synthetic and real images is narrowed by GAN refinement.</p><p>Recall that the real images used as a benchmark distribution for GAN refinement are neither from UWA nor NUCLA dataset. We impose no dependence on the type of real images used for SimGAN learning, because it is an unsupervised process and no pose labels are required. In the remaining experiments, we use HPM RGB and HPM 3D fine tuned with GAN refined synthetic images for comparison with other techniques. <ref type="table">Table 3</ref> compares our method with existing state-ofthe-art. The proposed HPM RGB alone achieves 68.0% average recognition accuracy for RGB videos, which is higher than the nearest RGB-only competitor R-NKTM <ref type="bibr" target="#b38">(Rahmani et al, 2017)</ref>, and for 8 out the 12 train-test combinations, our proposed HPM RGB features provide significant improvement in accuracy. This shows that the invariant features learned by the proposed HPM RGB are effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Comparison on the UWA3D Multiview-II Dataset</head><p>Combining HPM RGB and dense trajectory features (Traj) gives a significant improvement in accuracy. It improves the RGB recognition accuracy to 76.4%, which is 9% higher than the nearest RGB competitor. It is also higher than the depth only method HPM 3D . This shows that our method exploits the complementary in-  <ref type="table">Table 3</ref> Action recognition accuracy (%) on the UWA3D Multiview-II dataset. V 3 1,2 means that view 1 and 2 were used for training and view 3 alone was used for testing. References for the existing methods are DVV <ref type="bibr" target="#b25">(Li and Zickler, 2012)</ref>, Action Tube <ref type="bibr" target="#b9">(Gkioxari and Malik, 2015)</ref>, CVP <ref type="bibr" target="#b68">(Zhang et al, 2013)</ref>, LRCN <ref type="bibr" target="#b2">(Donahue et al, 2015)</ref>, AOG <ref type="bibr" target="#b56">(Wang et al, 2014)</ref>, Hankelets , JOULE <ref type="bibr" target="#b15">(Hu et al, 2015)</ref>, Two-stream <ref type="bibr" target="#b44">(Simonyan and Zisserman, 2014)</ref>, DT <ref type="bibr" target="#b53">(Wang et al, 2011)</ref>, C3D <ref type="bibr" target="#b49">(Tran et al, 2015)</ref>, nCTE <ref type="bibr" target="#b12">(Gupta et al, 2014)</ref>, NKTM <ref type="bibr" target="#b35">(Rahmani and Mian, 2015)</ref>, R-NKTM <ref type="bibr" target="#b38">(Rahmani et al, 2017)</ref>. The symbol † indicates that the original model was fine-tuned with our synthetic data before applying the testing protocol.</p><p>formation between the two modes of spatio-temporal representation, and enhances the recognition accuracy especially when there are large viewpoint variations. State-of-the-art RGB-D action recognition accuracy of 82.8% on UWA dataset is achieved when we combine HPM RGB , HPM 3D and dense trajectory features.</p><p>In <ref type="table">Table 3</ref>, the results reported for the existing methods are achieved by using the original public models and fine tuning them on UWA3D dataset under the used protocol. In contrast, HPMs are not fine tuned to any dataset once their training on the proposed synthetic data is completed. These models are used out-of-thebox for the test data. These settings hold for all the experiments conducted in this work. Indeed, fine tuning HPMs on the real test datasets further improves the results but we avoid this step to show their generalization power.</p><p>Our data generation technique endows HPMs with inherent robustness to viewpoint variations along robustness to changes in background, texture and clothing etc. The baseline methods lack in these aspects which is a one of the reasons for the improvement achieved by our approach over those methods. Note that, HPMs are unique in the sense that they model individual human poses in frames instead of actions. Therefore, our synthetic data generation method, which is an essential part of HPM training, also focuses on generating human pose frames. One interesting enhancement of our data generation technique is to produce synthetic videos instead. We can then analyze the performance gain of (video-based) baseline methods trained with our synthetic data. To explore this direction, we extended our technique to generate synthetic videos and applied it to C3D <ref type="bibr" target="#b49">(Tran et al, 2015)</ref> and LRCN <ref type="bibr" target="#b2">(Donahue et al, 2015)</ref> methods as follows.</p><p>For transparency, we selected all 'atomic' action sequences from CMU MoCap. Each of these sequences presents a single action, which also serves as the label of the video clip. To generate synthetic videos, the frames in the training clips were processed according to the procedure described in Section 3 and 4 with the following major differences. (1) No clustering was performed to learn representative poses because it was not required.</p><p>(2) The parameters (i.e. camera viewpoints, clothing etc.) were kept the same within a single synthetic video but different random settings were adopted for each video. The size of the generated synthetic data was matched to our "pose" synthetic data. We took the original C3D model that is pre-trained on the large scale dataset Sports-1M <ref type="bibr" target="#b20">(Karpathy et al, 2014)</ref> and <ref type="table">Table 4</ref> Action recognition accuracy (%) on the NUCLA Multiview dataset. V 3 1,2 means that view 1 and 2 were used for training and view 3 was used for testing. The symbol † indicates that the original model was fine-tuned with our synthetic data before applying the testing protocol. the original LRCN model that is pre-trained on UCF-101 <ref type="bibr" target="#b45">(Soomro et al, 2012)</ref> and fine-tuned these models using our synthetic videos. The fine-tuned models were then employed under the used protocol. We report the results of these experiments in Table 3 by denoting our enhancements of C3D and LRCN as C3D † and LRCN †. The results demonstrate that our synthetic data can improve the performance of baseline models for multi-view action recognition. The results also ascertain that the proposed approach exploits the proposed data very effectively to achieve significant performance improvement over the existing methods. We provide further discussion on the role of synthetic data in the overall performance of our approach in Section 9. <ref type="table">Table 4</ref> comparative results on the NUCLA dataset. The proposed HPM RGB alone achieves 77.8% average accuracy which is 8.4% higher than the nearest RGB competitor NKTM <ref type="bibr" target="#b35">(Rahmani and Mian, 2015)</ref>. HPM RGB +Traj further improves the average accuracy to 78.5%. Our RGB-D method (HPM RGB +HPM 3D +Traj) achieves 81.3% accuracy which is the highest accuracy reported on this dataset. <ref type="table" target="#tab_4">Table 5</ref> compares our method with existing state-ofthe-art on the NTU dataset. The proposed HPM RGB uses RGB frames only and achieves 68.5% cross-subject recognition accuracy, which is comparable to that of the best joints-based method ST-LSTM  69.2% even though joints have been estimated from depth data and do not contain action irrelevant noises. This demonstrates that the HPM RGB effectively learns features that are invariant to action irrelevant noises such as background, clothing texture and lighting etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Comparison on the Northwestern-UCLA Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Comparison on the NTU RGB+D Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of RGB Results:</head><p>Note that this paper is the first to provide RGB only human action recognition results on the challenging NTU dataset (see <ref type="table" target="#tab_4">Table 5</ref>). Our method (HPM RGB +Traj) outperforms all others by a significant margin while using only RGB data in both cross-subject and crossview settings. In the cross-subject setting, our method achieves 75.8% accuracy which is higher than stateof-the-art DSSCA-SSLM <ref type="bibr" target="#b42">(Shahroudy et al, 2017)</ref> even though DSSCA-SSLM uses both RGB and depth data whereas our method HPM RGB +Traj uses only RGB data. DSSCA-SSLM does not report cross-view results as it did not perform well in that setting <ref type="bibr" target="#b42">(Shahroudy et al, 2017)</ref> whereas our method achieves 83.2% accuracy for the cross-view case which is 7.7% higher than the nearest competitor ST-LSTM  which uses Joints data that is estimated from depth images. In summary, our 2D action recognition method outperforms existing 3D action recognition methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of RGB-D Results:</head><p>From <ref type="table" target="#tab_4">Table 5</ref>, we can see that our RGB-D method (HPM RGB + HPM 3D +Traj) achieves state-of-the-art results in both cross-subject and cross-view settings outperforming the nearest competitors by 6% and 8.4% respectively.  <ref type="table">Table 6</ref> shows the computation time for the major steps of our proposed method. Using a single core of a 3.4GHz CPU and the Tesla K-40 GPU, the proposed RGB (HPM RGB +Traj) and RGB-D (HPM RGB +HPM 3D +Traj) methods run at about 20 frames per second whereas the depth only method runs at about 46 frames per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion</head><p>When uncropped video frames are used to learn a neural network model, the background context is more dominant as it occupies more pixels. A recent study showed that by masking the human in the UCF-101 dataset, a 47.4% "human" action recognition accuracy could still be achieved which, using the same algorithm, is only 9.5% lower than when the humans are included <ref type="bibr" target="#b14">(He et al, 2016b)</ref>. Our HPM RGB learns human poses rather than the background context which is important for surveillance applications where the background is generally static and any action can be performed in the same background. Moreover, HPM RGB and HPM 3D are not fine tuned on any of the datasets on which they are tested. Yet, our models outperform all existing methods by a significant margin. For applications such as robotics and video retrieval where the background context is important, our HPM models can be used to augment the background context. For optimal performance, the cropped human images must be passed through the HPM RGB and HPM 3D . However, both HPMs are robust to cropping errors as many frames in the UWA dataset (especially view 4 in <ref type="figure" target="#fig_0">Fig. 12</ref>) and the NTU dataset have cropping errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Comparison with existing synthetic data</head><p>One of the major contributions of this work is synthetic data generation for robust action recognition. We note that SURREAL (Synthetic hUmans foR REAL tasks) <ref type="bibr" target="#b51">(Varol et al, 2017b)</ref> is another recent method to generate synthetic action data that can be used to train the proposed HPMs. However, there are some major differences between SURREAL and the proposed synthetic data. <ref type="formula" target="#formula_0">(1)</ref>  To demonstrate the use of SURREAL with our pipeline and quantitatively analyze the advantages of the proposed dataset for robust action recognition, we compare the performance of our underlying approach using the two datasets on UWA3D and NUCLA databases. We repeated our experiments using SURREAL as follows. First, we computed 339 representative poses from SURREAL using the HDBSCAN algorithm <ref type="bibr" target="#b30">(McInnes et al, 2017)</ref> and used the skeletal distance function in Eq. (1) to assign frames in the dataset to these poses. HPMs are then trained on these frames using the 339 pose labels, followed by temporal encoding and classification. This pipeline is exactly the same as the one used for our data in Section 8, except that the representative poses are now computed using SURREAL. <ref type="table">Table 7</ref> reports the mean recognition accuracies of HPMs on UWA3D and NUCLA datasets when trained using the SURREAL dataset and the proposed data. The table also includes results for the most challenging viewpoints in the datasets. For UWA3D, View 4 is challenging due to the large variations in both azimuth and elevation angles (see <ref type="figure" target="#fig_0">Fig. 12</ref>). For NUCLA, View 3 is particularly challenging as compared to the other viewpoints (see <ref type="figure" target="#fig_1">Fig. 13</ref>). From the results, we can see that the proposed data is able to achieve significant performance gain over SURREAL for these viewpoints. In our opinion, systematic coverage of 180 o of view in our data is the main reason behind this fact. Our data also achieves a consistent overall gain for both RGB and depth modalities of the test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Improvements with synthetic data</head><p>Our experiments in Section 8 demonstrate a significant performance gain over the current state-of-the-art. Whereas the contribution of the network architecture, data modalities and GAN to the overall performance is clear from the presented experiments, we further investigate the performance gain contributed by the proposed synthetic data. To that end, we first compare HPM RGB , which has been fine-tuned with the proposed data, to HPM Orig which is the original GoogLeNet model -not fine-tuned with our synthetic data. To ensure a fair comparison, all the remaining steps in the proposed pipeline, including temporal encoding and classification, are kept exactly the same for the two cases. The first two rows of <ref type="table">Table 8</ref> compare the mean recognition accuracies of HPM RGB and HPM Orig for UWA3D and NUCLA datasets. These results ascertain a clear performance gain with the proposed synthetic dataset.</p><p>The last two rows of <ref type="table">Table 8</ref> examine the performance gain of two popular baseline methods when finetuned on our synthetic data. Although significant, the average improvement in the accuracies of these methods is rather small compared to that of our method on our synthetic data (first two rows). Recall that our data generation method generates synthetic "poses" rather than videos and we had to extend our method to generate synthetic videos for the sake of this experiment. Details on synthetic video generation and training of the baseline methods are already provided in Section 8.3. From the results in <ref type="table">Table 8</ref>, we conclude that our proposed method exploits our synthetic data more effectively, and both the proposed method and our synthetic data contribute significantly to the overall performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Role of synthetic RGB data in action recognition</head><p>Although our approach deals with RGB, depth and RGB-D data; we find it necessary to briefly discuss  <ref type="table">Table 3</ref> and 4, yet our RGB synthetic data was able to boost their performance. Our synthetic data method easily and efficiently captures as many variations of the exact same action as desired, a real-world analogous to which is extremely difficult. We also tested the performance of our approach on UCF-101 dataset <ref type="bibr" target="#b45">(Soomro et al, 2012)</ref> to analyze the potential of synthetic data and HPMs for the standard action recognition benchmarks in the RGB domain. UCF-101 is a popular RGB-only action dataset, which includes video clips of 101 action classes. The actions covers 1) Human-Object Interaction, 2) Human Body Motion, 3) Human-Human Interaction, 4) Playing Musical Instruments, and 5) Sport Actions. Since we trained our HPMs to model human poses, the appearances of human poses in the test videos are important for a transparent analysis. Therefore, we selected 1910 videos from the dataset with 16 classes of Human Body Motion, and classified them using the proposed HPMs. <ref type="table">Table 9</ref> reports the performance of our approach along the accuracy of C3D on the same subset for comparison. The table also reports the accuracy of HPM RGB †, for which we used twice as many pose labels and training images as used for training HPM RGB . This improved the performance of our approach, indicating the advantage of easily producible synthetic data. Notice that, whereas the performance of HPMs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Human Body Motion C3D 84.8 HPM RGB 82.5 HPM RGB † 84.6 <ref type="table">Table 9</ref> Action recognition accuracy (%) on the Human Body Motion subset of UCF-101 dataset. For a transparent analysis, the results do not include augmentation by trajectory features and/or ensemble features for any of the approaches. The symbol † denotes larger (2×) synthetic training data size.</p><p>remains comparable to C3D, the latter is trained on Millions of 'videos' as compared to the few hundred thousand 'frames' used for training our model. Moreover, our model is nearly 7 times smaller than C3D in size. These facts clearly demonstrate the usefulness of the proposed method and synthetic data generation technique for standard RGB action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We proposed Human Pose Models for human action recognition in RGB, depth and RGB-D videos. The proposed models uniquely represent human poses irrespective of the camera viewpoint, clothing textures, background and lighting conditions. We proposed a method for synthesizing realistic RGB and depth training data for learning such models. The proposed method learns 339 representative human poses from MoCap skeleton data and then fits 3D human models to these skeletons. The human models are then rendered as RGB and depth images from 180 camera viewpoints where other variations such as body shapes, clothing textures, backgrounds and lighting conditions are applied. We adopted Generative Adversarial Networks (GAN) to reduce the distribution gap between the synthetic and real images. Thus, we were able to generate millions of realistic human pose images with known labels to train the Human Pose Models. The trained models contain complementary information between RGB and depth modalities, and also show good compatibility to the hand-crafted dense trajectory features. Experiments on three benchmark RGB-D datasets show that our method outperforms existing state-of-the-art on the challenging problem of cross-view and cross-person human action recognition by significant margins. The HPM RGB , HPM 3D and Python script for generating the synthetic data will be made public.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc>Samples from the 339 representative human poses learned from the CMU MoCap skeleton data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>3D human (male, heavy male, female, child) models generated with the MakeHuman software</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2 shows a few of the learned representative human poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>Sample clothing given by the MakeHuman software to the 3D modelsFig. 5 Each 3D human model is rendered from 180 camera viewpoints on a hemisphere. All cameras point to the center of the human model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>Human pose images rendered from multiple viewpoints</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7</head><label>7</label><figDesc>We use 262 textures for shirts and 183 for the trousers/shorts. The clothing textures of the human models are varied randomly through a Python script in BlenderFig. 8 Synthetic RGB images where variations are introduced in (a) human pose, (b) background and clothing texture, (c) body shapes and (d) lighting conditions era viewpoint, clothing types (not textures) and body shapes when synthesizing depth images. The virtual cameras are deployed in a similar way to the RGB image rendering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9</head><label>9</label><figDesc>Depth human pose images rendered from multiple viewpoints. The first row of the figure illustrates horizontal viewpoint change, and the second row illustrates vertical viewpoint change</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10</head><label>10</label><figDesc>Comparing real, raw synthetic and GAN-refined synthetic RGB images. Last row shows the different between raw and GAN-refined synthetic images. Since the backgrounds used are already real, differences are mostly on the synthetic human body especially at their edges</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>walking, (13) irregular walking, (14) lying down, (15) turning around, (16) drinking, (17) phone answering, (18) bending, (19)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13</head><label>13</label><figDesc>Sample frames from the Northwestern-UCLA Multiview Action dataset<ref type="bibr" target="#b56">(Wang et al, 2014)</ref> jumping jack,(20)running, (21) picking up, (22) putting down, (23) kicking, (24) jumping, (25) dancing, (26) moping floor, (27) sneezing, (28) sitting down (chair), (29) squatting, and (30) coughing. The four viewpoints are: (a) front, (b) left, (c) right, (d) top.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14</head><label>14</label><figDesc>RGB and depth sample frames from the NTU RGB+D Human Activity Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>NetworkLayer Dimension HPM RGB HPM 3D</figDesc><table><row><cell></cell><cell cols="3">UWA3D Multiview Activity-II</cell><cell></cell></row><row><cell>AlexNet</cell><cell>fc7</cell><cell>4096</cell><cell>61.2</cell><cell>72.1</cell></row><row><cell>ResNet-50</cell><cell>pool5</cell><cell>2048</cell><cell>65.4</cell><cell>74.0</cell></row><row><cell cols="2">GoogLeNet pool5</cell><cell>1024</cell><cell>64.7</cell><cell>74.1</cell></row><row><cell></cell><cell cols="3">Northwestern-UCLA Multiview</cell><cell></cell></row><row><cell>AlexNet</cell><cell>fc7</cell><cell>4096</cell><cell>69.9</cell><cell>78.7</cell></row><row><cell>ResNet-50</cell><cell>pool5</cell><cell>2048</cell><cell>75.7</cell><cell>77.3</cell></row><row><cell cols="2">GoogLeNet pool5</cell><cell>1024</cell><cell>76.4</cell><cell>79.8</cell></row><row><cell cols="5">Table 1 Comparison of feature dimensionality and action</cell></row><row><cell cols="5">recognition accuracy(%) for HPM RGB and HPM 3D trained</cell></row><row><cell cols="3">using different network architectures</cell><cell></cell><cell></cell></row><row><cell cols="2">8 Experiments</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Comparison of action recognition accuracy (%) for GoogLeNet based HPM RGB and HPM 3D when trained on raw synthetic images and GAN-refined synthetic images</figDesc><table><row><cell>Training Data</cell><cell cols="2">HPM RGB HPM 3D</cell></row><row><cell cols="2">UWA3D Multiview Activity-II</cell><cell></cell></row><row><cell>Raw synthetic images</cell><cell>64.7</cell><cell>73.8</cell></row><row><cell>GAN-refined synthetic images</cell><cell>68.0</cell><cell>74.8</cell></row><row><cell cols="2">Northwestern-UCLA Multiview</cell><cell></cell></row><row><cell>Raw synthetic images</cell><cell>76.4</cell><cell>78.4</cell></row><row><cell>GAN-refined synthetic images</cell><cell>77.8</cell><cell>79.7</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Action recognition accuracy (%) on the NTU RGB+D Human Activity Dataset. Our RGB only (HPM RGB +Traj) accuracies are higher than the nearest competitors which use RGB-D or Joints data.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Cross</cell><cell>Cross</cell></row><row><cell></cell><cell>Method</cell><cell cols="2">Data type Subject</cell><cell>View</cell></row><row><cell></cell><cell>Baseline</cell><cell></cell><cell></cell></row><row><cell></cell><cell>HON4D (Oreifej and Liu, 2013)</cell><cell>Depth</cell><cell>30.6</cell><cell>7.3</cell></row><row><cell>Our RGB-D method (HPM RGB + HPM 3D +Traj) outperforms all methods by significant margins in both settings</cell><cell cols="2">SNV (Yang and Tian, 2014) HOG-2 (Ohn-Bar and Trivedi, 2013) Skeletal Quads (Evangelidis et al, 2014) Joints Depth Depth Lie Group (Vemulapalli et al, 2014) Joints Deep RNN (Shahroudy et al, 2016a) Joints</cell><cell>31.8 32.4 38.6 50.1 56.3</cell><cell>13.6 22.3 41.4 52.8 64.1</cell></row><row><cell></cell><cell>HBRNN-L (Du et al, 2015)</cell><cell>Joints</cell><cell>59.1</cell><cell>64.0</cell></row><row><cell></cell><cell>Dynamic Skeletons (Hu et al, 2015)</cell><cell>Joints</cell><cell>60.2</cell><cell>65.2</cell></row><row><cell></cell><cell>Deep LSTM (Shahroudy et al, 2016a)</cell><cell>Joints</cell><cell>60.7</cell><cell>67.3</cell></row><row><cell></cell><cell>LieNet (Huang et al, 2016)</cell><cell>Joints</cell><cell>61.4</cell><cell>67.0</cell></row><row><cell></cell><cell>P-LSTM (Shahroudy et al, 2016a)</cell><cell>Joints</cell><cell>62.9</cell><cell>70.3</cell></row><row><cell></cell><cell>LTMD (Luo et al, 2017)</cell><cell>Depth</cell><cell>66.2</cell><cell>-</cell></row><row><cell></cell><cell>ST-LSTM (Liu et al, 2016)</cell><cell>Joints</cell><cell>69.2</cell><cell>77.7</cell></row><row><cell></cell><cell>DSSCA-SSLM (Shahroudy et al, 2017)</cell><cell>RGB-D</cell><cell>74.9</cell><cell>-</cell></row><row><cell></cell><cell>Proposed</cell><cell></cell><cell></cell></row><row><cell></cell><cell>HPM RGB</cell><cell>RGB</cell><cell>68.5</cell><cell>72.9</cell></row><row><cell></cell><cell>HPM RGB +Traj</cell><cell>RGB</cell><cell>75.8</cell><cell>83.2</cell></row><row><cell></cell><cell>HPM 3D</cell><cell>Depth</cell><cell>71.5</cell><cell>70.5</cell></row><row><cell></cell><cell>HPM RGB +HPM 3D</cell><cell>RGB-D</cell><cell>75.8</cell><cell>78.1</cell></row><row><cell></cell><cell>HPM RGB +HPM 3D +Traj</cell><cell>RGB-D</cell><cell>80.9</cell><cell>86.1</cell></row><row><cell>8.6 Timing</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Action recognition accuracy (%) for HPM RGB and HPM 3D when trained with SURREAL dataset<ref type="bibr" target="#b51">(Varol et al, 2017b)</ref> and the proposed synthetic data. V challenge represents the most challenging viewpoint in the multi-view test data, i.e. View 4 for UWA3D and View 3 for NUCLA dataset.</figDesc><table><row><cell></cell><cell>Data</cell><cell cols="4">HPM Feature Trajectory FTP</cell><cell>SVM</cell><cell>Total</cell><cell>Rate(fps)</cell></row><row><cell></cell><cell>RGB</cell><cell cols="2">2.13E-02</cell><cell>2.78E-02</cell><cell>4.39E-05 2.95E-04 4.94E-02</cell><cell>20.2</cell></row><row><cell></cell><cell>Depth</cell><cell cols="2">2.13E-02</cell><cell>-</cell><cell>4.64E-05 3.32E-04 2.17E-02</cell><cell>46.1</cell></row><row><cell></cell><cell cols="3">RGB-D 2.13E-02</cell><cell>2.78E-02</cell><cell>4.64E-05 6.80E-04 4.98E-02</cell><cell>20.1</cell></row><row><cell></cell><cell></cell><cell cols="4">Table 6 Execution time in seconds for the proposed method</cell></row><row><cell>Method</cell><cell cols="2">Training Data</cell><cell>V challenge</cell><cell>Mean</cell></row><row><cell cols="4">UWA3D Multiview Activity-II</cell><cell></cell></row><row><cell cols="2">HPM RGB SURREAL</cell><cell></cell><cell>61.6</cell><cell>67.4</cell></row><row><cell cols="3">HPM RGB Proposed data</cell><cell>69.0</cell><cell>68.0</cell></row><row><cell>HPM 3D</cell><cell>SURREAL</cell><cell></cell><cell>65.8</cell><cell>72.1</cell></row><row><cell>HPM 3D</cell><cell cols="2">Proposed data</cell><cell>74.7</cell><cell>74.8</cell></row><row><cell cols="5">Northwestern-UCLA Multiview</cell></row><row><cell cols="2">HPM RGB SURREAL</cell><cell></cell><cell>69.9</cell><cell>74.4</cell></row><row><cell cols="3">HPM RGB Proposed data</cell><cell>73.1</cell><cell>77.8</cell></row><row><cell>HPM 3D</cell><cell>SURREAL</cell><cell></cell><cell>68.1</cell><cell>77.4</cell></row><row><cell>HPM 3D</cell><cell cols="2">Proposed data</cell><cell>71.9</cell><cell>79.7</cell></row><row><cell>Table 7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SURREAL was originally proposed for body</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>segmentation and depth estimation whereas our dataset</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>aims at modeling distinctive human poses from multi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ple viewpoints. While both datasets provide sufficient</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>variations in clothing, human models, backgrounds, and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>illuminations; our dataset systematically covers 180 o</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>of view to enable viewpoint invariance, which is not</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>the case for SURREAL. (2) To achieve realistic view-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>point variations, our dataset uses 360 o spherical High</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dynamic Range Images whereas SURREAL uses the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSUN dataset (Yu et al, 2015) for backgrounds. Hence,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>our approach is more suitable for large viewpoint vari-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ations. (3) Finally, we use Generative Adversarial Net-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>work to reduce the distribution gap between synthetic</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and real data. Our results in Table 2 already verified</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>that this provides additional boost to the action recog-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>nition performance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table 8Analysis of performance gain due to the proposed data and method. HPM Orig is the original GoogLeNet. The symbols † and ↑ denote enhancement of the existing techniques with our data and the improvement in accuracy (%), respectively.the broader role of synthetic RGB data in human action recognition. In contrast to depth videos, multiple large scale RGB video action datasets are available to train deep action models. Arguably, this diminishes the need of synthetic data in this domain. However, synthetic data generation methods such as ours and<ref type="bibr" target="#b51">(Varol et al, 2017b</ref>) are able to easily ensure a wide variety of action irrelevant variations in the data, e.g. in camera viewpoints, textures, illuminations; up to any desired scale. In natural videos, such variety and scale of variations can not be easily guaranteed even in large scale datasets. For instance, in our experiments in Sections 8.3 and 8.4, both C3D and LRCN were originally pre-trained on large scale RGB video datasets in</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell cols="2">UWA3D NUCLA</cell></row><row><cell>HPM Orig</cell><cell>without synthetic data</cell><cell>62.8</cell><cell>66.7</cell></row><row><cell cols="2">HPM RGB with synthetic data</cell><cell>68.0</cell><cell>77.8</cell></row><row><cell>C3D †</cell><cell>with synthetic data</cell><cell>↑2.3</cell><cell>↑2.3</cell></row><row><cell>LRCN †</cell><cell>with synthetic data</cell><cell>↑3.5</cell><cell>↑1.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The code for this method will be made public.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.makehuman.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.blender.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The data synthesis script will be made public.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/carpedm20/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This research was sponsored by the Australian Research Council grant DP160101458. The Tesla K-40 GPU used for this research was donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4513" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to recognize activities from the wrong view point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tabrizi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="154" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A latent model of discriminative aspect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tabrizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="948" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d pose from motion for cross-view action recognition via non-linear circulant temporal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2601" to="2608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human action recognition without human</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirakabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for rgb-d activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning on lie groups for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latent tensor transfer learning for rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">R</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>arXiv:14085093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-view human action recognition from depth maps using spectral graph sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="108" to="126" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Max-margin heterogeneous information machine for rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="350" to="371" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross-view activity recognition using hankelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">I</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1362" to="1369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminative virtual views for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2855" to="2862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vlad3: Encoding dynamics of deep features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via view knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3209" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatiotemporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single view human action recognition using key pose matching and viterbi path searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Lv F, Nevatia R</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>IEEE conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">hdbscan: Hierarchical density based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Astels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Source Software</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint angles similarities and hog2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">View invariance for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">66</biblScope>
			<biblScope unit="page" from="83" to="101" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1913" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a non-linear knowledge transfer model for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2458" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Histogram of oriented principal components for crossview action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2430" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a deep model for human action recognition from novel viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">View-invariant representation and recognition of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="226" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multimodal multipart learning for action recognition in depth videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2123" to="2129" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep multimodal feature analysis for action recognition in rgb+ d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence Shakhnarovich G (2005) Learning task-specific similarity</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
		<idno>arXiv:161207828</idno>
		<title level="m">Learning from simulated and unsupervised images through adversarial training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>arXiv:12120402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hierarchical dynamic parsing and encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="202" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE international conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="4489" to="4497" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Crossview action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM on Multimedia Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improving human action recognition by non-action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2698" to="2707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Free viewpoint action recognition using motion history volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and image understanding</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="257" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Action recognition from arbitrary views using 3d exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Super normal vector for activity recognition using depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="804" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Actions sketch: A novel action representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="984" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Structure-preserving binary representations for rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1651" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via a continuous virtual path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2690" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning view-invariant sparse representations for cross-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3176" to="3183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1991" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
