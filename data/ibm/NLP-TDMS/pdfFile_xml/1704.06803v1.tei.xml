<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
						</author>
						<title level="a" type="main">Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationarity structures of user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines graph convolutional neural networks and recurrent neural networks to learn meaningful statistical graph-structured patterns and the non-linear diffusion process that generates the known ratings. This neural network system requires a constant number of parameters independent of the matrix size. We apply our method on both synthetic and real datasets, showing that it outperforms state-of-the-art techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recommender systems have become a central part of modern intelligent systems. Recommending movies on Netflix, friends on Facebook, furniture on Amazon, jobs on LinkedIn are a few examples of the main purpose of these systems. Two major approach to recommender systems are collaborative <ref type="bibr" target="#b4">(Breese et al., 1998)</ref> and content <ref type="bibr" target="#b28">(Pazzani &amp; Billsus, 2007)</ref> filtering techniques. Systems based on collaborative filtering use collected ratings of products by customers and offer new recommendations by finding similar rating patterns. Systems based on content filtering make 1 ICS USI Lugano, Switzerland 2 Tel Aviv University, Israel 3 Intel Perceptual Computing, Israel 4 TUM IAS, Germany 5 NTU, Singapore. Correspondence to: Federico Monti &lt;federico.monti@usi.ch&gt;.</p><p>Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&amp;CP. Copyright 2017 by the author(s). use of similarities between products and customers to recommend new products. Hybrid systems combine collaborative and content techniques.</p><p>Matrix completion. Mathematically, a recommendation method can be posed as a matrix completion problem <ref type="bibr" target="#b8">(Candes &amp; Recht, 2012)</ref>, where columns and rows represent users and items, respectively, and matrix values represent a score determining whether a user would like an item or not. Given a small subset of known elements of the matrix, the goal is to fill in the rest. A famous example is the Netflix challenge <ref type="bibr" target="#b20">(Koren et al., 2009</ref>) offered in 2009 and carrying a 1M$ prize for the algorithm that can best predict user ratings for movies based on previous ratings. The size of the Netflix is 480k movies × 18k users (8.5B entries), with only 0.011% known entries.</p><p>Recently, there have been several attempts to incorporate geometric structure into matrix completion problems <ref type="bibr" target="#b24">(Ma et al., 2011;</ref><ref type="bibr" target="#b17">Kalofolias et al., 2014;</ref><ref type="bibr" target="#b29">Rao et al., 2015;</ref><ref type="bibr" target="#b21">Kuang et al., 2016)</ref>, e.g. in the form of column and row graphs representing similarity of users and items, respectively. Such additional information makes well-defined e.g. the notion of smoothness of data and was shown beneficial for the performance of recommender systems. These approaches can be generally related to the field of signal processing on graphs <ref type="bibr" target="#b32">(Shuman et al., 2013)</ref>, extending classical harmonic analysis methods to non-Euclidean domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric deep learning.</head><p>Of key interest to the design of recommender systems are deep learning approaches. In the recent years, deep neural networks and, in particular, convolutional neural networks (CNNs) <ref type="bibr" target="#b22">(LeCun et al., 1998)</ref> have been applied with great success to numerous computer vision-related applications. However, original CNN models cannot be directly applied to the recommendation problem to extract meaningful patterns in users, items and ratings because these data are not Euclidean structured, i.e. they do not lie on regular lattices like images but irregular domains like graphs or manifolds. This strongly motivates the development of geometric deep learning <ref type="bibr" target="#b5">(Bronstein et al., 2016)</ref> techniques that can mathematically deal with graph-structured data, which arises in numerous applications, ranging from computer graphics and vision <ref type="bibr" target="#b1">Boscaini et al., 2015;</ref><ref type="bibr" target="#b3">2016b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b27">Monti et al., 2017)</ref> to chemistry <ref type="bibr" target="#b11">(Duvenaud et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1704.06803v1 [cs.LG] 22 Apr 2017</head><p>The earliest attempts to apply neural networks to graphs are due to <ref type="bibr" target="#b12">Scarselli et al. 2005;</ref> (see more recent formulation <ref type="bibr" target="#b23">(Li et al., 2015;</ref><ref type="bibr" target="#b34">Sukhbaatar et al., 2016)</ref>). <ref type="bibr" target="#b6">Bruna et al. 2013</ref>; 2015 formulated CNN-like deep neural architectures on graphs in the spectral domain, employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator <ref type="bibr" target="#b32">(Shuman et al., 2013)</ref>. In a follow-up work,  proposed an efficient filtering scheme using recurrent Chebyshev polynomials, which reduces the complexity of CNNs on graphs to the same complexity of standard CNNs (on grids). This model was later extended to deal with dynamic data <ref type="bibr" target="#b31">(Seo et al., 2016)</ref>. Kipf and Welling 2016 proposed a simplification of Chebychev networks using simple filters operating on 1-hop neighborhoods of the graph. <ref type="bibr" target="#b27">Monti et al. 2017</ref> introduced a spatial-domain generalization of CNNs to graphs local patch operators represented as Gaussian mixture models, showing a significant advantage of such models in generalizing across different graphs.</p><p>Main contribution. In this work, we treat matrix completion problem as deep learning on graph-structured data. We introduce a novel neural network architecture that is able to extract local stationary patterns from the highdimensional spaces of users and items, and use these meaningful representations to infer the non-linear temporal diffusion mechanism of ratings. The spatial patterns are extracted by a new CNN architecture designed to work on multiple graphs. The temporal dynamics of the rating diffusion is produced by a Long-Short Term Memory (LSTM) recurrent neural network (RNN) <ref type="bibr" target="#b14">(Hochreiter &amp; Schmidhuber, 1997)</ref>. To our knowledge, our work is the first application of graph-based deep learning to matrix completion problem.</p><p>The rest of the paper is organized as follows. Section 2 reviews the matrix completion models. Section 3 presents the proposed approach. Section 4 presents experimental results demonstrating the efficiency of our techniques on synthetic and real-world datasets, and Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Matrix Completion</head><p>Matrix completion problem. Recovering the missing values of a matrix given a small fraction of its entries is an ill-posed problem without additional mathematical constraints on the space of solutions. A well-posed problem is to assume that the variables lie in a smaller subspace, i.e., that the matrix is of low rank,</p><formula xml:id="formula_0">min X rank(X) s.t. x ij = y ij , ∀ij ∈ Ω,<label>(1)</label></formula><p>where X denotes the matrix to recover, Ω is the set of the known entries and y ij are their values. To make (1) robust against noise and perturbation, the equality constraint can be replaced with a penalty</p><formula xml:id="formula_1">min X rank(X) + µ 2 Ω • (X − Y) 2 F ,<label>(2)</label></formula><p>where Ω is the indicator matrix of the known entries Ω and • denotes the Hadamard pointwise product.</p><p>Unfortunately, rank minimization turns out an NP-hard combinatorial problem that is computationally intractable in practical cases. The tightest possible convex relaxation of the previous problem is</p><formula xml:id="formula_2">min X X + µ 2 Ω • (X − Y) 2 F ,<label>(3)</label></formula><p>where · is the nuclear norm of a matrix equal to the sum of its singular values <ref type="bibr" target="#b7">(Candès &amp; Recht, 2009</ref>). Candès and Recht 2009 proved that the 1 relaxation of the SVD lead to solutions that recover almost exactly the original low-rank matrix.</p><p>Geometric matrix completion An alternative relaxation of the rank operator in (1) is to constraint the space of solutions to be smooth w.r.t. some geometric structure of the matrix rows and columns <ref type="bibr" target="#b24">(Ma et al., 2011;</ref><ref type="bibr" target="#b17">Kalofolias et al., 2014;</ref><ref type="bibr" target="#b29">Rao et al., 2015;</ref><ref type="bibr" target="#b0">Benzi et al., 2016)</ref>. The simplest model is proximity structure represented as an undirected weighted column graph</p><formula xml:id="formula_3">G c = ({1, . . . , n}, E c , W c ) with adjacency matrix W c = (w c ij ), where w c ij = w c ji , w c ij = 0 if (i, j) / ∈ E c and w c ij &gt; 0 if (i, j) ∈ E c .</formula><p>In our notation, the column graph could be thought of as a social network capturing relations between users and the similarity of their tastes. The row graph G r = ({1, . . . , m}, E r , W r ) representing the similarities of the items is defined in a similar manner.</p><p>On each of these graphs one can construct the (unnormalized) graph Laplacian, an n × n symmetric positive-</p><formula xml:id="formula_4">semidefinite matrix ∆ = I − D −1/2 WD −1/2 , where D = diag j =i w ij is the degree matrix.</formula><p>We denote the Laplacian associated with row and column graphs by ∆ r and ∆ c , respectively. Considering the columns (respectively, rows) of matrix X as vector-valued functions on the column graph G c (respectively, row graph G r ), their smoothness can be expressed as the Dirichlet norm X 2 Gr = trace(X ∆ r X) (respecitvely, X 2 Gc = trace(X∆ c X )). The geometric matrix completion problem thus boils down to minimizing Factorized models. Matrix completion algorithms introduced in the previous section are well-posed as convex optimization problems, guaranteeing existence, uniqueness and robustness of solutions. Besides, fast algorithms have been developed in the context of compressed sensing to solve the non-differential nuclear norm problem. However, the variables in this formulation are the full m × n matrix X, making such methods hard to scale up to large matrices such as the notorious Netflix challenge.</p><formula xml:id="formula_5">min X X 2 Gr + X 2 Gc + µ 2 Ω • (X − Y) 2 F ,<label>(4)</label></formula><p>A solution is to use a factorized representation <ref type="bibr" target="#b33">(Srebro et al., 2004;</ref><ref type="bibr" target="#b20">Koren et al., 2009;</ref><ref type="bibr" target="#b24">Ma et al., 2011;</ref><ref type="bibr" target="#b36">Yanez &amp; Bach, 2012;</ref><ref type="bibr" target="#b29">Rao et al., 2015;</ref><ref type="bibr" target="#b0">Benzi et al., 2016</ref>) X = WH , where W, H are m × r and n × r matrices, respectively, with r min(m, n). The use of factors W, H reduce the number of degrees of freedom from O(mn) to O(m + n); this representation is also attractive as solving the matrix completion problem often assumes the original matrix to be low-rank, and rank(WH ) ≤ r by construction. <ref type="figure" target="#fig_0">Figure 1</ref> shows the full and factorized settings of the matrix completion problem.</p><p>The nuclear norm minimization problem in the previous section can be equivalently rewritten in a factorized form as <ref type="bibr" target="#b33">(Srebro et al., 2004)</ref>:</p><formula xml:id="formula_6">min W,H 1 2 W 2 F + 1 2 H 2 F + µ 2 Ω • (WH − Y) 2 F . (5)</formula><p>and the factorized formulation of the graph-based minimization problem (4) as</p><formula xml:id="formula_7">min W,H 1 2 W 2 Gr + 1 2 H 2 Gc + µ 2 Ω • (WH − Y) 2 F . (6)</formula><p>The limitation of model <ref type="formula">(6)</ref> is to decouple the regularization process applied simultaneously on the rows and columns of X in (4), but the advantage is linear instead of quadratic complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep learning on graphs</head><p>The key idea to our work is geometric deep learning, an extension of the popular CNNs to graphs. A graph Laplacian admits a spectral eigendecomposition of the form ∆ = ΦΛΦ , where Φ = (φ 1 , . . . φ n ) denotes the matrix of orthonormal eigenvectors and Λ = diag(λ 1 , . . . , λ n ) is the diagonal matrix of the corresponding eigenvalues. The eigenvectors play the role of Fourier atoms in classical harmonic analysis and the eigenvalues can be interpreted as frequencies. Given a function x = (x 1 , . . . , x n ) on the vertices of the graph, its graph Fourier transform is given byx = Φ x. The spectral convolution of two functions x, y can be defined as the element-wise product of the respective Fourier transforms,</p><formula xml:id="formula_8">x y = Φ(Φ x) • (Φ y) = Φ diag(ŷ 1 , . . . ,ŷ n )x. (7)</formula><p>Bruna et al. 2013 used the spectral definition of convolution <ref type="formula">(7)</ref> to generalize CNNs on graphs. A spectral convolutional layer has the form</p><formula xml:id="formula_9">x l = ξ   q l =1 ΦŶ ll Φ x l   , l = 1, . . . , q,<label>(8)</label></formula><p>where q , q denote the number of input and output channels, respectively,Ŷ ll = diag(ŷ ll ,1 , . . . ,ŷ ll ,n ) is a diagonal matrix of spectral multipliers representing a learnable filter in the spectral domain, and ξ is a nonlinearity (e.g. ReLU) applied on the vertex-wise function values. Unlike classical convolutions carried out efficiently in the spectral domain using FFT, the computations of the forward and inverse graph Fourier transform incur expensive O(n 2 ) multiplication by the matrices Φ, Φ , as there are no FFTlike algorithms on general graphs. Furthermore, there is no guarantee that the filters represented in the spectral domain are localized in the spatial domain, which is an important property of classical CNNs.</p><p>To address these issues, <ref type="bibr" target="#b9">Defferrard et al. 2016</ref> proposed using an explicit expansion in the Chebyshev polynomial basis to represent the spectral filters</p><formula xml:id="formula_10">τ θ (∆) = p−1 j=0 θ j T j (∆) = p−1 j=0 θ j ΦT j (Λ)Φ , (9) X X (t)X(t) MGCNN RNN dX (t) X (t+1) = X (t) + dX (t)</formula><p>row+column filtering where∆ = 2λ −1 n ∆ − I is the rescaled Laplacian such that its eigenvaluesΛ = 2λ −1 n Λ − I are in the interval [−1, 1], θ is the p-dimensional vector of polynomial coefficients parametrizing the filter, and T j (λ) = 2λT j−1 (λ)−T j−2 (λ) denotes the Chebyshev polynomial of degree j defined in a recursive manner with T 1 (λ) = λ and T 0 (λ) = 1. 1 This approach benefits from several advantages. First, it does not require an explicit computation of the Laplacian eigenvectors, and due to the recursive definition of the Chebyshev polynomials, the computation of the filter incurs applying the Laplacian p times. Multiplication by Laplacian has the cost of O(|E|), and assuming the graph has |E| = O(n) edges (which is the case for k-nearest neighbors graphs and most real-world networks), the overall complexity is O(n) rather than O(n 2 ) operations, which is the same complexity than standard CNNs. Moreover, since the Laplacian is a local operator affecting only 1-hop neighbors of a vertex and accordingly its (p − 1)st power affects the p-hop neighborhood, the resulting filters are spatially localized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our approach</head><p>In this paper, we propose formulating matrix completion as a learnable diffusion process applied to the score values. The deep learning architecture considered for this purpose consists of a spatial part extracting spatial features from the matrix (we consider two different approaches working on the full and factorized matrix models), and a temporal part using a recurrent LSTM network. The two architectures are 1 Tj(λ) = cos(j cos −1 (λ)) is an oscillating function on [−1, 1] with j roots, j + 1 equally spaced extrema, and a frequency linearly dependent on j. Chebyshev polynomials form an orthogonal basis for the space of smooth functions on [−1, 1] and are thus convenient to compactly represent spectral filters. summarized in Figures 2 and 3 and described in details in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Graph CNNs</head><p>Multi-graph convolution. Our first goal is to extend the notion of the aforementioned graph Fourier transform to matrices whose rows and columns are defined on row-and column-graphs. We recall that a classical two-dimensional Fourier transform of an image (matrix) can be thought of as applying a one-dimensional Fourier transform to its rows and columns. In our setting, the analogy of the twodimensional Fourier transform has the form</p><formula xml:id="formula_11">X = Φ r XΦ c<label>(10)</label></formula><p>where Φ c , Φ r and Λ c , Λ r denote the n × n and m × m eigenvector-and eigenvalue matrices of the column-and row-graph Laplacians ∆ c , ∆ r , respectively. The multigraph version of the spectral convolution <ref type="formula">(7)</ref> is given by</p><formula xml:id="formula_12">X Y = Φ r (X •Ŷ)Φ c .<label>(11)</label></formula><p>Representing the filters as their spectral multipliersŶ would yield O(mn) parameters, prohibitive in any practical application. To overcome this limitation, we resort to the representation of the filters in Chebychev polynomial bases of degree p,</p><formula xml:id="formula_13">τ Θ (λ c ,λ r ) = p j,j =0 θ jj T j (λ c )T j (λ r ),<label>(12)</label></formula><p>where Θ = (θ jj ) is the (p + 1) × (p + 1) matrix of coefficients, i.e., O(1) parameters. The application of such filters to the matrix X A Multi-Graph CNN (MGCNN) using this parametrization of filters (13) in the convolutional layer is applied to the m×n matrix X (single input channel), producing q outputs (i.e., a tensor of size m × n × q).</p><formula xml:id="formula_14">X = p j,j =0 θ jj T j (∆ r )XT j (∆ c )<label>(13)</label></formula><formula xml:id="formula_15">W H H (t)H(t) W (t)W(t) GCNN RNN GCNN RNN dH (t) dW (t) W (t+1) = W (t) + dW (t) H (t+1) = H (t) + dH (t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Separable convolution.</head><p>A simplification of the multigraph convolution is obtained considering the factorized form of the matrix X = WH and applying onedimensional convolution on the respective graph to each factor,w <ref type="bibr">. . . , q, (15)</ref> whereŶ ll r = diag(ŷ r ll ,1 , . . . ,ŷ r ll ,m ) andŶ ll c = diag(ŷ c ll ,1 , . . . ,ŷ c ll ,n ) are the row-and column-filters resulting in a total of O(m + n) parameters. Similarly to the previous case, we can express the filters resorting to Chebyshev polynomials,</p><formula xml:id="formula_16">l = q l =1 Φ rŶ ll r Φ r w l , l = 1, . . . , q (14) h l = q l =1 Φ cŶ ll c Φ c h l , l = 1,</formula><formula xml:id="formula_17">w l = q l =1 p j=0 θ r ll ,j T j (∆ r )w l (16) h l = q l =1 p j =0 θ c ll ,j T j (∆ c )h l<label>(17)</label></formula><p>with 2(p + 1)qq parameters in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Matrix diffusion with RNN</head><p>The next step of our approach is to feed the features extracted from the matrix by the MGCNN (or alternatively, the row-and column-GCNNs) to a Recurrent Neural Network (RNN) implementing the score diffusion process. We use the classical Long-Short Term Memory (LSTM) RNN architecture <ref type="bibr" target="#b14">(Hochreiter &amp; Schmidhuber, 1997)</ref>, which has demonstrated to be highly efficient to learn the dynamical property of data sequences as LSTM is able to keep long-term internal states (in particular, avoiding the vanishing gradient issue). The input of the LSTM gate is given by the static features extracted from the MGCNN, which can be seen as a projection or dimensionality reduction of the original matrix in the space of the most meaningful and representative information (the disentanglement effect). This representation coupled with LSTM appears particularly well-suited to keep a long term internal state, which allows to predict accurate small changes dX of the matrix X (or dW, dH of the factors W, H) that can propagate through the full temporal steps.</p><p>Figures 2 and 3 provides an illustration of the proposed matrix completion model. We also give a precise description of the two settings of our model in Algorithms 1 and 2. We refer to the whole architecture combining the MGCNN and RNN in the full matrix completion setting as Recurrent Graph CNN (RGCNN). The factorized version with two GCNNs and RNN is referred to as separable Recurrent <ref type="figure">Graph CNN (sRGCNN)</ref>.</p><p>The complexity of Algorithm 1 scales quadratically as O(mn) due to the use of MGCNN. For large matrices, we can opt for Algorithm 2 that processes the rows and columns separately with standard GCNNs and scales linearly as O(m + n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Full matrix completion model using RGCNN</head><p>input m × n matrix X (0) containing initial values 1: for t = 0 : T do 2:</p><p>Apply the Multi-Graph CNN (13) on X (t) producing an m×n×q outputX (t) containing a q-dimensional feature vector for each matrix element.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for all elements (i, j) do 4:</p><p>Apply RNN to feature vectorx</p><formula xml:id="formula_18">(t) ij = (x (t) ij1 , . . . ,x (t)</formula><p>ijq ) producing the predicted incremental value dx Update X (t+1) = X (t) + dX (t) 7: end for Algorithm 2 Factorized matrix completion model using sRGCNN input m×r factor H (0) and n×r factor W (0) representing the matrix X (0) 1: for t = 0 : T do 2:</p><p>Apply the Graph CNN on H (t) producing an n × q outputH (t) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for j = 1 : n do 4:</p><p>Apply RNN to feature vectorh</p><formula xml:id="formula_19">(t) j = (h (t) j1 , . . . ,h (t)</formula><p>jq ) producing the predicted incremental value dh Apply the Graph CNN on W (t) producing an m × q outputW (t) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>for i = 1 : m do 9:</p><p>Apply RNN to feature vectorw</p><formula xml:id="formula_20">(t) i = (w (t) i1 , . . . ,w (t)</formula><p>iq ) producing the predicted incremental value dw Update W (t+1) = W (t) + dW (t) 12: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>Training of the networks is performed by minimizing the loss</p><formula xml:id="formula_21">(Θ, σ) = X (T ) Θ,σ 2 Gr + X (T ) Θ,σ 2 Gc + µ 2 Ω•(X (T ) Θ,σ −Y) 2 F .<label>(18)</label></formula><p>Here, T denotes the number of diffusion iterations (applications of the RNN), and we use the notation X (T ) Θ,σ to emphasize that the matrix depends on the parameters of the MGCNN (Chebyshev polynomial coefficients Θ) and those of the LSTM (denoted by σ).</p><p>In the factorized setting, we use the loss</p><formula xml:id="formula_22">(θ r , θ c , σ) = W (T ) θr,σ 2 Gr + H (T ) θc,σ 2 Gc (19) + µ 2 Ω • (W (T ) θr,σ (H (T ) θc,σ ) − Y) 2 F .</formula><p>where θ c , θ r are the parameters of the two GCNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Experimental settings. We closely followed the experimental setup of <ref type="bibr" target="#b29">(Rao et al., 2015)</ref>, using five standard datasets: Synthetic dataset from <ref type="bibr" target="#b17">(Kalofolias et al., 2014)</ref>, MovieLens <ref type="bibr" target="#b26">(Miller et al., 2003)</ref>, Flixster <ref type="bibr" target="#b16">(Jamali &amp; Ester, 2010)</ref>, Douban <ref type="bibr" target="#b24">(Ma et al., 2011)</ref>, and YahooMusic <ref type="bibr" target="#b10">(Dror et al., 2012)</ref>. Classical Matrix Completion (MC) <ref type="bibr" target="#b8">(Candes &amp; Recht, 2012)</ref>, Inductive Matrix Completion (IMC) <ref type="bibr" target="#b15">(Jain &amp; Dhillon, 2013;</ref><ref type="bibr" target="#b35">Xu et al., 2013)</ref>, Geometric Matrix Completion (GMC) <ref type="bibr" target="#b17">(Kalofolias et al., 2014)</ref>, and Graph Regularized Alternating Least Squares (GRALS) <ref type="bibr" target="#b29">(Rao et al., 2015)</ref> were used as baseline methods.</p><p>In all the experiments, we used the following settings for our RGCNNs: Chebyshev polynomials of order p = 5, outputting k = 32-dimensional features, LSTM cells with 32 features and T = 10 diffusion steps. All the models were implemented in Google TensorFlow and trained using the Adam stochastic optimization algorithm <ref type="bibr" target="#b18">(Kingma &amp; Ba, 2014)</ref> with learning rate 10 −3 . In factorized models, rank r = 15 and 10 was used for the synthetic and real datasets, respectively. For all methods, hyperparameters were chosen by cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic data</head><p>We start our experimental evaluation showing the performance of our approach on a small synthetic dataset, in which the user and item graphs have strong communities structure. Though rather simple, such a dataset allows to study the behavior of different algorithms in controlled settings. The performance of different matrix completion methods is reported in <ref type="table" target="#tab_1">Table 1</ref>, along with their theoretical complexity. Our RGCNN model achieves the best accuracy, followed by the separable RGCNN. Different diffusion time steps of these two models are visualized in <ref type="figure" target="#fig_6">Figure  4</ref>. <ref type="figure">Figure 5</ref> shows the convergence rates of different methods. <ref type="figure" target="#fig_7">Figures 6 and 7</ref> depict the spectral filters learnt by the MGCNN and row-and column-GCNNs.</p><p>We repeated the same experiment considering only the column (users) graph to be given. In this setting, the RGCNN cannot be applied, while the sRGCNN has only one GCNN applied on the factor H, and the other factor W is free.    <ref type="figure">Figure 5</ref>. Convergence rates of the tested algorithms over the Synthetic Netflix dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Real data</head><p>Following <ref type="bibr" target="#b29">(Rao et al., 2015)</ref>, we evaluated the proposed approach on the MovieLens, Flixster, Douban and YahooMusic datasets. For the MovieLens dataset we constructed the user and item (movie) graphs as unweighted 10-nearest neighbor graphs in the space of user and movie features, respectively. For Flixster, the user and item graphs were constructed from the scores of the original matrix. On this dataset, we also performed an experiment using only the users graph. For the Douban dataset, we used only the user graph (the provided social network of the user). For the Ya-hooMusic dataset, we used only the item graph, constructed with unweighted 10-nearest neighbors in the space of item features (artists, albums, and genres). For the latter three datasets, we used a sub-matrix of 3000 × 3000 entries for evaluating the performance.</p><p>Tables 3 and 4 summarize the performance of different methods. RGCNN outperforms the competitors in all the experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented a new deep learning approach for matrix completion based on a specially designed multigraph convolutional neural network architecture. Among the key advantages of our approach compared to traditional methods is its low computational complexity and constant number of degrees of freedom independent of the matrix size. We showed that the use of deep learning for matrix completion allows to beat current state-of-the-art recommender system methods. To our knowledge, our work is the first application of deep learning on graphs to this class of problems. We believe that it shows the potential of the nascent field of geometric deep learning on non-Euclidean domains, and will encourage future works in this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Full (top) and factorized (bottom) geometric matrix completion representations. The column and row graphs represent the relationships between users and items, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Recurrent GCNN (RGCNN) architecture using the full matrix completion model and operating simultaneously on the rows and columns of the matrix X. The output of the Multi-Graph CNN (MGCNN) module is a q-dimensional feature vector for each element of the input matrix. The number of parameters to learn is O(1) and the learning complexity is O(mn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Separable Recurrent GCNN (sRGCNN) architecture using the factorized matrix completion model and operating separately on the rows and columns of the factors W, H . The output of the GCNN module is a q-dimensional feature vector for each input row/column, respectively. The number of parameters to learn is O(1) and the learning complexity is O(m + n). results in O(mn) complexity. Here, as previously,∆ r ,∆ c denote the scaled Laplacians with eigenvalues in the interval [−1, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t+1) = H (t) + dH (t) 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Evolution of the matrix X (t) with our architecture using full matrix completion model RGCNN (top) and factorized matrix completion model sRGCNN (bottom). Numbers indicate the RMS error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Absolute value of the first 8 spectral filters learnt by our bidimensional convolution. On the left the first filter with the reference axes associated to the row and column graph eigenvalues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Absolute value of the first four spectral filters learned by the user (solid) and items (dashed) GCNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>summarizes the results of this experiment, again,</cell></row><row><cell>showing that our approach performs the best.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison</figDesc><table><row><cell></cell><cell></cell><cell cols="5">of different matrix completion methods us-</cell></row><row><cell cols="7">ing users+items graphs in terms of number of parameters (opti-</cell></row><row><cell cols="7">mization variables) and computational complexity order (opera-</cell></row><row><cell cols="7">tions per iteration). Rightmost column shows the RMS error on</cell></row><row><cell cols="3">Synthetic dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>METHOD</cell><cell cols="4">PARAMETERS COMPLEXITY RMSE</cell></row><row><cell></cell><cell>GMC</cell><cell>mn</cell><cell></cell><cell>mn</cell><cell>0.3693</cell></row><row><cell></cell><cell>GRALS</cell><cell>m + n</cell><cell></cell><cell>m + n</cell><cell>0.0114</cell></row><row><cell></cell><cell>RGCNN</cell><cell>1</cell><cell></cell><cell>mn</cell><cell>0.0053</cell></row><row><cell></cell><cell>sRGCNN</cell><cell>1</cell><cell></cell><cell>m + n</cell><cell>0.0106</cell></row><row><cell cols="7">Table 2. Comparison of different matrix completion methods us-</cell></row><row><cell cols="7">ing users graph only in terms of number of parameters (optimiza-</cell></row><row><cell cols="7">tion variables) and computational complexity order (operations</cell></row><row><cell cols="7">per iteration). Rightmost column shows the RMS error on Syn-</cell></row><row><cell cols="2">thetic dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>METHOD</cell><cell cols="4">PARAMETERS COMPLEXITY RMSE</cell></row><row><cell></cell><cell>GRALS</cell><cell>m + n</cell><cell></cell><cell>m + n</cell><cell>0.0452</cell></row><row><cell></cell><cell>sRGCNN</cell><cell>m</cell><cell></cell><cell>m + n</cell><cell>0.0362</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GMC</cell></row><row><cell></cell><cell>10 −2</cell><cell></cell><cell></cell><cell></cell><cell>GRALS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sRGCNN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RGCNN</cell></row><row><cell>RMSE</cell><cell>10 −3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10 −4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Training iterations</cell><cell>·10 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Performance (RMS error) of different matrix completion methods on the MovieLens dataset. Matrix completion results on several datasets (RMS error). For Douban and YahooMusic, a single graph (of users and items, respectively) was used. For Flixter, two settings are shown: users+items graphs / only users graph.</figDesc><table><row><cell>METHOD</cell><cell></cell><cell></cell><cell></cell><cell>RMSE</cell></row><row><cell cols="2">GLOBAL MEAN</cell><cell></cell><cell></cell><cell>1.154</cell></row><row><cell>USER MEAN</cell><cell></cell><cell></cell><cell></cell><cell>1.063</cell></row><row><cell cols="2">MOVIE MEAN</cell><cell></cell><cell></cell><cell>1.033</cell></row><row><cell cols="2">MC (CANDES &amp; RECHT, 2012)</cell><cell></cell><cell></cell><cell>0.973</cell></row><row><cell cols="4">IMC (JAIN &amp; DHILLON, 2013; XU ET AL., 2013)</cell><cell>1.653</cell></row><row><cell cols="3">GMC (KALOFOLIAS ET AL., 2014)</cell><cell></cell><cell>0.996</cell></row><row><cell cols="2">GRALS (RAO ET AL., 2015)</cell><cell></cell><cell></cell><cell>0.945</cell></row><row><cell>sRGCNN</cell><cell></cell><cell></cell><cell></cell><cell>0.929</cell></row><row><cell>METHOD</cell><cell>FLIXSTER</cell><cell cols="3">DOUBAN YAHOOMUSIC</cell></row><row><cell>GRALS</cell><cell>1.3126 / 1.2447</cell><cell>0.8326</cell><cell cols="2">38.0423</cell></row><row><cell>sRGCNN</cell><cell>1.1788 / 0.9258</cell><cell>0.8012</cell><cell cols="2">22.4149</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Song recommendation with non-negative matrix factorization and graph total variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Benzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning classspecific descriptors for deformable shapes using localized spectral convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>1467-8659</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="13" to="23" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anisotropic diffusion descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="431" to="441" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical Analysis of Predictive Algorithms for Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Breese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Uncertainty in Artificial Intelligence</title>
		<meeting>Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08097</idno>
		<title level="m">Geometric deep learning: going beyond euclidean data</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exact Matrix Completion via Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="111" to="119" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Yahoo! music dataset and KDD-Cup&apos;11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Cup</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0626</idno>
		<title level="m">Provable inductive matrix completion</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A matrix factorization technique with trust propagation for recommendation in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Recommender Systems</title>
		<meeting>Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.1717</idno>
	</analytic>
	<monogr>
		<title level="j">Matrix completion on graphs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A harmonic extension approach for collaborative ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bertozzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05127</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Web Search and Data Mining</title>
		<meeting>Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DRR</title>
		<meeting>3DRR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MovieLens unplugged: experiences with an occasionally connected recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intelligent User Interfaces</title>
		<meeting>Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Content-based Recommendation Systems. The Adaptive Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Billsus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaborative filtering with graph information: Consistency and scalable methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc. Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Maximum-Margin Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07736</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Speedup matrix completion with side information: Application to multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Primal-dual algorithms for nonnegative matrix factorization with the kullback-leibler divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yanez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1788</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
