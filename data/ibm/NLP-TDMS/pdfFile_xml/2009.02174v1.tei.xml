<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Self-Organizing Maps with Unsupervised Feature Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyes</forename><surname>Khacef</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LEAT</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Rodriguez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LEAT</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoît</forename><surname>Miramond</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LEAT</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Self-Organizing Maps with Unsupervised Feature Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Self-Organizing Map (SOM) is a brain-inspired neural model that is very promising for unsupervised learning, especially in embedded applications. However, it is unable to learn efficient prototypes when dealing with complex datasets. We propose in this work to improve the SOM performance by using extracted features instead of raw data. We conduct a comparative study on the SOM classification accuracy with unsupervised feature extraction using two different approaches: a machine learning approach with Sparse Convolutional Auto-Encoders using gradient-based learning, and a neuroscience approach with Spiking Neural Networks using Spike Timing Dependant Plasticity learning. The SOM is trained on the extracted features, then very few labeled samples are used to label the neurons with their corresponding class. We investigate the impact of the feature maps, the SOM size and the labeled subset size on the classification accuracy using the different feature extraction methods. We improve the SOM classification by +6.09% and reach state-of-the-art performance on unsupervised image classification.</p><p>Keywords: brain-inspired computing · self-organizing map · unsupervised learning · feature extraction · sparse convolutional auto-encoders · spiking neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the fast expansion of Internet of Things (IoT) devices, a huge amount of unlabeled data is gathered everyday. While it is a big opportunity for Artificial Intelligence (AI) and Machine Learning (ML), the difficult task of labeling these data makes Deep Learning (DL) techniques slowly reaching the limits of supervised learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. Hence, unsupervised learning is becoming one of the most important and challenging topics in ML. In this work, we use the Self-Organizing Map (SOM) proposed by Kohonen <ref type="bibr" target="#b20">[20]</ref>, an Artificial Neural Network (ANN) that is very popular in the unsupervised learning category <ref type="bibr" target="#b22">[22]</ref>. Inspired from the cortical synaptic plasticity and its self-organization properties, the SOM is a powerful vector quantization algorithm which models the probability density function of the data into a set of prototype vectors that are represented by the neurons synaptic weights <ref type="bibr" target="#b35">[34]</ref>. It has been shown that SOMs perform better in representing overlapping structures compared to classical clustering techniques such as partitive clustering or K-means <ref type="bibr" target="#b2">[3]</ref>.</p><p>In addition, SOMs are well suited to hardware implementation based on cellular neuromorphic architectures <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b38">37]</ref>. Thanks to a fully distributed architecture with local connectivity amongst hardware neurons, the energy-efficiency of the SOM is highly improved since there is no communication between a centralized controller and a shared memory unit, as it is the case in classical Von-Neumann architectures. Moreover, the connectivity and computational complexities of the SOM become scalable with respect to the number of neurons <ref type="bibr" target="#b34">[33]</ref>. SOMs are used in a large range of applications <ref type="bibr" target="#b21">[21]</ref> going from high-dimensional data analysis to more recent developments such as identification of social media trends <ref type="bibr" target="#b37">[36]</ref>, incremental change detection <ref type="bibr" target="#b29">[28]</ref> and energy consumption minimization on sensor networks <ref type="bibr" target="#b23">[23]</ref>.</p><p>This work is an extension of the work done in <ref type="bibr" target="#b13">[14]</ref>, where we introduced the problem of post-labeled unsupervised learning: no label is available during training and representations are learned in an unsupervised fashion, then very few labels are available for assigning each representation the class it represents. The latter is called the labeling phase. In <ref type="bibr" target="#b13">[14]</ref>, we used the MNIST dataset <ref type="bibr" target="#b24">[24]</ref> to demonstrate the potential of this unsupervised learning method on the classification problem and compared different training and labelling techniques. In order to improve the classification accuracy of the SOM and be able to work with more complex datasets, we need to extract useful features from the raw data that will then be classified with the SOM. In the context of unsupervised learning, feature extraction can be done using two different approaches: a classical "machine learning approach" using Sparse Convolutional Auto-Encoders (SCAEs), and a "neuroscience approach" using Spiking Neural Networks (SNNs). The SCAE is trained using gradient back-propagation while the SNN is trained using Spike Timing Dependant Plasticity (STDP). The goal of this work is to compare the performance of both approaches when using a SOM classifier. We also experiment a supervised Convolutional Neural Network (CNN) with the same topology for approximating the best accuracy we can expect from the feature extraction. Section 2 describes the unsupervised feature extraction methods and details the SOM training and labeling algorithms. Then, Section 3 presents the implementation details of each feature extractor. Next, Section 4 presents the experiments and results on MNIST unsupervised classification. Finally, Section 5 and Section 6 discuss and conclude our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work and methodology</head><p>In this section, we review the related work and present the proposed methodology. We begin with the unsupervised feature extraction learning part, then how to train the SOM, and we finally explain the labeling procedure. Our first step is to extract relevant features from the raw data using unsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised feature extraction</head><p>Sparse Convolutional Auto-Encoders (SCAEs) Introduced by Rumelhart, Hinton and Williams <ref type="bibr" target="#b36">[35]</ref>, AEs were designed to address the problem of back propagation without supervisor via taking the input data itself as the supervised label <ref type="bibr" target="#b0">[1]</ref>. Today, AEs are typically used for dimensionality reduction or weights initialization in CNNs to improve the classification accuracy <ref type="bibr" target="#b26">[26]</ref>  <ref type="bibr" target="#b19">[19]</ref>. In this work, we want to use AEs as feature extractors with unsupervised learning. In such cases, the feature map representation of a Convolutional AE (CAE) is most of the time of a much higher dimensionality than the input image. While this feature representation seems well-suited in a supervised CNN, the overcomplete representation becomes problematic in an AE since it gives the autoencoder the possibility to simply learn the identity function by having only one weight on in the convolutional kernels <ref type="bibr" target="#b26">[26]</ref>. Without any further constraints, each convolutional layer in the AE could easily learn a simple point filter that copies the input onto a feature map <ref type="bibr" target="#b19">[19]</ref>. While this would later simplify a perfect reconstruction of the input, the CAE does not find any more suitable representation for our data. To prevent this problem, some constraints have to be applied in the CAE to increase the sparsity of the features representation.</p><p>The concept of sparsity was introduced in computational neuroscience, as sparse representations resemble the behavior of simple cells in the mammalian primary visual cortex, which is believed to have evolved to discover efficient coding strategies <ref type="bibr" target="#b32">[31]</ref>. It has been proven that encouraging sparsity when learning the transformed representation can improve the performance of classification tasks <ref type="bibr" target="#b10">[11]</ref>. Indeed, the overcomplete architecture of a CAE allows a larger number of hidden units in the code, but this requires that for the given input, most of hidden neurons result in very little activation <ref type="bibr" target="#b31">[30]</ref>. In a Sparse CAE (SCAE), activations of the encoding layer need to have low values in average. Units in the hidden layers usually do not fire <ref type="bibr" target="#b3">[4]</ref> so that the few non-zero elements represent the most salient features <ref type="bibr" target="#b31">[30]</ref>.</p><p>In order to increase the sparsity of the CAE's feature representation, several methods can be found in the literature. In <ref type="bibr" target="#b26">[26]</ref>, the authors use max-pooling to enforce the learning of plausible filters, but the filters are then fine-tuned with supervised learning for the classification. Since we do not want to use any label in the training process, we apply additional constraints in the SCAE, namely weights and activity constraints of types L2 and L1, respectively <ref type="bibr" target="#b30">[29]</ref>.</p><p>Spiking Neural Networks (SNNs) Spiking Neural Networks (SNNs) are a brain-inspired family of ANNs used for large-scale simulations in neuroscience <ref type="bibr" target="#b9">[10]</ref> and efficient hardware implementations for embedded AI <ref type="bibr" target="#b5">[6]</ref>. SNNs are characterized by the spike-based information coding, a computational model of the electrical impulses amongst the biological neurons. The amplitude and duration of all spikes are almost the same, so they are mainly characterized by their emission time <ref type="bibr" target="#b17">[17]</ref>. Furthermore, spiking neurons appear to fire a spike only when they have to send an important message, which leads to the fast and extremely energy-efficient neural computation in the brain.</p><p>Moreover, SNNs have a great potential for unsupervised learning through STDP <ref type="bibr" target="#b6">[7]</ref>, a biologically plausible local learning mechanism that uses the spiketiming correlation to update the synaptic weights. <ref type="bibr">Kheradpisheh et al. proposed</ref> in <ref type="bibr" target="#b17">[17]</ref> a SNN architecture that implements convolutional and pooling layers for spike-based unsupervised feature extraction. The SNN processes image inputs as follow. The first layer of the network uses Difference of Gaussians (DoG) filters to detect contrasts in the input image. It encodes the strength of the edges in the latencies of its output spikes, i.e. the higher the contrast, the shorter the latency. On the one hand, neurons in convolutional layers detect complex features by integrating input spikes from the previous layer, and emit a spike as soon as they detect their "preferred" visual feature. A Winner-Take-All (WTA) mechanism is implemented so that the neurons that fire earlier perform the STDP learning and prevent the others from firing. Hence, more salient and frequent features tend to be learned by the network. On the other hand, neurons in the pooling layers provide translation invariance by using a temporal maximum operation, and help the network to compress the flow of visual data by propagating the first spike received from neighboring neurons in the previous layer which are selective to the same feature. However, in <ref type="bibr" target="#b17">[17]</ref>, the extracted features were classified using a supervised Support Vector Machine (SVM). In this work, we use the unsupervised SOM classifier to keep the unsupervised training from end to end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised classification with Self-Organizing Maps (SOMs)</head><p>SOM learning The next step consists in training a SOM using the extracted features. We use a two-dimensional array of k neurons, that are randomly initialized and updated thanks to the following algorithm based on <ref type="bibr" target="#b20">[20]</ref>:</p><p>Initialize the network as a two-dimensional array of k neurons, where each neuron n with m inputs is defined by a two-dimensional position p n and a randomly initialized m-dimensional weight vector w n . for t from 0 to t f do for every input vector v do for every neuron n in the network do Compute the afferent activity a n from the distance d:</p><formula xml:id="formula_0">d = v − w n (1) a n = e − d α<label>(2)</label></formula><p>end for Compute the winner s such that:</p><formula xml:id="formula_1">a s = k−1 max n=0 (a n )<label>(3)</label></formula><p>for every neuron n in the network do Compute the neighborhood function h σ (t, n, s):</p><formula xml:id="formula_2">h σ (t, n, s) = e − pn−ps 2 2σ(t) 2<label>(4)</label></formula><p>Update the weight w n of the neuron n:</p><formula xml:id="formula_3">w n = w n + (t) × h σ (t, n, s) × (v − w n )<label>(5)</label></formula><p>end for end for Update the learning rate (t):</p><formula xml:id="formula_4">(t) = i f i t/t f (6)</formula><p>Update the width of the neighborhood σ(t):</p><formula xml:id="formula_5">σ(t) = σ i σ f σ i t/t f<label>(7)</label></formula><p>end for It is to note that t f is the number of epochs, i.e. the number of times the whole training dataset is presented. The α hyper-parameter is the width of the Gaussian kernel. Its value in Equation 2 is fixed to 1 in the SOM training, but it does not have any impact in the training phase since it does not change the neuron with the maximum activity. Its value becomes critical though in the labeling process. The SOM hyper-parameters are reported in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOM labeling</head><p>The labeling is the step between training and test where we assign each neuron the class it represents in the training dataset. We proposed in <ref type="bibr" target="#b13">[14]</ref> a labeling algorithm based on very few labels. We randomly took a labeled subset of the training dataset, and we tried to minimize its size while keeping the best classification accuracy. Our study showed that we only need 1% of randomly taken labeled samples from the training dataset for MNIST classification. In this work, we will extend the so-called post-labeled unsupervised learning to SOM classification with features extracted by different means.</p><p>The labeling algorithm detailed in <ref type="bibr" target="#b13">[14]</ref> can be summarized in five steps. First, we calculate the neurons activations based on the labeled input samples from the euclidean distance following Equation 2, where v is the input vector, w n and a n are respectively the weights vector and the activity of the neuron n. The parameter α is the width of the Gaussian kernel that becomes a hyper-parameter for the method. Second, the Best Matching Unit (BMU), i.e. the neuron with the maximum activity is elected. Third, each neuron accumulates its normalized activation (simple division) with respect to the BMU activity in the corresponding class accumulator, and the three steps are repeated for every sample of the labeling subset. Fourth, each class accumulator is normalized over the number of samples per class. Fifth and finally, the label of each neuron is chosen according to the class accumulator that has the maximum activity. The complete GPUbased source code is available in https://github.com/lyes-khacef/GPU-SOM.</p><p>3 Implementation details <ref type="bibr">MNIST [24]</ref> is a dataset of 70000 handwritten digits (60000 for training and 10000 for test) of 28 × 28 pixels. In order to compare the feature extraction performance, we use the following topologies for the two approaches: 28 × 28 × 1 − 64c5 − Xc5 − p5 for the SCAE and 28 × 28 × 1 − 64c5 − p2 − Xc5 − p2 for the SNN, i.e. two convolutional layers of 64 maps and X maps respectively. Each uses 5 × 5 kernels followed by a max-pooling layer. The reason for the different pooling mechanisms is explained in Section 3.3. We explore the impact of the number of features X on the classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CNN training</head><p>The CNN is modeled in TensorFlow/Keras and trained with Adadelta <ref type="bibr" target="#b40">[39]</ref> gradient-based algorithm for 100 epochs with a learning rate of 1.0. Since the goal is to estimate the maximum accuracy we can expect from each topology, the CNN is trained with the labeled training set by using 10 neurons with a Softmax activation function on top of the last pooling layer. This network is noted as CNN+MLP in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SCAE training</head><p>The SCAE is also modeled in TensorFlow/Keras and trained using Adadelta <ref type="bibr" target="#b40">[39]</ref> gradient-based algorithm for 100 epochs with a learning rate of 1.0. However, no label is used in the training process, as the goal of the SCAE is to reconstruct the input in the output. The complete SCAE topology is 28×28×1−64c5−Xc5−p5− u5 − 64d5 − 1d5, where u stands for up-sampling and d stands for deconvolution (or transposed convolution) layers. The complete architecture is thus symetric. We add to every convolution and deconvoltion layer a weight constraint of type L2, and we add to the second convolution layer that produces the features an activity constraint of type L1. The weights and activity regularisation rates are set to 10 −4 . Therefore, the objective function of the SCAE takes in account both the image reconstruction and the sparsity constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SNN training</head><p>The SNN is modeled in SpykeTorch <ref type="bibr" target="#b27">[27]</ref>, an open-source simulator of convolutional SNNs based on PyTorch <ref type="bibr" target="#b33">[32]</ref>. The SNN is trained with STDP layer by layer, with a different pooling mechanism than the CNN and SCAE. Except for the number of feature maps and kernel sizes, we kept the same hyper-parameters as the original implementation of <ref type="bibr" target="#b17">[17]</ref> that can be found on <ref type="bibr" target="#b27">[27]</ref>. Hence, we used a pooling layer of 2 × 2 after each convolutional layer, with a padding of 1 before the second convolutional layer. The threshold of the neurons in the last convolutional layer were set to be infinite so that their final potentials can be measured <ref type="bibr" target="#b17">[17]</ref>. Finally, the global pooling neurons compute the maximum potential at their corresponding receptive field and produce the features that will be used as input for the SOM. Our experimental study showed that the added padding and the pooling mechanism proposed in <ref type="bibr" target="#b27">[27]</ref> performs better than the one used in the CNN and SCAE (i.e. no pooling and one polling layer), with a gain of 1.43% on the maximum achievable accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head><p>The SOM training hyper-parameters were found with a grid search: i = 1.0, f = 0.01, η i = 10.0, η f = 0.01, α = 1.0 and the number of epochs is 10. First, <ref type="figure" target="#fig_0">figure 1</ref> shows the impact of the number of feature maps in the second convolutional layer, using 256 neurons in the SOM and 10% of labels. We deliberately use a large number of labels to avoid any bias due to the labeling performance, and focus on the impact of the feature maps. The accuracy of the CNN+SOM and SCAE+SOM is increasing with respect to the number of feature maps, reaching a maximum at 256 maps. Interestingly, the CNN+SOM performs better with 8 maps (97.56%) than with 16 (97.25%), 32 (97.00%), 64 (97.26%) or 128 (97.31%) maps. This is due to the tradeoff between additional information and additional noise induced by more feature maps according to the SOM classification. In fact, the CNN+MLP supervised baseline accuracy is increasing from 98.7% to 99% when the feature maps increase from 8 to 512. This observation is more pronounced when we look at the SNN+SOM that reaches a maximum accuracy for 64 maps then drastically decreases with more feature maps. Following the approach of <ref type="bibr" target="#b17">[17]</ref>, we used a SNN+SVM supervised baseline and its accuracy increases from 97% to 98% when the feature maps increase from 64 to 512. It means that the increasing number of feature maps for the SNN produces noisy features that do not affect the supervised classification but do decrease the unsupervised classification accuracy, because the SOM prototypes overlap and become less descriminative. Thus, we choose 256 maps for the CNN and SCAE that produce a feature size of 4096, and 64 maps for the SNN that produces feature maps of size 3136. We remark that the SNN features size is different from the CNN/SCAE features size, which is due to the to the added padding and the different pooling mechanism as explained in Section 3.3. Second, with the above mentioned topologies, we investigate the impact of the SOM size with 10% of labels, from 16 to 10000 neurons. We see in <ref type="figure">Figure 2</ref> that the accuracy of the four systems is increasing with respect to the number of neurons. We notice that the SNN-SOM reaches the same accuracy as the SCAE+SOM starting from 1024 neurons. Nevertheless, for the next step of the study, it is important to keep the same number of neurons. Hence, we have chosen the number of neurons for which one of the SCAE+SOM or SNN+SOM reaches the maximum accuracy, which is equal to 256 neurons with respect to the SCAE+SOM accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOM neurons</head><p>Third, using 256 neurons for the SOM, we investigate the impact of the labeling subset size in terms of % of the training set. <ref type="figure" target="#fig_1">Figure 3</ref> shows that the accuracy increases when the labeled subset increases. Interestingly, the CNN+SOM and SCAE+SOM reach their maximum accuracy with only 1% of labeled data, while the SNN+SOM and SOM need approximately 5% of labeled data. Since the SCAE+SOM performs better than the SNN+SOM, we only need 1% of labeled data. It confirms the results obtained in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Finally, the comparative study of the four settings with the best topology of each, using 256 neurons for the SOM and 1% of labeled data for the neurons labeling is summarized in <ref type="figure">Figure 4</ref>. As expected, the SOM without feature extraction has the worst accuracy of 90.91% ± 0.15 and the CNN+SOM with  supervised feature extraction reaches the best accuracy of 97.94% ± 0.22. More interestingly, with fully unsupervised learning, the SCAE performs better than the SNN (+1.53%), with 96.9% ± 0.24 and 95.37% ± 0.58 respectively. <ref type="table" target="#tab_2">Table 1</ref> shows the gap between supervised and unsupervised methods for feature extraction and classification. Interestingly, we only lose about 1% of accuracy when going from CNN+MLP to CNN+SOM, and another 1% when going from CNN+SOM to SCAE+SOM. The gap is slightly higher when going from SCAE+SOM to SNN+SOM, which is about 1.5%. In return, the hardware cost decreases when using SOMs and SNNs, thanks to the brain-inspired computing paradigm (distributed and local). Indeed, we showed in <ref type="bibr" target="#b12">[13]</ref> that the SNN has a gain of approximately 50% in hardware resources and power consumption when implemented in dedicated FPGA and ASIC hardware. <ref type="table">Table 2</ref>. MNIST unsupervised learning with AE-based feature extraction: state of the art reported from <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Method Accuracy (%) AE + K-means <ref type="bibr" target="#b1">[2]</ref> 81.2 Sparse AE + K-means <ref type="bibr" target="#b31">[30]</ref> 82.7 Denoising AE + K-means <ref type="bibr" target="#b39">[38]</ref> 83.2 Variational Bayes AE + K-means <ref type="bibr" target="#b18">[18]</ref> 83.2 SWWAE + K-means <ref type="bibr" target="#b41">[40]</ref> 82.5 Adversarial AE <ref type="bibr" target="#b25">[25]</ref> 95.9 Sparse CAE + SOM [Our work] 96.9</p><p>Overall, the SCAE+SOM reaches the best accuracy of 96.9% ± 0.24 on MNIST classification with unsupervised learning. As shown in <ref type="table">Table 2</ref>, we achieved state of the art accuracy compared to similar works that followed an AE-based approach. The sparsity constraints of the SCAE through the weights and activities regularization significantly improved the SOM classification accuracy. Indeed, without these constraints, the CAE+SOM with the same configuration achieves an accuracy of 94.9% ± 0.24, which means a loss of −2%.</p><p>A similar study was conducted in <ref type="bibr" target="#b8">[9]</ref>, but it was limited to one layer SCAE and SNN, and a supervised SVM was used for classification. The authors concluded that the SCAE reaches a better classification accuracy. Our study extands their finding to multiple convolutional layers by using unsupervised learning for both feature extraction and classification. Nevertheless, the SNN+SOM remains attractive due to the hardware-efficient computation of spiking neurons <ref type="bibr" target="#b12">[13]</ref> associated to the cellular neuromorphic architecture of the SOM <ref type="bibr" target="#b34">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and further works</head><p>In the context of unsupervised learning, we conducted a comparative study for unsupervised feature extraction, and concluded that the SCAE+SOM achieves a better accuracy thanks to the sparsity constraints that were applied to the SCAE through weights and activities regularization. However, the SNN+SOM remains interesting due to the hardware efficiency of spiking neurons. We achieved state of the art performance on MNIST unsupervised classification, using post-labeled unsupervised learning with the SOM. The future works will focus on using the feature extraction on more complex datasets to improve the accuracy of a multimodal unsupervised learning mechanism <ref type="bibr" target="#b16">[16]</ref> based on SOMs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FeatureFig. 1 .</head><label>1</label><figDesc>SOM classification accuracy using CNN, SCAE and SNN feature extraction vs. number of feature maps with 256 SOM neurons and 10% of labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>SOM classification accuracy using CNN, SCAE and SNN feature extraction vs. % of labeled data from the training subset for the neurons labeling with the optimal topologies and 256 SOM neurons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>SOM classification accuracy using CNN, SCAE and SNN feature extraction vs. number of SOM neurons with the optimal topologies and 10% of labels.</figDesc><table><row><cell></cell><cell>SOM</cell><cell cols="2">SNN+SOM</cell><cell>SCAE+SOM</cell><cell>CNN+SOM</cell></row><row><cell></cell><cell>100.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Classification accuracy (%)</cell><cell>70.00 80.00 90.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>50</cell><cell>100</cell><cell>500</cell><cell>1000</cell><cell>5000 10000</cell></row><row><cell>Fig. 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>SOM classification accuracy using CNN, SCAE and SNN feature extraction vs. summary of the comparative study with the optimal topologies, 256 SOM neurons and 1% of labels.</figDesc><table><row><cell></cell><cell>100.00</cell></row><row><cell>Classification accuracy (%)</cell><cell>85.00 90.00 95.00</cell></row><row><cell></cell><cell>80.00</cell></row><row><cell></cell><cell>SOM</cell><cell>SNN+SOM SCAE+SOM CNN+SOM</cell></row><row><cell>Fig. 4.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparison of unsupervised feature extraction and classification techniques in terms of accuracy and hardware cost.</figDesc><table><row><cell>Feature extraction</cell><cell>Classification</cell><cell></cell><cell>Performance</cell></row><row><cell cols="5">Model Learning Model Learning Accuracy (%) Error (%) Hardware cost</cell></row><row><cell cols="2">CNN Supervised MLP Supervised</cell><cell>99.00</cell><cell>1.00</cell><cell>High</cell></row><row><cell cols="2">CNN Supervised SOM Unsupervised</cell><cell>97.94</cell><cell>2.06</cell><cell>Medium</cell></row><row><cell cols="2">SCAE Unsupervised SOM Unsupervised</cell><cell>96.90</cell><cell>3.10</cell><cell>Medium</cell></row><row><cell cols="2">SNN Unsupervised SOM Unsupervised</cell><cell>95.37</cell><cell>4.63</cell><cell>Low</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This material is based upon work supported by the French National Research Agency (ANR) and the Swiss National Science Foundation (SNSF) through SOMA project ANR-17-CE24-0036.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v27/baldi12a.html" />
		<title level="m">Proceedings of ICML Workshop on Unsupervised and Transfer Learning. Proceedings of Machine Learning Research</title>
		<editor>Silver, D.</editor>
		<meeting>ICML Workshop on Unsupervised and Transfer Learning. Machine Learning Research<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2012-07-02" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
	<note>Autoencoders, unsupervised learning, and deep architectures</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Neural Information Processing Systems. p. 153160. NIPS06</title>
		<meeting>the 19th International Conference on Neural Information Processing Systems. p. 153160. NIPS06<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparing the performance of traditional cluster analysis, self-organizing maps and fuzzy c-means method for strategic grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Budayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Birgonul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="11772" to="11781" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A practical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models, software and guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Charte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Charte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><surname>Del Jesus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.inffus.2017.12.007</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S1566253517307844" />
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="78" to="96" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond supervised learning: A computer vision perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Indian Institute of Science</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="199" />
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Loihi: A neuromorphic manycore processor with on-chip learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chinya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Choday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dimou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mathaikutty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="99" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of digit recognition using spike-timingdependent plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2015.00099</idno>
		<ptr target="https://doi.org/10.3389/fncom.2015.00099" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">99</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep unsupervised network for multimodal perception, representation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Droniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ivaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.robot.2014.11.005</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0921889014002474" />
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>emerging Spatial Competences: From Machine Perception to Sensorimotor Intelligence</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual feature learning with spike-timing-dependent plasticity: How far are we from traditional feature learning approaches?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Falez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tirilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Bilasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Devienne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boulet</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2019.04.016</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0031320319301621" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="418" to="429" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The spinnaker project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Plana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="652" to="665" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14571469</biblScope>
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9864" to="9873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Confronting machinelearning with neuroscience for neuromorphic architectures design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khacef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abderrahmane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2018.8489241</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2018.8489241" />
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-organizing neurons: toward brain-inspired unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khacef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barrientos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Upegui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/IJCNN.2019.8852098</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2019.8852098" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neuromorphic hardware as a self-organizing computing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khacef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Rougier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Upegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN 2018 Neuromorphic Hardware In Practice and Use workshop</title>
		<meeting><address><addrLine>Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Rio de Janeiro</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Brain-inspired self-organization with cellular neuromorphic computing for multimodal unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khacef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stdp-based spiking deep convolutional neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjtabesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2017.12.005</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0893608017302903" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kohlbrenner</surname></persName>
		</author>
		<title level="m">Pre-training cnns using convolutional autoencoders</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The self-organizing map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.58325</idno>
		<ptr target="https://doi.org/10.1109/5.58325" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1464" to="1480" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Engineering applications of the self-organizing map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Simula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Visa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kangas</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.537105</idno>
		<ptr target="https://doi.org/10.1109/5.537105" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1358" to="1384" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<title level="m">Self-Organizing Maps</title>
		<editor>T.S.</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>3rd edn.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Energy consumption minimization on lorawan sensor network by using an artificial neural network based application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kromes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Verdier</surname></persName>
		</author>
		<idno type="DOI">10.1109/SAS.2019.8705992</idno>
		<ptr target="https://doi.org/10.1109/SAS.2019.8705992" />
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors Applications Symposium (SAS). pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<title level="m">Adversarial autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked convolutional autoencoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th International Conference on Artificial Neural Networks -Volume Part I</title>
		<meeting>the 21th International Conference on Artificial Neural Networks -Volume Part I<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">5259</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spyketorch: Efficient simulation of convolutional spiking neural networks with at most one spike per neuron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjtabesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nowzari-Dalini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">625</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<idno type="DOI">10.3389/fnins.2019.00625</idno>
		<ptr target="https://doi.org/10.3389/fnins.2019.00625" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Intelligent detection of driver behavior changes for effective coordination between autonomous and human driven vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nallaperuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IECON 2018 -44th Annual Conference of the IEEE Industrial Electronics Society</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3120" to="3125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An empirical analysis of different sparse penalties for autoencoder in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenge</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2015.7280568</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2015.7280568" />
	</analytic>
	<monogr>
		<title level="m">2015 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf" />
	</analytic>
	<monogr>
		<title level="m">Lecture notes CS294A. Stanford University</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alché-Buc, F., Fox, E., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A distributed cellular approach of large scale SOM models for hardware implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khacef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miramond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Image Processing and Signals</title>
		<meeting><address><addrLine>Sophia-Antipolis, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic self-organising map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rougier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boniface</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2010.06.034</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2010.06.034" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing, Elsevier</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1840" to="1847" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning Internal Representations by Error Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page">673695</biblScope>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Machine learning to support social media empowered patients in cancer care and cancer treatment decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K B</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Bandaragoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adikari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Iddamalgoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Lawrentschuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Persad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Osipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An fpga distributed implementation model for embedded som with on-line learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>De Abreu De Sousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Del-Moral-Hernandez</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2017.7966351</idno>
		<ptr target="https://doi.org/10.1109/IJCNN.2017.7966351" />
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">33713408</biblScope>
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Stacked what-where auto-encoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
