<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PPFNet: Global Context Aware Local Features for Robust 3D Point Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
							<email>haowen.deng@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universitat München</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Siemens AG</orgName>
								<address>
									<settlement>München</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
							<email>tolga.birdal@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universitat München</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Siemens AG</orgName>
								<address>
									<settlement>München</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
							<email>slobodan.ilic@siemens.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universitat München</orgName>
								<address>
									<settlement>Germany</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Siemens AG</orgName>
								<address>
									<settlement>München</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PPFNet: Global Context Aware Local Features for Robust 3D Point Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present PPFNet -Point Pair Feature NETwork for deeply learning a globally informed 3D local feature descriptor to find correspondences in unorganized point clouds. PPFNet learns local descriptors on pure geometry and is highly aware of the global context, an important cue in deep learning. Our 3D representation is computed as a collection of point-pair-features combined with the points and normals within a local vicinity. Our permutation invariant network design is inspired by PointNet and sets PPFNet to be ordering-free. As opposed to voxelization, our method is able to consume raw point clouds to exploit the full sparsity. PPFNet uses a novel N-tuple loss and architecture injecting the global information naturally into the local descriptor. It shows that context awareness also boosts the local feature representation. Qualitative and quantitative evaluations of our network suggest increased recall, improved robustness and invariance as well as a vital step in the 3D descriptor extraction performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Local description of 3D geometry plays a key role in 3D vision as it precedes fundamental tasks such as correspondence estimation, matching, registration, object detection or shape retrieval. Such wide application makes the local features amenable for use in robotics <ref type="bibr" target="#b6">[7]</ref>, navigation (SLAM) <ref type="bibr" target="#b38">[37]</ref> and scene reconstruction for creation of VR contents and digitalization. Developing such a generalpurpose tool motivated scholars to hand-craft their 3D feature descriptors/signatures for decades <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b36">35]</ref>. Unfortunately, we now notice that this quest has not been very fruitful in generating the desired repeatable and discriminative local descriptors for 3D point cloud data, especially when the input is partial or noisy <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b25">25]</ref>.</p><p>The recent trends carefully follow the paradigm shift to <ref type="bibr">Figure 1</ref>. PPFNet generates repeatable, discriminative descriptors and can discover the correspondences simultaneously given a pair of fragments. Point sets are colored by a low dimensional embedding of the local feature for visualization. 3D data and the illustrative image are taken from 7-scenes dataset <ref type="bibr" target="#b40">[39]</ref>.</p><p>deep neural networks, but the latest works either base the representation on a hand-crafted input encoding <ref type="bibr" target="#b24">[24]</ref> or try to naively extend the networks from 2D domain to 3D <ref type="bibr" target="#b50">[49]</ref>. Both approaches are sub-optimal, as they do not address an end-to-end learning of the raw data, point sets.</p><p>In this paper, we present PPFNet network for deep learning of fast and discriminative 3D local patch descriptor with increased tolerance to rotations. To satisfy its desirable properties, we first represent the local geometry with an augmented set of simple geometric relationships: points, normals and point pair features (PPF) <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b9">9]</ref>. We then design a novel loss function, which we term as N-tuple loss, to simultaneously embed multiple matching and nonmatching pairs into a Euclidean domain. Our loss resembles the contrastive loss <ref type="bibr" target="#b15">[15]</ref>, but instead of pairs, we consider an N-combination of the input points within two scene fragments to boost the separability. Thanks to this many-tomany loss function, we are able to inject the global context into the learning, i.e. PPFNet is aware of the other local features when establishing correspondence for a single one. Also because of such parallel processing, PPFNet is very fast in inference. Finally, we combine all these contributions in a new pipeline, which trains our network from correspondences in 3D fragment pairs. PPFNet extends Point-Net <ref type="bibr" target="#b32">[31]</ref> and thereby is natural for point clouds and neutral to permutations. <ref type="figure">Fig. 1</ref> visualizes our features and illustrates the robust matching we can perform.</p><p>Our extensive evaluations show that PPFNet achieves the state of the art performance in accuracy, speed, robustness to point density and tolerance to changes in 3D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Hand-crafted 3D Feature Descriptors Similar to its 2D counterpart, extracting meaningful and robust local descriptors from 3D data kept the 3D computer vision researchers busy for a long period of time <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b14">14]</ref>. Unfortunately, contrary to 2D, the repeatability and distinctiveness of 3D features were found to be way below expectations <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b25">25]</ref>. Many of those approaches try to discover a local reference frame (LRF), which is by its simplest definition, non-unique <ref type="bibr" target="#b13">[13]</ref>. This shifts the attention to LRF-free methods such as the rotation invariant point pair features (PPF) to be used as basis for creating powerful descriptors like PPFH <ref type="bibr" target="#b37">[36]</ref> and FPFH <ref type="bibr" target="#b36">[35]</ref>. PPFs are also made semi-global to perform reasonably well under difficult scenarios, such as clutter and occlusions <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b2">3]</ref>. Thus, they have been applied in many problems to estimate 3D poses or retrieve and recognize objects <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b45">44]</ref>. Thanks to the simplicity and invariance properties of PPFs, along with the raw points and normals, we use it in PPFNet to describe the local geometry and learn a strong local descriptor.</p><p>Learned 3D Feature Descriptors With the advent of deep learning, several problems like 3D retrieval, recognition, segmentation and descriptor learning have been addressed using 3D data <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b41">40]</ref>. A majority of them operate on depth images <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b46">45]</ref>, while just a handful of works target point clouds directly. As we address the problem of 3D descriptor learning on point sets, we opt to review the data representations regardless the application and then consider learning local descriptors for point cloud matching.</p><p>There are many ways to represent sparse unstructured 3D data. Early works exploited the apparent dense voxels <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b33">32]</ref>, usually in form of a binary-occupancy grid. This idea is quickly extended to more informative encoding such as TDF <ref type="bibr" target="#b42">[41]</ref>, TSDF <ref type="bibr" target="#b50">[49]</ref> (Truncated Signed Distance Field), multi-label occupancy <ref type="bibr" target="#b47">[46]</ref> and other different ones <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b49">48]</ref>. Since mainly used in the context of 3D retrieval, entire 3D objects were represented with small voxel grids 30 3 limited by the maximal size of 3D convolutions kernels <ref type="bibr" target="#b31">[30]</ref>. These representations have also been used for describing the local neighbours in the context of 3D descriptor learning. One such contemporary work is 3DMatch <ref type="bibr" target="#b50">[49]</ref>. It is based on a robust volumetric TSDF encoding with a contrastive loss to learn correspondences. Albeit straightforward, 3DMatch ignores the raw nature of the input: sparsity and unstructured-ness. It uses dense local grids and 3D CNNs to learn the descriptor and thus can fall short in training/testing performance and recall.</p><p>A parallel track of works follow a view based scheme <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b20">20]</ref>, where the sub-spaces of 3D information in form of projections or depth map are learned with well studied 2D networks. Promising potential, these methods do not cover for sparse point sets. Another spectrum of research exploits graph networks <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b8">8]</ref> also to represent point sets <ref type="bibr" target="#b35">[34]</ref>. This new direction prospers in other domains but graph neural networks are not really suited to point clouds, as they require edges, not naturally arising from point sets. Khoury et. al. <ref type="bibr" target="#b24">[24]</ref> try to overcome the local representation problem by a hand-crafted approach and use a deep-network only for a dimensionality reduction. Their algorithm also computes the non-unique LRF and taking such a path deviates from the efforts of end-to-end learning.</p><p>The low hanging vital step is taken by PointNet <ref type="bibr" target="#b32">[31]</ref>, a network designed for raw 3D point input. PointNet demonstrated that neural networks can be designed in a permutation invariant manner to learn segmentation, classification or keypoint extraction. It is then extended to PointNet++ to better handle the variations in point density <ref type="bibr" target="#b34">[33]</ref>. However, this work in its original form, cannot tackle the problem of local geometry description as we successfully do.</p><p>As we will describe, PPFNet, trained on PPFs, points and normals of local patches, boosts the notion of global context in the semi-local features of PointNet by optimizing combinatorial matching loss between multitudes of patches, resulting in powerful features, outperforming the prior art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>Motivation Before explaining our local descriptor, let us map out our setting. Consider two 3D point sets X ∈ R n×3 and Y ∈ R n×3 . Let x i and y i denote the coordinates of the i th points in the two sets, respectively. Momentarily, we assume that for each x i , there exists a corresponding y i , a bijective map. Then following <ref type="bibr" target="#b30">[29]</ref> and assuming rigidity, the two sets are related by a correspondence and pose (motion), represented by a permutation matrix P ∈ P n and a rigid transformation T = {R ∈ SO(3), t ∈ R 3 }, respectively. Then the L 2 error of point set registration reads: </p><formula xml:id="formula_0">d(X, Y|R, t, P) = 1 n n i=1 x i − Ry i(P) − t 2 (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat Features</head><p>Patch Description - <ref type="figure">Figure 2</ref>. PPFNET, our inference network, consists of multiple PointNets, each responsible for a local patch. To capture the global context across all local patches, we use a max-pooling aggregation and fusing the output back into the local description. This way we are able to produce stronger and more discriminative local representations. where x i and y i(P) are matched under P. We assume X and Y are of equal cardinality (|X| = |Y| = n). In form of homogenized matrices, the following is equivalent:</p><formula xml:id="formula_1">d(X, Y|T, P) = 1 n X − PYT T 2<label>(2)</label></formula><p>Two sets are ideally matching if d(X, Y|T, P) ≈ 0. This view suggests that, to learn an effective representation, one should preserve a similar distance in the embedded space:</p><formula xml:id="formula_2">d f (X, Y|T, P) = 1 n f (X) − f (PYT T ) 2<label>(3)</label></formula><p>and d f (X, Y|T, P) ≈ 0 also holds for matching points sets under any action of (T, P). Thus, for invariance, it is desirable to have: f (Y) ≈ f (PYT T ). Ideally we would like to learn f being invariant to permutations P and as intolerant as possible to rigid transformations T. Therefore, in this paper we choose to use a minimally handcrafted point set to deeply learn the representation. This motivates us to exploit PointNet architecture <ref type="bibr" target="#b32">[31]</ref> which intrinsically accounts for unordered sets and consumes sparse input. To boost the tolerance to transformations, we will benefit from point-pairfeatures, the true invariants under Euclidean isometry.</p><p>Point Pair Features (PPF) Point pair features are antisymmetric 4D descriptors, describing the surface of a pair of oriented 3D points x 1 and x 2 , constructed as:</p><formula xml:id="formula_3">ψ 12 = ( d 2 , ∠(n 1 , d), ∠(n 2 , d), ∠(n 1 , n 2 )) (4)</formula><p>where d denotes the difference vector between points, n 1 and n 2 are the surface normals at x 1 and x 2 . · is the Euclidean distance and ∠ is the angle operator computed in a numerically robust manner as in <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_4">∠(v 1 , v 2 ) = atan2 v 1 × v 2 , v 1 · v 2 (5) ∠(v 1 , v 2 )</formula><p>is guaranteed to lie in the range [0, π). By construction, this feature is invariant under Euclidean transformations and reflections as the distances and angles are preserved between every pair of points.</p><p>PointNet PointNet <ref type="bibr" target="#b32">[31]</ref> is an inspiring pioneer addressing the issue of consuming point clouds within a network architecture. It is composed of stacking independent MLPs anchored on points up until the last layers where a high dimensional descriptor is synthesized. This descriptor is weak and used in max-pooling in order to aggregate to a global information, which is then fed into task specific losses. Use of the max-pooling function makes the network inconsiderate of the input ordering and that way extends notions of deep learning to point sets. It showed potential on tasks like 3D model classification and segmentation. Yet, local features of PointNet are only suitable for the tasks it targets and are not generic. Moreover, the spatial transformer layer employed can bring only marginal improvement over the basic architectures. It is one aspect of PPFNet to successfully cure these drawbacks for the task of 3D matching. Note, in our work, we use the vanilla version of PointNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PPFNet</head><p>Overview We will begin by explaining our input preparation to compute a set describing the local geometry of a 3D point cloud. We then elaborate on PPFNet architecture, which is designed to process such data with merit. Finally, we explain our training method with a new loss function to solve the combinatorial correspondence problem in a global manner. In the end, the output of our network is a local descriptor per each sample point as shown in <ref type="figure">Fig. 2</ref> Encoding of Local Geometry Given a reference point lying on a point cloud x r ∈ X, we define a local region Ω ⊂ X and collect a set of points {m i } ∈ Ω in this local vicinity. We also compute the normals of the point set <ref type="bibr" target="#b19">[19]</ref>. The associated local reference frame <ref type="bibr" target="#b43">[42]</ref> then aligns the patches with canonical axes. Altogether, the oriented {x r ∪ {x i }} represent a local geometry, which we term a local patch. We then pair each neighboring point i with the reference r and compute the PPFs. Note that complexity-wise, this is indifferent than using the points themselves, as we omit the quadratic pairing thanks to fixation of central reference point x r . As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, our final local geometry description and input to PPFNet is a combined set of points normals and PPFs:</p><formula xml:id="formula_5">. = 0 … 0 … , 0 , , 0 , 1 , , 1 … , , ,</formula><formula xml:id="formula_6">F r = {x r , n r , x i , · · · , n i , · · · , ψ ri , · · · } (6)</formula><p>Network architecture The overall architecture of PPFNet is shown in <ref type="figure">Fig. 2</ref>   To this end, the state of the art seems to adopt two loss functions: contrastive <ref type="bibr" target="#b50">[49]</ref> and triplet <ref type="bibr" target="#b24">[24]</ref>, which try to consider pairs and triplets respectively. Yet, a fragment consists of more than 3 patches and in that case the widely followed practice trains networks by randomly retrieving 2/3-tuples of patches from the dataset. However, networks trained in such manner only learn to differentiate maximum 3 patches, preventing them from uncovering the true matching, which is combinatorial in the patch count.</p><p>Generalizing these losses to N-patches, we propose Ntuple loss, an N -to-N contrastive loss, to correctly learn to solve this combinatorial problem by catering for the manyto-many relations as depicted in <ref type="figure" target="#fig_3">Fig. 4</ref>. Given the ground truth transformation T, N-tuple loss operates by constructing a correspondence matrix M ∈ R N ×N on the points of the aligned fragments. M = (m ij ) where:</p><formula xml:id="formula_7">m ij = 1( x i − Ty j 2 &lt; τ )<label>(7)</label></formula><p>1 is an indicator function. Likewise, we compute a feature-</p><formula xml:id="formula_8">space distance matrix D ∈ R N ×N and D = (d ij ) where d ij = f (x i ) − f (y j ) 2<label>(8)</label></formula><p>The N-tuple loss then functions on the two distance matrices solving the correspondence problem. For simplisity of expression, we define an operation * (·) to sum up all the elements in a matrix. N-tuple loss can be written as:</p><formula xml:id="formula_9">L = * M • D M 2 2 + α max(θ − (1 − M) • D, 0) N 2 − M 2 2<label>(9)</label></formula><p>Here • stands for Hadamard Product -element-wise multiplication. α is a hyper-parameter balancing the weight between matching and non-matching pairs and θ is the lowerbound on the expected distance between non-correspondent pairs. We train PPFNet via N-tuple loss, as shown in Based on these features a feature distance matrix is computed for all the patch pairs. Meanwhile, a distance matrix of local patches is formed based on the ground-truth rigid pose between the fragments. By binarizing the distance matrix, we get a correspondence matrix to indicate all the matching and non-matching relationships between patches. N-tuple loss is then calculated by coupling the feature distance matrix and correspondence matrix to guide the PPFNet to find an optimal feature space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Setup Our input encoding uses a 17-point neighborhood to compute the normals for the entire scene, using the well accepted plane fitting <ref type="bibr" target="#b19">[19]</ref>. For each fragment, we anchor 2048 sample points distributed with spatial uniformity. These sample points act as keypoints and within their 30cm vicinity, they form the patch, from which we compute the local PPF encoding. Similarly, we down-sample the points within each patch to 1024 to facilitate the training as well as to increase the robustness of features to various point density and missing part. For occasional patches with insufficient points in the defined neighborhood, we randomly repeat points to ensure identical patch size. PPFNet extracts compact descriptors of dimension 64. PPFNet is implemented in the popular Tensorflow <ref type="bibr" target="#b0">[1]</ref>. The initialization uses random weights and ADAM <ref type="bibr" target="#b26">[26]</ref> optimizer minimizes the loss. Our network operates simultaneously on all 2048 patches. Learning rate is set at 0.001 and exponentially decayed after every 10 epochs un-til 0.00001. Due to the hardware constraints, we use a batch size of 2 fragment pairs per iteration, containing 8192 local patches from 4 fragments already. This generates 2 × 2048 2 combinations for the network per batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Datasets</head><p>We concentrate on real sets rather than synthetic ones and therefore our evaluations are against the diverse 3DMatch RGBD benchmark <ref type="bibr" target="#b50">[49]</ref>, in which 62 different real-world scenes retrieved from the pool of datasets Analysis-by-Synthesis <ref type="bibr" target="#b44">[43]</ref>, 7-Scenes <ref type="bibr" target="#b40">[39]</ref>, SUN3D <ref type="bibr" target="#b48">[47]</ref>, RGB-D Scenes v.2 <ref type="bibr" target="#b28">[28]</ref> and Halber et <ref type="bibr" target="#b16">[16]</ref>. This collection is split into 2 subsets, 54 for training and validation, 8 for testing. The dataset typically includes indoor scenes like living rooms, offices, bedrooms, tabletops, and restrooms. See <ref type="bibr" target="#b50">[49]</ref> for details. As our input consists of only point geometry, we solely use the fragment reconstructions captured by Kinect sensor and not the color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can PPFNet outperform the baselines on real data?</head><p>We evaluate our method against hand-crafted baselines of  <ref type="figure">Figure 6</ref>. Evaluating PPFNet on real datasets: (a) Our method consistently outperforms the state-of-the-art on matching task (no RANSAC is used) in terms of recall. (b) Thanks to its careful design, PPFNet clearly yields the highest robustness to change in the sparsity of the input, even when only 6.25% of the input data is used. (c, d) Assessing different elements of the input on training and validation sets, respectively. Note that combining cues of global information and point pair features help the network to achieve the top results.</p><p>Spin Images <ref type="bibr" target="#b22">[22]</ref>, SHOT <ref type="bibr" target="#b39">[38]</ref>, FPFH <ref type="bibr" target="#b36">[35]</ref>, USC <ref type="bibr" target="#b43">[42]</ref>, as well as 3DMatch <ref type="bibr" target="#b50">[49]</ref>, the state of the art deep learning based 3D local feature descriptor, the vanilla PointNet <ref type="bibr" target="#b32">[31]</ref> and CGF <ref type="bibr" target="#b24">[24]</ref>, a hybrid hand-crafted and deep descriptor designed for compactness. To set the experiments more fair, we also show a version of 3DMatch, where we use 2048 local patches per fragment instead of 5K, the same as in our method, denoted as 3DMatch-2K. We use the provided pretrained weights of CGF <ref type="bibr" target="#b24">[24]</ref>. We keep the local patch size same for all methods. Our evaluation data consists of fragments from 7-scenes <ref type="bibr" target="#b40">[39]</ref> and SUN3D <ref type="bibr" target="#b48">[47]</ref> datasets. We begin by showing comparisons without applying RANSAC to prune the false matches. We believe that this can show the true quality of the correspondence estimator. Inspired by <ref type="bibr" target="#b24">[24]</ref>, we accredit recall as a more effective measure for this experiment, as the precision can always be improved by better corresponding pruning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref>. Our evaluation metric directly computes the recall by averaging the number of matched fragments across the datasets:</p><formula xml:id="formula_10">R = 1 M M s=1 1 1 |Ω| (i,j∈Ω) 1 (x i − Ty j ) &lt; τ 1 &gt; τ 2 (10)</formula><p>where M is the number of ground truth matching fragment pairs, having at least 30% overlap with each other under ground-truth transformation T and τ 1 = 10cm. (i, j) denotes an element of the found correspondence set Ω. x and y respectively come from the first and second fragment under matching. The inlier ratio is set as τ 2 = 0.05. As seen from Tab. 1, PPFNet outperforms all the hand crafted counterparts in mean recall. It also shows consistent advantage over 3DMatch-2K, using an equal amount of patches. Finally and remarkably, we are able to show ∼ 2.7% improvement on mean recall over the original 3DMatch, using only ∼ 40% of the keypoints for matching. The performance boost from 3DMatch-2K to 3DMatch also indicates that having more keypoints is advantageous for matching.</p><p>Our method expectedly outperforms both vanilla PointNet and CGF by 15%. We show in Tab. 2 that adding more samples brings benefit, but only up to a certain level (&lt; 5K). For PPFNet, adding more samples also increases the global context and thus, following the advent in hardware, we have the potential to further widen the performance gap over 3DMatch, by simply using more local patches. To show that we do not cherry-pick τ 2 but get consistent gains, we also plot the recall computed with the same metric for different inlier ratios in <ref type="figure" target="#fig_2">Fig. 6(a)</ref>. There, for the practical choices of τ 2 , PPFNet persistently remains above all others. Application to geometric registration Similar to <ref type="bibr" target="#b50">[49]</ref>, we now use PPFNet in a broader context of transformation estimation. To do so, we plug all descriptors into the well established RANSAC based matching pipeline, in which the transformation between fragments is estimated by running a maximum of 50,000 RANSAC iterations on the initial correspondence set. We then transform the source cloud to the target by estimated 3D pose and compute the pointto-point error. This is a well established error metric <ref type="bibr" target="#b50">[49]</ref>. Tab. 3 tabulates the results on the real datasets. Overall, PPFNet is again the top performer, while showing higher recall on a majority of the scenes and on the average. It is noteworthy that we always use 2048 patches, while allowing 3DMatch to use its original setting, 5K. Even so, we could get better recall on more than half of the scenes. When we feed 3DMatch 2048 patches, to be on par with our sampling level, PPFNet dominates performance-wise on most scenes with higher average accuracy.</p><p>Robustness to point density Changes in point density, a.k.a. sparsity, is an important concern for point clouds, as it <ref type="table">Table 3</ref>. Our evaluations on the 3D-match benchmark after RANSAC. Kitchen is from 7-scenes <ref type="bibr" target="#b40">[39]</ref> and the rest from SUN3D <ref type="bibr" target="#b48">[47]</ref>.</p><p>Spin Images <ref type="bibr" target="#b22">[22]</ref> SHOT <ref type="bibr" target="#b39">[38]</ref> FPFH <ref type="bibr" target="#b36">[35]</ref> USC <ref type="bibr" target="#b43">[42]</ref> PointNet <ref type="bibr" target="#b32">[31]</ref> CGF <ref type="bibr" target="#b24">[24]</ref> 3DMatch <ref type="bibr" target="#b50">[49]</ref>   can change with sensor resolution or distance for 3D scanners. This motivates us to evaluate our algorithm against others in varying sparsity levels. We gradually decrease point density on the evaluation data and record the accuracy. <ref type="figure" target="#fig_2">Fig. 6(b)</ref> shows the significant advantage of PPFNet, especially under severe loss of density (only 6.5% of points kept). Such robustness is achieved due to the PointNet backend and the robust point pair features.</p><p>How fast is PPFNet? We found PPFNet to be lightning fast in inference and very quick in data preparation since we consume a very raw representation of data. Majority of our runtime is spent in the normal computation and this is done only once for the whole fragment. The PPF extraction is carried out within the neighborhoods of only 2048 sample points. Tab. 4 shows the average running times of different methods and ours on an NVIDIA TitanX Pascal GPU supported by an Intel Core i7 3.2GhZ 8 core CPU. Such dramatic speed-up in inference is enabled by the parallel-PointNet backend and our simultaneous correspondence estimation during inference for all patches. Currently, to prepare the input for the network, we only use CPU, leaving GPU idle for more work. This part can be easily implemented on GPU to gain even further speed boosts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study N-tuple loss</head><p>We train and test our network with 3 different losses: contrastive (pair) <ref type="bibr" target="#b15">[15]</ref>, triplet <ref type="bibr" target="#b18">[18]</ref> and our N-tuple loss on the same dataset with identical network configuration. Inter-distance distribution of correspondent pairs and non-correspondent pairs are recorded for the train/validation data respectively. Empirical results in <ref type="figure" target="#fig_6">Fig.  7</ref> show that the theoretical advantage of our loss immediately transfers to practice: Features learned by N-tuple are better separable, i.e. non-pairs are more distant in the embedding space and pairs enjoy a lower standard deviation.  N-tuples loss repels non-pairs further in comparison to contrastive and triplet losses because of its better knowledge of global correspondence relationships. Our N-tuple loss is general and thus we strongly encourage the application also to other domains such as pose estimation <ref type="bibr" target="#b46">[45]</ref>.  validation set as opposed to our baseline version of Vanilla PointNet • , which is free of global context and PPFs. Such significance indicates that global features aid discrimination and are valid cues also for local descriptors.</p><p>What does adding PPF bring? We now run a similar experiment and train two versions of our network, with/without incorporating PPF into the input. The contribution is tabulated in Tab. 5. There, a gain of 1% in training and 5% in validation is achieved, justifying that inclusion of PPF increases the discriminative power of the final features. While being a significant jump, this is not the only benefit of adding PPF. Note that our input representation is composed of 33% rotation-invariant and 66% variant representations. This is already advantageous to the state of the art, where rotation handling is completely left to the network to learn from data. We hypothesize that an input guidance of PPF would aid the network to be more tolerant to rigid transformations. To test this, we gradually rotate fragments around z-axis to 180 • with a step size of 30 • and then match the fragment to the non-rotated one. As we can observe from Tab. 6, with PPFs, the feature is more robust to rota-• Note that this doesn't 100% correspond to the original version, as we modified PointNet with task specific losses for our case. tion and the ratio in matching performance of two networks opens as rotation increases. In accordance, we also show a visualization of the descriptors at <ref type="figure">Fig. 9</ref> under small and large rotations. To assign each descriptor an RGB color, we use PCA projection from high dimensional feature space to 3D color space by learning a linear map <ref type="bibr" target="#b24">[24]</ref>. It is qualitatively apparent that PPF can strengthen the robustness towards rotations. All in all, with PPFs we gain both accuracy and robustness to rigid transformation, the best of seemingly contradicting worlds. It is noteworthy that using only PPF introduces full invariance besides the invariance to permutations and renders the task very difficult to learn for our current network. We leave this as a future challenge. A major limitation of PPFNet is quadratic memory footprint, limiting the number of used patches to 2K on our hardware. This is, for instance, why we cannot outperform 3DMatch on fragments of Home-2. With upcoming GPUs, we expect to reach beyond 5K, the point of saturation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented PPFNet, a new 3D descriptor tailored for point cloud input. By generalizing the contrastive loss to N-tuple loss to fully utilize available correspondence relatioships and retargeting the training pipeline, we have shown how to learn a globally aware 3D descriptor, which outperforms the state of the art not only in terms of recall but also speed. Features learned from our PPFNet is more capable of dealing with some challenging scenarios, as shown in <ref type="figure">Fig. 8</ref>. Furthermore, we have shown that designing our network suitable for set-input such as point pair features are advantageous in developing invariance properties.</p><p>Future work will target memory bottleneck and solving the more general rigid graph matching problem. A. Appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Further Architectural Details</head><p>Due to the constraint of GPU memory, we adopt a minimized version of vanilla PointNet in our implementation. <ref type="figure">Fig. A.2</ref> demonstrates a pipeline for processing one single patch and more details of our network.</p><p>The size of a local patch is n × d, where n = 1024 is number of points in the patch and d depends on the specific representation of local patch. For only point coordinates and normals, d = 6; for the one with extra PPF, d = 10.</p><p>A patch is first sent into a mini-PointNet with three layers, each has 32 nodes, and then a max pooling function aggregates all the information into a 32-dimensional local feature. After combining with the 32-dimensional global feature, it is further processed by a two-layer MLP, in which each layer has 64 nodes. The dimension of the final feature for the local patch is 64. <ref type="figure">Fig. 11</ref> demonstrates some qualitative fragment registration results in Section 5 in the paper, showing that learned features by PPFNet are able to cope with challenging point cloud matching problems under different situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Algorithmic Details</head><p>Sampling algorithm How we sample the point cloud down to 2048 keypoints (samples) plays an important role in learning. We try to be spatially as uniformly distributed as possible so that the samples are further apart and less dependent on one another. For this, we use the greedy algorithm given in Algorithm 1 inspired by <ref type="bibr" target="#b3">[4]</ref>. Note that the algorithm involves a search over the so-far-sampled cloud, which we speed up using a voxel-grid.</p><p>Normal computation To achieve speed-up in point pair feature calculation, we pre-compute the normals of the input fragment. To compute a normal, the tangent plane to each local neighborhood is approximated by the leastsquare fitting as proposed in <ref type="bibr" target="#b19">[19]</ref>. Computing the equation of the plane then boils down to a analysis of a covariance </p><formula xml:id="formula_11">C = 1 n n i=1 (x i −x)(x i −x) T<label>(11)</label></formula><p>wherex denotes the mean, or the center of the local patch. The equation of the plane is then computed from the eigenvectors of C. Due to the sign ambiguity in eigenvector analysis, the direction of the resulting normal is unknown. Thus, we use the convention where each surface normal is flipped towards the camera by ensuring the dot product between the viewpoint vector and surface normal be acute: −p · n &lt; π/2. target source registra�on result <ref type="figure">Figure 11</ref>. Qualitative registration results of 5 fragment pairs</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Simplistic encoding of a local patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(a )</head><label>)</label><figDesc>Pair Samples (b) Triplet SamplesIn this figure, patches connected are aware of each other, and the length of the line indicates the distance of patches in the feature space. Red lines connect similar pairs, they should be short. Green lines connect non-similar pairs, they should be long. In both Pair Samples and Triplet Samples, although all the connections satisfy the requirements already, but because of the constrained awareness field of each patch, there are still some non-similar patches which are close in the feature space and some similar patches distant from each other. But with our N-Tuple method, each patch is aware about its relationships with all the other patches, which guarantees all the similar patches to be close and non-similar patches to be distant.(c) N-Tuple Configuration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of N-tuple sampling in feature space. Green lines link similar pairs, which are coerced to keep close. Red lines connect non-similar pairs, pushed further apart. Without N-tuple loss, there remains to be some non-similar patches that are close in the feature space and some distant similar patches. Our novel Ntuple method pairs each patch with all the others guaranteeing that all the similar patches remain close and non-similar ones, distant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 ,Figure 5 .</head><label>55</label><figDesc>by drawing random pairs of fragments instead of patches. This also eases the preparation of training data. Overall training pipeline of PPFNet. Local patches are sampled from a pair of fragments respectively, and feed into PPFNet to get local features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>N-tuple Loss (c) lets the PPFNet better separate the matching vs non-matching pairs w.r.t. the traditional contrastive (a) and triplet (b) losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .PPFFigure 9 .</head><label>89</label><figDesc>Visualization of estimated transformations. Thanks to its robustness and understanding of global information, PPFNet can operate under challenging scenarios with confusing, repetitive structures as well as mostly planar scenes with less variation in geometry.(a) Original Fragment (b) Rotate 30 o around z axis (c) Rotate 60 o around z axis Without PPF With Inclusion of PPF makes the network more robust to rotational changes as shown, where the appearance across each row is expected to stay identical, for a fully invariant feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Pipeline for processing a single local patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Weights and gradients are shared across all PointNets during training. A max pooling layer then aggregates all the local features into a global one, summarizing the distinct local information to the global context of the whole fragment. This global feature is then concatenated to every local feature. A group of MLPs are used to further fuse the global and local features into the final global-context aware local descriptor.N-tuple loss Our goal is to use PPFNet to extract features for local patches, a process of mapping from a high dimensional non-linear data space into a low dimensional linear feature space. Distinctiveness of the resulting features are closely related to the separability in the embedded space.</figDesc><table /><note>. Our input consists of N local patches uniformly sampled from a fragment. Due to sparsity of point-style data representation and efficient GPU utilization of PointNet, PPFNet can absorb those N patches concurrently. The first module of PPFNet is a group of mini-PointNets, extracting features from local patches.Ideally, the proximity of neighboring patches in the data space should be preserved in the feature space.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Our evaluations on the 3DMatch benchmark before RANSAC. Kitchen is from 7-scenes<ref type="bibr" target="#b40">[39]</ref> and the rest from SUN3D<ref type="bibr" target="#b48">[47]</ref>.</figDesc><table><row><cell>ours)</cell></row></table><note>Spin Images [22] SHOT [38] FPFH [35] USC [42] PointNet [31] CGF [24] 3DMatch [49] 3DMatch-2K [49] PPFNet (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Recall of 3DMatch for different sample sizes.</figDesc><table><row><cell cols="2">Samples 128 256 512 1K 2K 5K 10K 20K 40K</cell></row><row><cell>Recall</cell><cell>0.24 0.32 0.40 0.47 0.51 0.59 0.59 0.56 0.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. recall prec. recall prec. recall prec. recall prec. recall prec. recall prec. recall prec.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3DMatch-2K</cell><cell cols="2">PPFNet</cell></row><row><cell cols="5">recall recall precRed Kitchen 0.27 prec. 0.49 0.21 0.44</cell><cell>0.36</cell><cell>0.52</cell><cell>0.52</cell><cell>0.60</cell><cell>0.76</cell><cell>0.60</cell><cell>0.72</cell><cell>0.54</cell><cell>0.85</cell><cell>0.72</cell><cell>0.80</cell><cell>0.54</cell><cell>0.90</cell><cell>0.66</cell></row><row><cell>Home 1</cell><cell>0.56</cell><cell>0.14</cell><cell>0.37</cell><cell>0.13</cell><cell>0.56</cell><cell>0.16</cell><cell>0.35</cell><cell>0.16</cell><cell>0.53</cell><cell>0.16</cell><cell>0.69</cell><cell>0.18</cell><cell>0.78</cell><cell>0.35</cell><cell>0.79</cell><cell>0.21</cell><cell>0.58</cell><cell>0.15</cell></row><row><cell>Home 2</cell><cell>0.35</cell><cell>0.10</cell><cell>0.30</cell><cell>0.11</cell><cell>0.43</cell><cell>0.13</cell><cell>0.47</cell><cell>0.24</cell><cell>0.42</cell><cell>0.13</cell><cell>0.46</cell><cell>0.12</cell><cell>0.61</cell><cell>0.29</cell><cell>0.52</cell><cell>0.14</cell><cell>0.57</cell><cell>0.16</cell></row><row><cell>Hotel 1</cell><cell>0.37</cell><cell>0.29</cell><cell>0.28</cell><cell>0.29</cell><cell>0.29</cell><cell>0.36</cell><cell>0.53</cell><cell>0.46</cell><cell>0.45</cell><cell>0.38</cell><cell>0.55</cell><cell>0.38</cell><cell>0.79</cell><cell>0.72</cell><cell>0.74</cell><cell>0.45</cell><cell>0.75</cell><cell>0.42</cell></row><row><cell>Hotel 2</cell><cell>0.33</cell><cell>0.12</cell><cell>0.24</cell><cell>0.11</cell><cell>0.36</cell><cell>0.14</cell><cell>0.20</cell><cell>0.17</cell><cell>0.31</cell><cell>0.18</cell><cell>0.49</cell><cell>0.15</cell><cell>0.59</cell><cell>0.41</cell><cell>0.60</cell><cell>0.22</cell><cell>0.68</cell><cell>0.22</cell></row><row><cell>Hotel 3</cell><cell>0.32</cell><cell>0.16</cell><cell>0.42</cell><cell>0.12</cell><cell>0.61</cell><cell>0.21</cell><cell>0.38</cell><cell>0.14</cell><cell>0.43</cell><cell>0.11</cell><cell>0.65</cell><cell>0.16</cell><cell>0.58</cell><cell>0.25</cell><cell>0.58</cell><cell>0.14</cell><cell>0.88</cell><cell>0.20</cell></row><row><cell cols="2">Study Room 0.21</cell><cell>0.07</cell><cell>0.14</cell><cell>0.07</cell><cell>0.31</cell><cell>0.11</cell><cell>0.46</cell><cell>0.17</cell><cell>0.48</cell><cell>0.16</cell><cell>0.48</cell><cell>0.16</cell><cell>0.63</cell><cell>0.27</cell><cell>0.57</cell><cell>0.17</cell><cell>0.68</cell><cell>0.16</cell></row><row><cell>MIT Lab</cell><cell>0.29</cell><cell>0.06</cell><cell>0.22</cell><cell>0.09</cell><cell>0.31</cell><cell>0.09</cell><cell>0.49</cell><cell>0.19</cell><cell>0.43</cell><cell>0.14</cell><cell>0.42</cell><cell>0.10</cell><cell>0.51</cell><cell>0.20</cell><cell>0.42</cell><cell>0.09</cell><cell>0.62</cell><cell>0.13</cell></row><row><cell>Average</cell><cell>0.34</cell><cell>0.18</cell><cell>0.27</cell><cell>0.17</cell><cell>0.40</cell><cell>0.21</cell><cell>0.43</cell><cell>0.27</cell><cell>0.48</cell><cell>0.23</cell><cell>0.56</cell><cell>0.23</cell><cell>0.67</cell><cell>0.40</cell><cell>0.63</cell><cell>0.24</cell><cell>0.71</cell><cell>0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Average per-patch runtime of different methods.</figDesc><table><row><cell></cell><cell cols="2">input preparation inference / patch</cell><cell>total</cell></row><row><cell cols="2">3DMatch 0.31ms on GPU</cell><cell>2.9ms on GPU</cell><cell>3.21ms</cell></row><row><cell>PPFNet</cell><cell>2.24ms on CPU</cell><cell>55µs on GPU</cell><cell>2.25ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Effect of different components in performance: Values depict the number of correct matches found to be 5% inlier ratio.</figDesc><table><row><cell>Method</cell><cell>Train</cell><cell>Validation</cell></row><row><cell>Without points and normals</cell><cell>0%</cell><cell>6%</cell></row><row><cell>Vanilla PointNet [31]</cell><cell>47%</cell><cell>41%</cell></row><row><cell>Without global context</cell><cell>48%</cell><cell>46%</cell></row><row><cell>Without PPF</cell><cell>65%</cell><cell>48%</cell></row><row><cell>All combined</cell><cell>67%</cell><cell>56%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>How useful is global context for local feature extraction? We argue that local features are dependent on the context. A corner belonging to a dining table should not share the similar local features of a picture frame hanging on the wall. A table is generally not supposed to be attached vertically on the wall. To assess the returns obtained from adding global context, we simply remove the global feature concatenation, keep the rest of the settings unaltered, and re-train and test on two subsets of pairs of fragments. Our results are shown in Tab. 5, where injecting global information into local features improves the matching by 18% in training and 7% in</figDesc><table><row><cell>No-colored Version</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Effect of point pair features in robustness to rotations.</figDesc><table><row><cell>z-rotation</cell><cell>0 •</cell><cell>30 •</cell><cell>60 •</cell><cell>90 •</cell><cell>120 • 150 • 180 •</cell></row><row><cell cols="6">with PPF 100.0% 53.3% 35.0% 20.0% 8.3% 5.0% 0.0%</cell></row><row><cell>w/o PPF</cell><cell cols="5">100.0% 38.3% 23.3% 11.7% 1.7% 0.0% 0.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Algorithm 1 Distance Constrained SamplingRequire: Source point cloud X, Relative threshold τ Ensure: Sampled point cloud S and its normals N</figDesc><table><row><cell>Compute normals for X</cell><cell></cell></row><row><cell>S ← []</cell><cell></cell></row><row><cell>N ← []</cell><cell></cell></row><row><cell>R d ← diameter(X)</cell><cell></cell></row><row><cell>for x ∈ X do</cell><cell></cell></row><row><cell>d min = min (t∈S) |x − t|</cell><cell></cell></row><row><cell>if (d min &gt; τ R d ) then</cell><cell></cell></row><row><cell>S ← S x</cell><cell></cell></row><row><cell>N ← N n(x)</cell><cell>sample the normal as well</cell></row><row><cell>end if</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell cols="2">matrix created from the nearest neighbors:</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Point pair features based object detection and pose estimation revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cad priors for accurate and flexible instance reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A point sampling algorithm for 3d matching of irregular geometries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Globallyoptimal inlier set maximisation for simultaneous camera pose and feature correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient globally optimal consensus maximisation with tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Purkait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2413" to="2421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Voting-based pose estimation for robotic assembly using a 3d sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1724" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="998" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d point cloud registration for localization using a deep neural network autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comprehensive performance evaluation of 3d local feature descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="89" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance evaluation of 3d local feature descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="178" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rotational projection statistics for 3d local surface description and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rops: A local feature descriptor for 3d rigid objects based on rotational projection statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications, Signal Processing, and their Applications (ICCSPA), 2013 1st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fine-to-coarse global registration of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Going further with point pair features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Surface reconstruction from unorganized points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duchamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stuetzle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>ACM</publisher>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning local shape descriptors with view-based convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<idno>abs/1706.04496</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2016 23rd International Conference on</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2670" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A performance evaluation of point pair features. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kiforenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Buch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for 3d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3050" to="3057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The 3d-3d registration problem revisited. In Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation, 2009. ICRA&apos;09. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Persistent Point Feature Histograms for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Intelligent Autonomous Systems (IAS-10)</title>
		<meeting>the 10th International Conference on Intelligent Autonomous Systems (IAS-10)<address><addrLine>Baden-Baden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Slam++: Simultaneous localisation and mapping at the level of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Salas-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1352" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Shot: Unique signatures of histograms for surface and texture description. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional-recursive deep learning for 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unique shape context for 3d data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM workshop on 3D object retrieval</title>
		<meeting>the ACM workshop on 3D object retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="57" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to navigate the energy landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="323" to="332" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Surflet-pairrelation histograms: a statistical 3d-shape representation for rapid classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hillenbrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hirzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Fourth International Conference on</title>
		<meeting>Fourth International Conference on</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="474" to="481" />
		</imprint>
	</monogr>
	<note>3-D Digital Imaging and Modeling</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Geometric features for voxel-based surface recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarotsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04249</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning local geometric descriptors from rgb-d reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
