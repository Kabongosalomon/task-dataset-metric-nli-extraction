<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Intermediate Representation for Monocular Vehicle Pose Estimation Scene Feature Maps Instance 3D Translation Scene with Improved Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengqiang</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
							<email>timcheng@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Intermediate Representation for Monocular Vehicle Pose Estimation Scene Feature Maps Instance 3D Translation Scene with Improved Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Input T Ego-Net E Instance x i . . . Instance Heatmaps φ g (x i ) ψ(x i ) 2D/3D Intermediate Representation . . . 1 https://github.com/Nicholasli1995/EgoNet Part of this work was done when the first author was an intern at Sense-Time Research Shanghai Autonomous Driving Group.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: A novel deep model, Ego-Net, is proposed for accurate vehicle orientation estimation in the camera coordinate system via extracting a set of explicit 2D/3D geometrical representations. A 3D vehicle detection system (upper branch) can enjoy improved orientation estimation accuracy by replacing its pose estimation module with Ego-Net (lower branch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a new learning-based framework to recover vehicle pose in SO(3) from a single RGB image. In contrast to previous works that map from local appearance to observation angles, we explore a progressive approach by extracting meaningful Intermediate Geometrical Representations (IGRs) to estimate egocentric vehicle orientation. This approach features a deep model that transforms perceived intensities to IGRs, which are mapped to a 3D representation encoding object orientation in the camera coordinate system. Core problems are what IGRs to use and how to learn them more effectively. We answer the former question by designing IGRs based on an interpolated cuboid that derives from primitive 3D annotation readily. The latter question motivates us to incorporate geometry knowledge with a new loss function based on a projective invariant. This loss function allows unlabeled data to be used in the training stage to improve representation learning. Without additional labels, our system outperforms previous monocular RGB-based methods for joint vehicle detection and pose estimation on the KITTI benchmark, achieving performance even comparable to stereo methods. Code and pre-trained models are available at this HTTPS URL 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>T</p><formula xml:id="formula_0">Ego-Net E Instance x i . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Heatmaps</head><formula xml:id="formula_1">φ g (x i ) ψ(x i ) 2D/3D Intermediate Representation .</formula><p>. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Feature Maps Instance 3D Translation</head><p>Scene with Improved Pose Estimation <ref type="figure">Figure 1</ref>: A novel deep model, Ego-Net, is proposed for accurate vehicle orientation estimation in the camera coordinate system via extracting a set of explicit 2D/3D geometrical representations. A 3D vehicle detection system (upper branch) can enjoy improved orientation estimation accuracy by replacing its pose estimation module with Ego-Net (lower branch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a new learning-based framework to recover vehicle pose in SO(3) from a single RGB image. In contrast to previous works that map from local appearance to observation angles, we explore a progressive approach by extracting meaningful Intermediate Geometrical Representations (IGRs) to estimate egocentric vehicle orientation. This approach features a deep model that transforms perceived intensities to IGRs, which are mapped to a 3D representation encoding object orientation in the camera coordinate system. Core problems are what IGRs to use and how to learn them more effectively. We answer the former question by designing IGRs based on an interpolated cuboid that derives from primitive 3D annotation readily. The latter question motivates us to incorporate geometry knowledge with a new loss function based on a projective invariant. This loss function allows unlabeled data to be used in the training stage to improve representation learning. Without additional labels, our system outperforms previous monocular RGB-based methods for joint vehicle detection and pose estimation on the KITTI benchmark, achieving performance even comparable to stereo methods. Code and pre-trained models are available at this HTTPS URL 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>"The usefulness of a representation depends upon how well suited it is to the purpose for which it is used".</p><p>-David Marr <ref type="bibr" target="#b42">[43]</ref> Understanding 3D properties of the surrounding world is critical for vision-based autonomous driving and traffic surveillance systems <ref type="bibr" target="#b14">[15]</ref>. Accurate 3D vehicle orientation estimation (VOE) can imply a driver's intent of travel direction, help predicting its position a moment later and help identifying anomalous behaviors. To pursue high accuracy, recent studies have exploited active sensors to obtain point clouds <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b46">47]</ref> or the use of stereo cameras <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b45">46]</ref>. The former enjoys the convenience of directly working with 3D data and the latter benefit from multi-view geometry. As downsides, LiDAR sensors cost more than commodity RGB cameras and stereo systems incur extra complexity.</p><p>Pushing the limit of a single-view RGB-based method is not only cost-effective, but can also complement other configurations through sensor fusion. However, estimating 3D parameters from a single RGB image is highly challenging due to the lack of depth information and other geometrical cues. Thanks to the strong representation learning power brought by recent advances in deep architectures, state-of-the-art (SOTA) RGB-based methods can regress 3D parameters from visual appearance in an end-to-end manner <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">52]</ref>. In this paradigm, paired images and 3D annotations are specified as inputs and learning targets respectively for supervising a deep <ref type="figure">Figure 2</ref>: Detailed architecture of Ego-Net. Feature maps are first computed with a fully convolution model H from a detected instance. Heatmaps representing the projection of a 3D cuboid are regressed and mapped to local coordinates with several strided convolution layers. The local coordinates are transformed to screen coordinates φ g (x i ) and mapped to a 3D cuboid representation ψ(x i ), whose orientation directly represent egocentric pose in the camera coordinate system. k=33 when q=2 as in Sec. 3.</p><formula xml:id="formula_2">Input x i 256×256×3 FCN H FCN C Heatmaps h(x i ) 64×64×k φ l (x i ) 2×k φ g (x i ) 2×k A i Lifter L d + + ψ(x i ) 3×(k-1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>model. No intermediate representation is designed in such</head><p>end-to-end solutions, which need a large amount of training pairs to approximate the highly non-linear mapping from the pixel space to 3D geometrical quantities.</p><p>In contrast to the aforementioned methods, we take a different approach and ask can a neural network extract explicit geometrical quantities from RGB images and use them for monocular vehicle pose estimation? Our inspiration is from the representational framework for vision <ref type="bibr" target="#b42">[43]</ref>, which first extracts geometrically meaningful 2 1 2 -D sketch from perceived intensities and converts them to 3D representations. We are also inspired by recent advances in two-stage 3D human pose estimation <ref type="bibr" target="#b35">[36]</ref>. Previous approaches <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b51">52]</ref> specify the start (pixels) and the end (3D pose) of such a process while ignoring the learning of any intermediate representation. Apart from the motivation from the framework <ref type="bibr" target="#b42">[43]</ref>, geometrical representations have two other benefits: (1) they are easier for human to comprehend than the dense feature maps when debugging safetycritical applications, (2) the mapping from them to 3D pose is easy to learn as a few layers would suffice in our experiments.</p><p>Towards this end, we propose a novel VOE approach featuring learning Intermediate Geometrical Representations (IGRs). We first explicitly define a set of IGRs based on existing ground truth without any extra manual annotation. We then design a novel deep model, Ego-Net, which first regresses IGRs and then maps them to the 3D space, achieving SOTA performance. Apart from the high accuracy, we propose a novel loss function for self-supervising the learning of IGRs based on prior knowledge of projective invariants. With a hybrid source of supervision, our model can benefit from unlabeled data for enhanced generalization. Our contributions can be summarized in the following:</p><p>• To our best knowledge, our proposed learning-based model, Ego-Net, is the first one which regresses intermediate geometrical representations for direct estimation of egocentric vehicle pose.</p><p>• Our novel loss function based on the preservation of projective invariants introduces self-supervision for the learning of such representations.</p><p>• Our approach does not require additional labels, and outperforms previous monocular RGB-based VOE methods. Our monocular system achieves performance comparable to some stereo methods for joint vehicle detection and pose estimation on KITTI.</p><p>• Ego-Net, a plug-and-play module, can be incorporated into any existing 3D object detection models for improved VOE. Equipped with Ego-Net, a 3D detection system can achieve high 3D location and orientation estimation performance simultaneously, while previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b3">4]</ref> usually fail to achieve both.</p><p>After discussing relevant works in Sec. 2, we detail the design of Ego-Net in Sec. 3. In Sec. 4 we demonstrate how unlabeled data can be utilized to learn IGRs with prior geometry knowledge. Sec. 5 presents experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Deep learning-based 3D object detection aims to learn a mapping from sensor input to instance attributes such as 3D orientation and 2D/3D location. Depending on the sensor modality, recent works can be categorized into LiDARbased approaches <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b43">44]</ref>, RGB-based methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref>, and hybrid ones <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b56">57]</ref>. Our method falls into the second category, which does not require expensive devices to acquire point clouds. Compared to stereo approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b10">11]</ref> that need two RGB cameras, our system only takes a single view input. For convenience in implementation, many previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51</ref>] employ a multi-task architecture composed of a backbone for feature extraction and 3D parameter estimation heads. Each head shares similar design and VOE is treated as a sub-task with equal importance as other tasks such as dimension estimation. In contrast, we present a dedicated module for this task which outperforms previous ones. Pose estimation in 3D object detection systems targets at recovering instance orientation. We focus on the works that use RGB images and classify them into learning-based methods and geometrical optimization-based ones.</p><p>Learning-based methods learn a function from pixels to orientation. Conventionally, visual features were extracted and fed to a model for discrete pose classification (DPC) or continuous regression. Early works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b58">59]</ref> utilized hand-crafted features <ref type="bibr" target="#b13">[14]</ref> and boosted trees for DPC. Recent works replace the feature extraction stage with predesigned deep models. ANN <ref type="bibr" target="#b63">[64]</ref> and DAVE <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref> classify instance feature maps extracted by CNN into discrete bins. To deal with images containing multiple instances, Fast-RCNN-like architectures were employed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> where region-of-interst (ROI) features were used to represent instance appearance and a classification head gives pose prediction. Deep3DBox <ref type="bibr" target="#b44">[45]</ref> proposed Multi-Bin loss for joint pose classification and residual regression. Wasserstein loss was promoted in <ref type="bibr" target="#b38">[39]</ref> for DPC. LSTM was employed in <ref type="bibr" target="#b20">[21]</ref> to refine vehicle pose using temporal information. Our work is also a learning-based approach but possesses key differences compared to previous works. Our approach promotes learning explicit IGRs while previous works do not. With IGRs, later part of our model can directly reason in the geometrical space. Such design enables our model to directly estimate global (egocentric) pose in the camera coordinate system while previous works can only estimate relative (allocentric) pose and require a second step to obtain the global pose.</p><p>Optimization-based methods first extract 2D evidence from RGB images and fit a 3D model to the evidence for pose estimation. Deepmanta <ref type="bibr" target="#b5">[6]</ref> detected pre-annotated semantic object parts and used Epnp <ref type="bibr" target="#b28">[29]</ref> for pose optimization. Mono3D++ <ref type="bibr" target="#b19">[20]</ref> introduced task priors into a joint energy function and conducted optimization using the Ceres solver <ref type="bibr" target="#b0">[1]</ref>. Compared to these works, our approach does not need the construction of a 3D shape model neither any extra manual labeling. Such labeling was required in <ref type="bibr" target="#b5">[6]</ref> to train a model for detecting image evidence. Neither does our approach need initialization and iterative optimization for inference. In fact, our approach can serve as the initialization step and be integrated with these works. Self-supervision from geometry knowledge can be incorporated into deep systems to regularize representation learning. Epipolar geometry was used in <ref type="bibr" target="#b24">[25]</ref> to generate supervisory signal for a fully convolutional network. Loop consistency <ref type="bibr" target="#b6">[7]</ref> and projective consistency <ref type="bibr" target="#b17">[18]</ref> were employed for 3D human pose estimation. Flow fields and disparity maps were extracted in <ref type="bibr" target="#b15">[16]</ref> for self-supervised video representation learning. We propose a new self-supervision mechanism by utilizing projective invariants to supervise our intermediate representations. This approach can utilize unlabeled images during training yet does not require multiview setting as <ref type="bibr" target="#b24">[25]</ref>. It is also camera-agnostic and does not require camera intrinsics as <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Egocentric 3D Vehicle Pose Estimation</head><p>Ego-Net E estimates 3D orientation from cropped instance appearance. It can be combined with a 3D location estimation module T to form a 3D object detection model M = (E, T ) as shown in <ref type="figure">Fig. 1</ref>. We detail E in the following and some examples of composing M are in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Single-step Progressive Mapping</head><p>For visual perception and traffic surveillance systems, expressing the poses of multiple objects in the same coordinate system is favorable and convenient for downstream planing and decision-making. Previous works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13]</ref> achieved this by adopting the computational graph in Eq. 1. A CNN-based model N is used to map local instance appearance x i to allocentric pose, i.e., 3D orientation in the object coordinate system {i, j, k}, which is later converted to the egocentric pose, i.e., orientation in the camera coordinate system. The difference between these two coordinates systems is shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. This two-step design is a workaround since an object with the same egocentric pose θ i can produce different local appearance depending on its location <ref type="bibr" target="#b26">[27]</ref> and learning the mapping from x i to θ i is an ill-posed problem. In this two-step design, {i, j, k} needs to be estimated by another parallel module and N can not recover θ i independently without the assist of this module. The error of this module can propagate to the final estimation of egocentric poses. Even with this workaround, the problem remains challenging as the mapping from x i to α i is highly non-linear and difficult to approximate.  We believe that such two-step computation is unnecessary and propose to learn a direct mapping E from visual information to egocentric pose. However, instead of relying on a black box model to fit such a non-linear mapping, we assume this mapping to be a progressive process, where a series of IGRs are gradually extracted from pixels and eventually lifted to the 3D target. Specifically, we design Ego-Net as a composite function consisting of {H, C, L} described as follows. Given the cropped patch of one detected instance x i , Ego-Net predicts its egocentric pose as <ref type="figure">Fig. 2</ref> depicts Ego-Net, whose computational graph is shown in Eq. 2. H is assumed to extract heatmaps h(x i ) for salient 2D object parts that are mapped by C to coordinates representing their lo-</p><formula xml:id="formula_3">x i N − → α i convert − −−−− → θ i {i, j, k} assist<label>(1)</label></formula><formula xml:id="formula_4">E(x i ) = L(A i (C(H(x i )))) = θ i .</formula><formula xml:id="formula_5">cal location φ l (x i ). φ l (x i ) is converted to the global image plane coordinates φ g (x i ) with an affine transformation A i parametrized with scaling and 2D translation. φ g (x i ) is fur- ther lifted into a 3D representation ψ(x i ) by L. The final pose prediction derives from ψ(x i ). x i H − → h(x i ) C − → φ l (x i ) Ai − − → φ g (x i ) L − → ψ(x i ) → θ i (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Labor-free Intermediate Representations</head><p>The abstraction in Eq. 2 does not mathematically define what geometrical quantity to use as IGRs, here we define them based on three considerations: Availability: The IGRs should be easily derived from existing ground truth annotations with none or minimum extra manual effort. While using CAD models such as the wireframe model in <ref type="bibr" target="#b5">[6]</ref> and 3D voxel pattern in <ref type="bibr" target="#b58">[59]</ref> can provide high level-of-detail, such representations require extensive manual effort and are not scalable. A new driving scenario may requires collecting a new training set, and can incur significant burden if the IGRs cannot be readily obtained. In addition, no evidence has shown that detailed object shapes are necessary for orientation estimation. Discriminative: The IGRs should be indicative for 3D pose estimation, so that it can serve as a good bridge between visual appearance input and the geometrical target. Transparency: The IGRs should be easy to understand, which makes them debugging-friendly and trustworthy for applications such as autonomous driving. While network visualization techniques <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34]</ref> can be used to understand learned representations, designing IGRs with explicit meaning offers more convenience in practice.</p><p>With the above considerations, we define the 3D representation ψ(x i ) as sparse 3D point cloud representing an interpolated cuboid. Autonomous driving datasets such as KITTI <ref type="bibr" target="#b16">[17]</ref> usually label instance 3D bounding boxes from captured point clouds where an instance x i is associated with its centroid location in the camera coordinate system t i = [t x , t y , t z ], size [h i , w i , l i ], and its egocentric pose θ i . As shown in <ref type="figure">Fig. 4</ref>, denote the 12 lines enclosing the vehicle as {l j } 12 j=1 , where each line is represented by two endpoints (start and end) as l j = [p s j ; p e j ]. p v j (v is s or e) is a w l h <ref type="figure">Figure 4</ref>: An instance x i with primitive 3D bounding box annotation (left) and its 3D point cloud representation ψ(x i ) (right) in the camera coordinate system as the collection of red crosses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-vector</head><formula xml:id="formula_6">(X v j , Y v j , Z v j )</formula><p>representing the point's location in the camera coordinate system. As a complexity-controlling parameter, q more points are derived from each line with a pre-defined interpolation matrix B q×2 as</p><formula xml:id="formula_7">    p 1 j p 2 j . . . p q j     = B q×2 p s j p e j =     β 1 1 − β 1 β 2 1 − β 2 . . . . . . β q 1 − β q     p s j p e j .<label>(3)</label></formula><p>The 8 endpoints, the instance's centroid, and the interpolated points for each of the 12 lines form a set of 9 + 12q points. The concatenation of these points forms a 9+12q by 3 matrix τ (x i ). Since we do not need the 3D target ψ(x i ) to encode location, we deduct the instance translation t i from τ (x i ) and represent ψ(x i ) as a set of 8 + 12q points representing the shape relative to the centroid</p><formula xml:id="formula_8">ψ(x i ) = {(X v j − t x , Y v j − t y , Z v j − t z )}<label>(4)</label></formula><p>where v ∈ {s, 1, . . . , q, e} and j ∈ {1, 2, . . . , 12}. Larger q provides more cues for inferring pose yet increases complexity. In practice we choose q = 2 and the right figure of <ref type="figure">Fig. 4</ref> shows an example with B 2×2 = Serving as the 2D representation to be located by H and C, φ g (x i ) is defined to be the projected screen coordinates of τ (x i ) given camera intrinsics K 3×3 as</p><formula xml:id="formula_9">φ g (x i ) = K 3×3 τ (x i ).<label>(5)</label></formula><p>φ g (x i ) implicitly encodes instance location on the image plane so that it is not an ill-posed problem to estimate egocentric pose from it directly. In summary, these IGRs can be computed with zero extra manual annotation, are easy to understand and contain rich information for estimating the instance orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning IGRs With Geometrical Prior</head><p>Sec. 3 defines what IGRs to learn, here we discuss how to learn such representations more effectively. We propose to use geometrical prior knowledge for self-supervision and propose a mixed training strategy to take advantage of cheap unlabeled data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-ratio = 4/3 Cross-ratio = 4/3</head><p>Project Image Plane </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Self-supervising by Preserving Cross-ratio</head><p>The cross-ratio of 4 collinear points are invariant during perspective projection <ref type="bibr" target="#b18">[19]</ref>. Once ψ(x i ) is defined with a fixed interpolation matrix B q×2 , this ratio is fixed and invariant after perspective projection as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. If the network E predicts a different value for such a ratio, one can penalize the network as supervisory signal. For every 4 points in φ l (x i ), we denote their ground truth 2D coordinates as φ l (x i ) = [v 1 , v 2 , v 3 , v 4 ] and the ground truth cross-ratio as cr = v3−v1 v4−v2 v3−v2 v4−v1 . Denote the predicted coordinates for such 4 points as [v 1 ,v 2 ,v 3 ,v 4 ], we propose the cross-ratio loss function as</p><formula xml:id="formula_10">L cr = SmoothL1(cr 2 − v 3 −v 1 2 v 4 −v 2 2 v 3 −v 2 2 v 4 −v 1 2 ),<label>(6)</label></formula><p>where we choose cr 2 as the target because v p −v q 2 can be easily computed with vector inner product &lt;v p − v q ,v p −v q &gt; to avoid computing the square-root for v p −v q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Utilizing Unlabeled Imagery</head><p>Evaluating L cr does not require any ground truth information, which means an unlabeled vehicle instance can be used to supervise the learning of φ l (x i ). In addition, the preservation of cross-ratio holds irrespective of the camera intrinsics as long as a center projection model is used. This implies that imagery captured with different cameras at different locations can be used collectively and jointly. In our experiments, we crop unlabeled cars from <ref type="bibr" target="#b53">[54]</ref> which were <ref type="figure">Figure 6</ref>: A random training batch with ground truth φ l (x) (blue) and predictionφ l (x) (red). By introducing the crossratio loss function, images with no labels can be employed into the training stage to regularize the learning of φ l (x). captured in a country different from those of KITTI <ref type="bibr" target="#b16">[17]</ref> and used cameras with different intrinsics.</p><p>Ego-Net is supervised by hybrid types of loss functions and Algorithm 1 shows the forward-pass given a data batch consisting of both labeled and unlabeled instances. For the labeled ones, ground truth IGRs are available to evaluate heatmap loss L hm and 2D/3D representation losses L 2d and L 3d . For the unlabeled images, only the proposed crossratio loss L cr is evaluated as an extra supervisory signal. <ref type="figure">Fig. 6</ref> shows a training batch containing both labeled and unlabeled cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Mixed training with unlabeled data</head><p>Input: Labeled data with ground truth</p><formula xml:id="formula_11">D 1 = {x i , h(x i ), φ l (x i ), ψ(x i )} N 1 i=1 , unlabeled image inputs D 2 = {x j } N 2 i=1</formula><p>, model E = (H, C, L), loss functions for heatmaps, preserving cross-ratio, 2D and 3D representations as L hm , Lcr, L 2d and L 3d Output: Total loss L total for i=1:  <ref type="table">Table 1</ref>: System-level evaluation by comparing Average Orientation Similarity (AOS) with the SOTA learning-based methods for the KITTI test set in the car category. The second-highest performance among the single-view RGB-based approaches is shown in blue and our improvement over it follows the ↑ signs. Without using LiDAR data <ref type="bibr" target="#b25">[26]</ref> or temporal information <ref type="bibr" target="#b3">[4]</ref>, our system out-performs previous monocular RGB-based methods and even stereo methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b54">55]</ref>.</p><formula xml:id="formula_12">N 1 dô h(x i ) = H(x i ); φ l (x i ) = C(ĥ(x i )); ψ(x i ) = L(A i (φ l (x i ))); end for j=1:N 2 dô φ l (x j ) = C(H(x j )); end L 1 = 1 N 1 N 1 i=1 L hm (ĥ(x i ), h(x i )); L 2 = 1 N 1 N 1 i=1 L 2d (φ l (x i ), φ l (x i )); L 3 = 1 N 1 N 1 i=1 L 3d (ψ(x i ), ψ(x i )); L 4 = 1 N 1 +N 2 [ N 1 i=1 Lcr(φ l (x i )) + N 2 j=1 Lcr(φ l (x j ))]; L total = 4 i=1 L i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We first introduce the evaluation benchmark, followed by an ablation study to demonstrate the effectiveness of IGRs and geometrical self-supervision. Finally we compare the overall system performance with recent SOTA works and demonstrate how Ego-Net can improve other 3D object detection systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Metrics</head><p>Datasets. We employ the KITTI object detection benchmark <ref type="bibr" target="#b16">[17]</ref> for evaluation, which contains RGB images captured in outdoor scenes. The dataset is split into 7,481 training images and 7,518 testing images. The ground truth of the testing images is not released and thus all predictions were sent to the official server for evaluation. The training images are further split into a train set and a validation set containing 3,682 and 3,799 images respectively. Our Ego-Net is trained using only the train set and the validation set is used for hyper-parameter tuning. To obtain unlabeled car instances for training, we randomly crop images from Apol-loCar3D <ref type="bibr" target="#b53">[54]</ref> dataset. Evaluation Metrics. We use the standard metric, Average Orientation Similarity (AOS), to assess the overall system performance for joint vehicle detection and pose estimation. AOS is defined as AOS = 1 11 r∈{0,0.1,...,1} maxr :r≥r s(r) where r is the detection recall and s(r) ∈ [0, 1] is the orientation similarity (OS) at recall level r. OS is defined as</p><formula xml:id="formula_13">s(r) = 1 |D(r)| i∈D(r) 1+cos∆ (i) θ 2 δ i , where D(r)</formula><p>denotes the set of all object detections at recall rate r and ∆ (i) θ is the difference in angle between estimated and ground truth vehicle pose of detection i. For the AOS evaluation, vehicle instances are grouped based on three difficulty levels: easy, moderate and hard. A smaller instance with more severe occlusion and truncation is considered to be more difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>A detected instance is cropped and resized to a 256 by 256 patch as input to Ego-Net. HRNet-w48 <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b57">58]</ref> is used as the fully-convolutional sub-model H to regress heatmaps of size 64 by 64. L hm is implemented as mean square loss, L2 loss is used for L hm and L 3d , while L1 loss is used for L 2d . More training details are included in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>Direct regression vs. learning IGRs To evaluate the effectiveness of our proposed IGRs, we compare the pose estimation accuracy with learned IGRs to that of a baseline which directly regresses pose from the instance feature maps. AOS is influenced by the recall of car detection and is upper-bounded by 2D Average Precision (AP 2D ) of the detector <ref type="bibr" target="#b16">[17]</ref>. To eliminate the impact of the used object detector, we compare AOS on all annotated vehicle instances in the validation set. This is equivalent to measuring AOS with AP 2D = 1 so that the orientation estimation accuracy becomes the only focus. The comparison is summarized in Tab. 2. Note learning IGRs outperforms the baseline by a significant margin. Deep3DBox <ref type="bibr" target="#b44">[45]</ref> is another popular architecture 3 that performs direct angle regression, but utilizes a complex MultiBin loss function for joint classification and regression. Without designing such loss functions, our approach outperforms it with the novel design of IGRs and does not need discrete pose classification. Quali- tative visualization of pose predictions on the validation set is shown in <ref type="figure" target="#fig_3">Fig. 7</ref>  Effect of the mixed training strategy. To study whether the introduction of L cr and unlabeled data can improve the learning of φ g (x i ) and result in improved accuracy of locating φ g (x i ), we re-train Ego-Net without using the crossratio component and unlabeled data. The quantitative comparison is shown in Tab. 3. Percentage of Correct Keypoints (PCK@X) gives the ratio of correctly located joints that have pixel error smaller than X times a pre-defined threshold. This threshold is different for each instance and depends on the instance size so that PCK is not influenced by car size variation. We define the threshold as one third of the bounding box height for each instance. Tab. 3 shows that the accuracy of locating screen coordinates improves after adding unlabeled data for training. Despite the unlabeled images do not have any 3D annotation and were taken in a different country with different camera intrinsics, they benefit model generalization. We believe the reason is that the introduction of these unlabeled images mitigates the appearance bias towards the training data.    <ref type="table">Table 5</ref>: AP BEV evaluated on KITTI validation set. Ego-Net can correct the erroneous pose predictions from <ref type="bibr" target="#b12">[13]</ref> as shown in <ref type="figure" target="#fig_5">Fig. 9</ref>. While AOS depends on the detection performance of these methods, fusing Ego-Net consistently improves the pose estimation accuracy of these approaches. This indicates that Ego-Net is robust despite the performance of a vehicle detector varies with different recall levels. For these detected instances (true positives) we plot the distribution of orientation estimation error versus different depth ranges and occlusion levels <ref type="bibr" target="#b3">4</ref> in <ref type="figure" target="#fig_4">Fig. 8</ref>. The error in each bin is the averaged egocentric pose error for those instances that fall into it. While the performance of M3D-RPN <ref type="bibr" target="#b2">[3]</ref> and D4LCN <ref type="bibr" target="#b12">[13]</ref> degrades significantly for distant and occluded cars, the errors of our approach increase gracefully. We believe that explicitly learning the vehicle parts makes our model more robust to occlusion as the visible parts can provide rich information for pose estimation.</p><p>Bird's eye view (BEV) evaluation compared with D4LCN <ref type="bibr" target="#b12">[13]</ref> is visualized in <ref type="figure" target="#fig_5">Fig. 9</ref>. The egocentric pose predictions are shown under bird's eye view and the arrows point to the heading direction of those vehicles. From the local appearance it can be hard to tell whether certain cars head towards or away from the camera, as validated by the erroneous predictions of D4LCN <ref type="bibr" target="#b12">[13]</ref> since it regresses pose from local features. In comparison, our approach gives accurate egocentric pose predictions for these instances and others that are partially occluded. Quantitative comparison <ref type="bibr" target="#b3">4</ref> The occlusion levels are annotated for each instance in KITTI. with several 3D object detection systems on the KITTI validation set is shown in Tab. 5. Note that utilizing Ego-Net can correct wrong pose predictions especially for difficult instances, which leads to improved 3D IoU and results in significantly improved AP BEV .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a new deep model for estimating 3D vehicle pose in the camera coordinate system. The heatmaps of object parts are first extracted from local object appearance, which are mapped to the screen coordinates and then further lifted to a 3D object pose. A novel loss function that encourages the preservation of cross-ratio is proposed to improve the estimation accuracy of the screen coordinates. Based on the definition of intermediate representations and the crossratio loss, unlabeled instances can be used jointly with labeled objects to improve representation learning. Our system hits a record high for the vehicle detection and pose estimation task on KITTI among the monocular RGB-based deep models.</p><p>Several future directions remain to be explored. For example, the definition of intermediate representations could consider instance point-clouds if LiDAR data is available. More geometrical prior knowledge apart from preserving cross-ratio can be incorporated. Apart from unlabeled instances, graphics-rendered cars can also be incorporated whose 3D ground truth are known.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Local appearance cannot uniquely determine egocentric pose. Existing solutions first estimate allocentric pose in the object coordinate system (blue) and convert it to egocentric pose in the camera coordinate system (green) based on the object location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>for each line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Perspective projection of τ (x i ) (left) and the zoomed image plane (right). Since the cross-ratio of every 4 collinear points is invariant after camera projection, the same value should be expected for a line on the screen as that of the corresponding line on the 3D bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results on the KITTI validation set where the red arrows indicate the vehicle poses. More examples can be found in our supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Average orientation error (AOE) on KITTI validation set in different depth ranges and occlusion levels. Our approach is robust to distant and partially occluded instances.Ego-Net as a module can be used as the pose estimation branch of other 3D object detection systems. To assess if Ego-Net can help improve the pose estimation accuracy of other 3D object detection systems, compared to their original pose estimation branch, we fuse Ego-Net with some recent works with open-source implementations. We took the detected instances produced by other approaches and compared the estimated poses by Ego-Net with those produced by their original pose estimation module. The result is summarized in Tab. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Detected instances in KITTI validation set along with the comparison of the predicted egocentric poses in bird's eye view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Module-level evaluation assuming perfect object detection (AP 2D = 100.00) in the KITTI validation set.</figDesc><table /><note>B: baseline using direct regression. +IGR: adding interme- diate geometric representations. Improvements compared with [45] follow the ↑ signs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of coordinate localization performance on KITTI validation set with and without using unlabeled data for mixed training. Higher PCK is better.</figDesc><table><row><cell cols="3">5.4. Comparison with SOTA works</cell><cell></cell></row><row><cell cols="4">Joint vehicle detection and pose estimation performance</cell></row><row><cell cols="4">is measured by AOS. Tab. 1 compares the AOS of our sys-</cell></row><row><cell cols="4">tem using Ego-Net with other SOTA approaches on KITTI</cell></row><row><cell cols="4">test set. Among the single-view RGB-based approaches,</cell></row><row><cell cols="4">Ego-Net outperforms all other deep models by a clear mar-</cell></row><row><cell cols="4">gin. Ego-Net even outperforms DSGN [11] and Disp R-</cell></row><row><cell cols="4">CNN [55] that use stereo cameras. In addition, our approach</cell></row><row><cell cols="4">outperforms Kinematic3D [4] which exploits temporal in-</cell></row><row><cell cols="4">formation using RGB video, while Ego-Net using only a</cell></row><row><cell cols="4">single frame for inference. This result indicates that vehicle</cell></row><row><cell cols="4">pose can be reliably estimated with a single frame. In fact,</cell></row><row><cell cols="4">our approach can also serve for per-frame initialization of</cell></row><row><cell>temporal models.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">AOS&amp;AP 2D , AOS ≤ AP 2D</cell></row><row><cell>Method</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell></cell><cell></cell><cell>AP 2D</cell><cell></cell></row><row><cell>M3D-RPN [3]</cell><cell>90.28</cell><cell>83.75</cell><cell>67.72</cell></row><row><cell>D4LCN [13]</cell><cell>92.80</cell><cell>84.43</cell><cell>67.89</cell></row><row><cell>Method</cell><cell></cell><cell>AOS</cell><cell></cell></row><row><cell>M3D-RPN [3]</cell><cell>88.79</cell><cell>81.25</cell><cell>65.37</cell></row><row><cell>M3D-RPN + Ego-Net</cell><cell>90.20 ↑1.6%</cell><cell>83.60 ↑2.9%</cell><cell>67.53 ↑3.3%</cell></row><row><cell>D4LCN [13]</cell><cell>91.74</cell><cell>82.96</cell><cell>66.45</cell></row><row><cell>D4LCN + Ego-Net</cell><cell>92.62 ↑1.0%</cell><cell>84.25 ↑1.6%</cell><cell>67.60 ↑1.7%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>AOS evaluation on KITTI validation set. After employing Ego-Net, the vehicle pose estimation accuracy of SOTA 3D object detection systems can be improved. The space for AOS improvement is upper-bounded by AP 2D .</figDesc><table><row><cell>Method</cell><cell>Easy</cell><cell>AP BEV Moderate</cell><cell>Hard</cell></row><row><cell>ROI-10D [42]</cell><cell>14.04</cell><cell>3.69</cell><cell>3.56</cell></row><row><cell>Mono3D++ [20]</cell><cell>16.70</cell><cell>11.50</cell><cell>10.10</cell></row><row><cell>MonoDIS [52]</cell><cell>24.26</cell><cell>18.43</cell><cell>16.95</cell></row><row><cell>D4LCN [13]</cell><cell>31.53</cell><cell>22.58</cell><cell>17.87</cell></row><row><cell>D4LCN + Ego-Net (Ours)</cell><cell>33.60</cell><cell>25.38</cell><cell>22.80</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Nicholasli1995/EgoNet Part of this work was done when the first author was an intern at Sense-Time Research Shanghai Autonomous Driving Group.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/smallcorgi/3D-Deepbox</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was partially supported by Hong Kong Research Grants Council (RGC) General Research Fund (GRF) 16203319. We acknowledge the support of NVIDIA Corporation with the donation of one Titan Xp GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keir</forename><surname>Mierle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename><surname>Ceres</surname></persName>
		</author>
		<ptr target="http://ceres-solver.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-rcnn: Joint object detection and pose estimation using 3d object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Flohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1546" to="1551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9287" to="9296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kinematic 3d object detection in monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrick</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of European Conference on Computer Vision, Virtual</title>
		<meeting>eeding of European Conference on Computer Vision, Virtual</meeting>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection with decoupled structured polygon estimation and heightguided depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10478" to="10485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaonary</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Céline</forename><surname>Teulière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5714" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals using stereo imagery for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1259" to="1272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12536" to="12545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monopair: Monocular 3d object detection using pairwise spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12093" to="12102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Serge Belongie, and Pietro Perona. Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual surveillance for moving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James M Ferryman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony D</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Worrall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="197" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5589" to="5597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10905" to="10914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mono3d++: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8409" to="8416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint monocular 3d vehicle detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou-Ning</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5390" to="5399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07744</idno>
		<title level="m">Perspectivenet: 3d object detection from a single rgb image via perspective points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time pose estimation piggybacked on object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Juranek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Herout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markéta</forename><surname>Dubská</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Zemcik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2381" to="2389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gsnet: Joint vehicle pose and shape reconstruction with geometrical and scene-aware supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11867" to="11876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Epnp: An accurate o (n) solution to the pnp problem. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">155</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaici</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feidao</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Facial age estimation by deep residual decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10737</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Visualizing the decision-making process in deep neural decision forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09201</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6173" to="6183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep fitting degree scoring network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1057" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Reinforced axial refinement network for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chufan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.13748</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conservative wasserstein training for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bvk</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8262" to="8272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Smoke: Singlestage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Tóth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="996" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking pseudo-lidar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2069" to="2078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Vision: A computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>MIT press</publisher>
			<biblScope unit="page" from="32" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Weakly supervised 3d object detection from lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11901</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsalan</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ida-3d: Instancedepth-aware 3d object detection from stereo vision for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Hao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13015" to="13024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Distancenormalized unified representation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuepeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="91" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Disentangling monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>López-Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12365</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5452" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Disp r-cnn: Stereo 3d object detection via shape prior guided instance disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10548" to="10557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4604" to="4612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1903" to="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Zoomnet: Part-aware adaptive zooming neural network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12557" to="12564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Object detection and viewpoint estimation with auto-masking neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="441" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">3d object detection using scale invariant and feature reweighting networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruolan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9267" to="9274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dave: A unified framework for fast vehicle detection and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mellor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="278" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fast automatic vehicle annotation for urban traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mellor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1973" to="1984" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
