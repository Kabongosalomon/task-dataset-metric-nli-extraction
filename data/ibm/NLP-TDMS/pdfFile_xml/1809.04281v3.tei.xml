<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MUSIC TRANSFORMER: GENERATING MUSIC WITH LONG-TERM STRUCTURE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Cheng-Zhi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Dinculescu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<title level="a" type="main">MUSIC TRANSFORMER: GENERATING MUSIC WITH LONG-TERM STRUCTURE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer <ref type="bibr" target="#b20">(Vaswani et al., 2017)</ref>, a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance <ref type="bibr" target="#b16">(Shaw et al., 2018)</ref>. This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minutelong compositions (thousands of steps, four times the length modeled in Oore et al. (2018)) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies 1 . We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A musical piece often consists of recurring elements at various levels, from motifs to phrases to sections such as verse-chorus. To generate a coherent piece, a model needs to reference elements that came before, sometimes in the distant past, repeating, varying, and further developing them to create contrast and surprise. Intuitively, self-attention <ref type="bibr" target="#b13">(Parikh et al., 2016)</ref> appears to be a good match for this task. Self-attention over its own previous outputs allows an autoregressive model to access any part of the previously generated output at every step of generation. By contrast, recurrent neural networks have to learn to proactively store elements to be referenced in a fixed size state or memory, potentially making training much more difficult. We believe that repeating self-attention in multiple, successive layers of a Transformer decoder <ref type="bibr" target="#b20">(Vaswani et al., 2017)</ref> helps capture the multiple levels at which self-referential phenomena exist in music.</p><p>In its original formulation, the Transformer relies on absolute position representations, using either positional sinusoids or learned position embeddings that are added to the per-position input representations. Recurrent and convolutional neural networks instead model position in relative terms: RNNs through their recurrence over the positions in their input, and CNNs by applying kernels that effectively choose which parameters to apply based on the relative position of the covered input representations.</p><p>Music has multiple dimensions along which relative differences arguably matter more than their absolute values; the two most prominent are timing and pitch. To capture such pairwise relations between representations, <ref type="bibr" target="#b16">Shaw et al. (2018)</ref> introduce a relation-aware version of self-attention which they use successfully to modulate self-attention by the distance between two positions. We extend this approach to capture relative timing and optionally also pitch, which yields improvement in both sample quality and perplexity for JSB Chorales. As opposed to the original Transformer, samples from a Transformer with our relative attention mechanism maintain the regular timing grid present in this dataset. The model furthermore captures global timing, giving rise to regular phrases.</p><p>The original formulation of relative attention <ref type="bibr" target="#b16">(Shaw et al., 2018)</ref> requires O(L 2 D) memory where L is the sequence length and D is the dimension of the model's hidden state. This is prohibitive for long sequences such as those found in the Piano-e-Competition dataset of human-performed virtuosic, classical piano music. In Section 3.4, we show how to reduce the memory requirements to O(LD), making it practical to apply relative attention to long sequences.</p><p>The Piano-e-Competition dataset consists of MIDI recorded from performances of competition participants, bearing expressive dynamics and timing on the granularity of &lt; 10 miliseconds. Discretizing time on a fixed grid that would yield unnecessarily long sequences as not all events change on the same timescale. We hence adopt a sparse, MIDI-like, event-based representation from <ref type="bibr" target="#b12">(Oore et al., 2018)</ref>, allowing a minute of music with 10 milisecond resolution to be represented at lengths around 2K, as opposed to 6K to 18K on a fixed-grid representation with multiple performance attributes. As position in sequence no longer corresponds to time, a priori it is not obvious that relative attention should work as well with such a representation. However, we will show in Section 4.2 that it does improve perplexity and sample quality over strong baselines.</p><p>We speculate that idiomatic piano gestures such as scales, arpeggios and other motifs all exhibit a certain grammar and recur periodically, hence knowing their relative positional distances makes it easier to model this regularity. This inductive bias towards learning relational information, as opposed to patterns based on absolute position, suggests that the Transformers with relative attention could generalize beyond the lengths it was trained on, which our experiments in Section 4.2.1 confirm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">CONTRIBUTIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain contributions</head><p>We show the first successful use of Transformers in generating music that exhibits long-term structure. Before our work, LSTMs were used at timescales of 15s (~500 tokens) on the Piano-e-Competition dataset <ref type="bibr" target="#b12">(Oore et al., 2018)</ref>. Our work show that Transformers not only achieve state-of-the-art perplexity on modeling these complex expressive piano performances, but can also generate them at the scale of 60s (~2000 tokens) with remarkable internal consistency. Our relative attention mechanism is essential to the model's quality. In listening tests (see Section 4.2.3), samples from models with relative self-attention were perceived as more coherent than the baseline Transformer model from <ref type="bibr" target="#b20">Vaswani et al. (2017)</ref>. Relative attention not only enables Transformers to generate continuations that elaborate on a given motif, but also to generalize and generate in consistent fashion beyond the length it was trained on (see Section 4.2.1). In a seq2seq setup, Transformers can generate accompaniments conditioned on melodies, enabling users to interact with the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithmic contributions</head><p>The space complexity of the relative self attention mechanism in its original formulation <ref type="bibr" target="#b16">(Shaw et al., 2018)</ref> made it infeasible to train on sequences of sufficient length to capture long-range structure in longer musical compositions. Addressing this we present a crucial algorithmic improvement to the relative self attention mechanism, dramatically reducing its memory requirements from O(L 2 D) to O(LD). For example, we reduce the memory consumption per layer from 8.5 GB to 4.2 MB (per head from 1.1 GB to 0.52 MB) for a sequence of length L = 2048 and hidden-state size D = 512 (per head D h = D H = 64, where number of heads is H = 8) (see <ref type="table" target="#tab_0">Table 1</ref>), allowing us to use GPUs to train the relative self-attention Transformer on long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Sequence models have been the canonical choice for modeling music, from Hidden Markov Models to RNNs and Long Short Term Memory networks (e.g., <ref type="bibr" target="#b3">Eck &amp; Schmidhuber, 2002;</ref><ref type="bibr" target="#b10">Liang, 2016;</ref><ref type="bibr" target="#b12">Oore et al., 2018)</ref>, to bidirectional LSTMs (e.g., <ref type="bibr" target="#b5">Hadjeres et al., 2017)</ref>. Successful application of sequential models to polyphonic music often requires serializing the musical score or performance into a single sequence, for example by interleaving different instruments or voices. Alternatively, a 2D pianoroll-like representation (see A.1 for more details) can be decomposed into a sequence of multi-hot pitch vectors, and their joint probability distributions can be captured using Restricted Boltzmann Machines <ref type="bibr" target="#b17">(Smolensky, 1986;</ref><ref type="bibr" target="#b6">Hinton et al., 2006)</ref> or Neural Autoregressive Distribution Estimators (NADE; <ref type="bibr" target="#b8">Larochelle &amp; Murray, 2011)</ref>. Pianorolls are also image-like and can be modeled by CNNs trained either as generative adversarial networks (e.g., <ref type="bibr" target="#b2">Dong et al., 2018)</ref> or as orderless NADEs <ref type="bibr" target="#b18">(Uria et al., 2014;</ref>) (e.g., <ref type="bibr" target="#b7">Huang et al., 2017)</ref>. <ref type="bibr" target="#b9">Lattner et al. (2018)</ref> use self-similarity in style-transfer fashion, where the self-similarity structure of a piece serves as a template objective for gradient descent to impose similar repetition structure on an input score. Self-attention can be seen as a generalization of self-similarity; the former maps the input through different projections to queries and keys, and the latter uses the same projection for both.</p><p>Dot-product self-attention is the mechanism at the core of the Transformer, and several recent works have focused on applying and improving it for image generation, speech, and summarization <ref type="bibr" target="#b15">Povey et al., 2018;</ref><ref type="bibr" target="#b11">Liu et al., 2018)</ref>. A key challenge encountered by each of these efforts is scaling attention computationally to long sequences. This is because the time and space complexity of self-attention grows quadratically in the sequence length. For relative self-attention <ref type="bibr" target="#b16">(Shaw et al., 2018)</ref> this is particularly problematic as the space complexity also grows linearly in the dimension, or depth, of the per-position representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DATA REPRESENTATION</head><p>We take a language-modeling approach to training generative models for symbolic music. Hence we represent music as a sequence of discrete tokens, with the vocabulary determined by the dataset. Datasets in different genres call for different ways of serializing polyphonic music into a single stream and also discretizing time.</p><p>The JSB Chorale dataset consists of four-part scored choral music, which can be represented as a matrix where rows correspond to voices and columns to time discretized to sixteenth notes. The matrix's entries are integers that denote which pitch is being played. This matrix can than be serialized in raster-scan fashion by first going down the rows and then moving right through the columns (see A.1 for more details). Compared to JSB Chorale, the piano performance data in the Piano-e-Competition dataset includes expressive timing information at much finer granularity and more voices. For the Piano-e-Competition we therefore use the performance encoding proposed by <ref type="bibr" target="#b12">Oore et al. (2018)</ref> which consists of a vocabulary of 128 NOTE_ON events, 128 NOTE_OFFs, 100 TIME_SHIFTs allowing for expressive timing at 10ms and 32 VELOCITY bins for expressive dynamics (see A.2 for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BACKGROUND: SELF-ATTENTION IN TRANSFORMER</head><p>The Transformer decoder is a autoregressive generative model that uses primarily self-attention mechanisms, and learned or sinusoidal position information. Each layer consists of a self-attention sub-layer followed by a feedforward sub-layer.</p><p>The attention layer first transforms a sequence of L D-dimensional vectors X = (x 1 , x 2 , . . . , x L ) into queries Q = XW Q , keys K = XW K , and values V = XW V , where W Q , W K , and W V are each D × D square matrices. Each L × D query, key, and value matrix is then split into H L × D h parts or attention heads, indexed by h, and with dimension D h = D H , which allow the model to focus on different parts of the history. The scaled dot-product attention computes a sequence of vector outputs for each head as</p><formula xml:id="formula_0">Z h = Attention(Q h , K h , V h ) = Softmax Q h K h √ D h V h .<label>(1)</label></formula><p>The attention outputs for each head are concatenated and linearly transformed to get Z, a L by D dimensional matrix. A upper triangular mask ensures that queries cannot attend to keys later in the sequence. For other details of the Transfomer model, such as residual connections and learning rates, the reader can refer <ref type="bibr" target="#b20">Vaswani et al. (2017)</ref>. The feedforward (FF) sub-layer then takes the output Z from the previous attention sub-layer, and performs two layers of point-wise dense layers on the depth D dimension, as shown in Equation 2. W 1 , W 2 , b 1 , b 2 are weights and biases of those two layers.</p><formula xml:id="formula_1">FF(Z) = ReLU(ZW 1 + b 1 )W 2 + b 2 (2) 3.3 RELATIVE POSITIONAL SELF-ATTENTION</formula><p>As the Transformer model relies solely on positional sinusoids to represent timing information, <ref type="bibr" target="#b16">Shaw et al. (2018)</ref> introduced relative position representations to allow attention to be informed by how far two positions are apart in a sequence. This involves learning a separate relative position embedding E r of shape (H, L, D h ), which has an embedding for each possible pairwise distance r = j k − i q between a query and key in position i q and j k respectively. The embeddings are ordered from distance −L + 1 to 0, and are learned separately for each head. In <ref type="bibr" target="#b16">Shaw et al. (2018)</ref>, the relative embeddings interact with queries and give rise to a S rel , an L × L dimensional logits matrix which modulates the attention probabilities for each head as:</p><formula xml:id="formula_2">RelativeAttention = Softmax QK + S rel √ D h V.<label>(3)</label></formula><p>We dropped head indices for clarity. Our work uses the same approach to infuse relative distance information in the attention computation, while significantly improving upon the memory footprint for computing S rel . For each head, <ref type="bibr" target="#b16">Shaw et al. (2018)</ref> instantiate an intermediate tensor R of shape (L, L, D h ), containing the embeddings that correspond to the relative distances between all keys and queries. Q is then reshaped to an (L, 1, D h ) tensor, and S rel = QR . 2 This incurs a total space complexity of O(L 2 D), restricting its application to long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MEMORY EFFICIENT IMPLEMENTATION OF RELATIVE POSITION-BASED ATTENTION</head><p>We improve the implementation of relative attention by reducing its intermediate memory requirement from O(L 2 D) to O(LD), with example lengths shown in <ref type="table" target="#tab_0">Table 1</ref>. We observe that all of the terms we need from QR are already available if we directly multiply Q with E r , the relative position embedding. After we compute QE r , its (i q , r) entry contains the dot product of the query in position i q with the embedding of relative distance r. However, each relative logit (i q , j k ) in the matrix S rel from Equation 3 should be the dot product of the query in position i q and the embedding of the relative distance j k − i q , to match up with the indexing in QK . We therefore need to "skew" QE r so as to move the relative logits to their correct positions, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> and detailed in the next section. The time complexity for both methods are O(L 2 D), while in practice our method is 6x faster at length 650.  Hence, we propose a "skewing" procedure to transform an absolute-by-relative (i q , r) indexed matrix into an absolute-by-absolute (i q , j k ) indexed matrix. The row indices i q stay the same while the columns indices are shifted according to the following equation: j k = r − (L − 1) + i q . For example in <ref type="figure" target="#fig_0">Figure 1</ref> the upper right green dot in position (0, 2) of QE r after skewing has a column index of 2 − (3 − 1) + 0 = 0, resulting in a position of (0, 0) in S rel .</p><p>We outline the steps illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> below.</p><p>1. Pad a dummy column vector of length L before the leftmost column.</p><p>2. Reshape the matrix to have shape (L+1, L). (This step assumes NumPy-style row-major ordering.)</p><p>3. Slice that matrix to retain only the last l rows and all the columns, resulting in a (L, L) matrix again, but now absolute-by-absolute indexed, which is the S rel that we need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">RELATIVE LOCAL ATTENTION</head><p>For very long sequences, the quadratic memory requirement of even baseline Transformer is impractical. Local attention has been used for example in Wikipedia and image generation <ref type="bibr" target="#b11">(Liu et al., 2018;</ref> by chunking the input sequence into non-overlapping blocks. Each block then attends to itself and the one before, as shown by the smaller thumbnail on the top right corner of <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>To extend relative attention to the local case, we first note that the right block has the same configuration as in the global case (see <ref type="figure" target="#fig_0">Figure 1</ref>) but much smaller: ( L M ) 2 (where M is the number of blocks, and N be the resulting block length) as opposed to L 2 . The left block is unmasked with relative indices running from -1 (top right) to -2N + 1 (bottom left). Hence, the learned E r for the local case has shape (2N − 1, N ).</p><p>Similar to the global case, we first compute QE r and then use the following procedure to skew it to have the same indexing as QK , as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>1. Pad a dummy column vector of length N after the rightmost column.</p><p>2. Flatten the matrix and then pad with a dummy row of length N − 1.</p><p>3. Reshape the matrix to have shape (N + 1, 2N − 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Slice that matrix to retain only the first N rows and last N columns, resulting in a (N, N ) matrix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>4.1 J.S. BACH CHORALES J.S. Bach chorales is a canonical dataset used for evaluating generative models for music 3 (e.g., <ref type="bibr" target="#b0">Allan &amp; Williams, 2005;</ref><ref type="bibr" target="#b1">Boulanger-Lewandowski et al., 2012;</ref><ref type="bibr" target="#b10">Liang, 2016;</ref><ref type="bibr" target="#b4">Hadjeres et al., 2016;</ref><ref type="bibr" target="#b7">Huang et al., 2017)</ref>. It consists of score-based four-part chorales. We first discretize the scores onto a 16th-note grid, and then serialize it by iterating through all the voices within a time step and then advancing time (see A.1 for more details). As there is a direct correspondence between position in sequence and position on the timing/instrument grid in a piece, adding relative position representations could make it easier to learn this grammar. We indeed see relative attention drastically improve negative log-likelihood (NLL) over baseline Transformer <ref type="table" target="#tab_1">(Table 2)</ref>. This improvement is also reflected in sample quality. The samples now maintain the necessary timing/instrument grid, always advancing four steps before advancing in time. As local timing is maintained, the model is able to capture timing on a more global level, giving rise to regular phrasing, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. In addition to relative attention, we also explored enhancing absolute timing through concatenating instead of adding the sinusoids to the input embeddings. This allows the model to more directly learn its absolute positional mapping. This further improves performance for both the baseline and relative transformer <ref type="table" target="#tab_1">(Table 2)</ref>. We compare against COCONET as it is one of the best-performing models that has also been evaluated on the 16-note grid using the canonical dataset split. To directly compare, we re-evaluated COCONET to obtain note-wise losses on the validation set 4 . For the Transformer models (abbreviated as TF), we implemented our attention mechanisms in the Tensor2Tensor framework . We use 8 heads, and keep the query, key (att) and value hidden size (hs) fixed within a config. We tuned number of layers (L in {4,5,6}), attention hidden size (att in {256, 512}) and pointwise feedforward hidden size (ff in {512, 1024}).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">GENERALIZING RELATIVE ATTENTION TO CAPTURE RELATIONAL INFORMATION</head><p>A musical event bears multiple attributes, such as timing, pitch, instrument etc. To capture more relational information, we extend relative attention to capture pairwise distances on additional attributes. We learn separate relative embeddings for timing E t and also pitch E p . E t has entries corresponding to how many sixteenth notes apart are two positions, while E p embeds the pairwise pitch interval. However this approach is not directly scalable beyond J.S. Bach Chorales because it involves explicitly gathering relative embeddings for R t and R p , resulting in a memory complexity of O(L 2 D) as in <ref type="bibr" target="#b16">Shaw et al. (2018)</ref>. This is due to relative information being computed based on content as opposed to content-invariant information such as position in sequence. It was sufficient to add the extra timing signals to the first layer, perhaps because it is closest to the raw input content. Here, the relative logits are computed from three terms, S rel = Skew(QE r ) + Q(R t + R p ) in contrast with other layers that only have one term, Skew(QE r ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PIANO-E-COMPETITION</head><p>We use the first 6 years of of Piano-e-Competition because these years have corresponding MIDI data released 5 , resulting in about 1100 pieces, split 80/10/10. Each piece is MIDI data capturing a classical piano performance with expressive dynamics and timing, encoded with the MIDI-like representation described in Section A.2. We trained on random crops of 2000-token sequences and employed two kinds of data augmentation: pitch transpositions uniformly sampled from {−3, −2, . . . , 2, 3} half-steps, and time stretches uniformly sampled from the set {0.95, 0.975, 1.0, 1.025, 1.05}.</p><p>We compare to Magenta's PerformanceRNN (LSTM, which first used this dataset) <ref type="bibr" target="#b12">(Oore et al., 2018)</ref> and LookBack RNN (LSTM with attention) <ref type="bibr" target="#b22">(Waite, 2016)</ref>. LookBack RNN uses an input representation that requires monophonic music with barlines which is information that is not present in performed polyphonic music data, hence we simply adopt their architecture. <ref type="table" target="#tab_2">Table 3</ref> shows that Transformer-based architectures fits this dataset better than LSTM-based models.  We implemented our attention mechanisms in the Tensor2Tensor framework , and used the default hyperparameters for training, with 0.1 learning rate, 0.1 dropout, and early stopping. We compare four architectures, varying on two axes: global versus local, and regular versus relative attention. We found that reducing the query and key hidden size (att) to half the hidden size (hs) works well and use this relationship for all of the models, while tuning on number of layers (L) and filter size (fs). We use block size (bs) 512 for local attention. We set the maximum relative distance to consider to half the training sequence length for relative global attention, and to the full memory length (which is two blocks) for relative local attention. <ref type="table" target="#tab_2">Table 3</ref> show that relative attention (global or local) outperforms regular self-attention (global or local). All else being equal, local and global attention perform similarly. Each though local attention does not see all the history at once, it can build up a larger receptive field across layers. This can be an advantage in the future for training on much longer sequences, as local attention requires much less memory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">QUALITATIVE PRIMING EXPERIMENTS</head><p>When primed with an initial motif (Chopin's Étude Op. 10, No. 5) shown in the top left corner of <ref type="figure" target="#fig_3">Figure 4</ref>, we see the models perform qualitatively differently. Transformer with relative attention elaborates the motif and creates phrases with clear contour which are repeated and varied. Baseline Transformer uses the motif in a more uniform fashion, while LSTM uses the motif initially but soon drifts off to other material. Note that the generated samples are twice as long as the training sequences. Relative attention was able to generalize to lengths longer than trained but baseline Transformer deteriorates beyond its training length. See Appendix C for visualizations of how the our Relative Transformer attends to past motifs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">HARMONIZATION: CONDITIONING ON MELODY</head><p>To explore the sequence-to-sequence setup of Transformers, we experimented with a conditioned generation task where the encoder takes in a given melody and the decoder has to realize the entire performance, i.e. melody plus accompaniment. The melody is encoded as a sequence of tokens as in <ref type="bibr" target="#b22">Waite (2016)</ref>, quantized to a 100ms grid, while the decoder uses the performance encoding described in Section 3.1 (and further illustrated in A.2). We use relative attention on the decoder side and show in <ref type="table" target="#tab_3">Table 4</ref> it also improves performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">HUMAN EVALUATIONS</head><p>To compare the perceived sample quality of models trained on the Piano-e-Competition dataset, and their ability to generate a continuation for a priming sequence, we carried out a listening test study comparing the baseline Transformer, our Transformer with relative-attention, PerformanceRNN (LSTM), and the validation set. Participants were presented with two musical excerpts (from two different models that were given the same priming sequence) and asked to rate which one is more musical on a Likert scale. For each model, we generated 10 samples each with a different prime, and compared them to three other models, resulting in 60 pairwise comparisons. Each pair was rated by 3 different participants, yielding a total of 180 comparisons. <ref type="figure" target="#fig_4">Figure 5</ref> shows the number of comparisons in which an excerpt from each model was selected as more musical. The improvement in sample quality from using relative attention over the baseline Transformer model was statistically significant (see Appendix B for the analysis), both in aggregate and between the pair. Even though in aggregate LSTMs performed better in the study than the Transformer, despite having higher perplexity, but when compared against each other head to head, the results were not statistically significant (see <ref type="table" target="#tab_4">Table 5</ref> in Appendix B). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work we demonstrated that the Transformer equipped with relative attention is very well-suited for generative modeling of symbolic music. The compelling long-term structure in the samples from our model leaves us enthusiastic about this direction of research. Moreover, the ability to expand upon a primer, in particular, suggests potential applications as creative tool.</p><p>The significant improvement from relative attention highlights a shortcoming of the original Transformer that might also limit its performance in other domains. Improving the Transformer's ability to capture periodicity at various time scales, for instance, or relations between scalar features akin to pitch could improve time-series models. Our memory-efficient implementation enables the application of relative attention to much longer sequences such as long texts or even audio waveforms, which significantly broadens the range of problems to which it could be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DOMAIN-SPECIFIC REPRESENTATIONS</head><p>Adapting sequence models for music requires making decisions on how to serialize a polyphonic texture. The data type, whether score or performance, makes certain representations more natural for encoding all the information needed while still resulting in reasonable sequence lengths.</p><p>A.1 SERIALIZED INSTRUMENT/TIME GRID (J.S.BACH CHORALES)</p><p>The first dataset, J.S. Bach Chorales, consists of four-part score-based choral music. The time resolution is sixteenth notes, making it possible to use a serialized grid-like representation. <ref type="figure">Figure 6</ref> shows how a pianoroll (left) can be represented as a grid (right), following <ref type="bibr" target="#b7">(Huang et al., 2017)</ref>. The rows show the MIDI pitch number of each of the four voices, from top to bottom being soprano (S), alto (A), tenor (T ) and bass (B), while the columns is discretized time, advancing in sixteenth notes.</p><p>Here longer notes such as quarter notes are broken down into multiple repetitions. To serialize the grid into a sequence, we interleave the parts by first iterating through all the voices at time step 1, and then move to the next column, and then iterate again from top to bottom, and so on. The resulting sequence is S 1 A 1 T 1 B 1 S 2 A 2 T 2 B 2 ..., where the subscript gives the time step. After serialization, the most common sequence length is 1024. Each token is represented as onehot in pitch. <ref type="bibr">S: 67, 67, 67, 67 A: 62, 62, 62, 62 T: 59, 59, 57, 57 B: 43, 43, 45, 45</ref> Figure 6: The opening measure of BWV 428 is visualized as a pianoroll (left, where the x-axis is discretized time and y-axis is MIDI pitch number), and encoded in grid representation with sixteenth note resolution (right). The soprano and alto voices have quarter notes at pitches G4 (67) and D4 (62), the tenor has eighth notes at pitches B3 (59) and A3 (57), and the bass has eighth notes at pitches A2 (45) and G2 (43).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MIDI-LIKE EVENT-BASED (PIANO-E-COMPETITION)</head><p>The second dataset, Piano-e-Competition, consists of polyphonic piano performances with expressive timing and dynamics. The time resolution here is on the millisecond level, so a grid representation would result in sequences that are too long. Instead, the polyphonic performance is serialized into a sequence of one hot encoded events as proposed in <ref type="bibr" target="#b12">(Oore et al., 2018)</ref>.</p><p>First, the input MIDI files are preprocessed to extend note durations based on sustain pedal control events. The sustain pedal is considered to be down whenever a sustain control change is encountered with a value &gt;= 64; the sustain pedal is then considered up after a control change with a value &lt; 64. Within a period where the sustain pedal is down, the duration of each note is extended to either the beginning of the next note of the same pitch or the end of the sustain period, whichever happens first. If the original duration extends beyond the time when the sustain pedal is down, that original duration is used.</p><p>Next, the MIDI note events are converted into a sequence from the following set of vocabulary: 128 NOTE_ON events for starting a note of with one of the 128 MIDI pitches, 128 NOTE_OFF events for ending a note with one of the 128 MIDI pitches, 100 TIME_SHIFT events representing forward time shifts in 10ms increments from 10ms to 1s, and 32 SET_VELOCITY events representing the velocity for future NOTE_ON events in the form of the 128 possible MIDI velocities quantized into 32 bins. An example performance encoding is illustrated in <ref type="figure">Figure 7</ref>. <ref type="table" target="#tab_0">SET_VELOCITY&lt;80&gt;, NOTE_ON&lt;60&gt;  TIME_SHIFT&lt;500&gt;, NOTE_ON&lt;64&gt;  TIME_SHIFT&lt;500&gt;, NOTE_ON&lt;67&gt;  TIME_SHIFT&lt;1000&gt;, NOTE_OFF&lt;60&gt;, NOTE_OFF&lt;64&gt;,  NOTE_OFF&lt;67&gt;  TIME_SHIFT&lt;500&gt;, SET_VELOCITY&lt;100&gt;, NOTE_ON&lt;65&gt;</ref> TIME_SHIFT&lt;500&gt;, NOTE_OFF&lt;65&gt; <ref type="figure">Figure 7</ref>: A snippet of a piano performance visualized as a pianoroll (left) and encoded as performance events (right, serialized from left to right and then down the rows). A C Major chord is arpeggiated with the sustain pedal active. At the 2-second mark, the pedal is released, ending all of the notes. At the 3-second mark, an F is played for .5 seconds. The C chord is played at velocity 80 and the F is played at velocity 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SUPPLEMENT OF LISTENING TEST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 STUDY PROCEDURE</head><p>Participants were presented with two musical excerpts that shared a common priming sequence. For each excerpt, the priming sequence was played, followed by 2.5 seconds of silence, followed by the priming sequence again and a continuation of that sequence. The continuations were either sampled from one of the models or extracted from our validation set. We evaluated all possible pairs in the space of data and model samples, except from the same model. Each continuation had a length of 512 events using the encoding described in Section A.2. This corresponds to the length the models were trained on to remove the deteriorating effect that happens with baseline Transformer when asked to generate beyond the length it was trained on. Participants were asked which excerpt they thought was more musical on a Likert scale of 1 to 5. The pair is laid out left versus right, with 1 indicating the left is much more musical, 2 the left is slightly more musical, 3 being a tie, 4 being the right is slightly more musical, and 5 the right is much more musical. For each model, we generated 10 samples each with a different prime, and compared them to three other models, resulting in 60 pairwise comparisons. Each pair was rated by 3 different participants, yielding a total of 180 comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 ANALYSIS</head><p>A Kruskal-Wallis H test of the ratings showed that there was a statistically significant difference between the models: χ 2 (2) = 63.84, p = 8.86e-14&lt; 0.01. <ref type="table" target="#tab_4">Table 5</ref> show a post-hoc analysis on the comparisons within each pair, using the Wilcoxon signed-rank test for matched samples. <ref type="table" target="#tab_5">Table 6</ref> shows a post-hoc analysis of how well each model performed when compared to all pairs, and compares each model's aggregate against each other, using the Mann-Whitney U test for independent samples. We use a Bonferroni correction on both to correct for multiple comparisons. The win and loss counts bucket scores 4, 5 and scores 1, 2 respectively, while the tieing score is 3.</p><p>Both within pairs and between aggregates, participants rated samples from our relative Transformer as more musical than the baseline Transformer with p &lt; 0.01/6.</p><p>For within pairs, we did not observe a consistent statistically significant difference between the other model pairs, baseline transformer versus LSTM and LSTM versus relative Transformer.</p><p>When comparing between aggregates, LSTM was overall perceived as more musical than baseline Transformer. Relative Transformer came a bit close to outperforming LSTM with p = 0.018. When we listen to the samples from the two, they do sound qualitatively different. Relative Transformer often exhibits much more structure (as shown in <ref type="figure" target="#fig_3">Figure 4</ref>), but the effects were probably less pronounced in the listening test because we used samples around 10s to 15s, which is half the length of those shown in <ref type="figure" target="#fig_3">Figure 4</ref> to prevent the baseline Transformer from deteriorating. This weakens the comparison on long-term structure.</p><p>When compared to real music from the validation set, we see that in aggregates, real music was better than LSTM and baseline Transformer. There was no statistical significant difference between real music and relative Transformer. This is probably again due to the samples being too short as real music is definitely still better.  One advantage of attention-based models is that we can visualize its attention distribution 3. This gives us a glimpse of how the model might be building up recurring structures and how far it is attending back. The pianorolls in the visualizations below is a sample generated from Transformer with relative attention. Each figure shows a query (the source of all the attention lines) and previous memories being attended to (the notes that are receiving more softmax probabiliy is highlighted in). The coloring of the attention lines correspond to different heads and the width to the weight of the softmax probability. <ref type="figure">Figure 8</ref>: This piece has a recurring triangular contour. The query is at one of the latter peaks and it attends to all of the previous high notes on the peak, all the way to beginning of the piece. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Steps 1</head><p>Steps 2,3: <ref type="figure" target="#fig_0">Figure 10</ref>: Relative global attention: Steps (from left to right) for "skewing" an absolute-by-relative (i q , r) indexed matrix into absolute-by-absolute (i q , j k ). Grey indicates self-attention masks or entries introduced by the skewing procedure. Positions with relative distance zero are marked. Entries outlined by purple are removed in step 3. Steps 1, 2</p><p>Steps 3 Steps 4 <ref type="figure" target="#fig_0">Figure 11</ref>: Relative local attention: Steps (from left to right) for "skewing" an (i q , r) indexed matrix with 2N − 1 ranged relative indices r into (i q , j k indexed. Shapes are indicated above the boxes, while indices in the boxes give relative distances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Relative global attention: the bottom row describes our memory-efficient "skewing" algorithm, which does not require instantiating R (top row, which is O(L 2 D)). Gray indicates masked or padded positions. Each color corresponds to a different relative distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Relative local attention: the thumbnail on the right shows the desired configuration for S rel . The "skewing" procedure is shown from left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Unconditioned samples from Transformer without (left) and with (right) relative selfattention. Green vertical boxes indicate the endings of (sub)phrases where cadences are held.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparing how models continue a prime (top left). Repeated motives and structure are seen in samples from Transformer with relative attention (top row), but less so from baseline Transformer (middle row) and PerformanceRNN (LSTM) (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Number of wins for each model. Error bars show standard deviations of mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>The query a note in the left-hand, and it attends to its immediate past neighbors and mostly to the earlier left hand chords, with most attention lines distributed in the lower half of the pianoroll.D PREVIOUS FIGURES FOR THE "SKEWING" PROCEDURE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparing the overall relative memory complexity (intermediate relative embeddings (R or E r ) + relative logits S rel ), the maximal training lengths that can fit in a GPU with 16GB memory assuming D h = 64, and the memory usage per layer per head (in MB).</figDesc><table><row><cell>Implementation</cell><cell cols="3">Relative memory Maximal L L = 650</cell><cell>L = 2048 L = 3500</cell></row><row><cell cols="2">Shaw et al. (2018) O(L 2 D + L 2 )</cell><cell>650</cell><cell cols="2">108 + 1.7 1100 + 16 3100 + 49</cell></row><row><cell>Ours</cell><cell>O(LD + L 2 )</cell><cell>3500</cell><cell cols="2">0.17 + 1.7 0.52 + 16 0.90 + 49</cell></row><row><cell cols="2">3.4.1 THE "SKEWING" PROCEDURE</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Note-wise validation NLL on J.S.Bach Chorales at 16th notes. Relative attention, more timing and relational information improve performance.</figDesc><table><row><cell>Model variation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Model variation</cell></row></table><note>Validation NLL for Piano-e-Competition dataset, with event-based representation with lengths L = 2048. Transformer with relative attention (with our efficient formulation) achieves state-of-the-art performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Validation conditional</cell></row><row><cell cols="2">NLL given groundtruth melody from</cell></row><row><cell>Piano-e-Competition.</cell><cell></cell></row><row><cell>Model variation</cell><cell>NLL</cell></row><row><cell>Baseline Transformer</cell><cell>2.066</cell></row><row><cell cols="2">Relative Transformer (ours) 1.786</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>A post-hoc comparison of each pair on their pairwise comparisons with each other, using the Wilcoxon signed-rank test for matched samples. p value less than 0.01/6=0.0016 yields a statistically significant difference and is marked by asterisk.</figDesc><table><row><cell>Pairs</cell><cell></cell><cell cols="3">wins ties losses p value</cell></row><row><cell cols="2">Our relative transformer real music</cell><cell>11</cell><cell>4</cell><cell>15 0.243</cell></row><row><cell cols="2">Our relative transformer Baseline transformer</cell><cell>23</cell><cell>1</cell><cell>6 0.0006*</cell></row><row><cell cols="2">Our relative transformer LSTM</cell><cell>18</cell><cell>1</cell><cell>11 0.204</cell></row><row><cell>Baseline transformer</cell><cell>LSTM</cell><cell>5</cell><cell>3</cell><cell>22 0.006</cell></row><row><cell>Baseline transformer</cell><cell>real music</cell><cell>6</cell><cell>0</cell><cell>24 0.0004*</cell></row><row><cell>LSTM</cell><cell>real music</cell><cell>6</cell><cell>2</cell><cell>22 0.0014</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparing each pair on their aggregates (comparisons with all models) in (wins, ties, losses), using the Mann-Whitney U test for independent samples.</figDesc><table><row><cell>Model</cell><cell>Model</cell><cell></cell><cell>p value</cell></row><row><cell cols="2">Our relative transformer (52, 6, 32) real music</cell><cell>(61, 6, 23)</cell><cell>0.020</cell></row><row><cell cols="3">Our relative transformer (52, 6, 32) Baseline transformer (17, 4, 69)</cell><cell>1.26e-9*</cell></row><row><cell cols="2">Our relative transformer (52, 6, 32) LSTM</cell><cell>(39, 6, 45)</cell><cell>0.018</cell></row><row><cell>Baseline transformer</cell><cell>(17, 4, 69) LSTM</cell><cell>(39, 6, 45)</cell><cell>3.70e-5*</cell></row><row><cell>Baseline transformer</cell><cell>(17, 4, 69) real music</cell><cell cols="2">(61, 6, 23) 6.73e-14*</cell></row><row><cell>LSTM</cell><cell>(39, 6, 45) real music</cell><cell>(61, 6, 23)</cell><cell>4.06e-5*</cell></row><row><cell cols="2">C VISUALIZING SOFTMAX ATTENTION</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We assume that the batch size is 1 here. With a batch size of B, Q would be reshaped to (L, B, D h ) and S rel would be computed with a batch matrix-matrix product.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">J.S. Bach chorales dataset: https://github.com/czhuang/JSB-Chorales-dataset 4 Some earlier papers report frame-wise losses to compare to models such as RNN-RBM which model "chords". Coconet can be evaluated under note-wise or frame-wise losses.5  Piano-e-Competition dataset (competition history): http://www.piano-e-competition.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">COCONET is an instance of OrderlessNADE, an ensemble over orderings. The chronological loss evaluates the model as autoregressive, from left to right. We can also evaluate the model as a mixture, by averaging its losses over multiple random orderings. This is a lower bound on log-likelihood. It is intractable to sample from exactly but can be approximated through Gibbs sampling.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENT</head><p>We thank many colleagues from the Transformer <ref type="bibr" target="#b20">(Vaswani et al., 2017)</ref> and Tensor2Tensor  papers for helping us along the way: Lukasz Kaiser, Ryan Sepassi, Niki Parmar and Llion Jones. Many thanks to Magenta and friends for their support throughout and for many insightful discussions: Jesse Engel, Adam Roberts, Fred Bertsch, Erich Elsen, Sander Dieleman, Sageev Oore, Carey Radebaugh, Natasha Jaques, Daphne Ippolito, Sherol Chan, Vida Vakilotojar, Dustin Tran, Ben Poole and Tim Cooijmans.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Harmonising chorales by probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moray</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao-Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Chia</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding temporal structure in music: Blues improvisation with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing</title>
		<meeting>the 12th IEEE Workshop on Neural Networks for Signal Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Style imitation and chord invention in polyphonic music with exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaëtan</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Sakellariou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Pachet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05152</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepbach: a steerable model for bach chorales generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaëtan</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Pachet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1362" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counterpoint by convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Music Information Retrieval</title>
		<meeting>the International Conference on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imposing higher-level structure in polyphonic music generation using convolutional restricted boltzmann machines and constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Grachten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Creative Music Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bachbot: Automatic composition in the style of bach chorales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feynman</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Masters thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">This time with feeling: Learning expressive musical performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sageev</forename><surname>Oore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03715</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A time-restricted self-attention layer for ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>eddings of the IEEE International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Information processing in dynamical systems: Foundations of harmony theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural autoregressive distribution estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benigno</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7184" to="7220" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>abs/1803.07416</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Generating long-term structure in songs and stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Waite</surname></persName>
		</author>
		<ptr target="https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
