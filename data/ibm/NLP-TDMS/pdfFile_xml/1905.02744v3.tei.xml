<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manikandasriram</forename><surname>Srinivasan Ramanagopal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Vasudevan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson-Roberson</surname></persName>
						</author>
						<title level="a" type="main">LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An accurate depth map of the environment is critical to the safe operation of autonomous robots and vehicles. Currently, either light detection and ranging (LIDAR) or stereo matching algorithms are used to acquire such depth information. However, a high-resolution LIDAR is expensive and produces sparse depth map at large range; stereo matching algorithms are able to generate denser depth maps but are typically less accurate than LIDAR at long range. This paper combines these approaches together to generate highquality dense depth maps. Unlike previous approaches that are trained using ground-truth labels, the proposed model adopts a self-supervised training process. Experiments show that the proposed method is able to generate high-quality dense depth maps and performs robustly even with low-resolution inputs. This shows the potential to reduce the cost by using LIDARs with lower resolution in concert with stereo systems while maintaining high resolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Depth estimation is one of the fundamental tasks in the computer vision field. Being able to acquire accurate depth maps is a first step for many vision tasks, such as 3D object detection <ref type="bibr" target="#b0">[1]</ref>, 3D mapping <ref type="bibr" target="#b1">[2]</ref> and localization <ref type="bibr" target="#b2">[3]</ref>. These tasks underlie a variety of applications including augmented reality, autonomous driving, and robotics.</p><p>One way to construct these depth maps is by solving the stereo matching problem. Traditionally, this has been done by applying window-based methods <ref type="bibr" target="#b3">[4]</ref> or global-optimization methods <ref type="bibr" target="#b4">[5]</ref> to construct a disparity map. Recently, deep convolution neural networks (DCNNs) have been applied to solve the stereo matching problem <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, since they are empirically verified to perform better than traditional approaches on a variety of vision tasks such as image classification <ref type="bibr" target="#b7">[8]</ref> and object detection <ref type="bibr" target="#b8">[9]</ref>. Generally, DCNNs are trained end-to-end with a large amount of ground-truth labels. Using sophisticated network architectures and groundtruth disparity labels, DCNNs have achieved impressive results as shown on KITTI benchmark stereo matching task <ref type="bibr" target="#b9">[10]</ref>. However, there are still some important challenges to be addressed. It is difficult to find correspondence in regions of high specularity, low-texture or under occlusion. In addition, the reliability of the estimated disparity map is range-dependent, which means that disparity estimated on distant regions tend to be less reliable than regions closer to the camera.</p><p>LIDAR, in contrast, is able to accurately measure depth over long ranges. The most common form of LIDAR relies on emitting pulses of light and measuring the time it takes those pulses to reflect off objects in the environment and return to the sensor. The primary limitation of such sensors is their angular resolution. This resolution is determined by the number of beams and is a function of the number of receivers. Since the cost to increase the number of beams is prohibitive, LIDAR typically generates sparse point clouds. This is especially problematic for distant objects. The depth map generated by stereo correspondence is much denser than one obtained from a LIDAR. But small errors in matching lead to large errors in range for distant objects. Previous research has exploited the high angular resolution of RGB images and use it to upsample the sparse depth map <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b13">[14]</ref>. The aim of these approaches is to calculate depth values at the high angular sampling rate of an RGB image by interpolating the empty pixels in the sparse depth map. Typically, ground-truth depth maps are required during training. With this pipeline, recent papers achieve impressive results on the KITTI benchmark depth completion task <ref type="bibr" target="#b14">[15]</ref>. Still, extrapolating in large regions without LIDAR depth remains an open problem. Additionally, collecting a large amount of ground-truth depth labels for training is expensive. This paper generates dense depth maps by utilizing stereo images and sparse depth maps collected from a LIDAR. We demonstrate that the stereo cues are more powerful than monocular color guidance, as they provide direct estimates of depth. The contributions of this paper are:</p><p>1. We propose a model that takes stereo images and LIDAR derived sparse depth maps as inputs and outputs accurate dense depth maps.</p><p>2. The proposed model can be trained in a self-supervised manner, which avoids the cost of collecting a large amount of ground-truth labels.</p><p>3. Experiments show the advantages of stereo cues over monocular images in terms of sparser input depth maps and the potential to reduce the cost by using LIDARs with lower resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Stereo Matching: In recent years, typical stereo matching pipelines have been gradually replaced by end-to-end training DCNNs. In DCNNs, stereo features extracted from a deep Siamese structure are passed into a correlation module. Disparity maps are typically derived from several layers of convolutional matching. For supervised approaches, the models are trained using ground-truth disparity maps. There are many works focusing on efficiently forming a correlation volume <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, designing architectures to extract features <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> and resorting to extra information to refine results <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Currently, the top methods on the KITTI benchmark stereo matching task are able to achieve an error rate around 1.5%. In addition to supervised approaches, unsupervised methods have gained popularity <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. Unsupervised approaches rely on a warping error to provide a training loss. The warping error measures the difference between the warped image from oneside of the stereo pair and the input image from the other side. The warping process is implemented using differential bilinear interpolation <ref type="bibr" target="#b24">[25]</ref>. Still, there is a noticeable gap between the supervised methods and unsupervised methods with respect to performance. Incorporating warping loss in supervised training has been shown to be beneficial over supervised training alone <ref type="bibr" target="#b25">[26]</ref>.</p><p>Depth estimation: Depth estimation is one of the fundamental tasks in computer vision. There is an extensive body of research that has been done on depth estimation from a single color image and the error rate of this task has been reduced significantly in recent years <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b30">[31]</ref>. Models in those approaches learn to exploit the 2D monocular cues to predict depth. However, predicting depth from a monocular image is not a fully-constrained problem. The current results are not accurate enough for practical deployment in applications such as motion planning. The 2D monocular cues, such as occlusion and perspective, are more useful in relative depth prediction <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. The depth completion task is a sub-problem of depth estimation. Instead of knowing nothing about the scene, the depth completion task has strong priors on scene depth. Sparsity-invariant operations have been developed for this task and have proved to be more effective than regular convolutions <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b33">[34]</ref>. With additional color images, the depth completion process can be guided by color information. Recent works <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref> show a performance boost using the color information contained in RGB data. Regular convolutions turn out to be successful in learning depth completion if dense color images are provided <ref type="bibr" target="#b37">[38]</ref>. Additionally, there are other approaches that focus on exploring extra information, such as edge and semantic cues <ref type="bibr" target="#b38">[39]</ref> and temporal frames <ref type="bibr" target="#b11">[12]</ref>. During training, camera poses are estimated between frames and the transformation matrix is used to warp images from one frame to the next <ref type="bibr" target="#b11">[12]</ref>. This warping loss enables self-supervised training. In <ref type="bibr" target="#b39">[40]</ref>, precomputed disparity maps and LIDAR derived depth maps are taken as inputs and end up with high-resolution disparity maps.</p><p>Specifically, our proposed method is intended for the depth completion task. Both <ref type="bibr" target="#b11">[12]</ref> and our proposed method use warping loss to enable the model to be trained in a selfsupervised manner. Unlike <ref type="bibr" target="#b11">[12]</ref>, we explore stereo geometry instead of temporal information. With time-synchronized stereo images, observed position can be attributed to parallax while temporal cues suffer from additional ambiguity as observed position can be attributable to both parallax and object motion. In essence, temporal frames just improve model's ability to exploit monocular cues for training, as they are not used during inference. This paper proposes a model which takes stereo images and sparse depth maps as inputs and outputs dense depth maps. In addition this proposed model, which we call LiStereo (a portmanteau for LIDAR and Stereo Images), can be trained in a selfsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>We present a model which generates dense depth maps by utilizing stereo images and sparse depth maps collected from a LIDAR. The whole architecture of our model is illustrated in <ref type="figure">Figure.</ref> 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture</head><p>There are two branches, the color image branch and the LIDAR branch. To extract features from stereo images, we borrow the Siamese structure in <ref type="bibr" target="#b21">[22]</ref>. The Siamese structure contains a ResNet50 structure <ref type="bibr" target="#b7">[8]</ref> and generates high-level features for both left and right images. Those feature maps are 1/8 of original input image size and are then processed to form a cost volume through the use of a correlation layer, similar to <ref type="bibr" target="#b6">[7]</ref>. The correlation layer correlates features from the left and right view horizontally. In the experiments, we consider a maximum displacement of 24 pixels in the feature map, which corresponds to maximum disparity of 192 pixels in the input image. The PSP module <ref type="bibr" target="#b40">[41]</ref> is used to extract more contextual information. The branch for processing sparse depth maps has the same structure as the Siamese network, but it does not share parameters. Instead of summing them together, we fuse the information by concatenating the cost volume, the context layer, the transformed layer and LIDAR features, and pass them to six residual blocks. The output from this is followed by four upsampling blocks in order to output depth maps at the same resolution as the input images. Injecting skip layers from the left image branch of the Siamese structure into later layers is helpful to maintain high resolution information from the color image. The output layer has 193 channels corresponding to the maximum disparity of 192 pixels, with a channel for zero disparity. Soft argmax <ref type="bibr" target="#b16">[17]</ref> is used to generate the final disparity maps, and the absolute depth is then computed given the extrinsic parameters of the stereo cameras.</p><p>In detail, the convolution layer in this paper refers to a convolution layer followed by a batch normalization layer <ref type="bibr" target="#b41">[42]</ref> and a leaky ReLU rectifier <ref type="bibr" target="#b42">[43]</ref>. The output layers of the Siamese structure, LIDAR branch and final output layer contain a regular convolution only. The size of all kernels is 3 except for the first two convolution layers in the Siamese structure and LIDAR branch, which are 7 and 5 respectively. The residual block is three-layer deep and the upsampling block consists of a bilinear interpolation layer, a regular convolution layer, a batch normalization layer and leaky  ReLU activation. It turns out that such upsampling blocks are able to avoid checkerboard artifacts generated by transposed convolution <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss components</head><p>Most current approaches for depth estimation are trained using ground-truth depth maps. However, the acquisition of ground-truth depth maps in the real world is difficult. In the KITTI dataset, the ground-truth depth map is semidense, covering 30% of the image, which is generated by enforcing consistency between laser scans and stereo reconstruction <ref type="bibr" target="#b14">[15]</ref>. Our proposed training process is selfsupervised, so no ground-truth depth map is required. The total loss consists of three components:</p><formula xml:id="formula_0">Loss = α × L depth + β × L photometric + γ × L smooth (1) • Sparse Depth Loss (L depth ):</formula><p>The goal of depth completion is to fill in the depth on pixels where there is no valid depth. The pixels where the input sparse depth map has a valid value should remain unchanged during the process. The L depth penalizes the difference on pixels with known depth before and after the depth completion task. The sparse depth supervision loss is defined as follows:</p><formula xml:id="formula_1">L depth (pred, sparse) = ||(pred − sparse) d&gt;0 || (2)</formula><p>where pred is the predicted dense depth map and sparse is the input sparse depth map. In the sparse depth map, pixels greater than zero are valid. During the experiments, mean absolute error is used. sparse is replaced with ground-truth depth maps if they are given. • Photometric Loss (L photometric ): Let I L and I R be the left and right images. Given the dense left disparity map D L , any pixel in the left image has a corresponding pixel in the right image. Therefore, we are able to create a warping function F (I, D) to synthesize the other camera's view using bilinear sampling, which is differentiable. I L = F (I R , D L ). We then penalize the visual difference between the input image and warped image. For the left side, the photometric loss is defined as follows:</p><formula xml:id="formula_2">L photometric (I L , I L ) = λ 1 S(I L , I L ) + λ 2 |I L − I L | (3)</formula><p>where S() is the structure similarity index <ref type="bibr" target="#b44">[45]</ref>, and we set λ 1 = 0.85 and λ 2 = 0.2 during experiments. • Smoothness Loss (L smooth ): Minimizing photometric loss tends to introduce high-frequency noise, which makes the depth map non-smooth and incorrect. The smoothness loss helps alleviate this problem. This smoothness loss computes the sum of the weighted second derivative of the disparity map and depth map, and the weight is the exponential of the second derivative of the input image. For the left side, smoothness loss on depth maps is defined as follows:</p><formula xml:id="formula_3">L smooth (I L , D L ) = 1 N (|∇ 2 x D L |e −|∇ 2 x I L | + |∇ 2 y D L |e −|∇ 2 y I L | )<label>(4)</label></formula><p>where N is number of pixels, ∇ 2 x and ∇ 2 y are second derivatives along the X and Y axes. We compute smoothness loss on both disparity maps and depth maps. Since disparity is inversely proportional to depth, those two smoothness loss terms are giving importance to both close and distant objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>In this section, we explain our implementation details and present qualitative and quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The depth completion task on the KITTI dataset <ref type="bibr" target="#b14">[15]</ref> provides stereo images, sparse depth maps and semi-dense ground-truth depth maps in training and validation sets, but only monocular images in the test set. The sparse depth map is generated by projecting 3D LIDAR point clouds (collected from Velodyne HDL-64E) onto the image plane. Ground-truth depth maps are generated by accumulating 11 frames of point clouds from the LIDAR. This accumulating process only produces semi-dense depth maps, and valid depth pixels only exist for the bottom part of the images. The dataset consists of 42,949 pairs of training images and 3,426 pairs of validation images with both left and right depth maps, including 1,000 cropped images with a fixed size. For convenience, we stick to take left sparse depth maps as inputs, so only half of the dataset is used.</p><p>We are not able to submit results on the KITTI benchmark, since the test set does not contain stereo images. Still, we report rough evaluation results on the KITTI dataset using part of the validation set. We randomly split the validation set into two sub-sets, 2,426 pairs of stereo images for testing and 1,000 for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The model is implemented in TensorFlow <ref type="bibr" target="#b45">[46]</ref>. Input stereo images are normalized to values ranging from -1 to 1. Depth inversion introduced by <ref type="bibr" target="#b46">[47]</ref> is conducted on input sparse depth maps since it creates a buffer gap between valid and empty pixel values. All parameters are randomly initialized using truncated zero-mean Gaussian distribution. During the training, images are randomly cropped down to 256x1024 patches on the bottom of images before being fed into the network. We use the Adam optimizer <ref type="bibr" target="#b47">[48]</ref> with β 1 = 0.9, β 2 = 0.999 and = 1e−8. We have a batch size of 10. The learning rate starts at 1e-4 and reduces to 5e-5 after training for 6 epochs. A total of 12 epochs are run. All experiments are run on two NVIDIA Titan-X GPUs. No data augmentation is performed in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation</head><p>We evaluate the proposed method on KITTI dataset. The main metric we use in this paper is root-mean-square error (RMSE in mm), which computes the L2 norm difference on all pixels where the ground-truth depth map is valid. We also report results of other metrics, such as mean-average error (MAE in mm), inverse mean-average error (iMAE in 1/km) and inverse-root-mean-square error (iRMSE in 1/km).</p><p>The evaluation results are listed in <ref type="table" target="#tab_2">Table I and Table II</ref> for supervised and self-supervised approaches respectively. Because no stereo images are provided in test set of KITTI dataset, we devise a strategy to roughly compare with approaches of the submitted results on the KITTI benchmark depth completion task leaderboard. In contrast to LiStereo, we have built a new model and call it LiMono, which only takes monocular images as inputs. It serves as the baseline. The LiMono is created by removing the right branch of the Siamese structure as shown in the <ref type="figure" target="#fig_0">Figure 1</ref>, and it is trained using ground-truth label. We submitted results of LiMono to the KITTI benchmark. The LiMono is able to achieve state-of-the-art performance. We also report results of both the LiMono and LiStereo in the table using the test set described in Section 4.1. With stereo information, LiStereo trained with ground-truth labels has a much lower RMSE than the LiMono does. This demonstrates that stereo cues have advantage over monocular cues and are able to improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study on Weights of Loss</head><p>The photometric loss is key for the model to learn to fill depth values in empty pixels and to be trained in a self-supervised manner. Here, we study the influence of the weight of the photometric loss on final results. We experiment with setting the photometric loss weight β between 0 and 2, and the results are shown in <ref type="table" target="#tab_2">Table III</ref>. Numbers are reported on the validation set described in Section 4.1. This table demonstrates that photometric loss does help complete depth on sparse input, reducing RSME from 1970mm to 1277mm with β = 0 and β = 0.5 respectively. However, too high a weight on photometric loss decreases performance. This is most likely due to the high weight on regions in which the photometric loss is poorly defined like those with low texture or specular reflections. So we set β = 0.5 throughout experiments. Besides, we set γ = 0.001 for models which are trained using ground-truth labels, since smoothness loss will over-smooth edges between objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study on Sparsity-invariant Convolutions</head><p>The sparsity-invariant operations are designed for extracting features from sparse input data, so they are suitable for the depth completion task. In this study we replace all convolution layers within LIDAR branch with the sparsityinvariant convolution introduced by <ref type="bibr" target="#b14">[15]</ref> and also remove all batch normalization layers. Quantitative results are shown in <ref type="table" target="#tab_2">Table IV</ref>. Numbers are reported on the test set described in Section 4.1. In the table, 'conv' refers to model using regular convolution layer. 'sparse conv' refers to model in which regular convolution layers are replaced with sparsityinvariant convolution layers in LIDAR branch. 'with GT' refers to model trained using ground-truth labels. 'w/o GT' refers to model trained in a self-supervised manner. It shows slightly better results of regular convolution than sparsityinvariant convolution in terms of RMSE. So the main results of the paper employ regular convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Analysis on sparsity of inputs during training</head><p>In this section, we investigate the influence of different sparsity of input depth maps by uniformly subsampling valid pixels in the input depth maps. This simulates situations where low-resolution LIDARs are used. We do analysis on both supervised and self-supervised versions of the proposed model. Qualitative results of self-supervised model are shown in <ref type="figure">Figure 2</ref>. During this study, we choose the self-supervised model (α = 1, β = 0.5 and γ = 0.01) and the supervised model      (α = 1, β = 0.5 and γ = 0.001). Models are trained and evaluated using different levels of sparsity of input depth maps and we report quantitative results in the We also compare our model with both the supervised and self-supervised versions of Sparse2Dense <ref type="bibr" target="#b11">[12]</ref>, which were trained using monocular images and temporal frames. Results are shown in <ref type="figure">Figure 3 (a)</ref>. Stereo cues turn out to be more robust to sparse input than monocular information in both supervised and self-supervised training process. Note the flatter error curve of for our model compared with that of Sparse2Dense. Especially, with only 1 percent density of the original sparse map, the RMSE of our self-supervised model is 3177.83mm, which is much smaller than around 11000mm achieved by self-supervised version of Sparse2Dense.</p><formula xml:id="formula_4">α = 1 γ = 0.01 β = 0.0 α = 1 γ = 0.01 β = 0.2 α = 1 γ = 0.01 β = 0.5 α = 1 γ = 0.01 β = 0.8 α = 1 γ = 0.01 β = 1 α = 1.0 γ = 0.01 β = 1.5 α = 1 γ = 0.01 β = 2.0 α = 1 γ = 0.001 β = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Analysis on sparsity of inputs during inference</head><p>We conduct an analysis on different sparsity levels of input depth maps during inference. In this case, the model is trained using original sparse depth maps but is provided different input sparsity levels during inference. This simulates situations where a primary model, which is trained <ref type="figure">Fig. 2</ref>: Qualitative results on different levels of input sparsity. The model is trained in a self-supervised manner. From left to right: corresponding left color image, predicted dense depth map from original sparse depth map, predicted dense depth map from input of sparsity level 0.1 and predicted dense depth map from input of sparsity level of 0.01. More details are shown with denser inputs. <ref type="bibr">Level</ref>    <ref type="bibr" target="#b11">[12]</ref>. 'w/o GT' means that the model is trained in a self-supervised manner and 'with GT' means that the model is trained using ground-truth label. (a) Errors during the training. Results are evaluated on split validation set introduced in Section 4.1. For each data point in the figure, the model is trained and evaluated using certain input sparsity specified by X axis; (b) Errors during the inference. Models are evaluated on the split validation set introduced in Section 4.1. We train the model using original depth maps but feed it with different sparsity levels of input depth maps during inference. Results of Sparse2dense is acquired by using the published pretrained model by <ref type="bibr" target="#b11">[12]</ref>. Both figures show that our proposed model is robust to sparse input and has the potential to take depth maps generated from low-resolution LIDARs.</p><p>and fed with dense data, is deployed to other applications in which only the low-resolution LIDAR is provided. The results is shown in the <ref type="figure">Figure 3 (b)</ref>. Results are reported on the validation set described in Section 4.1. We also report results of Sparse2Dense and LiMono. The LiMono model turns out to have the highest and steepest error curve, which means that training with temporal information used in Sparse2Dense or stereo cues used in our proposed model enable the model to be better to exploit information in the color image. Compared with Sparse2Dense, LiStereo trained in either supervised or self-supervised manner is more robust in terms of input sparsity, which means stereo images are able to provide more information than monocular images, even if multiple frames of monocular images are used during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we propose a model to upsample sparse depth map generated from LIDAR using stereo cues. Depth maps from LIDAR are accurate but sparse and stereo estimation generates less reliable depth maps which are dense. Our proposed method combines these two approaches together making use of the advantages of both to generate accurate high-resolution depth maps, which is important for autonomous driving and other robotic applications. Compared to using monocular images as the guidance, stereo cues turn out to be more robust to highly sparse inputs and so they reduces LIDAR resolution requirement. Thus, we can use the LIDAR with lower resolution and still produce comparable results as a high-resolution LIDAR. The highresolution LIDAR (Velodyne 128) is more than ten times of the cost of the low-resolution LIDAR (Velodyne 16) and stereo systems. So the proposed method would offer massive cost reduction for high resolution depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported by a grant from Ford Motor Company via the Ford-UM Alliance under award N022884.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Architecture of our proposed model. The pipeline of our model consists of following parts. (a) Inputs: rectified stereo images and corresponding left sparse depth map. (b) Feature extraction: features are extracted from stereo images and sparse depth map. The correlation layer computes correlation from one side of view to the other. Features from left color image are processed by transform layer to prepare for later sensor fusion. The PSP module is used to incorporate more contextual information. (c) Fusion: correlation information and features from the depth map are fused by concatenation. (d) Estimation: fused information is processed to perform depth estimation. (e) Output: a dense depth map and disparity map are generated. The output depth map is colorized for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Fusion Dense Depth MapPSP Left Image Right Image Left Sparse Depth Map Transform Layer Output Estimation Feature Extraction Input Images</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Convolution Layer</cell></row><row><cell></cell><cell>Residual Block</cell></row><row><cell></cell><cell>Correlation Layer</cell></row><row><cell></cell><cell>Context Layer</cell></row><row><cell></cell><cell>Upsampleing Block</cell></row><row><cell></cell><cell>Disparity map</cell></row><row><cell>PSP</cell><cell>PSP Modual</cell></row><row><cell></cell><cell>Skip Connection</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Comparison with other supervised methods on KITTI benchmark depth completion task. 'LiStereo' refers to proposed model taking stereo images as inputs. 'LiMono' refers to the model taking monocular images. 'test-set' refers to results that are reported on KITTI test set. 'split-test-set' refers to results that are reported on split test set from KITTI validation set introduced in Section 4.1. 'with GT' refers to training model using ground-truth label. With stereo information, our proposed method outperforms models with only monocular information.</figDesc><table><row><cell>Method</cell><cell>iRMSE(1/km)</cell><cell>iMAE(1/km)</cell><cell>RMSE(mm)</cell><cell>MAE(mm)</cell></row><row><cell>Sparse2dense(split-test-set + w/o GT) [12]</cell><cell>4.08</cell><cell>1.61</cell><cell>1301.05</cell><cell>352.22</cell></row><row><cell>Sparse2dense(test-set + w/o GT) [12]</cell><cell>4.07</cell><cell>1.57</cell><cell>1299.85</cell><cell>350.32</cell></row><row><cell>LiStereo(split-test-set + w/o GT)</cell><cell>3.83</cell><cell>1.32</cell><cell>1278.87</cell><cell>326.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc></figDesc><table /><note>Comparison with other self-supervised methods on KITTI benchmark depth completion task. 'LiStereo' refers to proposed model taking stereo images as inputs. 'test-set' refers to results that are reported on KITTI test set. 'split-test-set' refers to results that are reported on split test set from KITTI validation set introduced in Section 4.1. 'w/o GT' refers to training model in self-supervised training manner. With stereo information, our proposed method outperforms Sparse2Dense which is trained using temporal frames. Similar results of Sparse2Dense on split-test-set and test-set justify our strategy of split validation set introduced in Section 4.1.Loss Weights</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Ablation Study on Weights of Loss. Photometric loss does help complete depth on sparse input, reducing RMSE from 1970mm to 1277mm with β = 0 and β = 0.5 respectively. However, too much weight on photometric loss makes the results worse. The model with α = 1, γ = 0.01 and β = 0.5 achieves the lowest RMSE.</figDesc><table><row><cell>Method</cell><cell>iRMSE(1/km)</cell><cell>iMAE(1/km)</cell><cell>RMSE(mm)</cell><cell>MAE(mm)</cell></row><row><cell>LiStereo (sparse conv) + w/o GT</cell><cell>4.12</cell><cell>1.56</cell><cell>1379.31</cell><cell>355.42</cell></row><row><cell>LiStereo (conv) + w/o GT</cell><cell>3.83</cell><cell>1.32</cell><cell>1278.87</cell><cell>326.10</cell></row><row><cell>LiStereo (sparse conv) + with GT</cell><cell>2.22</cell><cell>1.02</cell><cell>894.56</cell><cell>268.69</cell></row><row><cell>LiStereo (conv) + with GT</cell><cell>2.19</cell><cell>1.10</cell><cell>832.16</cell><cell>283.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation study on regular convolution and sparsity-invariant convolution. All models take stereo images as inputs. 'conv' refers to model using regular convolution layer. 'sparse conv' refers to model in which regular convolution layers are replaced with sparsity-invariant convolution layers in LIDAR branch. 'with GT' refers to model trained using ground-truth label. 'w/o GT' refers to model trained in a self-supervised manner. Results are reported on split test set introduced in section 4.1. It shows slightly better results of regular convolution than sparsity-invariant convolution in terms of RMSE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table V .</head><label>V</label><figDesc>Level of sparsity (LoS) in the table means the ratio of valid pixels sampled from the original depth map, and RMSEs are reported. As expected Level of sparsity (LoS) in the table means the ratio of valid pixels sampled from the original depth map.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>Errors on different levels of input sparsity during training. RMSE (mm) from the supervised and the selfsupervised models are reported. Level of sparsity (LoS) in the table means the ratio of valid pixels sampled from the original depth map. Errors on different levels of input sparsity during training and inference. The X axis represents level of input sparity, and the Y axis represents RMSE (mm). 'LiStereo' refers to our proposed model. 'LiMono' refers to the model taking monocular images as inputs. 'Sparse2Dense' refers to model proposed by</figDesc><table><row><cell></cell><cell>9000 14000 11000</cell><cell></cell><cell cols="2">Sparse2Dense (w/o GT) Sparse2Dense (with GT) LiStereo (w/o GT)</cell><cell>3500</cell><cell></cell><cell></cell><cell cols="2">Sparse2Dense (w/o GT) Sparse2Dense (with GT) LiStereo (w/o GT)</cell></row><row><cell></cell><cell>7000</cell><cell></cell><cell cols="2">LiStereo (with GT)</cell><cell>3000</cell><cell></cell><cell></cell><cell>LiStereo (with GT) LiMono (with GT)</cell></row><row><cell>RMSE (mm)</cell><cell>3000 4000 5000</cell><cell></cell><cell></cell><cell>RMSE (mm)</cell><cell>2000 2500</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1500</cell><cell></cell><cell></cell><cell></cell><cell>1500</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell>1000</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>800</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.01</cell><cell>0.1</cell><cell>0.3</cell><cell>0.6 0.8 1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell></cell><cell cols="3">Levels of input sparsity during training</cell><cell></cell><cell cols="4">Levels of input sparsity during inference</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell></row><row><cell>Fig. 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual-lidar odometry and mapping: Low-drift, robust, and fast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2174" to="2181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual localization within lidar maps for automated urban driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Wolcott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="176" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stereo matching via selective multiple windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adhyapak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nadin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="25" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="328" to="341" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-32</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image guided depth upsampling using anisotropic total generalized variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rüther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cavalheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05356</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5695" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="887" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segstereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="636" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dispsegnet: Leveraging semantics for end-to-end learning of disparity estimation from stereo imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1162" to="1169" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the importance of stereo for accurate depth estimation: An efficient semi-supervised deep neural network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smolyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamenev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1007" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3-d depth reconstruction from a single still image</title>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep attention-based classification network for robust depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular relative depth perception with web stereo data supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hms-net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08685</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimating depth from rgb and sparse sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="167" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00761</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sparse and dense data with cnns: Depth completion and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wirbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nashashibi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantically guided depth upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">High-precision depth estimation with the 3d lidar and stereo fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2156" to="2163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deconvolution and checkerboard artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 15th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep Convolutional Compressed Sensing for LiDAR Depth Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Chodosh Chaoyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
