<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distribution Matching for Crowd Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Wang</surname></persName>
							<email>boywang@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huidong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
							<email>samaras@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
							<email>minhhoai@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11790</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distribution Matching for Crowd Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* indicates equal contribution</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In crowd counting, each training image contains multiple people, where each person is annotated by a dot. Existing crowd counting methods need to use a Gaussian to smooth each annotated dot or to estimate the likelihood of every pixel given the annotated point. In this paper, we show that imposing Gaussians to annotations hurts generalization performance. Instead, we propose to use Distribution Matching for crowd COUNTing (DM-Count). In DM-Count, we use Optimal Transport (OT) to measure the similarity between the normalized predicted density map and the normalized ground truth density map. To stabilize OT computation, we include a Total Variation loss in our model. We show that the generalization error bound of DM-Count is tighter than that of the Gaussian smoothed methods. In terms of Mean Absolute Error, DM-Count outperforms the previous state-of-the-art methods by a large margin on two large-scale counting datasets, UCF-QNRF and NWPU, and achieves the state-of-the-art results on the ShanghaiTech and UCF-CC50 datasets. DM-Count reduced the error of the state-of-the-art published result by approximately 16%. Code is available at https://github.com/cvlab-stonybrook/DM-Count.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image-based crowd counting is an important research problem with various applications in many domains including journalism and surveillance. Current state-of-the-art methods <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref> treat crowd counting as a density map estimation problem, where a deep neural network first produces a 2D crowd density map for a given input image and subsequently estimates the total size of the crowd by summing the density values across all spatial locations of the density map. For images of large crowds, this density map estimation approach has been shown to be more robust than the detection-then-counting approach <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b11">12]</ref> because the former is less sensitive to occlusion and it does not need to commit to binarized decisions at an early stage.</p><p>A crucial step in the development of a density map estimation method is the training of a deep neural network that maps from an input image to the corresponding annotated density map. In all existing crowd counting datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b50">51]</ref>, the annotated density map for each training image is a sparse binary mask, where each individual person is marked with a single dot on their head or forehead. The spatial extent of each person is not provided, due to the laborious effort needed for delineating the spatial extent, especially when there is too much occlusion ambiguity. Given training images with dot annotation, training the density map estimation network is equivalent to optimizing the parameters of the network to minimize a differentiable loss function that measures the discrepancy between the predicted density map and the dot-annotation map. Notably, the former is a dense real-value matrix, while the later is a sparse binary matrix. Given the sparsity of the dots, a function that is defined based on the pixel-wise difference between the annotated and predicted density maps is hard to train because the reconstruction loss is heavily unbalanced between the 0s and 1s in the sparse binary matrix. One approach to alleviate this problem is to turn each annotated dot into a Gaussian blob such that the ground truth is more balanced and thus the network is easier to train. Almost all prior crowd density map estimation methods <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref> have followed this convention. Unfortunately, the performance of the resulting network is highly dependent on the quality of this "pseudo ground truth", but it is not trivial to set the right widths for the Gaussian blobs given huge variation in the sizes and shapes of people in a perspective image of a crowded scene.</p><p>Recently, Ma et al. <ref type="bibr" target="#b30">[31]</ref> proposed a Bayesian loss to measure the discrepancy between the predicted and the annotated density maps. This method transforms a binary ground truth annotation map into N "smoothed ground truth" density maps, where N is the count number. Each pixel value of a smoothed ground truth density map is the posterior probability of the corresponding annotation dot given the location of that pixel. Empirically, this method has been shown to outperform other aforementioned approaches <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4]</ref>. However, there are two major problems with this loss function. First, it also requires a Gaussian kernel to construct the likelihood function for each annotated dot, which involves setting the kernel width. Second, this loss corresponds to an underdetermined system of equations with infinitely many solutions. The loss can be 0 for many density maps that are not similar to the ground truth density map. As a consequence, using this loss for training can lead to a predicted density map that is very different from the ground truth density map.</p><p>In this paper, we address the shortcomings in existing approaches with the following contributions.</p><p>• We theoretically and empirically show that imposing Gaussians to annotations will hurt the generalization performance of a crowd counting network. • We propose DM-Count, a method that performs Distribution Matching for crowd COUNTing.</p><p>Unlike previous works, DM-Count does not need any Gaussian smoothing ground truth annotations. Instead, we use Optimal Transport (OT) to measure the similarity between the normalized predicted density map and the normalized ground truth density map. To stabilize the OT computation, we further add a Total Variation (TV) loss. • We present the generalization error bounds for the counting loss, OT loss, TV loss and the overall loss in our method. All the bounds are tighter than those of the Gaussian smoothed methods. • Empirically, our method improved the state-of-the-art by a large margin on four challenging crowd counting datasets: UCF-QNRF, NWPU, ShanghaiTech, and UCF-CC50. Notably, our method reduced the published state-of-the-art MAE on the NWPU dataset by approximately 16%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Crowd Counting Methods</head><p>Crowd counting methods can be divided into three categories: detection-then-count, direct count regression, and density map estimation. Early methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b11">12]</ref> detect people, heads, or upper bodies in the image. However, accurate detection is difficult for dense crowds. Besides, it also requires bounding box annotation, which is a laborious and ambiguous process due to heavy occlusion. Later methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b6">7]</ref> avoid the detection problem and directly learn to regress the count from a feature vector. But their results are less interpretable and the dot annotation maps are underutilized. Most recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b41">42]</ref> are based on density map estimation, which has been shown to be more robust than detection-then-count and count regression approaches.</p><p>Density map estimation methods usually define the training loss based on the pixel-wise difference between the Gaussian smoothed density map and the predicted density map. Instead of using a single kernel width to smooth the dot annotation, <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b46">47]</ref> used adaptive kernel width. The kernel width is selected based on the distance to an annotated dot's nearest neighbors. Specifically, <ref type="bibr" target="#b14">[15]</ref> generated multiple smoothed ground truth density maps on different density levels. The final loss combines the reconstruction errors from multiple density levels. However, these methods assume the crowd is evenly distributed; in reality crowd distribution is quite irregular. The Bayesian loss method <ref type="bibr" target="#b30">[31]</ref> uses a Gaussian to construct a likelihood function for each annotated dot. However, it may not predict a correct density because the loss is underdetermined. Detailed analysis can be found in Sec 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Optimal Transport</head><p>We propose a novel loss function based on Optimal Transport (OT) <ref type="bibr" target="#b45">[46]</ref>. For a better understanding of the proposed method, we briefly review the Monge-Kantarovich OT formulation in this section.</p><p>Optimal Transport refers to the optimal cost to transform one probability distribution to another. Let</p><formula xml:id="formula_0">X = {x i |x i ∈ R d } n i=1</formula><p>and Y = {y j |y j ∈ R d } n j=1 be two sets of points on d-dimensional vector space. Let µ and ν be two probability measures defined on X and Y, respectively; µ, ν ∈ R n + and 1 T n µ = 1 T n ν = 1 (1 n is a n-dimensional vector of all ones). Let c : X × Y → R + be the cost function for moving from a point in X to a point in Y, and C be the corresponding n×n cost matrix for the two sets of points: C ij = c(x i , y j ). Let Γ be the set of all possible ways to transport probability mass from X to Y: Γ = {γ ∈ R n×n + : γ1 = µ, γ T 1 = ν}. The Monge-Kantorovich's Optimal Transport (OT) cost between µ and ν is defined as:</p><formula xml:id="formula_1">W(µ, ν) = min γ∈Γ C, γ .<label>(1)</label></formula><p>Intuitively, if the probability distribution µ is viewed as a unit amount of "dirt" piled on X and ν a unit amount of dirt piled on Y, the OT cost is the minimum "cost" of turning one pile into the other. The OT cost is a principal measurement to quantify the dissimilarity between two probability distributions, also taking into account the distance between "dirt" locations.</p><p>The OT cost can also be computed via the dual formulation:</p><formula xml:id="formula_2">W(µ, ν) = max α,β∈R n α, µ + β, ν , s.t. α i + β j ≤ c(x i , y j ), ∀i, j.<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DM-Count: Distribution Matching for Crowd Counting</head><p>We consider crowd counting as a distribution matching problem. In this section, we propose DM-Count: Distribution matching for crowd counting. A network for crowd counting inputs an image and outputs a map of density values. The final count estimate can be obtained by summing over the predicted density map. DM-Count is agnostic to different network architectures. In our experiments, we use the same network as in the Bayesian loss paper <ref type="bibr" target="#b30">[31]</ref>. Unlike all previous density map estimation methods which need to use Gaussians to smooth ground truth annotations, DM-Count does not need any Gaussian to preprocess ground truth annotations.</p><p>Let z ∈ R n + denote the vectorized binary map for dot-annotation andẑ ∈ R n + the vectorized predicted density map returned by a neural network. By viewing z andẑ as unnormalized density functions, we formulate the loss function in DM-Count using three terms: the counting loss, the OT loss, and the Total Variation (TV) loss. The first term measures the difference between the total masses, while the last two measures the difference between the distributions of the normalized density functions.</p><p>The Counting Loss. Let · 1 denote the L 1 norm of a vector, and so z 1 , ẑ 1 are the ground truth and predicted counts respectively. The goal of crowd counting is to make ẑ 1 as close as possible to z 1 , and the counting loss is defined as the absolute difference between them:</p><formula xml:id="formula_3">C (z,ẑ) = | z 1 − ẑ 1 |.<label>(3)</label></formula><p>The Optimal Transport Loss. Both z andẑ are unnormalized density functions, but we can turn them into probability density functions (pdfs) by dividing them by the their respective total mass. Apart from OT, the Kullback-Leibler divergence and Jensen-Shannon divergence can also measure the similarity between two pdfs. However, these measurements do not provide valid gradients to train a network if the source distribution does not overlap with the target distribution <ref type="bibr" target="#b31">[32]</ref>. Therefore, we propose the use of OT in this work. We define the OT loss as follows:</p><formula xml:id="formula_4">OT (z,ẑ) = W z z 1 ,ẑ ẑ 1 = α * , z z 1 + β * ,ẑ ẑ 1 ,<label>(4)</label></formula><p>where α * and β * are the solutions of Problem <ref type="bibr" target="#b1">(2)</ref>. We use the quadratic transport cost, i.e., c (z(i),ẑ(j)) = z(i) −ẑ(j) 2 2 , where z(i) andẑ(j) are 2D coordinates of locations i and j, respectively. To avoid the division-by-zero error, we add a machine precision to the denominator.</p><p>Since the entries inẑ are non-negative, the gradient of Eq. (4) with respect toẑ is:</p><formula xml:id="formula_5">∂ OT (z,ẑ) ∂ẑ = β * ẑ 1 − β * ,ẑ ẑ 2 1 .<label>(5)</label></formula><p>This gradient can be back-propagated to learn the parameters of the density estimation network.</p><p>Total Variation Loss. In each training iteration, we use the Sinkhorn algorithm <ref type="bibr" target="#b33">[34]</ref> to approximate α * and β * . The time complexity is O(n 2 log n/ 2 ) <ref type="bibr" target="#b8">[9]</ref>, where is the desired optimality gap, i.e., the upper bound for the difference between the returned objective and the optimal objective. When optimizing with the Sinkhorn algorithm, the objective decreases dramatically at the beginning but only converges slowly to the optimal objective in later iterations. In practice, we set the maximum number of iterations, and the Sinkhorn algorithm only returns an approximate solution. As a result, when we optimize the OT loss with the Sinkhorn algorithm, the predicted density map ends up close to the ground truth density map, but not exactly the same. The OT loss will approximate well the dense areas of the crowd, but the approximation might be poorer for the low density areas of the crowd. To address this issue, we additionally use the Total Variation (TV) loss, defined as 1 :</p><formula xml:id="formula_6">T V (z,ẑ) = z z 1 −ẑ ẑ 1 T V = 1 2 z z 1 −ẑ ẑ 1 1 .<label>(6)</label></formula><p>The TV loss will also increase the stability of the training procedure. Optimizing the OT loss with the Sinkhorn algorithm is a min-max saddle point optimization procedure, which is similar to GAN optimization <ref type="bibr" target="#b12">[13]</ref>. The stability of GAN training can be increased by adding a reconstruction loss, as shown in the Pix2Pix GAN <ref type="bibr" target="#b15">[16]</ref>. To this end, the TV loss is similar to the reconstruction loss, and also increases the stability of the training procedure.</p><p>The gradient of the TV loss with respect to the predicted density mapẑ is:</p><formula xml:id="formula_7">∂ T V (z,ẑ) ∂ẑ = − 1 2 sign(v) ẑ 1 − sign(v),ẑ ẑ 2 1 ,<label>(7)</label></formula><p>where v = z/ z 1 −ẑ/ ẑ 1 , and sign(·) is the Sign function on each element of a vector.</p><p>The Overall Objective. The overall loss function is the combination of the counting loss, the OT loss, and the TV loss:</p><formula xml:id="formula_8">(z,ẑ) = C (z,ẑ) + λ 1 OT (z,ẑ) + λ 2 z 1 T V (z,ẑ),<label>(8)</label></formula><p>where λ 1 and λ 2 are tunable hyper-parameters for the OT and TV losses. To ensure that the TV loss has the same scale as the counting loss, we multiply this loss term with the total count.</p><p>Given K training images {I k } K k=1 with corresponding dot annotation maps {z k } K k=1 , we will learn a deep neural network f for density map estimation by minimizing: L(f ) = 1 K K k=1 (z k , f (I k )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generalization Bounds and Theoretical Analysis</head><p>In this section, we analyze the theoretical properties of the Gaussian smoothed methods, the Bayesian loss, and the proposed DM-Count. The proofs of the theorems in this section can be found in the supplementary material. First, we introduce some notations below.</p><p>Let I denote the set of images and Z the set of dot annotation maps. Let D = {(I, z)} be the joint distribution of crowd images and corresponding dot annotation maps. Let H be a hypothesis space. Each h ∈ H maps from I ∈ I to each dimension of z ∈ Z. Let F = H × · · · × H (n times) be the mapping space. Each f ∈ F maps I ∈ I to z ∈ Z. Let t be the Gaussian smoothed density map of each z ∈ D, and letD = {(I, t)} be the joint distribution of (I, t). </p><formula xml:id="formula_9">Let S = {(I k , z k )} K k=1 , andS = {(I k , t k )} K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generalization Error Bounds of Gaussian Smoothed Methods</head><p>Many existing methods (e.g., <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref>) use Gaussian-smoothed annotation maps for training. Below we give generalization error bounds when using the 1 loss on the density maps. <ref type="bibr" target="#b0">1</ref> In the training loss context, Total Variation refers to the total variation distance of two probability measures. A formal definition can be found in <ref type="bibr">[</ref> Theorem 1 Assume that ∀f ∈ F and (I, t) ∼D, we have (t, f (I)) ≤ B. Then, for any 0 &lt; δ &lt; 1, with probability of at least 1 − δ, a) the upper bound of the generalization error is</p><formula xml:id="formula_10">R(D, fS 1 , 1 ) ≤ R(D, fD 1 , 1 ) + 2nR S (H) + 5B 2 log (8/δ)/K + E (I,z)∼D z − t 1 , b) the lower bound of the generalization error is R(D, fS 1 , 1 ) ≥ E (I,z)∼D z − t 1 − R(D, fS 1 , 1 ) .</formula><p>In this theorem, as the number of samples K grows to infinity, 2nR S (H) and 5B 2 log (8/δ)/K decrease to 0. Theorem 1.a) shows that the upper bound (worst case) of the expected risk R(D, fS 1 , 1 ), which is evaluated on real ground truth data using an empirical minimizer trained on the Gaussian smoothed ground truth, does not exceed R(D, fD 1 , 1 ) + E (I,z)∼D z − t 1 given sufficient training data. Theorem 1.b) shows that the lower bound (best case) of R(D, fS 1 , 1 ) is not smaller than |E (I,z)∼D z − t 1 − R(D, fS 1 , 1 )|. This means that if R(D, fS 1 , 1 ) ≤ E (I,z)∼D z − t 1 , then the smaller R(D, fS 1 , 1 ) is, the larger the expected risk R(D, fS 1 , 1 ) will be. In other words, the better a good model fS 1 performs on the Gaussian smoothed ground truthD, the poorer it generalizes on the real ground truth D. Furthermore, as long as R(D, fS 1 , 1 ) = E (I,z)∼D z − t 1 , we have R(D, fS 1 , 1 ) &gt; 0. R(D, fS 1 , 1 ) can be as large as E (I,z)∼D z − t 1 when R(D, fS 1 , 1 ) = 0. This is undesirable because we want the risk R(D, fS 1 , 1 ) evaluated on the real ground truth to be 0 as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Underdetermined Bayesian Loss</head><p>The Bayesian Loss <ref type="bibr" target="#b30">[31]</ref> is:</p><formula xml:id="formula_11">Bayesian (z,ẑ) = N i=1 |1 − p i ,ẑ |, where p i = N (q i , σ 2 1 2×2 ) N i=1 N (q i , σ 2 1 2×2 ) ,<label>(9)</label></formula><p>and N is number of people of z, and N (q i , σ 2 1 2×2 ) is a Gaussian distribution centered at q i with variance σ 2 1 2×2 . q i is the i th annotated dot in z. The dimension of p i and z is n, the number of pixels of the density map. However, since the number of annotated dots N is less than n, the Bayesian loss is underdetermined. For a ground truth annotation z, there are infinitely manyẑ with Bayesian (z,ẑ) = 0 andẑ = z. Therefore, the predicted density map could be very different from the ground truth density map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Generalization Error Bounds of the Losses in DM-Count</head><p>We give the generalization error bounds of the losses in the proposed method in the following theorem.</p><p>Theorem 2 Assume that ∀f ∈ F and (I, z) ∼ D, we have z 1 ≥ 1, f (I) 1 ≥ 1 (can be satisfied by adding a dummy dimension with value of 1 to both z and f (I)) and C (z, f (I)) ≤ B. Then, for any 0 &lt; δ &lt; 1, with probability of at least 1 − δ a) the generalization error bound of the counting loss is   <ref type="figure">Figure 1</ref>: Comparison of different methods on toy data. The pixel-wise loss generates a blurry density map with a higher counting error. The Bayesian loss produces dissimilar density maps from the ground truth, with high values in many locations with no annotations. DM-Count is able to produce more accurate crowd count and localization than the other two methods.</p><formula xml:id="formula_12">R(D, f S C , C ) ≤ R(D, f D C , C ) + 2nR S (H) + 5B 2 log (8/δ)/K, b) the generalization error bound of the OT loss is R(D, f S OT , OT ) ≤ R(D, f D OT , OT ) + 4C ∞ n 2 R S (H) + 5C ∞ 2 log (8/δ)/K, c) the generalization error bound of the TV loss is R(D, f S T V , T V ) ≤ R(D, f D T V , T V ) +</formula><p>In the above theorem, as K grows, R S (H) and 2 log (1/δ)K decrease. All the expected risks R(D, f S ∆ , ∆ ) using the empirical minimizers f S ∆ converge to the expected risks R(D, f D ∆ , ∆ ), ∆ ∈ {C, OT, T V, ∅} using optimal minimizers f D ∆ . This means that all the upper bounds are tight. In addition, all upper bounds are tighter than the upper bound of the Gaussian smoothed methods shown in Theorem 1.a). The bound of the OT loss in Theorem 2.b) is related to the maximum transport cost C ∞ . Therefore, we need to use a smaller transport cost in OT for better generalization performance. The coefficient of R S (H) for the counting loss is O(n), and for the OT loss and the TV loss is O(n 2 ). This means that for larger image size, we need more images to train. The number is linear to the size of z using solely the counting loss, and quadratic using solely the OT loss or the TV loss. When using all three losses, we need to set λ 1 and λ 2 to be small in order to balance the three losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we describe experiments on toy data and on benchmark crowd counting datasets. More detailed dataset descriptions, implementation details and experimental settings can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on Toy Data</head><p>To understand the empirical behavior of different methods, we consider a toy problem where the task is to move a source density mapẑ to a target density map z using the Pixel-wise loss, the Bayesian loss and DM-Count. The source density mapẑ is initialized from a uniform distribution between 0 and 0.01, and the target density map is shown in the leftmost figure in <ref type="figure">Fig. 1</ref>. All three methods start from the same source density map. <ref type="figure">Fig. 1</ref> visualizes the finalẑ at convergence. The Pixel-wise loss yields a blurry density map with a higher count. The Bayesian loss performs better than the Pixel-wise loss in terms of counting error, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity in Image (SSIM) <ref type="bibr" target="#b51">[52]</ref>, but the resulting density map is quite different from the target, with high values at many locations where no dots are annotated. This confirms our analysis that the Bayesian loss corresponds to an underdetermined system such that the output density map could be very different from the target density map. In contrast, DM-Count is able to produce a more accurate count and density map. DM-Count outperforms the Bayesian loss by a large margin in both PSNR and SSIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on Benchmark Datasets</head><p>We perform experiments on four challenging crowd counting datasets: UCF-QNRF <ref type="bibr" target="#b14">[15]</ref>, NWPU <ref type="bibr" target="#b50">[51]</ref>, ShanghaiTech <ref type="bibr" target="#b59">[60]</ref>, and UCF-CC-50 <ref type="bibr" target="#b13">[14]</ref>. It is worth noting that the NWPU dataset is the largestscale and most challenging crowd counting dataset publicly available today. The ground truth counts for test images are not released, and the results on the test set must be obtained by submitting to the evaluation server at https://www.crowdbenchmark.com/nwpucrowd.html. Following previous work <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b59">60]</ref>, we use the following metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Normalized Absolute Error (NAE) as evaluation metrics. For  all three metrics, the smaller the better. For a fair comparison, we use the same network as in the Bayesian loss paper <ref type="bibr" target="#b30">[31]</ref>. In all experiments, we set λ 1 = 0.1, λ 2 = 0.01, and the Sinkhorn entropic regularization parameter to 10. The number of Sinkhorn iterations is set to 100. On average, the OT computation time is 25ms for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF-QNRF</head><p>Quantitative Results. <ref type="table" target="#tab_2">Tables 1 and 2</ref>   <ref type="table" target="#tab_2">Table 1</ref>. Additionally, even without using a multi-scale architecture as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47]</ref>, or a deeper network as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">50]</ref>  Qualitative Results. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the predicted density maps of the Pixel-wise loss, the Bayesian loss and DM-Count. This figure demonstrates that: 1) DM-Count produces count numbers that are closer to the ground truth numbers, 2) DM-Count produces much sharper density maps than the   Pixel-wise and Bayesian losses. In <ref type="figure" target="#fig_1">Fig. 2</ref>  <ref type="figure">Fig. 3</ref> shows predicted density maps by DM-Count. The predicted density maps correspond well to crowd densities in both sparse and dense areas, demonstrating the effectiveness of DM-Count in spatial density estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>Hyper-parameter study. We tune λ 1 and λ 2 in DM-Count on the UCF-QNRF dataset. First, we fix λ 1 to 0.1 and tune λ 2 from 0.01, 0.05 to 0.1. The MAE varies from 85.6, 87.8 to 88.5. As λ 2 = 0.01 achieves the best result, we fix λ 2 to 0.01 and tune λ 1 from 0.01, 0.05 to 0.1. The MAE varies from 87.2, 86.2 to 85.6. Thus, we set λ 1 = 0.1, λ 2 = 0.01 and use them on all the datasets.</p><p>Effect of the number of Sinkhorn iterations.  Contribution of each component. The loss in DM-Count is composed of three components, the counting loss, the OT loss and the TV loss. We study the contribution of each component on the UCF-QNRF dataset. Results are listed in <ref type="table" target="#tab_10">Table 5</ref>. As seen in the <ref type="table">Table,</ref> all components are essential to the final performance. However, the OT loss is the most important component.</p><p>Robustness to noisy annotations. Crowd annotation is performed by placing a single dot on a person. Such process is ambiguous and could lead to inevitable annotation errors. We study how different loss functions perform w.r.t. annotation errors. We add uniform random noise to the original annotation and train different models with the same noisy annotation. The noise is randomly generated between 0 and 5% of the image height, and is about 80 pixels on average. As shown in <ref type="table" target="#tab_7">Table 4</ref>, the proposed DM-Count is more robust to annotation errors compared to the pixel-wise Bayesian losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have shown that using the Gaussian kernel to smooth the ground truth dot annotations can hurt the generalization bound of a model when testing on the real ground truth data. Instead, we consider crowd counting as a distribution matching problem and propose DM-Count, based on Optimal Transport, to address this problem. Unlike prior work, DM-Count does not need a Gaussian kernel to smooth the annotated dots. The generalization error bound of DM-Count is tighter than that of the Gaussian smoothed methods. Extensive experiments on four crowd counting benchmarks demonstrated that DM-Count significantly outperforms previous state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our work is able to more accurately estimate the crowd size in images or videos, such that it can guide crowd control and improve public safety. The estimated crowd count results are interpretable, with better crowd localization, which will increase transparency of the results for critical applications.</p><p>In an age when the size of the crowd in various political events often becomes a point of heated dispute, having transparent, accurate and objective counting methods could help the historical record, as well a public acceptance of the estimates. Our method could potentially be used to protect public health by monitoring social distancing which is becoming increasingly important during the current epidemic. This method does not leverage biases in the data. The proposed method for counting is general, with possible applications to biomedical cell counting, live stock counting and etc. Our work can be adapted to count moving crowds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>k=1 be the finite sets of K samples i.i.d. sampled from D andD, respectively. Let R S (H) denote the empirical Rademacher complexity [3] for H w.r.t S. Given a data set D ∈ {D, S,D,S}, a mapping f ∈ F and a loss function , let R(D, f, ) = E (I,s)∼D [ (s, f (I))] denote the expected risk. Let 1 (z,ẑ) = z −ẑ 1 . Let f D ∆ = argmin f ∈F R(D, f, ∆ ) be the minimizer of R(D, f, ∆ ) over a data set D using the loss ∆ , where D ∈ {D, S,D,S}, and ∆ ∈ {1, C, OT, T V, ∅}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>n 2 R</head><label>2</label><figDesc>S (H) + 5 2 log (8/δ)K, d) the generalization error bound of the overall loss isR(D, f S , ) ≤ R(D, f D , ) + (2n + 4λ 1 C ∞ n 2 + λ 2 N n 2 )R S (H) +5(B + λ 1 C ∞ + λ 2 N ) 2 log (8/δ)K,where C ∞ is the maximum cost in the cost matrix in OT, and N = sup{ z 1 | ∀(I, z) ∼ D} is the maximum count number over a dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>compare the performance of DM-Count against various methods. In all experiments, DM-Count outperforms all other methods except CAN under MSE in NWPU (where they are comparable). Although we use the same set of hyper-parameters for DM-Count in all experiments, DM-Count still achieves the best performance, suggesting that DM-Count's performance is stable across various datasets. DM-Count outperforms the Pixel-wise loss and the Bayesian loss, when used in the same network architecture and training procedure as DM-Count, in all the experiments. This demonstrates the effectiveness of the proposed loss. The pixel-wise loss is much worse than DM-Count in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 : 3 Figure 3 :</head><label>233</label><figDesc>Density map visualization. Comparison between Pixel-wise loss, Bayesian loss and DM-Count. The pixel-wise and Bayesian losses fail to localize people well in dense regions. DM-Count is able to localize people both in dense and sparse regions. The Count number, PSNR and SSIM metrics suggest that DM-Count produces more accurate count numbers and better density maps. Density map visualization on the NWPU validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on the UCF-QNRF, Shanghai Tech, and UCF-CC-50 datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">ShanghaiTech A ShanghaiTech B</cell><cell cols="2">UCF-CC-50</cell></row><row><cell></cell><cell cols="8">MAE RMSE MAE RMSE MAE RMSE MAE RMSE</cell></row><row><cell>Crowd CNN [58]</cell><cell>-</cell><cell>-</cell><cell>181.8</cell><cell>277.7</cell><cell>32.0</cell><cell>49.8</cell><cell cols="2">467.0 498.5</cell></row><row><cell>MCNN [60]</cell><cell>277</cell><cell>426</cell><cell>110.2</cell><cell>173.2</cell><cell>26.4</cell><cell>41.3</cell><cell cols="2">377.6 509.1</cell></row><row><cell>CMTL [41]</cell><cell>252</cell><cell>514</cell><cell>101.3</cell><cell>152.4</cell><cell>20.0</cell><cell>31.1</cell><cell cols="2">322.8 341.4</cell></row><row><cell>Switch CNN [2]</cell><cell>228</cell><cell>445</cell><cell>90.4</cell><cell>135.0</cell><cell>21.6</cell><cell>33.4</cell><cell cols="2">318.1 439.2</cell></row><row><cell>IG-CNN [1]</cell><cell>-</cell><cell>-</cell><cell>72.5</cell><cell>118.2</cell><cell>13.6</cell><cell>21.1</cell><cell cols="2">291.4 349.4</cell></row><row><cell>ic-CNN [35]</cell><cell>-</cell><cell>-</cell><cell>68.5</cell><cell>116.2</cell><cell>10.7</cell><cell>16.0</cell><cell cols="2">260.9 365.5</cell></row><row><cell>CSR Net [20]</cell><cell>-</cell><cell>-</cell><cell>68.2</cell><cell>115.0</cell><cell>10.6</cell><cell>16.0</cell><cell cols="2">266.1 397.5</cell></row><row><cell>SANet [4]</cell><cell>-</cell><cell>-</cell><cell>67.0</cell><cell>104.5</cell><cell>8.4</cell><cell>13.6</cell><cell cols="2">258.4 334.9</cell></row><row><cell>CL-CNN [15]</cell><cell>132</cell><cell>191</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PACNN [40]</cell><cell>-</cell><cell>-</cell><cell>62.4</cell><cell>102.0</cell><cell>7.6</cell><cell>11.8</cell><cell cols="2">241.7 320.7</cell></row><row><cell>CAN [27]</cell><cell>107</cell><cell>183</cell><cell>62.3</cell><cell>100.0</cell><cell>7.8</cell><cell>12.2</cell><cell cols="2">212.2 243.7</cell></row><row><cell>SFCN [50]</cell><cell>102</cell><cell>171</cell><cell>64.8</cell><cell>107.5</cell><cell>7.6</cell><cell>13.0</cell><cell cols="2">214.2 318.2</cell></row><row><cell>ANF [57]</cell><cell>110</cell><cell>174</cell><cell>63.9</cell><cell>99.4</cell><cell>8.3</cell><cell>13.2</cell><cell cols="2">250.2 340.0</cell></row><row><cell>Wan et al. [47]</cell><cell>101</cell><cell>176</cell><cell>64.7</cell><cell>97.1</cell><cell>8.1</cell><cell>13.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Pixel-wise Loss [31]</cell><cell cols="2">106.8 183.7</cell><cell>68.6</cell><cell>110.1</cell><cell>8.5</cell><cell>13.9</cell><cell cols="2">251.6 331.3</cell></row><row><cell>Bayesian Loss [31]</cell><cell>88.7</cell><cell>154.8</cell><cell>62.8</cell><cell>101.8</cell><cell>7.7</cell><cell>12.7</cell><cell cols="2">229.3 308.2</cell></row><row><cell cols="2">DM-Count (proposed) 85.6</cell><cell>148.3</cell><cell>59.7</cell><cell>95.7</cell><cell>7.4</cell><cell>11.8</cell><cell cols="2">211.0 291.5</cell></row><row><cell></cell><cell></cell><cell>Backbone</cell><cell cols="2">Validation set</cell><cell></cell><cell>Test set</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">MAE RMSE MAE RMSE NAE</cell><cell></cell></row><row><cell>MCNN [60]</cell><cell></cell><cell>FS</cell><cell cols="5">218.5 700.6 232.5 714.6 1.063</cell><cell></cell></row><row><cell>CSR net [20]</cell><cell></cell><cell>VGG-16</cell><cell cols="5">104.8 433.4 121.3 387.8 0.604</cell><cell></cell></row><row><cell cols="2">PCC-Net-VGG [10]</cell><cell>VGG-16</cell><cell cols="5">100.7 573.1 112.3 457.0 0.251</cell><cell></cell></row><row><cell>CAN [27]</cell><cell></cell><cell>VGG-16</cell><cell>93.5</cell><cell cols="4">489.9 106.3 386.5 0.295</cell><cell></cell></row><row><cell>SCAR [11]</cell><cell></cell><cell>VGG-16</cell><cell>81.5</cell><cell cols="4">397.9 110.0 495.3 0.288</cell><cell></cell></row><row><cell cols="2">Bayesian Loss [31]</cell><cell>VGG-19</cell><cell>93.6</cell><cell cols="4">470.3 105.4 454.2 0.203</cell><cell></cell></row><row><cell>SFCN [50]</cell><cell></cell><cell cols="2">ResNet-101 95.4</cell><cell cols="4">608.3 105.7 424.1 0.254</cell><cell></cell></row><row><cell cols="2">DM-Count (proposed)</cell><cell>VGG-19</cell><cell>70.5</cell><cell>357.6</cell><cell>88.4</cell><cell cols="2">388.6 0.169</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of various methods on the NWPU validation and test sets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, DM-Count still achieves state-of-the-art performance on all four datasets. This indicates the importance of having a good loss function in crowd counting. On the large-scale and challenging datasets UCF-QNRF and NWPU, DM-Count significantly outperforms the state-of-the-art methods. Specifically, on the UCF-QNRF dataset, DM-Count reduces the MAE and MSE of the Bayesian loss from 88.7 to 85.6 and from 154.8 to 148.3, respectively. Notably, on the NWPU test set (obtained by submitting to the evaluation server), DM-Count reduces the MAE and NAE by a large margin, from 105.4 to 88.4 in MAE and from 0.203 to 0.169 in NAE.</figDesc><table><row><cell>Image</cell><cell>Pixel-wise loss</cell><cell>Bayesian loss</cell><cell>DM-Count (proposed)</cell></row><row><cell>Count: 139</cell><cell>Count: 83.2</cell><cell>Count: 98.3</cell><cell>Count: 137.6</cell></row><row><cell></cell><cell>PSNR: 43, SSIM: 0.83</cell><cell>PSNR: 39, SSIM: 0.75</cell><cell>PSNR: 45, SSIM: 0.86</cell></row><row><cell>Count: 2638</cell><cell>Count: 1755.7</cell><cell>Count: 2236.3</cell><cell>Count: 2656.7</cell></row><row><cell></cell><cell>PSNR: 31, SSIM: 0.12</cell><cell>PSNR: 30, SSIM: 0.08</cell><cell>PSNR: 37, SSIM: 0.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Effect of # of Sinkhorn iterations.</figDesc><table><row><cell>Method</cell><cell>MAE RMSE</cell></row><row><cell cols="2">Pixel-wise loss 144.1 232.5</cell></row><row><cell cols="2">Bayesian loss 108.4 187.2</cell></row><row><cell>DM-Count</cell><cell>105.6 181.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Robustness to noisy annotations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>, DM-Count produces much higher PSNRs and SSIMs than the Pixel-wise and Bayesian losses. The average PSNR and SSIM over the whole UCF-QNRF test set for the Pixel-wise loss are 34.79 and 0.43, for the Bayesian loss are 34.55 and 0.42, and for DM-Count are 40.65 and 0.55, respectively. Because the Pixel-wise loss uses the Gaussian smoothed ground truth, it produces blurrier density maps than the real ground truth. This empirically verifies our theoretical analysis of the generalization bound of Gaussian smoothed methods. As shown in the figure, the Pixel-wise and Bayesian losses are unable to localize people in dense regions. In contrast, DM-Count localizes people well in both dense and sparse regions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3</head><label>3</label><figDesc>lists the results of DM-Count on the UCF-QNRF dataset using different numbers of Sinkhorn iterations. As shown in this table, using a small number of iterations lowers the performance of DM-Count, which indicates that we obtain inaccurate OT solutions. When the number of iterations increases to 100, DM-Count outperforms the previous state-of-the-art. The performance plateaued after the number of iterations crossed 100. Therefore, in all of our experiments, we use 100 Sinkhorn iterations for DM-Count.</figDesc><table><row><cell>Component</cell><cell>Combinations</cell></row><row><cell>Counting loss</cell><cell></cell></row><row><cell>OT loss</cell><cell></cell></row><row><cell>TV loss</cell><cell></cell></row><row><cell>MAE</cell><cell>103.1 94.9 89.3 85.6</cell></row><row><cell>RMSE</cell><cell>175.9 167.4 161.3 148.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Component analysis</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="34">34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2009.13077v2 [cs.CV] 25 Oct 2020</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was partially supported by US National Science Foundation Award IIS-1763981, the SUNY2020 Infrastructure Transportation Security Center, and Air Force Research Laboratory (AFRL) DARPA FA8750-19-2-1003, the Partner University Fund, and a gift from Adobe.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Divide and grow: capturing huge diversity in crowd images with incrementally growing cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Sajjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukundhan</forename><surname>Vr Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning spatial awareness to improve crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Qi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computational optimal transport: Complexity by accelerated gradient descent is better than by sinkhorn&apos;s algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Dvurechensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gasnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kroshnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pcc net: Perspective crowd counting via spatial convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scar: Spatial-/channel-wise attention regression networks for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Marked point processes for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weina</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhmmad</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishan</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somaya</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crowd counting and density estimation by trellis encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Where are the blobs: Counting by localization with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Issam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">O</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Density map regression guided detection network for rgb-d crowd counting and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimation of number of people in crowded scenes using perspective transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaw-Yeh</forename><surname>Sheng-Fuu Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent attentive zooming for joint crowd counting and precise localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decidenet: Counting varying density crowds through attention guided detection and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Crowd counting with deep structured scale integration network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adcrowdnet: An attentioninjective deformable convolutional network for crowd understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context-aware crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging unlabeled data for crowd counting by learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7661" to="7669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Point in, box out: Beyond counting persons in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Class-agnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian loss for crowd count estimation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjovsky</forename><surname>Sc Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Onoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto J López-Sastre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Computational optimal transport. Foundations and Trends® in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="355" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Iterative crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Uncertainty estimation and sample selection for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Top-down feedback for crowd counting convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babu</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Venkatesh</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Crowd counting via adversarial cross-scale consistency pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Revisiting perspective information for efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ha-ccn: Hierarchical attention-based crowd counting network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="323" to="335" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-level bottom-top and top-bottom feature fusion for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Padnet: Pan-density crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2714" to="2727" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Introduction to nonparametric estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexandre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive density map generation for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Residual regression with semantic prior for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Multimedia Conference</title>
		<meeting>the ACM Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning from synthetic data for crowd counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Nwpu-crowd: A large-scale benchmark for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03360</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">From open set to closed set: Counting objects by spatial divide-and-conquer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learn to scale: Generating multipolar normalized density maps for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Perspective-guided convolution networks for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Relational attention network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attentional neural fields for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Wide-area crowd counting via ground-plane density maps and multi-view fusion cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Leveraging heterogeneous auxiliary tasks to assist crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bayesian human segmentation in crowded situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
