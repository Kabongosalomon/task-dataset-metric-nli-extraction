<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIPREADING USING TEMPORAL CONVOLUTIONAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Research Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computing Department</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Research Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computing Department</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Research Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Computing Department</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LIPREADING USING TEMPORAL CONVOLUTIONAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Visual Speech Recognition</term>
					<term>Lip-reading</term>
					<term>Temporal Convolutional Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lip-reading has attracted a lot of research attention lately thanks to advances in deep learning. The current state-of-theart model for recognition of isolated words in-the-wild consists of a residual network and Bidirectional Gated Recurrent Unit (BGRU) layers. In this work, we address the limitations of this model and we propose changes which further improve its performance. Firstly, the BGRU layers are replaced with Temporal Convolutional Networks (TCN). Secondly, we greatly simplify the training procedure, which allows us to train the model in one single stage. Thirdly, we show that the current state-of-the-art methodology produces models that do not generalize well to variations on the sequence length, and we addresses this issue by proposing a variable-length augmentation. We present results on the largest publiclyavailable datasets for isolated word recognition in English and Mandarin, LRW and LRW1000, respectively. Our proposed model results in an absolute improvement of 1.2% and 3.2%, respectively, in these datasets which is the new state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Visual speech recognition, also known as lip-reading, relies on the lip movements in order to recognise speech without relying on the audio stream. This is particularly useful in noisy environments where the audio signal is corrupted, and can be used in combination with acoustic speech recognisers in order to compensate for the degraded performance due to noise.</p><p>Traditionally, a two-stage approach was followed where features were first extracted from the mouth region, with the Discrete Cosine Transform being the most popular feature extractor, and then fed to a Hidden Markov Model (HMMs) for modeling of the temporal dynamics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. The same two-step approach was also followed in the first deep learning works, where the feature extraction step was replaced by deep autoencoders and HMMs were replaced by Long-Short Term Memory (LSTM) networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Recently, several end-toend works have been presented. Such works use either fully connected <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> or convolutional layers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> to extract features from the mouth region, and then feed them into a recurrent neural network or to attention <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> / selfattention architectures <ref type="bibr" target="#b11">[12]</ref>.</p><p>The state-of-the-art approach for recognition of isolated words is the one proposed in <ref type="bibr" target="#b9">[10]</ref> which is further refined in <ref type="bibr" target="#b14">[15]</ref> and has been recently extended to a two-stream model in <ref type="bibr" target="#b15">[16]</ref>. It consists of a 3D convolutional layer followed by an 18-layer Residual Network (ResNet) <ref type="bibr" target="#b16">[17]</ref>, a Bidirectional Gated Recurrent Unit (BGRU) network and a softmax layer. It achieves the state-of-the-art performance on the LRW <ref type="bibr" target="#b12">[13]</ref> and LRW1000 <ref type="bibr" target="#b17">[18]</ref> datasets, which are the largest publicly available datasets for isolated word recognition.</p><p>In this work, we improve the performance of our stateof-the-art model <ref type="bibr" target="#b14">[15]</ref>. Firstly, we improve the overall performance to achieve a new state-of-the-art. This is achieved by replacing the BGRU layers with a Temporal Convolutional Network <ref type="bibr" target="#b18">[19]</ref>, which has been shown to achieve similar or even better performance than recurrent layers. Secondly, we simplify the training procedure, reducing training time from 3 weeks to 1 week GPU-time, and avoid relying on a cumbersome 3-stage sequential training. For this purpose, we adopt a cosine scheduler <ref type="bibr" target="#b19">[20]</ref> and show that training from scratch in one stage is not only feasible, but in fact can produce stateof-the-art results. Finally, we propose a variable-length augmentation procedure to improve the generalization capabilities of the trained model when applied to sequences of varying length 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATABASES</head><p>For the purposes of this study we use the Lip Reading in the Wild (LRW) <ref type="bibr" target="#b20">[21]</ref> and LRW1000 <ref type="bibr" target="#b17">[18]</ref> databases which are the largest publicly available lipreading datasets in English and Mandarin, respectively, in the wild. LRW consists of short segments (1.16 seconds) from BBC programs, mainly news and talk shows. There are more than 1000 speakers and a large variation in head pose and illumination and as a consequence is a challenging dataset. The number of words, 500, is also much higher than existing lipreading databases used for word recognition.</p><p>LRW1000 is also a very challenging dataset due to its <ref type="bibr" target="#b0">1</ref> Length of all LRW videos is 29 frames. large variations in scale, resolution and background clutter. There are 1000 word classes and a total of 718,018 samples with total duration of approximiately 57 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BACKGROUND</head><p>In the following we describe the methodology of <ref type="bibr" target="#b14">[15]</ref>, as it constitutes the starting point for this work. In particular, we describe the architectural design, of which we maintain the feature encoding part but change the sequence classification layers, and the training procedure, which we thoroughly revamp.</p><p>Architecture design: Given a video sequence, the input to the network is a B×T ×H ×W tensor, each dimension corresponding to batch, frames, height and width, respectively (input images have a single channel indicating gray level). A standard ResNet18 network is used as the video feature encoder, except that the first convolution is substituted by a 3D convolution with kernel 5 × 7 × 7, as proposed in <ref type="bibr" target="#b9">[10]</ref>. No temporal downsampling is performed throughout the network, and a global spatial average pooling is applied after the last convolution, resulting in a feature output of dimensions B × C × T , where C indicates the channel dimensionality (512 in this case). Finally, the sequence of feature vectors is fed into a two-layer bidirectional GRU followed by a dense softmax layer <ref type="bibr" target="#b14">[15]</ref>.</p><p>Training procedure: Training follows the three-stage optimization procedure originally proposed in <ref type="bibr" target="#b9">[10]</ref>. Training consists of three models trained sequentially, where the previous model is used as initialization to the subsequent one. Firstly, a model is trained with a variant of the network consisting of substituting the BGRU head with a single-layer Temporal Convolutional Network <ref type="bibr" target="#b18">[19]</ref>. The second model uses the final architecture, the BGRU parameters are randomly initialized, the feature encoding layers are frozen and only the BGRU layers are trained. The third and final step finetunes the full network together. The rationale of this procedure is that RNNs are hard to train, so the feature encoding layers do not have a strong-enough gradient to train properly. Furthermore, <ref type="bibr" target="#b14">[15]</ref> uses a custom reduce and reload scheduler, so at the end of an epoch, if validation performance drops, the previous checkpoint is loaded and the learning rate is lowered by 3%. Despite the strong performance demonstrated, the training procedure requires 3 weeks of GPU time to train a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PROPOSED METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Temporal Convolutional Networks</head><p>Recently, temporal convolutions have emerged as a promising alternative to LSTMs <ref type="bibr" target="#b18">[19]</ref>, in some cases showing remarkable success on a number of tasks <ref type="bibr" target="#b21">[22]</ref>. A temporal convolution takes a time-indexed sequence of feature vectors as input, and maps it into another such sequence (i.e., the length of the sequence is not altered) through the use of a 1D temporal convolution. Drawing a parallel to the ResNet's basic block, a temporal convolutional block consists of two sets of temporal conv-batchnorm-activation layers, and dropout can be used after each activation. A skip connection/downsample layer is also used, going from the block input to its output. Several such temporal convolutional blocks can be stack sequentially to act as a deep feature sequence encoder. Then, a dense layer is applied to each time-indexed feature vector. Finally, since the aim is sequence classification, a consensus function, in our case a simple averaging, is used.</p><p>Dilated convolutions are typically used within TCN to increase the receptive field at a faster rate. In particular, within block i, we use a stride of 2 i−1 . This architecture is illustrated in <ref type="figure" target="#fig_1">Fig. 1a</ref>. It is important to note that TCN can be designed to be causal, so at time t only information prior to it is used, or non-causal. Since we are classifying the whole sequence at once, we use the latter design.</p><p>Since the input and output of a temporal convolution have the same length, the receptive field of a TCN is defined by the kernel sizes and the stride. Thus, on a standard TCN, all activations at a specific layer share the same temporal receptive field. We would like to provide the network with visibility into multiple temporal scales, in a way that short term and long term information can be mixed up during the feature encoding. To this end, we propose a multi-scale TCN. In this TCN variant, each temporal convolution consists now of several branches, each with different kernel size. When using n branches, each branch has C/n kernels, and their outputs are simply combined through concatenation. In this way, every convolution layer mixes information at several temporal scales. We depict this architecture in <ref type="figure" target="#fig_1">Fig. 1b</ref>. The full lipreading model is shown in <ref type="figure" target="#fig_1">Fig. 1c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Do lipreading models require complex training?</head><p>Given that we use a purely convolutional architecture, it is reasonable to test whether is possible to train a lipreading model from scratch. We empirically found that in fact it is possible to successfully train the full-fledged model from scratch and achieve state of the art performance. To this end, we adopt a cosine scheduler, which has been shown to be particularly effective <ref type="bibr" target="#b19">[20]</ref>. Such training leads to competitive performance in 1 week GPU-time.</p><p>However, we observe that it is also possible to first pretrain on a subset of the 10% hardest words, which amounts to 50 classes for LRW 2 . Such initialization allows for faster training, and even yields a small performance improvement. Thus, we adopt this pre-training strategy as it adds a minimal training overhead.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Variable length augmentation</head><p>Models trained on LRW tend to overfit to the dataset scenario, where input sequences are composed of 29 frames and the target word is always at its center. A model trained in such biased setting can memorize these biases, and becomes sensitive to tiny changes being applied to the input. For example, simply removing one random frame from an input sequence results in a significant drop in performance. We propose to avoid this dataset bias by performing variablelength augmentation, by which each input training sequence is cropped temporally at a random point prior and after the target word boundaries. While this change does not lead to a direct improvement on the benchmark at hand, we argue it produces more robust models. We offer some experimental validation of this fact in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setting</head><p>Pre-processing: Each video sequence from the LRW dataset is processed by 1) doing face detection and face alignment, 2) aligning each frame to a reference mean face shape 3) cropping a fixed 96 × 96 pixels wide ROI from the aligned face image so that the mouth region is always roughly centered on the image crop 4) transform the cropped image to gray level, as there does not seem to be a performance difference with respect to using RGB. The mouth ROIs are pre-cropped in the LRW1000 dataset so there is no need for pre-processing. We train for 80 epochs using a cosine scheduler and use Adam as the optimizer, with an initial learning rate of 3e − 4, a weight decay of 1e − 4 and batch size of 32. We use random crop of 88 × 88 pixels and random horizontal flip as data augmentation, both applied in a consistent manner to all frames of the sequence. Finally, we use the standard Cross Entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with the current State-of-the-Art</head><p>In this section we compare against the most notable lipreading works in the literature. We attain the state-of-the-art by a wide margin on both LRW by 1.2%, and LRW1000 by 3.2% in terms of top-1 accuracy. Furthermore, the current stateof-the-art method on LRW, <ref type="bibr" target="#b15">[16]</ref>, relies on much heavier architectures (3D ResNet34), use an ensemble of two networks, and pre-trains on the Kinetics dataset <ref type="bibr" target="#b22">[23]</ref>, which is extremely compute-intensive. With regards to the baseline method <ref type="bibr" target="#b14">[15]</ref>, we achieve 1.9% better top1 accuracy. It is also interesting to note that we attain such improvements with a much simpler training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Fixed Length VS Variable Length Training</head><p>The LRW dataset was constructed with several dataset biases that can be exploited to improve on-dataset performance, such as the fact that sequences always have 29 frames, and that words are centered within the sequence. However, it is unrealistic to assume such biases in any real-world test scenario. In order to highlight this issue, we have designed an experiment in which we test the model performance when random frames are removed from the sequence, with the number of frames removed N going from 0 (full model performance) to 5. We further tested the models trained with the variable length augmentation proposed in Sec. 4.3, which is aimed to improve robustness of the model. Results for fixed and variable length training are shown in <ref type="table">Table 2</ref>. All models are trained and evaluated on the LRW dataset. It is clear that the BGRU model trained on fixed length sequences is significantly affected even when one frame is removed during testing. As expected the performance degrades as more and more frames are removed. On the other hand, the BGRU model trained</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone LRW (Accuracy) LRW-1000 (Accuracy) LRW <ref type="bibr" target="#b20">[21]</ref> VGG-M 61.1 -WAS <ref type="bibr" target="#b12">[13]</ref> VGG-M 76.2 -ResNet + LSTM <ref type="bibr" target="#b9">[10]</ref> ResNet34 * 83.0 38.2 3 End-to-end AVR <ref type="bibr" target="#b14">[15]</ref> ResNet18 * 83.4 4 -Multi-Grained <ref type="bibr" target="#b23">[24]</ref> ResNet34 + DenseNet3D 83.3 36.9 2-stream 3DCNN <ref type="bibr" target="#b15">[16]</ref> (3D ResNet34)× 2 84.1 -Multi-Scale TCN (Ours) ResNet18 * 85.3 41.4 <ref type="table">Table 1</ref>: Comparison with state-of-the-art methods in the literature on the LRW and LRW-1000 datasets. Performance is in terms of classification accuracy (the higher the better). We also indicate the backbone employed, as some works either use higher-capacity networks, or use an ensemble of two networks. Networks marked with * use 2D convolutions except for the first being a 3D one.  <ref type="table">Table 2</ref>: Classification accuracy of different models on LRW when frames are randomly removed from the test sequences. The model in the first row is the same as the one in <ref type="bibr" target="#b14">[15]</ref>. A better accuracy is achieved compared to the one presented in <ref type="table">Table 1</ref> due to the changes in training as explained in section 4. The baseline corresponds to the model presented in <ref type="bibr" target="#b14">[15]</ref>.</p><p>with variable length augmentation is much more robust to frame removals. In the extreme case where 5 frames are removed it results in an absolute improvement of 35.80% over the fixed length model. However, this robustness is achieved at the expense of reduced performance (84.60% vs 82.40%) when the full sequence is fed to the model. On the other hand, the combination of the TCN model with variable length augmentation achieves the same robustness as the BGRU model to frame removals and at the same time leads to superior performance when the full test sequence is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Audio-visual experiments</head><p>While lip-reading can be used in isolation, the most useful scenario is when combined with audio to improve performance on noisy environments. In this section we show the performance of our model when trained on audio only, visual only and audio-visual data under varying levels of babble noise. The audio-only and audio-visual models are based on <ref type="bibr" target="#b14">[15]</ref> but we apply the proposed changes as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>The performance under different Signal to Noise Ratio (SNR) levels is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. We also compute the performance of a TCN network trained with MFCC features. We use 13 coefficients (and their deltas) using a 40ms window and a 10ms step. Performance of MFCCs is similar to the audioonly model at high SNRs but becomes worse at low SNRs similarly to <ref type="bibr" target="#b14">[15]</ref>.</p><p>The audio-visual model is slightly better than the audioonly model at low SNRs but yielding a clear advantage at higher levels of noise (low SNR levels). In particular, when using a clean audio signal, the audio-only model attains 1.54% error rate, while the audio-visual model attains 1.04%. In the presence of heavy noise, e.g. 0 dB, the audio-visual error rate is 2.92%, while performance for the audio-only model goes down to 8.57%. Similarly at -5dB, the audiovisual model achieves an error rate of 6.53% significantly outperforming the audio-only model which has an error rate of 26.21%. We further compare the performance of the audio-only and audio-visual models with respect to our baseline <ref type="bibr" target="#b14">[15]</ref>, showing a clear gain throughout the different noise levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>In this work we have presented an improved version of the state-of-the-art model on isolated word recognition. We address the issue of the model not being able to generalise on sequences of varying length by using variable length augmentation. We also replace the BGRUs with temporal convolutional networks which enhance the model's performance. Finally, we simplify the training process so the model can be trained much quicker. The proposed model achieves the new stateof-the-art performance on the LRW and LRW1000 datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Temporal Convolutional Network (TCN). (b) Our Multi-scale Temporal Convolution, which is used in the lip-reading model (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Performance for audio-only (A), video-only (V) and audio-visual (AV) models under different babble noise levels.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The list of "hardest words" is obtained from<ref type="bibr" target="#b9">[10]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Result reported in<ref type="bibr" target="#b17">[18]</ref>.<ref type="bibr" target="#b3">4</ref> The paper reports 18% error, but the author's website shows better results resulting from further model finetuning.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recent advances in the automatic recognition of audiovisual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1306" to="1326" />
			<date type="published" when="2003-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Audio-visual speech modeling for continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2000-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="590" to="605" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="737" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep complementary bottleneck features for visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lip reading using a dynamic feature of lip images and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ariki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACIS Intl. Conf. on Computer and Information Science</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end visual speech recognition with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2592" to="2596" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lipreading with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end multi-view lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Combining residual networks with LSTMs for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large-scale visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bennett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions of Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audio-visual speech recognition with a hybrid ctc/attention architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with two-stream deep 3D CNNs for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LRW-1000: A naturally-distributed large-scale benchmark for lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conf. on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">WaveNet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the Kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-grained spatio-temporal modeling for lip-reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
