<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The IBM 2016 English Conversational Telephone Speech Recognition System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Saon</surname></persName>
							<email>gsaon@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Kwang</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The IBM 2016 English Conversational Telephone Speech Recognition System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: recurrent neural networks</term>
					<term>convolutional neural networks</term>
					<term>conversational speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.6% on the Switchboard subset of the Hub5 2000 evaluation testset. On the acoustic side, we use a score fusion of three strong models: recurrent nets with maxout activations, very deep convolutional nets with 3x3 kernels, and bidirectional long short-term memory nets which operate on FMLLR and i-vector features. On the language modeling side, we use an updated model "M" and hierarchical neural network LMs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The landscape of neural network acoustic modeling is rapidly evolving. Spurred by the success of deep feed-forward neural nets for LVCSR in <ref type="bibr" target="#b0">[1]</ref> and inspired by other research areas like image classification and natural language processing, many speech groups have looked at more sophisticated architectures such as deep convolutional nets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, deep recurrent nets <ref type="bibr" target="#b3">[4]</ref>, time-delay neural nets <ref type="bibr" target="#b4">[5]</ref>, and long-short term memory nets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. The trend is to remove a lot of the complexity and human knowledge that was necessary in the past to build good ASR systems (e.g. speaker adaptation, phonetic context modeling, discriminative feature processing, etc.) and to replace them with a powerful neural network architecture that can be trained agnostically on a lot of data. With the advent of numerous neural network toolkits which can implement these sophisticated models out-of-the-box and powerful hardware based on GPUs, the barrier of entry for building high performing ASR systems has been lowered considerably. First case in point: front-end processing has been simplified considerably with the use of CNNs which treat the log-mel spectral representation as an image and don't require extra processing steps such as PLP cepstra, LDA, FMLLR, fMPE transforms, etc. Second case in point: end-to-end ASR systems such as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref> bypass the need of having phonetic context decision trees and HMMs altogether and directly map the sequence of acoustic features to a sequence of characters or context independent phones. Third case in point: training algorithms such as connectionist temporal classification <ref type="bibr" target="#b10">[10]</ref> don't require an initial alignment of the training data which is typically done with a GMM-based baseline model.</p><p>The above points beg the question whether, in this age of readily available NN toolkits, speech recognition expertise is still necessary or whether one can simply point a neural net to the audio and transcripts, let it train, and obtain a good acoustic model. While it is true that, as the amount of training data increases, the need for human ASR expertise is lessened, at the moment the performance of end-to-end systems ultimately re-mains inferior to that of more traditional, i.e. HMM and decision tree-based, approaches. Since the goal of this work is to obtain the lowest possible WER on the Switchboard dataset regardless of other practical considerations such as speed and/or simplicity, we have focused on the latter approaches.</p><p>The paper is organized as follows: in section 2 we discuss acoustic and language modeling improvements and in section 3 we summarize our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System improvements</head><p>In this section we describe three different acoustic models that were trained on 2000 hours of English conversational telephone speech: recurrent nets with maxout activations and annealed dropout, very deep convolutional nets with 3×3 kernels, and bidirectional long short-term memory nets operating on FM-LLR and i-vector features. All models are used in a hybrid HMM decoding scenario by subtracting the logarithm of the HMM state priors from the log of the softmax output scores.</p><p>The training and test data, frontend processing, speaker adaptation are identical to <ref type="bibr" target="#b11">[11]</ref> and their description will be omitted. At the end of the section, we also provide an update on our vocabulary and language modeling experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Recurrent nets with maxout activations</head><p>We remind the reader that maxout nets <ref type="bibr" target="#b12">[12]</ref> generalize ReLu units by employing non-linearities of the form si = max j∈C(i) w T j x + bj where the subsets of neurons C(i) are typically disjoint. In <ref type="bibr" target="#b11">[11]</ref> we have shown that maxout DNNs and CNNs trained with annealed dropout outperform their sigmoid-based counterparts on both 300 hours and 2000 hours training regimes. What was missing there was a comparison between maxout and sigmoid for unfolded RNNs <ref type="bibr" target="#b3">[4]</ref>. The architecture of the maxout RNNs comprises one recurrent layer with 2828 units projected to 1414 units via non-overlapping 2 → 1 maxout operations. This layer is followed by 4 nonrecurrent layers with 2828 units (also projected to 1414) followed by a bottleneck with 1024→512 units and an output layer with 32000 neurons corresponding to as many contextdependent HMM states. The number of neurons for the maxout layers have been chosen such that the weight matrices have roughly the same number of parameters as the baseline sigmoid network which has 2048 units per hidden layer. The recurrent layer is unfolded backwards in time for 6 time steps t − 5 . . . t and has 340-dimensional inputs consisting of 6 spliced right context 40-dimensional FMLLR frames (t . . . t + 5) to which we append a 100-dimensional speaker-based ivector. The unfolded maxout RNN architecture is depicted in <ref type="figure">Figure 1</ref>.</p><p>The network is trained one hidden layer at a time with discriminative pretraining followed by 12 epochs of SGD CE training on randomized minibatches of 250 samples. The model is refined with Hessian-free sequence discriminative training <ref type="bibr" target="#b13">[13]</ref>  In <ref type="table" target="#tab_0">Table 1</ref> we report the error rates for sigmoid and maxout RNNs on the Switchboard and CallHome subsets of Hub5'00. The decodings are done with a small vocabulary of 30K words and a small 4-gram language model with 4M n-grams. Note that the sigmoid RNNs have better error rates than what was reported in <ref type="bibr" target="#b11">[11]</ref> because they have been retrained after the data has been realigned with the best joint RNN/CNN model. We observe that the maxout RNNs are consistently better and that, by themselves, they achieve a similar WER as our previous best model which was the joint RNN/CNN with sigmoid activations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Very deep convolutional networks</head><p>Very deep CNNs with small 3 × 3 kernels have recently been shown to achieve very strong performance as acoustic models in hybrid NN-HMM speech recognition systems. Results were provided after cross-entropy training on the 300 hours switchboard-1 dataset in <ref type="bibr" target="#b14">[14]</ref>, and results from sequence training on both switchboard-1 and the 2000 hours switch-board+Fisher dataset are in <ref type="bibr" target="#b15">[15]</ref>.</p><p>The very deep convolutional networks are inspired by the "VGG Net" architecture introduced in <ref type="bibr" target="#b16">[16]</ref> for the 2014 Ima-geNet classification challenge, with the central idea to replace large convolutional kernels by small 3 × 3 kernels. By stacking many of these convolutional layers with ReLU nonlinearities before pooling layers, the same receptive field is created with less parameters and more nonlinearity. <ref type="figure">Figure 2</ref> shows the design of the networks. Note that as we go deeper in the network, the time and frequency resolu-tion is reduced through pooling only, while the convolutions are zero-padded as to not reduce the size of the feature maps. We increase the number of feature maps gradually from 64 to 512 (indicated by the different colors). We pool right before the layer that increases the number of feature maps. Note that the indication of feature map size on the right only applies to the rightmost 2 designs. In contrast, the classical CNN architecture has only two layers, goes to 512 feature maps directly, and uses a large 9 × 9 kernel on the first layer. Our 10-layer CNN has about the same number of parameters as the classical CNN, converges in 5 times fewer epochs, but is computationally more expensive.</p><p>Results for 3 variations of the 10-layer CNN are in table 2. For model combination, we use the version with pooling, which is the exact same model without modifications from the original paper <ref type="bibr" target="#b14">[14]</ref>. <ref type="bibr" target="#b17">[17]</ref> 13.2 11.8 --Classic maxout <ref type="bibr" target="#b11">[11]</ref> 12.6 11.2 11.7* 9.9* (a) 10-conv Pool 11.8 10.5 10.2 9.4 (b) 10-conv No pool 11.5 10.9 10.7 9.7 (c) 10-conv No pool, no pad 11.9 10.8 10.8 9.7 <ref type="table">Table 2</ref>: WER on the SWB part of the Hub5'00 testset, for 3 variants of the 10-convolutional-layer CNN: with pooling in time (a), without pooling in time (b), and without pooling nor padding in time (c). For more details see <ref type="bibr" target="#b15">[15]</ref>. *New results.</p><formula xml:id="formula_0">CNN model SWB (300h) SWB (2000h) CE ST CE ST Classic sigmoid</formula><p>Our implementation was done in Torch <ref type="bibr" target="#b18">[18]</ref>. We adopt the balanced sampling from <ref type="bibr" target="#b14">[14]</ref>, by sampling from context dependent state CDi with probability pi = throughout the experiments during cross-entropy training. During CE training, we optimize with simple SGD or NAG, during ST we found NAG to be superior to SGD. We regularize the stochastic sequence training by adding the gradient of crossentropy loss, as proposed in <ref type="bibr" target="#b19">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Bidirectional LSTMs</head><p>Given the recent popularity of LSTMs for acoustic modeling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, we have experimented with such models on the Switchboard task using the Torch toolkit <ref type="bibr" target="#b18">[18]</ref>. We have looked at the effect of the input features on LSTM performance, the number of layers and whether start states for the recurrent layers should be reset or carried over. We use bidirectional LSTMs that are trained on non-overlapping subsequences of 20 frames. The subsequences coming from the same utterance are contiguous so that the left-to-right final states for the current subsequence can be copied to the left-to-right start states for the next subsequence (i.e. carried over). For processing speed and in order to get good gradient estimates, we group subsequences from multiple utterances into minibatches of size 256. Regardless of the number of LSTM layers, all models use a linear bottleneck of size 256 before the softmax output layer (of size 32000).</p><p>In one experiment, we compare the effect of input features on model performance. The baseline models are trained on 40-dimensional FMLLR + 100-dimensional ivector frames and have 1024 (or 512) LSTM units per layer and per direction (left-to-right and right-to-left). The forward and backward activations from the previous LSTM layer are concatenated and fed into the next LSTM layer. The contrast model is a sin-  Due to a bug that affected our earlier multi-layer LSTM results, we decided to go ahead with single layer bidirectional LSTMs on bottleneck features on the full 2000 hour training set. We also experimented with how to deal with the start states at the beginning of the left-to-right pass. One option is to carry them over from the previous subsequence and the other one is to reset the start states at the beginning of each subsequence. In <ref type="figure" target="#fig_2">Figure 3</ref> we compare the cross-entropy loss on held-out data between these two models.</p><p>As can be seen, the LSTM model with carried over start states is much better at predicting the correct HMM state. However, when comparing word error rates in <ref type="table" target="#tab_3">Table 4</ref>, the LSTM with start states that are reset has a better performance. We surmise that this is because the increased memory of the LSTM with carried over start states is in conflict with the state sequence constraints imposed by the HMM topology and the language model. Additionally, we show the WERs of the DNN used for Held-out loss Epoch start states reset start states carried over  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Model combination</head><p>In <ref type="table" target="#tab_5">Table 5</ref> we report the performance of the individual models (RNN, VGG and 4-layer LSTM) described in the previous subsections as well as the results after frame-level score fusion. All decodings are done with a 30K word vocabulary and a small 4-gram language model with 4M n-grams. We note that RNNs and VGG nets exhibit similar performance and have a strong complementarity which improves the WER by 0.6% and 0.9% on SWB and CH, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Language modeling experiments</head><p>Our language modeling strategy largely parallels that described in <ref type="bibr" target="#b11">[11]</ref>. For completeness, we will repeat some of the details here. The main difference is an increase in the vocabulary size from 30K words to 85K words. When comparing acoustic models in previous sections, we used a relatively small legacy language model used in previous publications: a 4M n-gram (n=4) language model with a vocabulary of 30.5K words. We wanted to increase the language model coverage in a manner that others can replicate. To this end, we increased the vocabulary size from 30.5K words to 85K words by adding the vocabulary of the publicly available Broadcast News task. We also added to the LM publicly available text data from LDC, including Switchboard, Fisher, Gigaword, and Broadcast News and Conversations. The most relevant data are the transcripts of the 1975 hour audio data used to train the acoustic model, consisting of about 24M words.</p><p>For each corpus we trained a 4-gram model with modified Kneser-Ney smoothing <ref type="bibr" target="#b20">[20]</ref>. The component LMs are linearly interpolated with weights chosen to optimize perplexity on a held-out set. Entropy pruning <ref type="bibr" target="#b21">[21]</ref> was applied, resulting in a single 4-gram LM consisting of 36M n-grams. This new n-gram LM was used together with our best acoustic model to decode and generate word lattices for LM rescoring experiments. The first two lines of <ref type="table" target="#tab_7">Table 6</ref> show the improvement using this larger n-gram LM with larger vocabulary trained on more data. The WER improved by 1.0% for SWB. Part of this improvement (0.1-0.2%) was due to also using a larger beam for decoding and a change in vocabulary tokenization.  We used two types of LMs for LM rescoring: model M, a class-based exponential model <ref type="bibr" target="#b22">[22]</ref> and feed-forward neural network LM (NNLM) <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>. We built a model M LM on each corpus and interpolated the models, together with the 36M n-gram LM. As shown in <ref type="table" target="#tab_7">Table 6</ref>, using model M results in an improvement of 0.6% on SWB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LM</head><p>We built two NNLMs for interpolation. One was trained on just the most relevant data: the 24M word corpus (Switchboard/Fisher/CallHome acoustic transcripts). Another was trained on a 560M word subset of the LM training data: in order to speed up training for this larger set, we employed a hierarchical NNLM approximation <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b27">27]</ref>. <ref type="table" target="#tab_7">Table 6</ref> shows that the NNLMs provided an additional 0.4% improvement over the model M result on SWB. Compared with the n-gram LM baseline, LM rescoring yielded a total improvement of 1.0% on SWB (7.6% to 6.6%) and 1.5% on CH (13.7% to 12.2%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conclusion</head><p>In our previous Switchboard system paper <ref type="bibr" target="#b11">[11]</ref> we have observed a good complementarity between recurrent nets and convolutional nets and their combination led to significant accuracy gains. In this paper we have presented an improved unfolded RNN (with maxout instead of sigmoid activations) and a stronger CNN obtained by adding more convolutional layers with smaller kernels and ReLu nonlinearities. These improved models still have good complementarity and their frame-level score combination in conjunction with a multi-layer LSTM leads to a 0.4%-0.7% decrease in WER over the LSTM. Multilayer LSTMs were the strongest performing model followed closely by the RNN and VGG nets. We also believe that LSTMs have more potential for direct sequence-to-sequence modeling and we are actively exploring this area of research. On the language modeling side, we have increased our vocabulary from 30K to 85K words and updated our component LMs.</p><p>At the moment, we are less than 3% away from achieving human performance on the Switchboard data (estimated to be around 4%). Unfortunately, it looks like future improvements on this task will be considerably harder to get and will probably require a breakthrough in direct sequence-to-sequence modeling and a significant increase in training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>.</head><label></label><figDesc>We keep γ = 0.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Single layer LSTM cross-entropy loss on held-out data with left-to-right start states which are either reset or carried over.the bottleneck features and of a 4-layer 512 unit LSTM. We observe that the 4 layer LSTM is significantly better than the DNN and the two single layer LSTMs trained on bottleneck features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>arXiv:1604.08242v2 [cs.CL] 22 Jun 2016</figDesc><table><row><cell></cell><cell></cell><cell cols="2">1024 512</cell></row><row><cell></cell><cell></cell><cell cols="2">2828 1414</cell></row><row><cell></cell><cell></cell><cell cols="2">2828 1414</cell></row><row><cell></cell><cell></cell><cell cols="2">2828 1414</cell></row><row><cell></cell><cell></cell><cell cols="2">2828 1414</cell></row><row><cell>2828 1414</cell><cell>...</cell><cell cols="2">2828 1414</cell></row><row><cell>t−5</cell><cell></cell><cell>t</cell></row><row><cell cols="4">Figure 1: Unfolded maxout RNN architecture (right arrows in</cell></row><row><cell cols="3">the boxes denote the maxout operation).</cell></row><row><cell>Model</cell><cell cols="3">WER SWB WER CH</cell></row><row><cell>RNN sigmoid (CE)</cell><cell></cell><cell>10.8</cell><cell>16.9</cell></row><row><cell>RNN maxout (CE)</cell><cell></cell><cell>10.4</cell><cell>16.2</cell></row><row><cell>RNN maxout (ST)</cell><cell></cell><cell>9.3</cell><cell>15.4</cell></row></table><note>using the state-based MBR criterion for 10 iterations.: Word error rates for sigmoid vs. Maxout RNNs trained with annealed dropout on Hub5'00 after cross-entropy training (and sequence training for Maxout).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Word error rates on Hub5 2000 for various LSTM models trained with cross-entropy on 300 hours.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Word error rates on Hub5 2000 for DNN and LSTM models. All models are trained on 2000 hours with crossentropy and sequence discriminative training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Word error rates for individual acoustic models and frame-level score fusions on Hub5 2000.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of word error rates for different LMs.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Acknowledgment</head><p>The authors wish to thank E. Marcheret, J. Cui and M. Nussbaum-Thom for useful suggestions about LSTMs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature engineering in context-dependent deep neural networks for conversational speech transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring convolutional neural network structures and optimization techniques for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3366" to="3370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for lvcsr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8614" to="8618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unfolded recurrent neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deepspeech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning acoustic frame labeling for speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4280" to="4284" />
		</imprint>
	</monogr>
	<note>2015 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08240</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep bi-directional recurrent networks over spectral windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>ASRU</publisher>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The IBM 2015 English conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable minimum Bayes risk training of deep neural network acoustic models using distributed Hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Very deep multilingual convolutional neural networks for lvcsr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<title level="m">Advances in very deep convolutional neural networks for lvcsr</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint training of convolutional and non-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">to Proc. ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="393" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Entropy-based pruning of backoff language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Broadcast News Transcription and Understanding Workshop</title>
		<meeting>DARPA Broadcast News Transcription and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="270" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shrinking exponential language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="468" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A neural syntactic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Baltimore, MD, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="518" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Empirical study of neural network language models for Arabic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mangu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large scale hierarchical neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arısoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vozila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
