<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revealing the Myth of Higher-Order Inference in Coreference Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Xu</surname></persName>
							<email>liyan.xu@emory.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">Emory University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
							<email>jinho.choi@emory.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution">Emory University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revealing the Myth of Higher-Order Inference in Coreference Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution. HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representation learning. To make a comprehensive analysis, we implement an endto-end coreference system as well as four HOI approaches, attended antecedent, entity equalization, span clustering, and cluster merging, where the latter two are our original methods. We find that given a high-performing encoder such as SpanBERT, the impact of HOI is negative to marginal, providing a new perspective of HOI to this task. Our best model using cluster merging shows the Avg-F1 of 80.2 on the CoNLL 2012 shared task dataset in English.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coreference resolution has always been considered one of the unsolved NLP tasks due to its challenging aspect of document-level understanding <ref type="bibr" target="#b12">(Wiseman et al., 2015</ref><ref type="bibr" target="#b13">(Wiseman et al., , 2016</ref><ref type="bibr">Manning, 2015, 2016;</ref><ref type="bibr" target="#b7">Lee et al., 2017)</ref>. Nonetheless, it has made a tremendous progress in recent years by adapting contextualized embedding encoders such as ELMo <ref type="bibr" target="#b3">Fei et al., 2019)</ref> and <ref type="bibr">BERT (Kantor and Globerson, 2019;</ref><ref type="bibr" target="#b5">Joshi et al., 2019</ref><ref type="bibr" target="#b4">Joshi et al., , 2020</ref>. The latest state-of-the-art model shows the improvement of 12.4% over the model introduced 2.5 years ago, where the major portion of the improvement is derived by representation learning <ref type="figure" target="#fig_0">(Figure 1</ref>).</p><p>Most of these previous models have also adapted higher-order inference (HOI) for the global optimization of coreference links, although HOI clearly has not been the focus of those works, for the fact that gains from HOI have been reported marginal. This has inspired us to analyze the impact of HOI on modern coreference resolution models in order to envision the future direction of this research.</p><p>To make thorough ablation studies among different approaches, we implement an end-to-end coreference system in PyTorch (Sec 3.1), and two HOI approaches proposed by previous work, attended antecedent and entity equalization (Sec 3.2), along with two of our original approaches, span clustering and cluster merging (Sec 3.3). These approaches are experimented with two Transformer encoders, BERT and SpanBERT, to assess how effective HOI is even when coupled with those high-performing encoders (Sec 4). To the best of our knowledge, this is the first work to make a comprehensive analysis on multiple HOI approaches side-by-side for the task of coreference resolution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most neural network-based coreference resolution models have adapted antecedent-ranking <ref type="bibr" target="#b12">(Wiseman et al., 2015;</ref><ref type="bibr" target="#b0">Clark and Manning, 2015;</ref><ref type="bibr" target="#b7">Lee et al., 2017</ref><ref type="bibr" target="#b5">Joshi et al., 2019</ref><ref type="bibr" target="#b4">Joshi et al., , 2020</ref>, which relies on the local decisions between each mention and its antecedents. To achieve deeper global optimization, <ref type="bibr" target="#b13">Wiseman et al. (2016)</ref>; <ref type="bibr" target="#b1">Clark and Manning (2016)</ref>; <ref type="bibr" target="#b15">Yu et al. (2020)</ref> built entity representations in the ranking process, whereas ; <ref type="bibr" target="#b6">Kantor and Globerson (2019)</ref> refined the mention representation by aggregating its antecedents' information.</p><p>It is no secret that the integration of contextualized embeddings has played the most critical role in this task. While the following are based on the same end-to-end coreference model <ref type="bibr" target="#b7">(Lee et al., 2017)</ref>, ; <ref type="bibr" target="#b3">Fei et al. (2019)</ref> reported 3.3% improvement by adapting ELMo in the encoders <ref type="bibr" target="#b10">(Peters et al., 2018)</ref>. <ref type="bibr" target="#b6">Kantor and Globerson (2019)</ref>; <ref type="bibr" target="#b5">Joshi et al. (2019)</ref> gained additional 3.3% by adapting BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref>. <ref type="bibr" target="#b4">Joshi et al. (2020)</ref> introduced SpanBERT that gave another 2.7% improvement over <ref type="bibr" target="#b5">Joshi et al. (2019)</ref>.</p><p>Most recently, <ref type="bibr" target="#b14">Wu et al. (2020)</ref> proposes a new model that adapts question-answering framework on coreference resolution, and achieves state-ofthe-art result of 83.1 on the CoNLL'12 shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">End-to-End Coreference System</head><p>We reimplement the end-to-end c2f-coref model introduced by   </p><p>where s(x, y) is the local score involving two parts: how likely the spans x and y are valid mentions, and how likely they refer to the same entity:</p><formula xml:id="formula_1">s(x, y) = s m (x) + s m (y) + s c (x, y) (2) s m (x) = w m FFNN m (g x ) s c (x, y) = w c FFNN c (g x , g y , φ(x, y))</formula><p>g x , g y are the span embeddings of x and y, φ(x, y) is the meta-information (e.g., speakers, distance), and w m , w c are the mention and coreference scores, respectively (FFNN: feedforward neural network). We use different Transformers-based encoders, and follow the "independent" setup for long documents as suggested by <ref type="bibr" target="#b5">Joshi et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Span Refinement</head><p>Two HOI methods presented by recent coreference work are based on span refinement that aggregates non-local features to enrich the span representation with more "global" information. The updated span representation g x can be derived as in Eq. 3, where g x is the interpolation between the current and refined representation g x and a x , and W f is the gate parameter. g x is used to perform another round of antecedent-ranking in replacement of g x .</p><formula xml:id="formula_2">g x = f x • g x + (1 − f x ) • a x (3) f x = σ(W f [g x , a x ])</formula><p>The following two methods share the same updating process for g x , but with different ways to obtain the refined span representation a x .</p><p>Attended Antecedent (AA) takes the antecedent information to enrich g x <ref type="bibr" target="#b3">Fei et al., 2019;</ref><ref type="bibr" target="#b5">Joshi et al., 2019</ref><ref type="bibr" target="#b4">Joshi et al., , 2020</ref>. The refined span a x is the attended antecedent representation over the current antecedent distribution P (y), where g y∈Y(x) is the antecedent representation:</p><formula xml:id="formula_3">a x = y∈Y(x) P (y) · g y (4)</formula><p>Entity Equalization (EE) takes the clustering relaxation as in Eq. 5 to model the entity distribution <ref type="bibr" target="#b6">(Kantor and Globerson, 2019)</ref>, where Q(x ∈ E y ) is the probability of the span x referring to an entity E y in which the span y is the first mention. P (y) is the current antecedent distribution.</p><formula xml:id="formula_4">Q(x ∈ E y ) =      x−1 k=y P (y = k) · Q(k ∈ E y ) y &lt; x P (y = ) y = x 0 y &gt; x<label>(5)</label></formula><p>The refined span a x is the attended entity repre-</p><formula xml:id="formula_5">sentation, where e (x)</formula><p>y is the entity representation to which the span y belongs till the span x:</p><formula xml:id="formula_6">e (t) x = t y=1 Q(y ∈ E x ) · g y (6) a x = x y=1 Q(x ∈ E y ) · e (x) y<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HOI with Clustering</head><p>This section introduces two new HOI methods for a more extensive study in HOI.</p><p>Span Clustering (SC) is also based on span refinement, and it constructs the actual clusters and obtains the "true" predicted entities using P (y) instead of modeling the "soft" entity clusters through the relaxation as in EE (Section 3.2). This way, although we lose the differentiable property, the obtaining of true entities with the same empirical inference time as EE has made SC desirable. The entity representation e i for an entity cluster C i is given by the attended spans in this cluster:</p><formula xml:id="formula_7">α t = w α FFNN α (g t ) α i,t = exp(α t ) k∈C i exp(α k ) e i = t∈C i α i,t · g t</formula><p>The entity clusters C i are constructed in the same way as in the final cluster prediction. The refined span a x is then equal to the representation of entity e i to which it belongs (g x ∈ C i ).</p><p>Cluster Merging (CM) performs sequential antecedent ranking combining both antecedent and entity information to gradually build up the entity clusters, which is distinguished from span refinement methods that simply re-rank antecedents. Algorithm 1 describes the ranking process for CM. g i is the i'th span, Y(i) is the indices of g i 's antecedents, and C i is the cluster that g i belongs to. The ranking score s x (y) consists of both antecedent score f a (see Eq. 2) and cluster score f c . To avoid overlapping between f a and f c , we set f c as 0 if the cluster is the initial cluster (L6). Thus, f c becomes the consultation such that when f c &gt; 0, the span g x is likely to match the cluster C y , and vice versa. f c is computed by FFNN similar to f a , and φ(C y ) is the meta-feature such as the cluster size.</p><p>Algorithm 1 Antecedent Ranking for CM 1: procedure RANKING(g1, · · · , gN ) 2:</p><p>Ci=1,··· ,N ← gi 3:</p><p>R ← ranking_order(g1, · · · , gN ) 4:</p><p>for x = R1 · · · RN do 5:</p><p>for y ∈ Y(x) do Parallelized 6:</p><p>fc(gx, Cy) ← 0 if Cy = gy 7: sx(y) ← fa(gx, gy) + fc(gx, Cy, φ(Cy)) 8:</p><p>y ← argmax y∈Y(x) sx(y) 9:</p><p>if y = then 10:</p><p>merge Cx and C y 11:</p><p>return s1, · · · , sN Two simple configurations can be tuned for CM. We can have the sequential left-to-right ranking order or the easy-first order (L3) whose sequence is ordered by each span's max antecedent score, building the most confident clusters first <ref type="bibr" target="#b9">(Ng and Cardie, 2002;</ref><ref type="bibr" target="#b1">Clark and Manning, 2016)</ref>. There can be element-wise mean or max-reduction among the spans in the two merging clusters (L10). Distinguished from <ref type="bibr" target="#b13">Wiseman et al. (2016)</ref>, clusters in CM are searched and merged in training without the use of oracle clusters, closing the gap between training and test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>For our experiments, the CoNLL 2012 English shared task dataset is used <ref type="bibr" target="#b11">(Pradhan et al., 2012)</ref>. Given the end-to-end coreference system in Section 3.1, six models are developed as follows: 2</p><p>• BERT: BERT <ref type="bibr" target="#b2">(Devlin et al., 2019)</ref> as the encoder Note that BERT and SpanBERT completely rely on only local decisions without any HOI. Particularly, +AA is equivalent to <ref type="bibr" target="#b4">Joshi et al. (2020)</ref>. <ref type="table" target="#tab_2">Table 1</ref> shows the best results in comparison to previous state-of-the-art systems. We also report the mean scores and standard deviations from 5 repeated developments, which we could not find from the previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>The impact of SpanBERT over BERT is clear, showing 2.4% improvement on average. However, none of the HOI models shows a clear advantage over SpanBERT which adapts no HOI. In fact, all HOI models except for CM show negative impact. The best result is achieved by CM with the Avg-F1 of 80.2, surpassing the previous best result of 79.6 based on c2f-coref reported by <ref type="bibr" target="#b4">Joshi et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact Analysis of HOI</head><p>Three HOI methods based on span refinement, AA, EE, and SC, show negative impact upon local decisions. We suspect that error propagation from antecedent-ranking may downgrade the quality of refinement. On the other hand, CM shows marginal improvement, suggesting that maintaining entity clusters can be superior to span refinement, at the 2 Appdendix A.1 provides details of our experimental settings.   In further investigation, we examine the change of coreferent links w.r.t their correctness. Specifically, <ref type="table" target="#tab_3">Table 2</ref> shows the four types of link changes before and after HOI. It demonstrates that the benefits from HOI is diminished because the effects are two-sided: there are roughly same amounts of links (about 1%) becoming correct or wrong after HOI, therefore neither HOI method leads to much improvement overall.</p><p>It is worth mentioning that the impact of HOI is not limited to only global decisions. HOI implicitly serves as a way of regularization that impacts local decisions as well, since HOI and local ranking are mutually dependent during training. Such indirect influence of HOI makes it difficult to assess its true impact, which we will explore more in the future.  <ref type="table">Table 3</ref>: Averaged statistics on the test set prediction of different approaches. SP is the number of coreferent links from Singular to Plural personal pronouns; vice versa for PS. FL (False Link) and WL (Wrong Link) is the number of conreferent link errors that involve two personal pronouns. BC is the number of clusters that contain both singular and plural pronouns, and the parentheses indicate the numbers of BC that contain ambiguous pronouns such as "you".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Pronoun Resolution</head><p>Direct Inference For the error analysis, we examine the direct inference between two personal pronouns. 3 SP/PS in <ref type="table">Table 3</ref> shows the numbers of links that one pronoun incorrectly selects another pronoun with different plurality as its antecedent.</p><p>We find that adapting HOI shows slightly higher impact than switching to a more advanced encoder. AA can reinforce the pronoun representation to bias towards singularity and lead to lower SP error and higher PS error, while the difference between BERT and SpanBERT is trivial on SP/PS.</p><p>We also look at the general types of coreferent errors involving two pronouns. False Link (FL) falsely links a non-anaphoric pronoun to another pronoun as antecedent; Wrong Link (WL) links an anaphoric pronoun to another wrong pronoun as antecedent. <ref type="table">Table 3</ref> shows that EE and CM reduce FL errors by 4+%, suggesting that the aggregation of non-local features indeed leads to more conservative linking decisions. However, adapting an advanced encoder shows higher impact on WL errors, as SpanBERT reduces almost 10% compared to BERT, implying that representation learning is still more important for semantic matching.</p><p>Indirect Inference The plurality of ambiguous pronouns such as you depends on the context. Two indirect links of (he, you) and (you, they) can be common to induce incorrect clusters that contain both singular and plural pronouns <ref type="bibr" target="#b13">(Wiseman et al., 2016;</ref>. <ref type="table">Table 3</ref> shows the numbers of these erroneous clusters in prediction. Surprisingly, very few of these clusters contain ambiguous pronouns in either approach. This observation moderates the long-standing movitation of HOI. Additionally, the change of representation from BERT to SpanBERT has far more impact that reduces 10% of these erroneous clusters, while the four HOI methods fail to show significant difference compared to SpanBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We implement the end-to-end coreference resolution model and investigate four higher-order inference methods, including two of our own methods. Our best model shows the new result of 80.2 on the CoNLL 2012 dataset. We thoroughly analyze the empirical effectiveness of HOI and demonstrate why it fails to boost performance on the CoNLL 2012 dataset compared to the improvement from encoders. We show that current HOI does not meet up with the original motivation, suggesting that a new perspective of HOI is needed for this task in the era of deep learning-based NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Experimental Settings</head><p>We implement the experimented models using Py-Torch. BERT Large and SpanBERT Large are used as encoders. For each experiment, the best performed model on the development set is selected and evaluated on the test set.</p><p>Hyperparameters and Implementation Similar to <ref type="bibr" target="#b5">Joshi et al. (2019</ref><ref type="bibr" target="#b4">Joshi et al. ( , 2020</ref>, documents are split into independent segments with maximum 384 word pieces for BERT Large and 512 for SpanBERT Large . In our final setting, BERTparameters and task-parameters have separate learning rates (1×10 −5 and 3×10 −4 respectively), separate linear decay schedule, and separate weight decay rates (10 −2 and 0 respectively). Models are trained 24 epochs with dropout rate 0.3.</p><p>The implementation of EE is based on the Tensorflow implementation from <ref type="bibr" target="#b6">Kantor and Globerson (2019)</ref> which requires O(k 2 ) memory with k being the number of extracted spans, while other HOI approaches only requires O(k) memory 4 . To keep the GPU memory usage within 32GB, we limit the maximum number of span candidates for EE to be 300, which may have a negative impact on the performance.</p><p>Experiments are conducted on Nvidia Tesla V100 GPUs with 32GB memory. The average training time is around 7 hours for BERT and SpanBERT without HOI, and ranges from 9 -15 hours with HOI methods. <ref type="table">Table 4</ref> reports the macro-average F1 scores out of 5 repeated developments of each approach. CM still has the best performance with 79.9 averaged F1 score. Span refinement-based HOI approaches, <ref type="bibr">4</ref> The maximum number of antecedents for all models is set to 50 which is constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Results</head><p>AA, EE, and SC, still have lower F1 scores than the local-only SpanBERT.</p><p>We do not find different configurations for CM make any huge impact to the performance. The final configuration for CM is sequential order and max reduction (Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Analysis</head><p>AA -0.02 (± 0.06) EE 0.03 (± 0.07) SC 0.11 (± 0.10) CM 0.04 (± 0.04)  <ref type="table" target="#tab_4">Table 5</ref> shows the averaged performance drop and its standard deviations w.r.t Avg-F1 after turning off the corresponding HOI in trained models, to see the direct performance impact of HOI over local decisions.</p><p>Pronoun Resolution In our analysis, the following personal pronouns are regarded as ambiguous pronouns: "you", "your", "yours".</p><formula xml:id="formula_8">MUC B 3 CEAF φ 4 F1 F1 F1 Avg. F1</formula><p>BERT 83.7 (± 0.1) 75.5 (± 0.1) 72.6 (± 0.1) 77.3 (± 0.1) SpanBERT 85.3 (± 0.1) 78.4 (± 0.1) 75.5 (± 0.3) 79.7 (± 0.1) + AA 85.2 (± 0.2) 78.1 (± 0.2) 75.0 (± 0.2) 79.4 (± 0.2) + EE 85.0 (± 0.1) 77.7 (± 0.2) 74.7 (± 0.2) 78.9 (± 0.4) + SC 85.1 (± 0.2) 77.9 (± 0.3) 74.7 (± 0.3) 79.2 (± 0.3) + CM 85.5 (± 0.2) 78.5 (± 0.3) 75.6 (± 0.2) 79.9 (± 0.2) <ref type="table">Table 4</ref>: Results on the test set of the CoNLL'12 English shared task data. Macro-average is reported for each F1 score from 5 repeated developments of each approach. See Section 4 for the approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Performance breakdown of the recent stateof-the-art models on the CoNLL 2012 shared task. W-16: Wiseman et al. (2016), C-16: Clark and Manning (2016), L-17: Lee et al. (2017), L-18: Lee et al. (2018), F-19: Fei et al. (2019), K-19: Kantor and Globerson (2019), J-19: Joshi et al. (2019), J-20: Joshi et al. (2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>that has been adapted by every coreference resolution model since then. It detects mention candidates through span enumeration and aggressive pruning. For each candidate span x, the model learns the distribution over its antecedents y ∈ Y(x): P (y) = e s(x,y) y ∈Y(x) e s(x,y )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>SpanBERT: SpanBERT (Joshi et al., 2020) as the encoder • +AA: SpanBERT with attended antecedent ( §3.2) • +EE: SpanBERT with entity equalization ( §3.2) • +SC: SpanBERT with span clustering ( §3.3) • +CM: SpanBERT with cluster merging ( §3.3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Best results on the test set of the CoNLL'12 English shared task. The averaged F1 of MUC, B 3 , CEAF φ4 is the main evaluation metric. Avg-M: the mean Avg-F1 and its standard deviation from five developments. The mean and stdev of other metrics are provided in Appendix A.2. SeeFigure 1for acronyms of the previous works. cost of more inference time from the sequential ranking process. To analyze the direct impact of HOI, we take the trained models of each HOI method and evaluate them on the test set while turning off HOI, making it compatible to SpanBERT.The averaged performance drop w.r.t Avg-F1 after turning off HOI is less than 0.2 for all methods (Appendix A.3), implying that none of the HOI method has a significantly direct impact to the final performance of the model using SpanBERT.</figDesc><table><row><cell>W2C</cell><cell>C2W</cell><cell>C2C</cell><cell>W2W</cell></row><row><cell cols="4">+ AA 240.8 (1.3) 241.2 (1.3) 16262.2 2168.4</cell></row><row><cell cols="4">+ EE 244.1 (1.3) 245.3 (1.3) 16183.3 2136.3</cell></row><row><cell cols="4">+ SC 248.2 (1.3) 262.0 (1.4) 16184.4 2146.0</cell></row><row><cell cols="4">+ CM 226.4 (1.2) 235.0 (1.2) 16446.0 2180.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Averaged statistics on the test set prediction of four HOI approaches. W2C represents the number of mentions that are linked to a Wrong antecedent before HOI and are linked to a Correct antecedent after HOI; vice versa for C2W. C2C/W2W is the number of mentions that are both linked to Correct/Wrong antecedents before and after HOI. Parentheses indicate the percentage of corresponding numbers per row.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance drop on CoNLL'12 English test set after turning off the corresponding HOI in trained models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source codes and models are available at https:// github.com/lxucs/coref-hoi.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Ambiguous pronouns such as "you" are excluded in direct inference analysis, and included in indirect inference analysis.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support of the AWS Machine Learning Research Awards (MLRA). Any contents in this material are those of the authors and do not necessarily reflect the views of AWS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1405" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end deep reinforcement learning based coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1064</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="660" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT for coreference resolution: Baselines and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1588</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5803" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coreference resolution with entity equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1066</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="673" to="677" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Higher-order coreference resolution with coarse-tofine inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving machine learning approaches to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL -Shared Task</title>
		<meeting><address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1137</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1416" to="1426" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="994" to="1004" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CorefQA: Coreference resolution as query-based span prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6953" to="6963" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A cluster ranking model for full anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Uma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
