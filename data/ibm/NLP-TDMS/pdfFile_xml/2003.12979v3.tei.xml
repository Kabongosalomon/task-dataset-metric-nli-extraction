<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatial Attention Pyramid Network for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University at Albany</orgName>
								<orgName type="institution" key="instit2">State University of New York</orgName>
								<address>
									<settlement>Albany</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">State Key Laboratory of Computer Science</orgName>
								<orgName type="institution">ISCAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">JD Finance America Corporation</orgName>
								<address>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejian</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">State Key Laboratory of Computer Science</orgName>
								<orgName type="institution">ISCAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatial Attention Pyramid Network for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Unsupervised Domain Adaptation · Spatial Attention Pyra- mid · Object Detection · Semantic Segmentation · Instance Segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation is critical in various computer vision tasks, such as object detection, instance segmentation, and semantic segmentation, which aims to alleviate performance degradation caused by domain-shift. Most of previous methods rely on a single-mode distribution of source and target domains to align them with adversarial learning, leading to inferior results in various scenarios. To that end, in this paper, we design a new spatial attention pyramid network for unsupervised domain adaptation. Specifically, we first build the spatial pyramid representation to capture context information of objects at different scales. Guided by the task-specific information, we combine the dense global structure representation and local texture patterns at each spatial location effectively using the spatial attention mechanism. In this way, the network is enforced to focus on the discriminative regions with context information for domain adaptation. We conduct extensive experiments on various challenging datasets for unsupervised domain adaptation on object detection, instance segmentation, and semantic segmentation, which demonstrates that our method performs favorably against the state-of-the-art methods by a large margin. Our source code is available at https://isrc.iscas.ac.cn/gitlab/research/domain-adaption.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>source domain and target domain are dissimilar or even completely different. To avoid expensive and time-consuming human annotation, unsupervised domain adaptation is proposed to learn discriminative cross-domain representation in such domain shift circumstance <ref type="bibr" target="#b29">[30]</ref>.</p><p>Most of previous methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b14">15]</ref> attempt to globally align the entire distributions between the source and target domains. However, it is challenging to generate a unified adaptation function for various scene layouts and appearance variation of different objects. Recent methods focus on transferring texture and color statistics within object instances or local patches. To deal with domain adaptation in the object detection and instance segmentation tasks, the basic idea in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45]</ref> is to exploit discriminative features in bounding boxes of objects and attempt to align them across both source and target domains. However, the context information around the objects is not fully exploited, causing inferior results in some scenarios. Meanwhile, some domain adaptation methods for semantic segmentation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b27">28]</ref> enforce the semantic consistency between the pixels or local patches of the two domains, leading to deficiencies of critical information from object-level patterns. To that end, recent methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref> concatenate global context feature and instance-level feature for distribution alignment, and optimize the model based on several loss terms for global-level and local-level features with pre-set weights. However, this method fails to exploit the context information of objects, which is not optimal in challenging scenarios.</p><p>In this paper, we design the spatial attention pyramid network (SAPNet) to solve unsupervised domain adaptation for object detection, instance segmentation, and semantic segmentation. Inspired by spatial pyramid pooling <ref type="bibr" target="#b11">[12]</ref>, we construct the spatial pyramid representation with multi-scale feature maps, which integrates full of holistic (image-level) and local (regions of interest) semantic information. Meanwhile, we design a task-specific guided spatial attention mechanism to capture multi-scale context information. In this way, discriminative semantic regions are attended in a soft manner to extract features for adversarial learning. Extensive experiments are conducted on various challenging domain-shift datasets, such as Cityscapes <ref type="bibr" target="#b4">[5]</ref> to FoggyCityscapes <ref type="bibr" target="#b34">[35]</ref>, PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> to Clipart <ref type="bibr" target="#b17">[18]</ref>, and GTA5 <ref type="bibr" target="#b31">[32]</ref> to Cityscapes <ref type="bibr" target="#b4">[5]</ref>. It is worth mentioning that the proposed method surpasses the state-of-the-art methods on various tasks, i.e., object detection, instance segmentation, and semantic segmentation. For example, our SAPNet improves from Cityscapes <ref type="bibr" target="#b4">[5]</ref> to FoggyCityscapes <ref type="bibr" target="#b34">[35]</ref> by 3% mAP in terms of object detection, and achieves comparable accuracy from GTA5 <ref type="bibr" target="#b31">[32]</ref> to Cityscapes <ref type="bibr" target="#b4">[5]</ref> in terms of semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions. 1)</head><p>We propose a new spatial attention pyramid network to solve the unsupervised domain adaptation task for object detection, instance segmentation, and semantic segmentation. 2) We develop a task-specific guided spatial attention pyramid learning strategy to merge multi-level semantic information in feature maps of the pyramid. 3) Extensive experiments conducted on various challenging domain-shift datasets for object detection, instance segmentation, and semantic segmentation, demonstrate the effectiveness of the proposed method, surpassing the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Unsupervised domain adaptation. Several methods have been proposed for unsupervised domain adaptation in terms of several tasks such as object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>, instance segmentation <ref type="bibr" target="#b44">[45]</ref> and semantic segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28]</ref>. For object detection domain adaptation, Chen et al. <ref type="bibr" target="#b3">[4]</ref> align source and target domain both on image level and instance level using the gradient reverse layer <ref type="bibr" target="#b7">[8]</ref>. Zhu et al. <ref type="bibr" target="#b44">[45]</ref> mine the discriminative regions (pertinent to object detection) using k-means clustering and align them across both domains, which is applied in object detection and instance segmentation. Recently, the strong-weak adaptation method is proposed in <ref type="bibr" target="#b33">[34]</ref>. It focuses the adversarial alignment loss toward images that are globally similar, and away from images that are globally dissimilar by employing focal loss <ref type="bibr" target="#b24">[25]</ref>. Shen et al. <ref type="bibr" target="#b35">[36]</ref> propose a gradient detach based stacked complementary losses method that adapt source domain and target domain on multiple layers. On the other hand, Hoffman et al. <ref type="bibr" target="#b15">[16]</ref> perform global domain alignment in a novel semantic segmentation network with fully convolutional domain adversarial learning. Tsai et al. <ref type="bibr" target="#b39">[40]</ref> learn discriminative feature representations of patches in the source domain by discovering multiple modes of patch-wise output distribution through the construction of a clustered space. Luo et al. <ref type="bibr" target="#b27">[28]</ref> introduce a category-level adversarial network to enforce local semantic consistency on the output space using two distinct classifiers. However, the aforementioned methods only consider domain adaptation in two levels, i.e., aligning the feature maps of the whole image or local regions with a fixed scale. Different from them, we design the spatial pyramid representation to capture multi-level semantic patterns within the image for better adaptation between the source domain and target domain. Attention mechanism. To focus on the most discriminative features, various attention mechanisms have been explored. SENet <ref type="bibr" target="#b16">[17]</ref> develops the Squeezeand-Excitation (SE) block that adaptively recalibrates channel-wise feature responses. Non-local networks <ref type="bibr" target="#b40">[41]</ref> capture long-range dependencies by computing the response at a position as a weighted sum of the features at all positions in the input feature maps. SKNet <ref type="bibr" target="#b22">[23]</ref> uses softmax attention to fuse multiple feature maps of different kernel sizes in a weighted manner to adaptively adjust the receptive field size of the input feature map. Except channel-wise attention, CBAM <ref type="bibr" target="#b42">[43]</ref> introduce spatial attention by calculating the inter-spatial relation of features. To highlight transferable regions in domain adaptation, Wang et al. <ref type="bibr" target="#b41">[42]</ref> use multiple region-level domain discriminators and single image-level domain discriminator to generate transferable local and global attention, respectively. Sun and Wu <ref type="bibr" target="#b37">[38]</ref> integrates atrous spatial pyramid, cascade attention mechanism and residual connections for image synthesis and image-to-image translation. As previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b23">24]</ref> have shown the importance of multi-scale information, we propose the attention pyramid learning to better adapt the source domain and target domain. Specifically, we employ the task-specific information to guide pyramid attention to make full use of semantic information of different feature maps at different levels.  Detection and segmentation networks. The performance of object detection and segmentation has boosted with the development of deep convolutional neural networks. Faster R-CNN <ref type="bibr" target="#b30">[31]</ref> is an object detection framework that predicts class-agnostic coarse object proposals using a region proposal network (RPN) and then extract fix-sized object features to classify object category and refine object location. Moreover, He et al. <ref type="bibr" target="#b10">[11]</ref> extend Faster R-CNN by adding a branch for predicting instance segmentation results. For semantic segmentation, the DeepLab-v2 method <ref type="bibr" target="#b0">[1]</ref> develops atrous spatial pyramid pooling (ASPP) modules to segment objects at multiple scales. For fair comparison, we propose the spatial attention pyramid network based on the same detection and segmentation frameworks as that in the previous domain adaptation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spatial Attention Pyramid Network</head><p>We design a spatial attention pyramid network (SAPNet) to solve various computer vision tasks, such as object detection, instance segmentation, and semantic segmentation. First of all, we define the labeled source domain D s and the unlabeled target domain D t , which are subject to the complex and unknown distributions in the source and target domains. Our goal is to find discriminative representation for the distributions of both source and target domains to capture various semantic information and local patterns in different tasks. The architecture of SAPNet is presented in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>Spatial pyramid representation. According to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>, spatial pyramid pooling can maintain spatial information by pooling in local spatial bins. To better adapt source and target domains, we develop a spatial pyramid representation to exploit the underlying distribution within an image. Specifically, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the feature mapf ∈ RĈ ×H×W is extracted from the backbone G of the network, whereĈ, H and W are the channel dimension, height, width of feature maps respectively. To improve efficiency, we first reduce the number of channels inf tof ∈ R C×H×W gradually by using three 1 × 1 convolutional layers, i.e., we set C = 256 in all our experiments. Second, we use multiple average pooling layers with different sizes to operate upon the feature mapf separately. The sizes of the pooling operation are {k n } N n=1 , where N is the number of pooling layers. That is, the rectangular pooling region with the size k n at each location off is down-sampled to the average value of each region, resulting in the pyramid of N pooled feature maps {f 1 , . . . , f N }. In this way, every pooled feature map f n ∈ R C×Hn×Wn in the pyramid can encode different semantic information of objects or layouts within the image.</p><p>It is worth mentioning that the proposed spatial pyramid representation is related with spatial pyramid pooling (SPP) for visual recognition <ref type="bibr" target="#b11">[12]</ref>. While they share the pooling concept, we would like to highlight two important differences. First, we use average pooling instead of max pooling to construct the spatial pyramid representation. It can better capture the overall strength of local patterns such as edges and textures, which is demonstrated in the ablation study. Second, SPP pools the features with just a few window sizes and concatenates them to generate fixed-length representation; while SAP is designed to capture multi-scale context information of all levels in the pyramid. Thus, it is difficult to use a large number of windows with different sizes for SPP due to computational complexity.</p><p>Attention mechanism. Moreover, we integrate the spatial attention pyramid strategy to enforce the network to focus on the most discriminative semantic regions and feature maps. There are mainly two advantages of introducing the attention mechanism in the spatial pyramid architecture. First, there exist different local patterns in each spatial location of feature maps. Second, different feature maps in the pyramid have different contributions to the semantic representation. The detailed learning method is described in two aspects as follows.</p><p>To facilitate highlighting the most discriminative semantic regions, the spatial attention masks for the pyramid {f 1 , . . . , f N } are learned based on the guided information from the task-specific heads (i.e., object detection, instance segmentation and semantic segmentation). For object detection and instance segmentation, the guided information is the output map with the size of A × H × W from the classification head of region proposal network (RPN). It can predict object's confidence in terms of the locations in feature maps for all the number of anchors A. That is, it encodes the distribution of objects which is suitable for object detection and instance segmentation problem. For semantic segmentation, the guided information is the output map with the size of C sem × H × W from the segmentation head, where C sem is the number of semantic categories. We denote the guided map asP. Then, we concatenate the guided mapP and feature mapf to generate guided feature mapP followed by 3 convolutional layers. The guided feature mapP is shared for all N scales. To adjust each scale of feature map f n in the spatial pyramid, we resizeP to the size C × H n × W n at each level. The spatial attention mask P n ∈ R Hn×Wn can be predicted by the followed 3 convolutional layers. Finally, P n is normalized using the softmax function to compute the spatial attention, i.e., ω n (x, y) = e P n (x,y)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hn i=1</head><p>Wn j=1 e P n (i,j)</p><p>,</p><p>where ω n (i, j) indicates the value of the attention mask ω n at (i, j). Thus, we have</p><formula xml:id="formula_1">Hn i=1</formula><p>Wn j=1 ω n (i, j) = 1. As shown in <ref type="figure">Fig. 2</ref>, we provide some examples with normalized attention masks ω n for different feature maps in the spatial pyramid when N = 7. We can conclude that the feature map with different scale k n focuses on different semantic regions. For example, in the forth row, the feature map with smaller pooling size (k = 3) pays more attention on the seagull, while the feature map with larger pooling size (k = 21) focuses on the sail boat and the neighbouring context. Based on different guided information, ω n recalibrates spatial responses in feature map f n adaptively.</p><p>On the other hand, it can be seen that not all the attention masks correspond to meaningful regions (see the attention mask with pooling size k = 37 in the forth row). Inspired by <ref type="bibr" target="#b22">[23]</ref>, we develop a dynamic weight selection mechanism to adjust the channel-wise weight of feature maps in the pyramid adaptively. To consider feature maps with different size, we normalize f n to an attention vector V n ∈ R C×1 using the corresponding spatial attention weight ω n as:</p><formula xml:id="formula_2">V n = Hn i=1 Wn j=1 f n · ω n ,<label>(2)</label></formula><p>where i and j enumerate all spatial positions of weighted feature map f n · ω n . Thus the attention vectors have the same size for all the feature maps in the pyramid. Given attention vectors {V 1 , . . . , V N }, we first fuse these vectors via an element-wise addition, i.e., v = N n=1 V n . Then, a compact feature z ∈ R d×1 is created to enable the guidance for adaptive selections by the batch normalization layer, where d is the dimension of the compact feature z, and we set it to C 2 in all experiments. After that, for each attention vector V n , we compute the channelwise attention weight φ n ∈ R C×1 as</p><formula xml:id="formula_3">φ n = e an·z N i=1 e ai·z ,<label>(3)</label></formula><p>where {a i , . . . , a N } are learnable parameters of fully connected layers for each scale. We have N c=1 φ n (c) = 1, where φ n (c) is the c-th element of φ n . In <ref type="figure">Fig. 2</ref>, we show the corresponding weights of each feature map in the spatial pyramid. Specifically, we compute the mean of channel-wise attention weight φ n for each scale in each image. Finally, the fused semantic vector V ∈ R C×1 is obtained through the channel-wise attention weight as V = N n=1 V n · φ n , where V is a highly embedded vector in the latent space that encodes the semantic information of different spatial locations, different channels and the relations among them.</p><p>Optimization. The whole network is trained by minimizing two loss terms, i.e., adversarial loss and task-specific loss. The adversarial loss is used to determine whether the sample comes from the source domain or target domain. Specifically, we calculate the probability x i of the sample belonging to the target domain based on the fused semantic vector V using a simple fully-connected layer. The proposed SAPNet is denoted as D. Then, the adversarial loss is computed as</p><formula xml:id="formula_4">L adv (G, D) = 1 |D s ∪ D t | xi∈Ds∪Dt H(D(G(x i )), y i ),<label>(4)</label></formula><p>where y i is the domain label (0 for source domain and 1 for target domain) and H is the cross-entropy loss function. On the other hand, the task loss L task is determined by the specific task, i.e., object detection, instance segmentation, and semantic segmentation. The loss is computed as where G and R are the backbone and task-specific components of the network, respectively. y s i is the ground-truth label of sample i in the source domain. We have L task-specific = {L det , L ins , L seg }. Taking object detection as an example, we denote the objective of Faster R-CNN as L det , which contains classification loss of object categories and regression loss of object bounding boxes. In summary, the overall objective is formulated as</p><formula xml:id="formula_5">L task (G, R) = 1 |D s | xi∈Ds L task-specific (R(G(x i )), y s i ),<label>(5)</label></formula><formula xml:id="formula_6">max D min G,R L task (G, R) − λL adv (G, D),<label>(6)</label></formula><p>where λ controls the trade-off between task-specific loss and adversarial training loss. Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref>, we use gradient reverse layer (GRL) <ref type="bibr" target="#b7">[8]</ref> to enable adversarial training where the gradient is reversed before back-propagating to G from D. We first train the networks with only source domain to avoid initially noisy predictions. Then we train the whole model with Adam optimizer and the initial learning rate is set to 10 −5 , then divided by 10 at 70, 000, 80, 000 iterations. The total number of training iterations is 90, 000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We implement our SAPNet method with PyTorch <ref type="bibr" target="#b28">[29]</ref>, which is evaluated in three domain adaptation tasks, including object detection, instance segmentation, and semantic segmentation. For fair comparison, we set the shorter side of the image to 600 following the implementation of <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref> with RoIAlign <ref type="bibr" target="#b10">[11]</ref> in object detection; for instance segmentation and semantic segmentation, we use the same settings as previous methods. To consider the trade-off between accuracy and complexity, the number of pyramid levels is set to N = 13 for object detection and instance segmentation, i.e., we have the spatial pooling size set K = {3, <ref type="bibr">6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 35, 37}</ref>. Note that we start from the initial pooling size 3 × 3 with the step of 3, and the last two pooling sizes are reduced from {38, 41} to {35, 37} because of the width limit of feature map. For semantic segmentation, the number of pyramid levels is set to N = 9 since semantic segmentation involves feature maps with higher resolution, i.e., K = {3, <ref type="bibr">9, 15, 21, 27, 33, 39, 45, 51}</ref>. The hyper-parameter λ is used to control the adaptation between source and target domains. Thus, we use different λ in different tasks. Empirically, we set a larger λ for adaptation between similar domains (e.g., Cityscapes→FoggyCityscapes), and set a smaller λ for adaptation between dissimilar domains (e.g., PASCAL VOC→WaterColor). We choose λ based on the performance on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Domain Adaptation for Detection</head><p>For object detection task, we conduct our experiments in 3 domain shift scenarios: (1) similar domains; (2) dissimilar domains; and (3) from synthetic to real images. We compare our model to the state-of-the-art methods on 6 domain shift datasets: Cityscapes <ref type="bibr" target="#b4">[5]</ref> to FoggyCityscapes <ref type="bibr" target="#b34">[35]</ref>, Cityscapes <ref type="bibr" target="#b4">[5]</ref> to KITTI <ref type="bibr" target="#b8">[9]</ref>, KITTI <ref type="bibr" target="#b8">[9]</ref> to Cityscapes <ref type="bibr" target="#b4">[5]</ref>, PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> to Clipart <ref type="bibr" target="#b17">[18]</ref>, PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> to Watercolor <ref type="bibr" target="#b17">[18]</ref>, Sim10K <ref type="bibr" target="#b18">[19]</ref> to Cityscapes <ref type="bibr" target="#b4">[5]</ref>. For fair comparison, we use ResNet101 and VGG-16 as the backbone and the last convolutional layer to enable domain adaptation as similar as that in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref>. Some qualitative adaptation results of object detection are shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>Cityscapes→FoggyCityscapes. Notably, we evaluate our model between Cityscapes <ref type="bibr" target="#b4">[5]</ref> and FoggyCityscapes <ref type="bibr" target="#b34">[35]</ref> (simulated attenuation coefficient β = 0.02) at the most difficult level. Specifically, Cityscapes is the source domain, while the target domain FoggyCityscape (Foggy for short) is rendered from the same images in Cityscape using depth information. We set λ = 1.0 in (6) empirically. As shown in <ref type="table" target="#tab_1">Table 1</ref>, our SAPNet gains 3.0% average accuracy improvement compared with the previous state-of-the-art methods. Specifically, in terms of person and car categories, our method outperforms the second performer with a huge margin (about 9% and 15% higher, respectively). Cityscapes↔KITTI. As shown in <ref type="table" target="#tab_2">Table 2</ref>, we present the comparison between our model and state-of-the-art on domain adaptation between Cityscapes <ref type="bibr" target="#b4">[5]</ref> and KITTI <ref type="bibr" target="#b8">[9]</ref>. Similar to the works in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>, we use KITTI training set that contains 7, 481 images. We set λ = 0.01 for Cityscapes → KITTI and λ = 0.2 for KITTI → Cityscapes in (6) empirically. The Strong-Weak and SCL <ref type="bibr" target="#b35">[36]</ref> methods only employ multi-stage feature maps from the backbone to align holistic features, resulting in inferior performance than our method on both directions. In summary, our method achieves 1.5% and 2.5% accuracy improvement of KITTI → Cityscapes and Cityscapes → KITTI, respectively.</p><p>PASCAL VOC→Clipart/WaterColor. Moreover, we evaluate our method on dissimilar domains, i.e., from real images to artistic images. According to <ref type="bibr" target="#b33">[34]</ref>, PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> is the source domain, where the PASCAL VOC 2007 and 2012 training and validation sets are used for training. For the target domain, we use Clipart <ref type="bibr" target="#b17">[18]</ref> and Watercolor <ref type="bibr" target="#b17">[18]</ref> as that in <ref type="bibr" target="#b33">[34]</ref>. ResNet-101 <ref type="bibr" target="#b12">[13]</ref> pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref> is used as the backbone network following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref>. We set λ = 0.1 and λ = 0.01 for Clipart <ref type="bibr" target="#b17">[18]</ref> and Watercolor <ref type="bibr" target="#b17">[18]</ref> respectively. As shown in <ref type="table" target="#tab_3">Table  3</ref> and <ref type="table" target="#tab_4">Table 4</ref>, our model obtains comparable results with SCL <ref type="bibr" target="#b35">[36]</ref>.</p><p>Sim10K→Cityscapes. In addition, we evaluate our model in the synthetic to real scenario. Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref>, we use Sim10K <ref type="bibr" target="#b18">[19]</ref> as the source domain that contains 10, 000 training images collected from the computer game Grand Theft Auto 5 (GTA5). We set λ = 0.1 in (6) empirically. As shown in <ref type="table" target="#tab_6">Table 5</ref>, our SAPNet obtains 3.3% improvement in terms of AP score compared with stateof-the-art methods. It is worth mentioning that BDC-Faster <ref type="bibr" target="#b33">[34]</ref> is also trained using crossentropy loss but the performance is significantly decreased. Therefore, The Strong-Weak method <ref type="bibr" target="#b33">[34]</ref> adapts the focal loss <ref type="bibr" target="#b24">[25]</ref> to balance different regions. Compared with <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref>, our proposed attention mechanism is much more effective and thus the focal loss module is no longer needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Domain Adaptation for Segmentation</head><p>Instance Segmentation. For instance segmentation task, we evaluate our model from Cityscapes <ref type="bibr" target="#b4">[5]</ref> to FoggyCityscapes <ref type="bibr" target="#b34">[35]</ref>. Similar to <ref type="bibr" target="#b44">[45]</ref>, we use the  VGG16 as the backbone network and add the segmentation head similar to that in Mask R-CNN <ref type="bibr" target="#b10">[11]</ref>. From <ref type="table" target="#tab_7">Table 6</ref>, we can conclude that our method outperforms SCDA <ref type="bibr" target="#b44">[45]</ref>   <ref type="figure" target="#fig_3">Fig. 4</ref>. Semantic Segmentation. For semantic segmentation task, we conduct experiments from GTA5 <ref type="bibr" target="#b31">[32]</ref> to Cityscapes <ref type="bibr" target="#b4">[5]</ref> and SYNTHIA <ref type="bibr" target="#b32">[33]</ref> to Cityscapes. Following <ref type="bibr" target="#b27">[28]</ref>, we use the DeepLab-v2 <ref type="bibr" target="#b0">[1]</ref> framework with ResNet-101 backbone that is pre-trained on ImageNet. Notably, the task-specific guided map for semantic segmentation naturally comes from the predicted output with the shape of C sem ×H ×W , where C sem is the number of semantic categories. As presented in <ref type="table" target="#tab_8">Table 7</ref> and <ref type="table" target="#tab_9">Table 8</ref>, our method achieves comparable segmentation accuracy with state-of-the-arts on the domain adaptation from GTA5 <ref type="bibr" target="#b31">[32]</ref> to Cityscapes <ref type="bibr" target="#b4">[5]</ref>, and from SYNTHIA <ref type="bibr" target="#b32">[33]</ref> to Cityscapes. Some visual examples of adaptation semantic segmentation results are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We further perform experiments to study the effect of important aspects in SAP-Net, i.e., task-specific guided map and spatial attention pyramid. Since PASCAL   VOC → Clipart, Sim10k → Cityscapes and Cityscapes → Foggy represent three different domain shift scenarios, we perform ablation study in terms of object detection datasets for comprehensive analysis.</p><p>Task-specific guided information: To investigate the importance of taskspecific guided information, we remove the task-specific guidance to generate the spatial attention mask, which is denoted as "w/o GM". In this way, the number of channels of the first convolutional layer after concatenation of feature maps in layer 3 and task-specific guided information is reduced (see <ref type="figure" target="#fig_1">Fig. 1</ref>). However, the impact is negligible since the channel number of guided map is small. As presented in <ref type="table" target="#tab_10">Table 9</ref>, the task-specific guided information improves the accuracy, especially for dissimilar domains PASCAL VOC and Clipart (42.2 vs. 37.1). We speculate that such guidance can facilitate focusing on the most discriminative semantic regions for domain adaptation. Spatial attention pyramid: To investigate the effectiveness of spatial attention pyramid, we construct the "w/o SA" variant of SAPNet, which indicates that we remove the spatial attention masks and global attention pyramid (since no multi-scale vectors are available) in <ref type="figure" target="#fig_1">Fig. 1</ref>. As shown in <ref type="table" target="#tab_10">Table 9</ref>, the performance drops dramatically without spatial attention pyramid. On the other hand, along with the increasing number of pooled feature maps in the pyramid, the performance is gradually improved. Specifically, we use the spatial  pooling size set K = {3, <ref type="bibr">6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 35, 37}</ref> when N = 13, K = {3, 9, 15, 21, 27, 33, 37} when N = 7 and K = {3, 21, 37} when N = 3. It indicates that the spatial pyramid with deep levels contains more discriminative semantic information for domain adaptation and our method can make full use of it. In addition, we compare average pooling and maximal pooling operations in spatial attention pyramid. We can conclude that average pooling achieves better performance in different datasets, which demonstrates the effectiveness of average pooling to capture discriminative local patterns for domain adaptation.</p><p>Channel-wise attention: To verify the effectiveness of channel-wise attention, we conduct two variants to compute the embedded vector V, where weighted summation V = N n=1 V n · w n and equal summation V = 1 N N n=1 V n are denoted as "w/ CA" and "w/o CA" respectively. The results are shown in <ref type="table" target="#tab_10">Table 9</ref>. Notably, for similar domains (e.g., Sim10k to Cityscapes or Cityscapes to Foggy Cityscapes), we obtain very similar result without channel-wise attention; while for dissimilar domains (e.g., PASCAL to Clipart or PASCAL to Watercolor), we observe an obvious drop in performance, i.e., 4.5% vs. 2.9%. This is maybe because similar/dissimilar domains share similar/different semantic information in each feature map of the spatial pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Image</head><p>Adapted (Ours) Ground-Truth Non-adapted  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose a general unsupervised domain adaptation framework for various computer vision tasks including object detection, instance segmentation and semantic segmentation. Given target-specific guided information, our method can make full use of feature maps in the spatial attention pyramid, which enforces the network to focus on the most discriminative semantic regions for domain adaptation. Extensive experiments conducted on various challenging domain adaptation datasets demonstrate the effectiveness of the proposed, which performs favorably against the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>The framework of spatial attention pyramid network. For clarity, we only show N = 3 levels in the spatial pyramid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Adaptation object detection results. From left to right: Foggy Cityscapes, Watercolor and Clipart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Instance segmentation results for Cityscapes → Foggy Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Semantic segmentation results for GTA5 → Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Adaptation detection results from Cityscapes to FoggyCityscapes.</figDesc><table><row><cell>Method</cell><cell>person rider car truck bus train cycle bicycle mAP</cell></row><row><cell cols="2">Faster R-CNN (w/o) 24.1 33.1 34.3 4.1 22.3 3.0 15.3 26.5 20.3</cell></row><row><cell>DA-Faster [4]</cell><cell>25.0 31.0 40.5 22.1 35.3 20.2 20.0 27.1 27.6</cell></row><row><cell>SCDA [45]</cell><cell>33.5 38.0 48.5 26.5 39.0 23.3 28.0 33.6 33.8</cell></row><row><cell>Strong-Weak [34]</cell><cell>29.9 42.3 43.5 24.5 36.2 32.6 30.0 35.3 34.3</cell></row><row><cell cols="2">Diversify&amp;match [21] 30.8 40.5 44.3 27.2 38.4 34.5 28.4 32.2 34.6</cell></row><row><cell>MAF [14]</cell><cell>28.2 39.5 43.9 23.8 39.9 33.3 29.2 33.9 34.0</cell></row><row><cell>SCL [36]</cell><cell>31.6 44.0 44.8 30.4 41.8 40.7 33.6 36.2 37.9</cell></row><row><cell>SAPNet</cell><cell>40.8 46.7 59.8 24.3 46.8 37.5 30.4 40.7 40.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Adaptation detection results between KITTI and Cityscapes. We report AP scores in terms of the car category on both directions, including KITTI → Cityscapes and Cityscapes → KITTI.</figDesc><table><row><cell>Method</cell><cell>KITTI → Cityscapes</cell><cell>Cityscapes → KITTI</cell></row><row><cell>Faster RCNN</cell><cell>30.2</cell><cell>53.5</cell></row><row><cell>DA-Faster [4]</cell><cell>38.5</cell><cell>64.1</cell></row><row><cell>Strong-Weak (impl. of [36])</cell><cell>37.9</cell><cell>71.0</cell></row><row><cell>SCL [36]</cell><cell>41.9</cell><cell>72.7</cell></row><row><cell>SCDA [45]</cell><cell>42.5</cell><cell>-</cell></row><row><cell>SAPNet</cell><cell>43.4</cell><cell>75.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Adaptation detection results from PASCAL VOC to Clipart.</figDesc><table><row><cell>Method</cell><cell>aero bicycle bird boat bottle bus car cat chair cow</cell></row><row><cell>FRCNN [31]</cell><cell>35.6 52.5 24.3 23.0 20.0 43.9 32.8 10.7 30.6 11.7</cell></row><row><cell cols="2">BDC-Faster [34] 20.2 46.4 20.4 19.3 18.7 41.3 26.5 6.4 33.2 11.7</cell></row><row><cell cols="2">DA-Faster [4] 15.0 34.6 12.4 11.9 19.8 21.1 23.2 3.1 22.1 26.3</cell></row><row><cell cols="2">WST-BSR [20] 28.0 64.5 23.9 19.0 21.9 64.3 43.5 16.4 42.2 25.9</cell></row><row><cell cols="2">Strong-Weak [34] 26.2 48.5 32.6 33.7 38.5 54.3 37.1 18.6 34.8 58.3</cell></row><row><cell>SCL [36]</cell><cell>44.7 50.0 33.6 27.4 42.2 55.6 38.3 19.2 37.9 69.0</cell></row><row><cell>Ours</cell><cell>27.4 70.8 32.0 27.9 42.4 63.5 47.5 14.3 48.2 46.1</cell></row><row><cell></cell><cell>table dog horse bike person plant sheep sofa train tv mAP</cell></row><row><cell>FRCNN [31]</cell><cell>13.8 6.0 36.8 45.9 48.7 41.9 16.5 7.3 22.9 32.0 27.8</cell></row><row><cell cols="2">BDC-Faster [34] 26.0 1.7 36.6 41.5 37.7 44.5 10.6 20.4 33.3 15.5 25.6</cell></row><row><cell cols="2">DA-Faster [4] 10.6 10.0 19.6 39.4 34.6 29.3 1.0 17.1 19.7 24.8 19.8</cell></row><row><cell cols="2">WST-BSR [20] 30.5 7.9 25.5 67.6 54.5 36.4 10.3 31.2 57.4 43.5 35.7</cell></row><row><cell cols="2">Strong-Weak [34] 17.0 12.5 33.8 65.5 61.6 52.0 9.3 24.9 54.1 49.1 38.1</cell></row><row><cell>SCL [36]</cell><cell>30.1 26.3 34.4 67.3 61.0 47.9 21.4 26.3 50.1 47.3 41.5</cell></row><row><cell>Ours</cell><cell>31.8 17.9 43.8 68.0 68.1 49.0 18.7 20.4 55.8 51.3 42.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Adaptation detection results from PASCAL VOC to WaterColor.</figDesc><table><row><cell>Method</cell><cell>bike</cell><cell>bird</cell><cell>car</cell><cell>cat</cell><cell>dog</cell><cell>person</cell><cell>mAP</cell></row><row><cell>Faster RCNN</cell><cell>68.8</cell><cell>46.8</cell><cell>37.2</cell><cell>32.7</cell><cell>21.3</cell><cell>60.7</cell><cell>44.6</cell></row><row><cell>DA-Faster [4]</cell><cell>75.2</cell><cell>40.6</cell><cell>48.0</cell><cell>31.5</cell><cell>20.6</cell><cell>60.0</cell><cell>46.0</cell></row><row><cell>Strong-Weak [34]</cell><cell>82.3</cell><cell>55.9</cell><cell>46.5</cell><cell>32.7</cell><cell>35.5</cell><cell>66.7</cell><cell>53.3</cell></row><row><cell>SCL [36]</cell><cell>82.2</cell><cell>55.1</cell><cell>51.8</cell><cell>39.6</cell><cell>38.4</cell><cell>64.0</cell><cell>55.2</cell></row><row><cell>Ours</cell><cell>81.1</cell><cell>51.1</cell><cell>53.6</cell><cell>34.3</cell><cell>39.8</cell><cell>71.3</cell><cell>55.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>significantly, i.e., 39.4 vs. 31.4. Some visual examples of adaptation instance segmentation results are shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Adaptation detection results from Sim10k to Cityscapes.</figDesc><table><row><cell>Method</cell><cell>AP on Car</cell></row><row><cell>Faster R-CNN</cell><cell>34.6</cell></row><row><cell>DA-Faster [4]</cell><cell>38.9</cell></row><row><cell>Strong-Weak [34]</cell><cell>40.1</cell></row><row><cell>SCL [36]</cell><cell>42.6</cell></row><row><cell>SCDA [45]</cell><cell>43.0</cell></row><row><cell>Ours</cell><cell>44.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Adaptation instance segmentation results from Cityscapes to Fog-gyCityscapes.</figDesc><table><row><cell>Method</cell><cell>mAP</cell></row><row><cell>Source Only</cell><cell>26.6</cell></row><row><cell>SCDA [45]</cell><cell>31.4</cell></row><row><cell>Ours</cell><cell>39.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Adaptation semantic segmentation results from GTA5 to Cityscapes. road side buil. wall fence pole light sign vege. terr. Source 75.8 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 ROAD [3] 76.3 36.1 69.6 28.6 22.4 28.6 29.3 14.8 82.3 35.3 TAN [39] 86.5 25.9 79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 CLAN [28] 87.0 27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 Ours 88.4 38.7 79.5 29.4 24.7 27.3 32.6 20.4 82.2 32.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4 CLAN [28] 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2 Ours 73.3 55.5 26.9 82.4 31.8 41.8 2.4 26.5 24.1 43.2</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">sky pers. rider car truck bus train motor bike mIoU</cell></row><row><cell>Source</cell><cell>70.3 53.8 26.4 49.9 17.2 25.9 6.5</cell><cell>25.3 36.0 36.6</cell></row><row><cell cols="2">ROAD [3] 72.9 54.4 17.8 78.9 27.7 30.3 4.0</cell><cell>24.9 12.6 39.4</cell></row><row><cell>TAN [39]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Adaptation semantic segmentation results from SYNTHIA to Cityscapes. road side buil. light sign vege. sky pers. rider car bus motor bike mIoU Source 55.6 23.8 74.6 6.1 12.1 74.8 79.0 55.3 19.1 39.6 23.3 13.7 25.0 38.6 TAN [39] 79.2 37.2 78.8 9.9 10.5 78.2 80.5 53.5 19.6 67.0 29.5 21.6 31.3 45.9 CLAN [28] 81.3 37.0 80.1 16.1 13.7 78.2 81.5 53.4 21.2 73.0 32.9 22.6 30.7 47.8 Ours 81.7 33.5 75.9 7.0 6.3 74.8 78.9 52.1 21.3 75.7 30.6 10.8 28.0 44.3</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Effectiveness of important aspects in SAPNet.</figDesc><table><row><cell>Variant</cell><cell cols="3">PASCAL VOC→Clipart Sim10k→Cityscapes Cityscapes→Foggy</cell></row><row><cell>w/o GM</cell><cell>37.1</cell><cell>43.8</cell><cell>38.3</cell></row><row><cell>w/ GM</cell><cell>42.2</cell><cell>44.9</cell><cell>40.9</cell></row><row><cell>w/o CA</cell><cell>37.7</cell><cell>45.6</cell><cell>40.4</cell></row><row><cell>w/ CA</cell><cell>42.2</cell><cell>44.9</cell><cell>40.9</cell></row><row><cell>w/o SA</cell><cell>35.4</cell><cell>38.3</cell><cell>36.6</cell></row><row><cell>w/ SA(N = 3)</cell><cell>39.6</cell><cell>43.9</cell><cell>39.0</cell></row><row><cell>w/ SA(N = 7)</cell><cell>40.2</cell><cell>45.9</cell><cell>40.5</cell></row><row><cell>w/ SA(N = 13)</cell><cell>42.2</cell><cell>44.9</cell><cell>40.9</cell></row><row><cell>max pooling</cell><cell>37.5</cell><cell>43.1</cell><cell>34.9</cell></row><row><cell>avg pooling</cell><cell>42.2</cell><cell>44.9</cell><cell>40.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ROAD: reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7892" to="7901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Domain adaptive faster R-CNN for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3339" to="3348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<editor>Bach, F.R., Blei, D.M.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boundary-aware instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="587" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-adversarial faster-rcnn for unrestricted object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1907.10343</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1994" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1612.02649</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5001" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks? In: ICRA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rosaen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="746" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Self-training and adversarial background regularization for unsupervised domain adaptive one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1909.00597</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Diversify and match: A domain adaptive representation learning paradigm for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12456" to="12465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="700" to="708" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Covariate shift and local learning by distribution matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quiñonero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Strong-weak distribution alignment for adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6956" to="6965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">SCL: towards accurate domain adaptive object detection via gradient detach based stacked complementary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<idno>abs/1911.02559</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning spatial pyramid attentive pooling in image synthesis and image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1901.06322</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno>abs/1901.05427</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks. In: CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Transferable attention for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="5345" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<editor>Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y.</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adapting object detectors via selective cross-domain alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
