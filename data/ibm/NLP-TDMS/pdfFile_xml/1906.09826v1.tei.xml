<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Communications and Networking</orgName>
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>210003</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Zhou</surname></persName>
							<email>quan.zhou@njupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Communications and Networking</orgName>
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>210003</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center of Communications and Networking</orgName>
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>210003</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ESNet: An Efficient Symmetric Network for Real-time Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Real-time Semantic Segmentation · DCNNs · Factorized Convolution</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent years have witnessed great advances for semantic segmentation using deep convolutional neural networks (DCNNs). However, a large number of convolutional layers and feature channels lead to semantic segmentation as a computationally heavy task, which is disadvantage to the scenario with limited resources. In this paper, we design an efficient symmetric network, called (ESNet), to address this problem. The whole network has nearly symmetric architecture, which is mainly composed of a series of factorized convolution unit (FCU) and its parallel counterparts. On one hand, the FCU adopts a widely-used 1D factorized convolution in residual layers. On the other hand, the parallel version employs a transform-split-transform-merge strategy in the designment of residual module, where the split branch adopts dilated convolutions with different rate to enlarge receptive field. Our model has nearly 1.6M parameters, and is able to be performed over 62 FPS on a single GTX 1080Ti GPU. The experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy trade-off for realtime semantic segmentation on CityScapes dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation plays a significant role in image understanding <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. From the perspective of computer vision, the task here is to assign a semantic label for each image pixel, which thus can be also considered as a dense prediction problem. Unlike conventional approaches that handle this challenge task by designing hand-craft features, deep convolutional neural networks (DCNNs) have shown their impressive capabilities in terms of end-to-end segmentation with full image resolution. The first prominent work in this field is fully convolutional networks (FCNs) <ref type="bibr" target="#b3">[4]</ref>, which are composed by a series of convolutional and max-pooling layers. After that, vast number of FCN-based network architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> have been proposed and the remarkable progress have been achieved within segmentation accuracy. However, multiple stages of spatial pooling and convolution stride significantly reduce the dimension of feature representation, thereby losing much of the finer image structure. In order to address this problem, a more deeper architecture, named encoder-decoder network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, has become a trend, where the encoder network is utilized to abstract image features and the decoder counterpart is employed to sequentially recover image details.</p><p>In the designment of network architecture, the residual network (ResNet) <ref type="bibr" target="#b1">[2]</ref> has been commonly adopted in recent years, where the residual layer allows to stack large amounts of convolutional layers, leading to the great improvement for both image classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11]</ref> and semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>In spite of achieving impressive results, these accurate DCNNs neglect the implementing efficiency, which is a significant factor in limited resources scenarios. Considering running DCNNs on the mobile platforms (e.g., drones, robots, and smartphones), the designed networks are not only required to perform reliably (stability), but also required to conduct fast (real-time), suitable for embedded devices with space and memory constraints (compactness), and have low power consumption due to limited energy overhead (energy-saving). With this in mind, some preliminary research work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> have been proposed to design lightweight networks that aim to develop efficient architectures for real-time semantic segmentation. However, these approaches usually focus on accelerating inference speed by aggressively reducing network parameters, which highly detriments segmentation performance. Therefore, pursuing the best performance with a good trade-off between accuracy and efficiency still remains an open research issue for the task of real-time semantic segmentation.</p><p>In this paper, we design a novel lightweight network called ESNet, adopting a nearly symmetric encoder-decoder architecture to address above problems. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, our ESNet is based on ResNet <ref type="bibr" target="#b1">[2]</ref>, which consists of four basic components, including down-sampling unit, upsampling unit, factorized convolution unit (FCU) and its parallel version. The core element of our architecture is parallel factorized convolution unit (PFCU), where a novel transform-splittransform-merge strategy is employed in the designment of residual layer, approaching the representational power of large and dense layers, but at a considerably lower computational complexity. More specifically, the PFCU leverages the identity mappings and multi-path factorized convolutions with 1D filter kernels. While the identity mappings allow the convolutions to learn residual functions that facilitate training, the multi-path factorized convolutions allow a significant reduction of the convolutional layers. On the other hand, in contrast to previous lightweight networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> that abstract feature representation with fixed filter kernel size, the FCUs adopt the 1D factorized convolutions with different kernel size, where the receptive fields are adaptive to capture object instances with different scales. The FCUs and PFCUs are symmetrically stacked to construct our encoder-decoder architecture, producing semantic segmentation output end-to-end in the same resolution as the input image. Although the focus of this paper is the task of semantic segmentation, the proposed FCUs and PFCUs is directly transferable to any existing network that makes use of residual layers, including both classification and segmentation architectures. In summary, our contributions are three-folds: (1) The symmetrical architecture of ESNet leads to the great reduction of network complexity, accelerating the entire inference process; (2) Using multiple branch parallel convolutions in the residual layer leverages network size and powerful feature representation, still resulting in the whole network can be trained end-to-end. (3) We evaluate the performance of ESNet on CityScapes dataset <ref type="bibr" target="#b19">[20]</ref>, and the experimental results show that compared with recent mainstream lightweight networks, it achieves the best available trade-off in terms of accuracy and efficiency.</p><p>The remainder of this paper is organized as follows. After a brief discussion of related work in Section 2, a fast and compact architecture named ESNet is proposed in Section 3. The proposed network has been evaluated on CityScapes dataset, where the benchmark is constructed on a single NVIDIA GTX 1080Ti GPU. These experiments can be found in Section 4. Finally, the conclusion remarks and future work are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent success in DCNNs has brought significant progress on semantic segmentation in the past few years <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. As a pioneer work, Farabet et. al <ref type="bibr" target="#b20">[21]</ref> utilized DCNNs to abstract hierarchical feature representation for semantic segmentation. In <ref type="bibr" target="#b3">[4]</ref>, Long et. al first proposed an end-to-end segmentation based on VGG-16 network, where the fully connected layers in traditional DCNNs are replaced by convolutional layers to upsample feature maps. So far, a large amount of FCN-based networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> have been designed to deal with semantic segmentation challenge. To enlarge receptive fields, the dilated convolution <ref type="bibr" target="#b23">[24]</ref> or atrous convolution <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref> is also employed in FCN to capture large scale context. Due to continuous pooling, however, the resolution of feature maps are significant reduced and directly adapting FCNs always leads to the poor estimation outputs. To refine predict results, the encoder-decoder networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref> are commonly-used to develop FCN architecture by sequentially recovering fine image details. For instance, Noh et al. employ deconvolution to upsample low resolution feature responses <ref type="bibr" target="#b9">[10]</ref>. SegNet <ref type="bibr" target="#b7">[8]</ref> reuses the recorded pooling indices to upsample feature maps, and learns extra deconvolutional layers to densify the feature responses. Through adding skip connections, U-Net <ref type="bibr" target="#b24">[25]</ref> designs an elegant symmetric network architecture, which stacks convolutional features from the encoder to the decoder activations. More recently, more attention have been paid to RefineNets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, which adopt ResNet <ref type="bibr" target="#b1">[2]</ref> in encoder-decoder structure, and have been demonstrated very effective on several semantic segmentation benchmarks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>In spite of achieving promising performance, these advances are at the sacrifice of running time and speed. In order to overcome this problem, many lightweight networks, initially designed for image classification task <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, have been designed to balance the segmentation accuracy and implementing efficiency <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. ENet <ref type="bibr" target="#b16">[17]</ref> is the first work that considers the efficiency issue, where the point-wise convolution is adopted in the residual layer. Apart from this initial designment, some recent work always employ convolution factorization principle <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref> in their network architecture, where the 2D standard convolution is replaced by depthwise separable convolution. For example, Zhao et al. <ref type="bibr" target="#b34">[35]</ref> investigate the high-level label cues to improve performance. ERFNet <ref type="bibr" target="#b18">[19]</ref> leverages skip connections and 1D convolutions in residual block designment, greatly reducing network parameters while maintaining high efficiency. In <ref type="bibr" target="#b17">[18]</ref>, Mehta et al. design an efficient spatial pyramid convolution network for semantic segmentation. Some similar networks also use symmetrical encoder-decoder architecture <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref>, while the other approaches take the contextual clues into account <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref> to balance performance and efficiency. Unlike these lightweight networks, our ESNet utilizes multiple branch parallel factorized convolution, achieving real-time inference and higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ESNet</head><p>In this section, we first introduce the whole architecture of ESNet, and then elaborate on the designed details of each unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Overview</head><p>As shown in <ref type="table" target="#tab_0">Table 1</ref> and illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, our ESNet has a symmetric encoder-decoder architecture, where an encoder produces downsampled feature maps, and a decoder upsamples the feature maps to match input resolution. The entire network is composed of 18 convolution layers, where the residual module is adopted as our core element. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the encoder and decoder has nearly same number of convolution layers, and utilize similar convolution type. For instance, both Block 1 and Block 5 employ FCU with K = 3, while Block 2 and Block 4 also employ FCU with K = 5. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the input image first undergoes a down-sampling unit to form initial feature maps, which are fed into the subsequent residual layers. Downsampling enables more deeper network to gather context, while at the same time helps to reduce computation. Additionally, two types of residual convolution module, called FCU and PFCU are employed, where the first one uses factorized convolution to extract low-level features, and the second one utilizes multi-branch dilated convolution to enlarge receptive fields to capture high-level semantics. In <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37]</ref>, the designed networks are began with sustained downsampling, however, such kind of operation may be harmful to feature abstraction, which highly detriment segmentation accuracy. In order to address this problem, our ESNet postpones downsampling oepration in encoder, with the similar spirit of <ref type="bibr" target="#b35">[36]</ref>. In the following, we will describe how to design FCU and PFCU, which focus on solving the efficiency limitation that is essentially present in the residual layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FCU Module</head><p>To reduce computation budget, the pointwise convolutions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref> and factorized convolutions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref> are widely used to take place of traditional stan-dard convolution in residual layer. Essentially, pointwise convolution (e.g., 1 × 1) speeds up computation by reducing the number of convolutional channels, which thus can be also considered as dimensionality reduction of feature maps. On the other hand, factorized convolution attempts to perform convolution with smaller filter kernel size. As a result, the recent years have witnessed multiple successful instances of lightweight residual layer <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>, such as Non-bottleneck ( <ref type="figure">Figure  2 (a)</ref>), Bottleneck <ref type="figure">(Figure 2 (b)</ref>), and Non-bottleneck-1D <ref type="figure">(Figure 2 (c)</ref>). More specifically, the Bottleneck module comes from the standard residual layer of ResNet <ref type="bibr" target="#b1">[2]</ref>, which requires less computational resources with respect to Nonbottleneck module. Although it is commonly adopted in state-of-the-art networks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>, the performance descend drastically when network goes deeper. Another outstanding residual module is Non-bottleneck-1D <ref type="bibr" target="#b18">[19]</ref>, which can be considered as a special case of our FCU with K = 3. In this module, a standard 3×3 convolution in the bottleneck is decomposed into two 1D convolutions (e.g., 3 × 1 and 1 × 3), yet the fixed kernel size of factorized convolution limits the field-of-view, leading to the decrease of performance. As shown in <ref type="figure">Figure 2</ref> (d), we again employ Non-bottleneck-1D module <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref> to design our FCU, as this is helpful for greatly reducing the number of parameters and accelerating the training and inference process. Unlike previous approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref>, however, the size of convolutional kernel is unfixed, allowing FCU to adaptively broaden receptive fields. For example, in the encoder, the shallow layers (Block 1 in <ref type="table" target="#tab_0">Table 1</ref>) prefer to use smaller kernel size (K = 3) to abstract low-level image features, while deeper layers (Block 2 in <ref type="table" target="#tab_0">Table 1</ref>) resort to larger kernel size (K = 5) for capturing wide-scale context. Conversely, in the decoder, the shallow layers (Block 4 in <ref type="table" target="#tab_0">Table 1</ref>) utilize a larger kernel size (K = 5) to gather long-ranged information to enhance prediction accuracy, while deeper layers (Block 5 in <ref type="table" target="#tab_0">Table 1</ref>) symmetrically employ smaller kernel size (K = 3) to recover image details by smoothing filter responses of short-ranged neighborhood pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PFCU Module</head><p>We focus on solving the accuracy and efficiency trade-off as a whole, without sitting on only one of its sides. To this end, this section introduces PFCU, as depicted in <ref type="figure">Figure 2</ref>(e). Motivated from <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36]</ref>, a transform-split-transformmerge strategy is employed in the designment of our PFCU, where each branch employs dilated convolution with different rate to broaden receptive fields. The dilated convolutions with parallel multiple branch are adaptive to capture objects within different scales, approaching the representational power of large and dense layers, but at a considerably lower computational complexity. At the beginning of each PFCU, the input is first transferred by a set of specialized 1D filters (e.g., 1×3 and 3×1), and the convolutional outputs pass through three parallel dilated convolution with rates r 1 = 2, r 2 = 5, and r 3 = 9, respectively. To facilitate training, finally, the outputs of three convolutional branches are added with input through the branch of identity mapping. After merging, the next PFCU begins. It is clear that our PFCU is not only efficient, but also accurate. Firstly, <ref type="table">Table 2</ref>. Comparison of weights used in different type of residue blocks. Three parameters of "Size" are number of layers, feature channels and convolutional kernels in corresponding residue block, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Encoder the powerful representation ability of PFCU allows us to use less convolution layers. Secondly, in PFCU, each branch shares the same convolutional feature maps. This can be regarded as a kind of feature reuse, which to some extent enlarges network capacity without significantly increasing complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison of Network Complexity</head><p>In this section, we analyze the network complexity of our ESNet and compare with recent state-of-the-art ERFNet <ref type="bibr" target="#b18">[19]</ref>. In addition, we also compare our new implementation of the residual layer that makes use of the parallel 1D factorization to accelerate and reduce the parameters. <ref type="table">Table 2</ref> summarizes the total dimensions of the weights on the convolutions of every residual block. As shown in <ref type="figure">Figure 2</ref>, Non-Bottleneck-1D has the simplest structure and fewest parameters. Since the standard 3 × 3 convolution has been decomposed into 1 × 3 and 3 × 1 convolution, the number of convolutional kernels is only 12 in each residual layer. As our FCU (K =3) module also adopts 1D factorized convolution, it has the same number of kernels with respect to Non-Bottleneck-1D. On the other hand, FCU (K = 5) and PFCU involve larger kernel size or more factorized convolution, leading to the increase of convolutional kernels (20 for FCU (K = 5), and 24 for PFCU) used in corresponding residual layers. However, the total weights are not only decided by filter kernel size, but also depend on the number of feature channels and convolution layers. Due to the parallel design of PFCU that facilitates the reduction of convolution layers, the entire size of ESNet is still smaller than ERFNet <ref type="bibr" target="#b18">[19]</ref>. For example, in contrast to ERFNet <ref type="bibr" target="#b18">[19]</ref> that contains 8 layers of dilated convolution (8 × 128 × 12 = 12, 288), our ESNet only has 3 layers of PFCU, resulting in more fewer parameters (3 × 128 × 24 = 9, 216) while achieving higher accuracy. As for the total parameters, our ESNet design is clearly more benefited, by receiving a direct 13.5% reduction, and thus greatly accelerates its execution. This is also consistent with the results of <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we carry on the experiments to demonstrate the potential of our segmentation architecture in terms of accuracy and efficiency trade-off. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The widely-used CityScapes dataset <ref type="bibr" target="#b19">[20]</ref>, including 19 object classes and one additional background, is selected to evaluate our ESNet. Beside the images with fine pixel-level annotations that contain 2,975 training, 500 validation and 1,525 testing images, we also use the 20K coarsely annotated images for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>To show the advantages of ESNet, we selected 6 state-of-the-art lightweight networks as baselines, including SegNet <ref type="bibr" target="#b7">[8]</ref>, ENet <ref type="bibr" target="#b16">[17]</ref>, ERFNet <ref type="bibr" target="#b18">[19]</ref>, ICNet <ref type="bibr" target="#b34">[35]</ref>, CGNet <ref type="bibr" target="#b14">[15]</ref>, and ESPNet <ref type="bibr" target="#b17">[18]</ref>. We adopt mean intersection-over-union (mIOU) averaged across all classes and categories to evaluate segmentation accuracy, while running time, inference speed (FPS), and model size (number of parameters) to measure implementing efficiency. For fair comparison, all the methods are conducted on the same hardware platform of DELL workstation with a single GTX 1080Ti GPU. We favor a large minibatch size (set as 4) to make full use of the GPU memory, where the initial learning rate is 5 × 10 −4 and the 'poly' learning rate policy is adopted with power 0.9, together with momentum and weight decay are set to 0.9 and 10 −4 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Results</head><p>In <ref type="table" target="#tab_2">Table 3</ref> and  <ref type="bibr" target="#b7">[8]</ref>. Although ENet <ref type="bibr" target="#b16">[17]</ref>, an anther efficient network, is nearly 1.2× efficient, and has 5× less parameters than our ESNet, but delivers poor segmentation accuracy of 12.4% and 7% drops in terms of class and category mIOU, respectively. Another interesting results is the comparison with CGNet <ref type="bibr" target="#b14">[15]</ref> in <ref type="table" target="#tab_2">Table 3</ref>, where it has 3× fewer parameters, while performs slightly slower than our ESNet. This is probably because that ESNet has more simpler architecture and less convolution layers, yielding more efficient in inference process. <ref type="figure">Figure 3</ref> shows some visual examples of segmentation outputs on the CityScapes validation set. It is demonstrated that, compared with baselines, our ESNet not only correctly classifies object with different scales (especially for very small object instance, such as "traffic sign" and "traffic light"), but also produces consistent qualitative results for all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion Remark and Future Work</head><p>This paper has proposed an architecture that achieves accurate and fast pixelwise semantic segmentation. In contrast to top-accurate networks that are computationally expensive with complex and deep architectures, our ESNet focuses more on developing the core elements of network architecture: the convolutional blocks. The transform-split-transform-merge scheme is adopted to redesign the commonly-used residual layers, leading to the multi-branch parallel 1D decomposed convolution, which is more efficient while retaining a similar learning performance. As this design can be directly used in existing encoder-decoder <ref type="figure">Fig. 3</ref>. The visual comparison on CityScapes val dataset. From left to right are input images, ground truth, segmentation outputs from our ESNet, SegNet <ref type="bibr" target="#b7">[8]</ref>, ENet <ref type="bibr" target="#b16">[17]</ref>, ERFNet <ref type="bibr" target="#b18">[19]</ref>, ESPNet <ref type="bibr" target="#b17">[18]</ref>, ICNet <ref type="bibr" target="#b34">[35]</ref>, and CGNet <ref type="bibr" target="#b14">[15]</ref>. (Best viewed in color) networks, we propose an ESNet that completely leverages its benefits to reach state-of-the-art segmentation accuracy and efficiency. The experimental results show that our ESNet achieves best available trade-off on CityScapes dataset in terms of segmentation accuracy and implementing efficiency. The future work includes incorporating contextual branch, as well as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref> does, to further improve performance while remaining few parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overall symmetric architecture of the proposed ESNet. The entire network is composed by four components: down-sampling unit, upsampling unit, factorized convolution unit and its parallel version. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The architecture of ESNet. "Size" denotes the dimension of output feature maps, C is the number of classes.</figDesc><table><row><cell cols="3">Stage Name Layer</cell><cell></cell><cell>Type</cell><cell></cell><cell>Size</cell></row><row><cell></cell><cell>Block 1</cell><cell>1 2-4</cell><cell cols="3">Down-sampling Unit 3× FCU (K = 3)</cell><cell>512 × 256 × 16 512 × 256 × 16</cell></row><row><cell>Encoder</cell><cell>Block 2 Block 3</cell><cell cols="5">5 6-7 8 9-11 3× PFCU (dilated r1 = 2, r2 = 5, r3 = 9) 128 × 64 × 128 Down-sampling Unit 256 × 128 × 64 2× FCU (K = 5) 256 × 128 × 64 Down-sampling Unit 128 × 64 × 128</cell></row><row><cell>Decoder</cell><cell>Block 4 Block 5</cell><cell>12 13-14 15 15-17</cell><cell cols="3">Up-sampling Unit 2× FCU (K = 5) Up-sampling Unit 2× FCU (K = 3)</cell><cell>256 × 128 × 64 256 × 128 × 64 512 × 256 × 16 512 × 256 × 16</cell></row><row><cell></cell><cell cols="2">Full Conv 18</cell><cell cols="3">Up-sampling Unit</cell><cell>1024 × 512 × C</cell></row><row><cell cols="2">BN ReLU 3×3 Conv</cell><cell>BN ReLU 1×1 Conv</cell><cell>3×1 Conv 1×3 Conv ReLU</cell><cell>K×1 Conv 1×K Conv ReLU</cell><cell></cell><cell>3×1 Conv BN ReLU 1×3 Conv ReLU</cell></row><row><cell></cell><cell></cell><cell>3×3 Conv</cell><cell>BN ReLU</cell><cell>BN ReLU</cell><cell></cell></row><row><cell></cell><cell></cell><cell>BN ReLU</cell><cell>3×1 Conv</cell><cell>K×1 Conv</cell><cell>3×1 DConv (r1)</cell><cell>3×1 DConv (r2)</cell><cell>3×1 DConv (r3)</cell></row><row><cell cols="2">3×3 Conv BN ReLU</cell><cell>1×1 Conv</cell><cell>1×3 Conv</cell><cell>ReLU 1×K Conv</cell><cell>ReLU 1×3 DConv(r1)</cell><cell>ReLU 1×3 DConv(r2) 1×3 DConv(r3) ReLU</cell></row><row><cell></cell><cell></cell><cell>BN</cell><cell>BN</cell><cell>BN</cell><cell>BN</cell><cell>BN</cell><cell>BN</cell></row><row><cell>Add</cell><cell></cell><cell>Add</cell><cell>Add</cell><cell>Add</cell><cell></cell><cell>Add</cell></row><row><cell cols="2">ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell></cell><cell>ReLU</cell></row><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell></cell><cell>(e)</cell></row></table><note>ReLU Fig. 2. Comparison of different residual layer modules. From left to right are (a) Non- bottleneck [2], (b) Bottleneck [17], (c) Non-bottleneck-1D [19], (d) FCU and (e) PFCU module. "DConv" denotes the dilated convolution, where r1, r2, and r3 are dilated rates for each split branch, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>× 16 × 12 2 × 64 × 20 3 × 128 × 24 2 × 64 × 20 2 × 16 × 12</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decoder</cell><cell>#Para</cell></row><row><cell>ERFNet[19]</cell><cell>Type Non-bt-1D Size 5 × 64 × 12</cell><cell>--</cell><cell cols="2">Non-bt-1D Non-bt-1D Non-bt-1D 17,688 8 × 128 × 12 2 × 64 × 12 2 × 16 × 12</cell></row><row><cell>ESNet</cell><cell cols="2">Type FCU(K=3) FCU(K=5) Size 3</cell><cell>PFCU</cell><cell>FCU(K=5) FCU(K=3) 15,296</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison with the state-of-the-art approaches in terms of segmentation accuracy and implementing efficiency.</figDesc><table><row><cell>Method</cell><cell>Cla(%)</cell><cell>Cat(%)</cell><cell>Time(ms)</cell><cell>Speed(Fps)</cell><cell>Para(M)</cell></row><row><cell>SegNet[8]</cell><cell>57.0</cell><cell>79.1</cell><cell>67</cell><cell>15</cell><cell>29.5</cell></row><row><cell>ENet[17]</cell><cell>58.3</cell><cell>80.4</cell><cell>13</cell><cell>77</cell><cell>0.36</cell></row><row><cell>ESPNet[18]</cell><cell>60.3</cell><cell>82.2</cell><cell>18</cell><cell>54</cell><cell>0.40</cell></row><row><cell>CGNet[15]</cell><cell>64.8</cell><cell>85.7</cell><cell>20</cell><cell>50</cell><cell>0.50</cell></row><row><cell>ERFNet [19]</cell><cell>66.3</cell><cell>86.5</cell><cell>21</cell><cell>48</cell><cell>2.10</cell></row><row><cell>ICNet [35]</cell><cell>69.5</cell><cell>86.4</cell><cell>33</cell><cell>30</cell><cell>7.80</cell></row><row><cell>Ours</cell><cell>70.7</cell><cell>87.4</cell><cell>16</cell><cell>63</cell><cell>1.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>, we have reported the quantitative results compared with state-of-the-art baselines. The results demonstrate that ESNet achieves the best available trade-off in terms of accuracy and efficiency. Without data augmentation, our ESNet obtains comparable results with respect to ICNet<ref type="bibr" target="#b34">[35]</ref> (only slight 0.4% drop of class mIOU, but 0.4% improvement of category mIOU). After augmented with 20K additional data with coarse annotations, our ESNet yields 70.7% class mIOU and 87.4% category mIOU, respectively, where 16 out of the 19 categories obtains best scores. Regarding to the efficiency, ESNet is nearly 4× faster and 18× smaller than SegNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Individual category results on the CityScapes test set in terms of class and category mIOU scores. Methods trained using both fine and coarse data are marked with superscript ' †'. The best performance for each individual class is marked with bold-face number.</figDesc><table><row><cell>Method</cell><cell cols="2">Roa Sid Bui Wal Fen Pol TLi TSi Veg Ter Cla</cell></row><row><cell cols="3">SegNet [8] 96.4 73.2 84.0 28.4 29.0 35.7 39.8 45.1 87.0 63.8 57.0</cell></row><row><cell>ENet [17]</cell><cell cols="2">96.3 74.2 75.0 32.2 33.2 43.4 34.1 44.0 88.6 61.4 58.3</cell></row><row><cell cols="3">ESPNet [18] 97.0 77.5 76.2 35.0 36.1 45.0 35.6 46.3 90.8 63.2 60.3</cell></row><row><cell cols="3">CGNet [15] 95.5 78.7 88.1 40.0 43.0 54.1 59.8 63.9 89.6 67.6 64.8</cell></row><row><cell cols="3">ERFNet [19] 97.2 80.0 89.5 41.6 45.3 56.4 60.5 64.6 91.4 68.7 66.3</cell></row><row><cell cols="3">ICNet [35] 97.1 79.2 89.7 43.2 48.9 61.5 60.4 63.4 91.5 68.3 69.5</cell></row><row><cell>Ours</cell><cell cols="2">97.1 78.5 90.4 46.5 48.1 60.1 60.4 70.9 91.1 59.9 69.1</cell></row><row><cell>Ours  †</cell><cell cols="2">98.1 80.4 92.4 48.3 49.2 61.5 62.5 72.3 92.5 61.5 70.7</cell></row><row><cell>Method</cell><cell>Sky Ped Rid Car Tru Bus Tra Mot Bic</cell><cell>Cat</cell></row><row><cell cols="2">SegNet [8] 91.8 62.8 42.8 89.3 38.1 43.1 44.1 35.8 51.9</cell><cell>79.1</cell></row><row><cell>ENet [17]</cell><cell>90.6 65.5 38.4 90.6 36.9 50.5 48.1 38.8 55.4</cell><cell>80.4</cell></row><row><cell cols="2">ESPNet [18] 92.6 67.0 40.9 92.3 38.1 52.5 50.1 41.8 57.2</cell><cell>82.2</cell></row><row><cell cols="2">CGNet [15] 92.9 74.9 54.9 90.2 44.1 59.5 25.2 47.3 60.2</cell><cell>85.7</cell></row><row><cell cols="2">ERFNet [19] 94.2 76.1 56.4 92.4 45.7 60.6 27.0 48.7 61.8</cell><cell>86.5</cell></row><row><cell cols="2">ICNet [35] 93.5 74.6 56.1 92.6 51.3 72.7 51.3 53.6 70.5</cell><cell>86.4</cell></row><row><cell>Ours</cell><cell>93.2 74.3 51.8 92.3 61.0 72.3 51.0 43.3 70.2</cell><cell>86.8</cell></row><row><cell>Ours  †</cell><cell>94.4 76.6 53.2 94.4 62.5 74.3 52.4 45.5 71.4</cell><cell>87.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR</title>
		<imprint>
			<biblScope unit="page" from="580" to="587" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiaoxiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhiwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chenchange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiaoou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6459" to="6468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roberto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Refinenet: multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guosheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chunhua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCV</title>
		<imprint>
			<biblScope unit="page" from="1520" to="1528" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large kernel matters: Improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1743" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring context with deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1352" to="1366" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Can: Contextual aggregating network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cgnet: A light-weight context guided network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08201v1.</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bodenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06815v3.</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TITS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Panqu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pengfei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zehua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiaodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV</title>
		<imprint>
			<biblScope unit="page" from="1451" to="1460" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang-Chieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomas</surname></persName>
		</author>
		<editor>MICCAI.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3309" to="3318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4877" to="4885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mobilenets: efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="6848" to="6856" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Changqian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jingbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Changxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00897.</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08545v2.</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast semantic segmentation for scene perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TII</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
