<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-05-09">9 May 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Imaging Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SICE</orgName>
								<orgName type="department" key="dep2">Telecommunications ‡ Applied Mathematics and Statistics</orgName>
								<orgName type="institution" key="instit1">Beijing University of Posts</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Imaging Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-05-09">9 May 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art subspace clustering methods are based on expressing each data point as a linear combination of other data points while regularizing the matrix of coefficients with ℓ 1 , ℓ 2 or nuclear norms. ℓ 1 regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad theoretical conditions, but the clusters may not be connected. ℓ 2 and nuclear norm regularization often improve connectivity, but give a subspace-preserving affinity only for independent subspaces. Mixed ℓ 1 , ℓ 2 and nuclear norm regularizations offer a balance between the subspacepreserving and connectedness properties, but this comes at the cost of increased computational complexity. This paper studies the geometry of the elastic net regularizer (a mixture of the ℓ 1 and ℓ 2 norms) and uses it to derive a provably correct and scalable active set method for finding the optimal coefficients. Our geometric analysis also provides a theoretical justification and a geometric interpretation for the balance between the connectedness (due to ℓ 2 regularization) and subspace-preserving (due to ℓ 1 regularization) properties for elastic net subspace clustering. Our experiments show that the proposed active set method not only achieves state-of-the-art clustering performance, but also efficiently handles large-scale datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many computer vision applications, including image representation and compression <ref type="bibr" target="#b19">[20]</ref>, motion segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b34">35]</ref>, temporal video segmentation <ref type="bibr" target="#b39">[40]</ref>, and face clustering <ref type="bibr" target="#b18">[19]</ref>, high-dimensional datasets can be well approximated by a union of low-dimensional subspaces. In this case, the problem of clustering a high-dimensional dataset into multiple classes or categories reduces to the problem of assigning each data point to its own subspace and recovering the underlying low-dimensional structure of the data, a problem known in the literature as subspace clustering <ref type="bibr" target="#b37">[38]</ref>.</p><p>Prior Work. Over the past decade, the subspace clustering problem has received a lot of attention in the literature and many methods have been developed. Among them, spectral clustering based methods have become extremely popular <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37]</ref> (see <ref type="bibr" target="#b37">[38]</ref> for details). These methods usually divide the problem into two steps: a) learning an affinity matrix that characterizes whether two points are likely to lie in the same subspace, and b) applying spectral clustering to this affinity. Arguably, the first step is the most important, as the success of spectral clustering depends on having an appropriate affinity matrix.</p><p>State-of-the-art methods for constructing the affinity matrix are based on the self-expressiveness model <ref type="bibr" target="#b9">[10]</ref>. Under this model, each data point x j is expressed as a linear combination of all other data points, i.e., x j = i =j x i c ij +e j , where the coefficient c ij is used to define an affinity between points i and j, and the vector e j captures deviations from the self-expressive model. The coefficients are typically found by solving an optimization problem of the form min cj ,ej r(c j ) + γ · h(e j ) s.t. x j = Xc j + e j , c jj = 0, <ref type="bibr" target="#b0">(1)</ref> where X = [x 1 , · · · , x N ] is the data matrix, c j = [c 1j , . . . , c N j ] ⊤ is the vector of coefficients, r(·) is a properly chosen regularizer on the coefficients, h(·) is a properly chosen regularizer on the noise or corruption, and γ &gt; 0 is a parameter that balances these two regularizers.</p><p>The main difference among state-of-the-art methods lies in the choice of the regularizer r(·). The sparse subspace clustering (SSC) method <ref type="bibr" target="#b9">[10]</ref> searches for a sparse representation using r(·) = · 1 . While under broad theoretical conditions (see <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47]</ref>) the representation produced by SSC is guaranteed to be subspace preserving (i.e., c ij = 0 only if x i and x j are in the same subspace), the affinity matrix may lack connectedness <ref type="bibr" target="#b29">[30]</ref> (i.e., data points from the same subspace may not form a connected component of the affinity graph due to the sparseness of the connections, which may cause over-segmentation). Other recently proposed sparsity based methods, such as orthogonal matching pursuit (OMP) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b45">46]</ref> and nearest subspace neighbor (NSN) <ref type="bibr" target="#b32">[33]</ref>, also suffer from the same connectivity issue.</p><p>As an alternative, the least squares regression (LSR) method <ref type="bibr" target="#b28">[29]</ref> uses the regularizer r(·) = 1 2 · 2 2 . One benefit of LSR is that the representation matrix is generally dense, which alleviates the connectivity issue of sparsity based methods. However, the representation is known to be subspace preserving only when the subspaces are independent, <ref type="bibr" target="#b0">1</ref> which significantly limits its applicability. Nuclear norm regularization based methods, such as low rank representation (LRR) <ref type="bibr" target="#b26">[27]</ref> and low rank subspace clustering (LRSC) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39]</ref>, also suffer from the same limitation <ref type="bibr" target="#b44">[45]</ref>.</p><p>To bridge the gap between the subspace preserving and connectedness properties, <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref> propose to use mixed norms. For example, the low rank sparse subspace clustering (LRSSC) method <ref type="bibr" target="#b44">[45]</ref>, which uses a mixed ℓ 1 and nuclear norm regularizer, is shown to give a subspace preserving representation under conditions which are similar to but stronger than those of SSC. However, the justification for the improvements in connectivity given by LRSSC is merely experimental. Likewise, <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b12">13]</ref> propose to use a mixed ℓ 1 and ℓ 2 norm given by</p><formula xml:id="formula_0">r(c) = λ c 1 + 1 − λ 2 c 2 2 ,<label>(2)</label></formula><p>where λ ∈ [0, 1] controls the trade-off between the two regularizers. However, <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b12">13]</ref> do not provide a theoretical justification for the benefits of the method. Other subspace clustering regularizers studied in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b21">[22]</ref> use the trace lasso <ref type="bibr" target="#b15">[16]</ref> and the k-support norm <ref type="bibr" target="#b0">[1]</ref>, respectively. However, no theoretical justification is provided in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22]</ref> for the benefit of their methods. Another issue with the aforementioned methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22]</ref> is that they do not provide efficient algorithms to deal with large-scale datasets. To address this issue, <ref type="bibr" target="#b4">[5]</ref> proposes to find the representation of X by a few anchor points that are sampled from X and then perform spectral clustering on the anchor graph. In <ref type="bibr" target="#b33">[34]</ref> the authors propose to cluster a small subset of the original data and then classify the rest of the data based on the learned groups. However, both of these strategies are suboptimal in that they sacrifice clustering accuracy for computational efficiency. Paper Contributions. In this paper, we exploit a mixture of ℓ 1 and ℓ 2 norms to balance the subspace preserving and connectedness properties. Specifically, we use r(·) as in <ref type="bibr" target="#b1">(2)</ref> and h(e) = 1 2 e 2 2 . The method is thus a combination of SSC and LSR and reduces to each of them when λ = 1 and λ = 0, respectively. In the statistics literature, the optimization program using this regularization is called Elastic Net and is used for variable selection in regression problems <ref type="bibr" target="#b49">[50]</ref>. Thus we refer to this method as the Elastic Net Subspace Clustering (EnSC).</p><p>This work makes the following contributions:</p><p>1. We propose an efficient and provably correct activeset based algorithm for solving the elastic net prob-lem. The proposed algorithm exploits the fact that the nonzero entries of the elastic net solution fall into an oracle region, which we use to define and efficiently update an active set. The proposed update rule leads to an iterative algorithm which is shown to converge to the optimal solution in a finite number of iterations.</p><p>2. We provide theoretical conditions under which the affinity generated by EnSC is subspace preserving, as well as a clear geometric interpretation for the balance between the subspace-preserving and connectedness properties. Our conditions depend on a local characterization of the distribution of the data, which improves over prior global characterizations.</p><p>3. We present experiments on computer vision datasets that demonstrate the superiority of our method in terms of both clustering accuracy and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Elastic Net: Geometry and a New Algorithm</head><p>In this section, we study the elastic net optimization problem, and present a new active-set based optimization algorithm for solving it. Consider the objective function</p><formula xml:id="formula_1">f (c; b, A) := λ c 1 + 1 − λ 2 c 2 2 + γ 2 b − Ac 2 2 , (3) where b ∈ IR D , A = [a 1 , · · · , a N ] ∈ IR D×N , γ &gt; 0,</formula><p>and λ ∈ [0, 1) (the reader is referred to the appendix for a study of the case λ = 1). Without loss of generality, we assume that b and {a j } N j=1 are normalized to be of unit ℓ 2 norm in our analysis. The elastic net model then computes</p><formula xml:id="formula_2">c * (b, A) := arg min c f (c; b, A).<label>(4)</label></formula><p>We note that c * (b, A) is unique since f (c; b, A) is a strongly convex function; we use the notation c * in place of c * (b, A) when the meaning is clear. In the next two sections, we present a geometric analysis of the elastic net solution, and use this analysis to design an active-set algorithm for efficiently solving (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Geometric structure of the elastic net solution</head><p>We first introduce the concept of an oracle point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.1 (Oracle Point). The oracle point associated with the optimization problem (4) is defined to be</head><formula xml:id="formula_3">δ(b, A) := γ · b − Ac * (b, A) .<label>(5)</label></formula><p>When there is no risk of confusion, we omit the dependency of the oracle point on b and A and write δ(b, A) as δ.</p><p>Notice that the oracle point is unique since c * is unique, and that the oracle point cannot be computed until the optimal solution c * has been computed. The next result gives a critical relationship involving the oracle point that is exploited by our active-set method.  <ref type="figure" target="#fig_6">Figure 1</ref>. Illustration of the structure of the solution c * for a data matrix A containing 100 randomly generated points in IR 2 , which are shown as blue dots in the x-y plane. The z direction shows the magnitude for each coefficient c * j . The red dot represents the oracle point δ(b, A), with its direction denoted by the red dashed line. The value for γ is fixed at 50, but the value for λ varies as depicted.</p><p>Theorem 2.1. The solution c * to problem (4) satisfies</p><formula xml:id="formula_4">(1 − λ)c * = T λ (A ⊤ δ),<label>(6)</label></formula><p>where T λ (·) is the soft-thresholding operator (applied com-</p><formula xml:id="formula_5">ponentwise to A ⊤ δ) defined as T λ (v) = sgn(v)(|v| − λ) if |v| &gt; λ and 0 otherwise.</formula><p>Theorem 2.1 shows that if the oracle point δ is known, the solution c * can be written out directly. Moreover, it follows from <ref type="formula" target="#formula_3">(5)</ref> and <ref type="formula" target="#formula_4">(6)</ref> that δ = 0 if and only if b = 0.</p><p>In <ref type="figure" target="#fig_6">Figure 1</ref>, we depict a two dimensional example of the solution to the elastic net problem (4) for different values of the tradeoff parameter λ. As expected, the solution c * becomes denser as λ decreases. Moreover, as predicted by Theorem 2.1, the magnitude of the coefficient c * j is a decaying function of the angle between the corresponding dictionary atom a j and the oracle point δ (shown in red). If a j is far enough from δ such that | a j , δ | ≤ λ holds true, then the corresponding coefficient c * j is zero. We call the region containing the nonzero coefficients the oracle region. We can formally define the oracle region by using the quantity µ(·, ·) to denote the coherence of two vectors, i.e.,</p><formula xml:id="formula_6">µ(v, w) := | v, w | v 2 w 2 .<label>(7)</label></formula><p>Definition 2.2 (Oracle Region). The oracle region associated with the optimization problem (4) is defined as</p><formula xml:id="formula_7">∆(b, A) := v ∈ IR D : v 2 = 1, µ(v, δ) &gt; λ δ 2 .<label>(8)</label></formula><p>The oracle region is composed of an antipodal pair of spherical caps of the unit ball of IR D that are located at the symmetric locations ±δ/ δ 2 , both with an angular radius of θ = arccos(λ/ δ 2 ) (see <ref type="figure">Figure 2</ref>). From the definition of the oracle region and Theorem 2.1, it follows that c * j = 0 if and only if a j ∈ ∆(b, A). In other words, the support of the solution c * are those vectors a j in the oracle region.</p><p>The oracle region also captures the behavior of the solution when columns from the matrix A are removed or new columns are added. This provides the key insight into designing an active-set method for solving the optimization.  <ref type="figure">Figure 2</ref>. The oracle region ∆(b, A) is illustrated in red. Note that the size of the oracle region increases as the quantity λ/ δ 2 decreases, and vice versa. </p><formula xml:id="formula_8">Proposition 2.1. For any b ∈ IR D , A ∈ IR D×N and A ′ ∈ IR D×N ′ , if no column of A ′ is contained in ∆(b, A), then c * (b, [A, A ′ ]) = [c * (b, A) ⊤ , 0 ⊤ N ′ ×1 ] ⊤ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 2.2. For any</head><formula xml:id="formula_9">b ∈ IR D , A ∈ IR D×N and A ′ ∈ IR D×N ′ , denote c * (b, [A, A ′ ]) = [c ⊤ A , c ⊤ A ′ ] ⊤ . If any column of A ′ lies within ∆(b, A) , then c ⊤ A ′ = 0.</formula><p>This result means that the solution to the elastic net problem will certainly be changed by adding new columns that lie within the oracle region to the dictionary.</p><p>In the next section, we describe an efficient algorithm for solving the elastic net problem (4) that is based on the geometric structure and concentration behavior of the solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">A new active-set algorithm</head><p>Although the elastic net optimization problem <ref type="bibr" target="#b49">[50]</ref> has been recently introduced for subspace clustering in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>, such prior work does not provide an efficient algorithm that can handle large-scale datasets. In fact, such prior work solves the elastic net problem using existing algorithms that require calculations involving the full data matrix A (e.g., the accelerated proximal gradient (APG) <ref type="bibr" target="#b1">[2]</ref> is used in <ref type="bibr" target="#b11">[12]</ref> and the linearized alternating direction method (LADM) <ref type="bibr" target="#b24">[25]</ref> is used in <ref type="bibr" target="#b31">[32]</ref>). Here, we propose to solve the elastic net problem (4) with an active-set algorithm that is more efficient than both APG and LADM, and can handle largescale datasets. We call our new method (see Algorithm 1) ORacle Guided Elastic Net solver, or ORGEN for short. The basic idea behind ORGEN is to solve a sequence of reduced-scale subproblems defined by an active set that is itself determined from the oracle region. Let T k be the active set at iteration k. Then, the next active set T k+1 is selected to contain the indices of columns that are in the oracle region ∆(b, A T k ), where A T k denotes the submatrix of A with columns indexed by T k . We use <ref type="figure" target="#fig_3">Figure 3</ref> for a conceptual illustration. In <ref type="figure" target="#fig_3">Figure 3</ref>(a) we show the columns of A that correspond to the active set T k by labeling the corresponding columns of A T k in red. The oracle region ∆(b, A T k ) is the union of the red arcs in <ref type="figure" target="#fig_3">Figure 3</ref>(b). Notice that at the bottom left there is one red dot that is not in ∆(b, A T k ) and thus must not be included in T k+1 , and two blue dots that are not in T k but lie in the oracle region ∆(b, A T k ) and thus must be included in T k+1 . In <ref type="figure" target="#fig_3">Figure  3</ref>(c) we illustrate T k+1 by green dots. This iterative procedure is terminated once T k+1 does not contain any new points, i.e., when T k+1 ⊆ T k , at which time T k+1 is the support for c * (b, A).</p><formula xml:id="formula_10">(a) Active set T k (b) ∆(b, A T k ) (c) Active set T k+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 ORacle Guided Elastic Net (ORGEN) solver</head><formula xml:id="formula_11">Input: A = [a 1 , . . . , a N ] ∈ IR D×N , b ∈ IR D , λ and γ.</formula><p>1: Initialize the support set T 0 and set k ← 0. Compute c * (b, A T k ) as in (4) using any solver. <ref type="bibr" target="#b3">4</ref>:</p><formula xml:id="formula_12">Compute δ(b, A T k ) from c * (b, A T k )</formula><p>as in (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Active set update:</p><formula xml:id="formula_13">T k+1 ← {j : a j ∈ ∆(b, A T k )}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>If T k+1 ⊆ T k , terminate; otherwise set k ← k + 1. 7: end loop Output: A vector c such that c T k = c * (b, A T k ) and zeros otherwise. Its support is T k+1 .</p><p>The next lemma helps explain why ORGEN converges.</p><formula xml:id="formula_14">Lemma 2.1. In Algorithm 1, if T k+1 T k , then f (c * (b, A T k+1 ); b, A T k+1 ) &lt; f (c * (b, A T k ); b, A T k ).</formula><p>The following convergence result holds for ORGEN.</p><p>Theorem 2.2. Algorithm 1 converges to the optimal solution c * (b, A) in a finite number of iterations.</p><p>The result follows from Lemma 2.1, because it implies that an active set can never be repeated. Since there are only finitely many distinct active sets, the algorithm must even-</p><formula xml:id="formula_15">tually terminate with T k+1 ⊆ T k . The remaining part of the proof establishes that if T k+1 ⊆ T k , then c * (b, A T k+1 )</formula><p>gives the nonzero entries of the solution.</p><p>ORGEN solves large-scale problems by solving a sequence of reduced-size problems in step 3 of Algorithm 1. If the active set T k is small, then step 3 is a small-scale problem that can be efficiently solved. However, there is no procedure in Algorithm 1 that explicitly controls the size of T k . To address this concern, we propose an alternative to step 5 in which only a small number of new points-the ones most correlated with δ-are added. Specifically,</p><formula xml:id="formula_16">5' : T k+1 = {j ∈ T k : a j ∈ ∆(b, A T k )} ∪ S k , (9)</formula><p>where S k holds the indices of the largest n entries in</p><formula xml:id="formula_17">{|a ⊤ j δ(b, A T k )| : j / ∈ T k , a j ∈ ∆(b, A T k )};</formula><p>ideally, n should be chosen so that the size of T k+1 is bounded by a predetermined value N max that represents the maximum size subproblem that can be handled in step 3. If N max is chosen large enough that the second set in the union in <ref type="formula">(9)</ref> is non-empty, then our convergence result still holds. Initialization. We suggest the following procedure for computing the initial active set T 0 . First, compute the solution to (4) with λ = 0, which has a closed form solution and can be computed efficiently if the ambient dimension D of the data is not too big. Then, the l largest entries (in absolute value) of the solution for some pre-specified value l are added to T 0 . Our experiments suggest that this strategy promotes fast convergence of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Elastic Net Subspace Clustering (EnSC)</head><p>Although the elastic net has been recently introduced for subspace clustering in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b12">13]</ref>, these works do not provide conditions under which the affinity is guaranteed to be subspace preserving or potential improvements in connectivity. In this section, we give conditions for the affinity to be subspace preserving and for the balance between the subspacepreserving and connectedness properties. To the best of our knowledge, this is the first time that such theoretical guarantees have been established.</p><p>We first formally define the subspace clustering problem. Problem 3.1 (Subspace Clustering). Let X ∈ IR D×N be a real-valued matrix whose columns are drawn from a union of n subspaces of IR D , say n ℓ=1 S ℓ , where the dimension d ℓ of the ℓ-th subspace satisfies d ℓ &lt; D for ℓ = 1, . . . , n. The goal of subspace clustering is to segment the columns of X into their representative subspaces.</p><formula xml:id="formula_18">Let X = [x 1 , · · · , x N ],</formula><p>where each x j is assumed to be of unit norm. Using the same notation as for <ref type="formula" target="#formula_2">(4)</ref>, the proposed EnSC computes c * (</p><formula xml:id="formula_19">x j , X −j ) for each {x j } N j=1 , i.e., c * (x j , X −j ) = arg min c f (c; x j , X −j ),<label>(10)</label></formula><p>where X −j is X with the j-th column removed. In this section, we focus on a given vector, say x j . We suppose that x j ∈ S ℓ for some ℓ, and use X ℓ −j to denote the submatrix of X with columns from S ℓ except that x j is removed. Since our goal is to use the entries of c * (x j , X −j ) to construct an affinity graph in which only points in the same subspace are connected, we desire the nonzero entries of c * (x j , X −j ) to be a subset of the columns X ℓ −j so that no connections are built between points from different subspaces. If this is the case, we say that such a solution c * (x j , X −j ) is subspace preserving. On the other hand, we also want the nonzero entries of c * (x j , X −j ) to be as dense as possible in X ℓ −j so that within each cluster the affinity graph is wellconnected 2 . To some extent, these are conflicting goals: if the connections are few, it is more likely that the solution is subspace preserving, but the affinity graph of each cluster is not well connected. Conversely, as one builds more connections, it is more likely that some of them will be false, but the connectivity is improved.</p><p>In the next two sections, we give a geometric interpretation of the tradeoff between the subspace preserving and connectedness properties, and provide sufficient conditions for a representation to be subspace preserving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Subspace-preserving vs. connected solutions</head><p>Our analysis is built upon the optimization problem min c f (c; x j , X ℓ −j ). Note that its solution is trivially subspace preserving since the dictionary X ℓ −j is contained in S ℓ . We then treat all points from other subspaces as newly added columns to X ℓ −j and apply Propositions 2.1 and 2.2. We get the following geometric result.</p><formula xml:id="formula_20">Lemma 3.1. Suppose that x j ∈ S ℓ . Then, the vector c * (x j , X −j ) is subspace preserving if and only if x k / ∈ ∆(x j , X ℓ −j ) for all x k / ∈ S ℓ .</formula><p>We illustrate the geometry implied by Lemma 3.1 in <ref type="figure">Figure 4</ref>, where we assume S ℓ is a two dimensional subspace <ref type="bibr" target="#b1">2</ref> In fact, even when each cluster is well-connected, further improving connectivity within clusters is still beneficial since it enhances the ability of the subsequent step of spectral clustering in correcting any erroneous connections in the affinity graph <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>S ℓ <ref type="figure">Figure 4</ref>. The structure of the solution for an example in IR 3 associated with a point xj (not shown) that lies in the 2-dimensional subspace S ℓ . The blue dots illustrate the columns of X ℓ −j , the union of the two red regions is the oracle region ∆(xj, X ℓ −j ), and the green points are vectors from other subspaces.</p><p>in IR 3 . The dictionary X ℓ −j is represented by the blue dots in the plane and the oracle region ∆(x j , X ℓ −j ) is denoted as the two red circles. The green dots are all other points in the dictionary. Lemma 3.1 says that c * (x j , X −j ) is subspace preserving if and only if all green dots lie outside of the red region.</p><p>To ensure that a solution is subspace preserving one desires a small oracle region, while to ensure connectedness one desires a large oracle region. These facts again highlight the trade-off between these two properties. Recall that the elastic net balances ℓ 1 regularization (promotes sparse solutions) and ℓ 2 regularization (promotes dense solutions). Thus, one should expect that the oracle region will decrease in size as λ is increased from 0 towards 1. Theorem 3.1 formalizes this claim, but first we need the following definition that characterizes the distribution of the data in X ℓ −j .</p><p>Definition 3.1 (inradius). The inradius of a convex body P is the radius r(P) of the largest ℓ 2 ball inscribed in P.</p><p>To understand the next result, we comment that the size of the oracle region ∆(x j , X ℓ −j ) is controlled by the quantity λ/ δ(x j , X ℓ −j ) 2 as depicted in <ref type="figure">Figure 2</ref>.</p><formula xml:id="formula_21">Theorem 3.1. If x j ∈ S ℓ , then λ δ(x j , X ℓ −j ) 2 ≥ r 2 j r j + 1−λ λ ,<label>(11)</label></formula><p>where r j is the inradius of the convex hull of the symmetrized points in X ℓ −j , i.e., r j := r(conv{±x k : x k ∈ S ℓ and k = j}).</p><p>We define the right-hand-side of (11) to be zero when λ = 0.</p><p>The above theorem allows us to determine an upper bound for the size of the oracle region. This follows since a lower bound on the size of λ/ δ(x j , X ℓ −j ) 2 implies an upper bound on the size of the oracle region (see <ref type="bibr" target="#b7">(8)</ref> and <ref type="figure">Figure 2</ref>). Also notice that the right hand side of (11) is in the range [0, r j ) and is monotonically increasing with λ.</p><p>Thus, it provides an upper bound on the area of the oracle region, which decreases as λ increases. This highlights that the trade-off between the subspace-preserving and connectedness properties is controlled by λ.</p><p>Remark 3.1. It would be nice if λ/ δ(x j , X ℓ −j ) 2 was increasing as a function of λ (we already know that its lower bound given in Theorem 3.1 is increasing in λ). However, one can show using the data x j = [0.22, 0.72, 0.66] ⊤ ,</p><formula xml:id="formula_23">X ℓ −j =   −0.55 −0.82 −0.05 0.22 0.22 0.57 0.84 0.78 −0.80 0.00 0.55 0.58   ,<label>(13)</label></formula><p>and parameter choice γ = 10, that λ/ δ (with λ = 0.88) is larger than λ/ δ (with λ = 0.95).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Conditions for a subspace-preserving solution</head><p>A sufficient condition for a solution to be subspace preserving is obtained by combining the geometry in Lemma 3.1 with the bound on the size of the oracle region implied by Theorem 3.1.</p><formula xml:id="formula_24">Theorem 3.2. Let x j ∈ S ℓ , δ j = δ(x j , X ℓ −j )</formula><p>be the oracle point, and r j be the inradius characterization of X ℓ −j as given by <ref type="bibr" target="#b11">(12)</ref>. Then, c * (x j , X −j ) is subspace preserving if</p><formula xml:id="formula_25">max k:x k / ∈S ℓ µ(x k , δ j ) ≤ r 2 j r j + 1−λ λ .<label>(14)</label></formula><p>Notice that in Theorem 3.2 the quantity δ j is determined from X ℓ −j and that it lies within the subspace S ℓ by definition of δ(x j , X ℓ −j ). Thus the left-hand-side of (14) characterizes the separation between the oracle point-which is in S ℓ -and the set of points outside of S ℓ . On the right-handside, r j characterizes the distribution of points in X ℓ −j . In particular, r j is large when points are well spread within S ℓ and not skewed toward any direction. Finally, note that the right-hand-side of <ref type="formula" target="#formula_2">(14)</ref> is an increasing function of λ, showing that the solution is more likely to be subspace preserving if more weight is placed on the ℓ 1 regularizer relative to the ℓ 2 regularizer. Theorem 3.2 has a close relationship to the sufficient condition for SSC to give a subspace preserving solution (the case λ = 1) <ref type="bibr" target="#b35">[36]</ref>. Specifically, <ref type="bibr" target="#b35">[36]</ref> shows that if max k:x k / ∈S ℓ µ(x k , δ j ) &lt; r j , then SSC gives a subspace preserving solution. We can observe that condition <ref type="bibr" target="#b13">(14)</ref> approaches the condition for SSC as λ → 1.</p><p>The result stated in Theorem 3.2 is a special case of the following more general result. Theorem 3.3. Let x j ∈ S ℓ , δ j = δ(x j , X ℓ −j ) be the oracle point, and κ j = max k =j,x k ∈S ℓ µ(x k , δ j ) be the coherence of δ j with its nearest neighbor in X ℓ −j . Then, the solution c * (x j , X −j ) is subspace preserving if</p><formula xml:id="formula_26">max k:x k / ∈S ℓ µ(x k , δ j ) ≤ κ 2 j κ j + 1−λ λ .<label>(15)</label></formula><p>The only difference between this result and that in Theorem 3.2 is that κ j is used instead of r j for characterizing the distribution of points in X ℓ −j . We show in Lemma C.1 that r j ≤ κ j , which makes Theorem 3.3 more general than Theorem 3.2. Geometrically, r j is large if the subspace S ℓ is well-covered by X ℓ j , while κ j is large if the neighborhood of the oracle closest to δ j is well-covered, i.e., there is a point in X ℓ −j that is close to δ j . Thus, while the condition in Theorem 3.2 requires each subspace to have global coverage by the data, the condition in Theorem 3.3 allows the data to be biased, and only requires a local region to be well-covered. In addition, condition <ref type="bibr" target="#b14">(15)</ref> can be checked when the membership of the data points is known. This advantage allows us to check the tightness of the condition <ref type="bibr" target="#b14">(15)</ref>, which is studied in more details in the appendix. In contrast, condition <ref type="bibr" target="#b13">(14)</ref> and previous work on SSC <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44]</ref> use the inradius r j , which is generally NP-hard to calculate <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b44">45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ORGEN on synthetic data</head><p>We conducted synthetic experiments to illustrate the computational efficiency of the proposed algorithm OR-GEN. Three popular solvers are exploited: the regularized feature sign search (RFSS) is an active set type method <ref type="bibr" target="#b20">[21]</ref>; the LASSO version of the LARS algorithm <ref type="bibr" target="#b8">[9]</ref> that is implemented in the sparse modeling software (SPAMS); and the gradient projection for sparse reconstruction (GPSR) algorithm proposed in <ref type="bibr" target="#b14">[15]</ref>. These three solvers are used to solve the subproblem in step 3 of ORGEN, resulting in three implementations of ORGEN. We also used the three solvers as stand-alone solvers for comparison purposes.</p><p>In all experiments, the vector b and columns of A are all generated independently and uniformly at random on the unit sphere of IR 100 . The results are averages over 50 trials.</p><p>In the first experiment, we test the scaling behavior of ORGEN by varying N ; the results are shown in <ref type="figure" target="#fig_5">Figure 5</ref>(a). We can see that our active-set scheme improves the computational efficiency of all three solvers. Moreover, as N grows, the improvement becomes more significant.</p><p>Next, we test the performance of ORGEN for various values of the parameter λ that controls the tradeoff between the subspace preserving and connectedness properties; the running times and sparsity level are shown in <ref type="figure" target="#fig_5">Figures 5(b)</ref> and 5(c), respectively. The performance of SPAMS is not  reported since it performs poorly even for moderately small values of λ. For all methods, the computational efficiency decreases as λ becomes smaller. For the two versions of ORGEN, this is expected since the solution becomes denser as λ becomes smaller (see <ref type="figure" target="#fig_5">Figure 5</ref>(c)). Thus the active sets become larger, which leads directly to larger and more time consuming subproblems in step 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">EnSC on real data</head><p>In this section, we use ORGEN to solve the optimization problems arising in EnSC, where each subproblem in step 3 is solved using the RFSS method. To compute the coefficient vectors c * (x j , X −j ), the parameter λ is set to be the same for all j, while the parameter γ is set as γ = αγ 0 where α &gt; 1 is a hyperparameter and γ 0 is the smallest value of γ such that c * (x j , X −j ) is nonzero. The algorithm is run for at most 2 iterations, as we observe that this is sufficient for the purpose of subspace clustering and that subsequent iterations do not boost performance. We measure clustering performance by clustering accuracy, which is calculated as the best matching rate between the label predicted by the algorithm and that of the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets.</head><p>We test our method on the four datasets presented in <ref type="table" target="#tab_0">Table 1</ref>. The Coil-100 dataset <ref type="bibr" target="#b30">[31]</ref> contains 7,200 grayscale images of 100 different objects. Each object has 72 images taken at pose intervals of 5 degrees, with the images being of size 32 × 32. The PIE dataset <ref type="bibr" target="#b16">[17]</ref> contains images of the faces of 68 people taken under 13 different poses, 43 different illuminations, and 4 different expressions. In the experiments, we use the five near frontal poses and all images under different illuminations and expressions. Each image is manually cropped and normalized to 32 × 32 pixels. The MNIST dataset <ref type="bibr" target="#b22">[23]</ref> contains 70,000 images of handwritten digits 0-9. For each image, we extract a feature vector of dimension 3,472 via the scattering convolution network <ref type="bibr" target="#b2">[3]</ref>, and then project to dimension 500 using PCA. Finally, the Covtype database 3 has been collected to predict forest cover type from 54 cartographic variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods.</head><p>We compare our method with several state-ofthe-art subspace clustering methods that may be categorized into three groups. The first group contains TSC <ref type="bibr" target="#b17">[18]</ref>, OMP <ref type="bibr" target="#b7">[8]</ref>, NSN <ref type="bibr" target="#b32">[33]</ref>, and SSC <ref type="bibr" target="#b9">[10]</ref>. TSC is a variant of the k-nearest neighbors method, OMP and NSN are two sparse greedy methods, and SSC is a convex optimization method. These algorithms build sparse affinity matrices and are computationally efficient, and therefore can perform large-scale clustering. For TSC and NSN we use the code provided by the respective authors. We note that the code may not be optimized for computational efficiency considerations. For OMP, we use our implementation, which has been optimized for subspace clustering. For SSC we use the SPAMS solver described in the previous section.</p><p>The second group consists of LRSC and SSC (with a different solver). We use the code provided by their respective authors, which uses the Alternating Direction Method of Multipliers (ADMM) to solve the optimization problems. To distinguish the two versions of SSC, we refer to this one as SSC-ADMM and to the previous one as SSC-SPAMS.</p><p>The final group consists of ENSC <ref type="bibr" target="#b31">[32]</ref> and KMP <ref type="bibr" target="#b21">[22]</ref>, and are the closest in spirit to our method. Our method and ENSC both balance the ℓ 1 and ℓ 2 regularizations, but ENSC uses h(e) = e 1 to penalize the noise (see <ref type="bibr" target="#b0">(1)</ref>) and the linearized alternating direction method to minimize their objective. In KMP, the k-support norm is used to blend the ℓ 1 and ℓ 2 regularizers. We implemented ENSC and KMP according to the descriptions in their original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>To the best of our knowledge, a comparison of all these methods on large scale datasets has not been reported in prior work. Thus, we run all experiments and tune the parameters for each method to give the best clustering accuracy. The results are reported in <ref type="table">Table 2</ref>. <ref type="table">Table 2</ref>. Performance of different clustering algorithms. The running time includes the time for computing the affinity matrix and for performing spectral clustering. The sparsity is the number of nonzero coefficients in each representation cj averaged over j = 1, · · · , N . The value "M" means that the memory limit of 16GB was exceeded, and the value "T" means that the time limit of seven days was reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TSC OMP NSN SSC-SPAMS SSC-ADMM LRSC ENSC KMP EnSC-ORGEN Clustering accuracy (%)</head><p>Coil-100 61. <ref type="bibr" target="#b31">32</ref>  We see that our proposed method achieves the best clustering performance on every dataset. Our method is also among the most efficient in terms of computing time. The methods SSC-ADMM, ENSC, LRSC and KMP cannot handle large-scale data because they perform calculations over the full data matrix and put the entire kernel matrix X ⊤ X in memory, which is infeasible for large datasets. The method of SSC-SPAMS uses an active set method that can deal with massive data, however, it is computationally much less efficient than our solver ORGEN.</p><p>For understanding the advantages of our method, in <ref type="table">Table 2</ref> we report the sparsity of the representation coefficients, which is the number of nonzero entries in c j averaged over all j = 1, . . . , N . For TSC, OMP and NSN, the sparsity is directly provided as a parameter of the algorithms. For SSC and our method EnSC-ORGEN, the sparsity is indirectly controlled by the parameters of the models. We can see that our method usually gives more nonzero entries than the sparsity based methods of TSC, OMP, NSN, and SSC. This shows the benefit of our method: while the number of correct connections built by OMP, NSN and SSC are in general upper-bounded by the dimension of the subspace, our method does not have this limit and is capable of constructing more correct connections and producing well-connected affinity graphs. On the other hand, the affinity graph of LRSC is dense, so although each cluster is self-connected, there are abundant wrong connections. This highlights the advantage of our method, which is flexible in controlling the number of nonzero entries by adjusting the trade-off parameter λ. Our results illustrate that this tradeoff improves clustering accuracy.</p><p>Finally, ENSC and KMP are two representatives of other methods that also exploit the trade-off between ℓ 1 and ℓ 2 regularizations. A drawback of both works is that the solvers for their optimization problems are not as effective as our ORGEN algorithm, as they cannot deal with large datasets due to memory requirements. Moreover, we observe that their algorithms converge to modest accuracy in a few iterations but can be very slow in giving a high precision solution. This may explain why their clustering accuracy is not as good as that of EnSC-ORGEN. Especially, we see that ENSC gives dense solutions although the true solution is expected to be sparser, and this is explained by the fact that the solution paths of their solver are dense solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We investigated elastic net regularization (i.e., a mixture of the ℓ 1 and ℓ 2 norms) for scalable and provable subspace clustering. Specifically, we presented an active set algorithm that efficiently solves the elastic net regularization subproblem by capitalizing on the geometric structure of the elastic net solution. We then gave theoretical justifications-based on a geometric interpretation for the trade-off between the subspace preserving and connectedness properties-for the correctness of subspace clustering via the elastic net. Extensive experiments verified that that our proposed active set method achieves state-of-the art clustering accuracy and can handle large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>The appendix is organized as follows. In Section A we present proofs for the geometric properties of the elastic net solution. In Section B we show the convergence of algorithm ORGEN. In Section C we prove the relevant results for the properties of the EnSC. In Section D, we use synthetically generated data to verify our results on the properties of the EnSC. In Section E, we study the special case of λ = 1, in which the EnSC method reduces to SSC. We show that the properties of EnSC as well as the ORGEN algorithm also apply to SSC with minor modifications, thus this work also offers additional understanding of SSC. In Section F we report the parameters of the algorithms used for real data experiments. Finally, in Section G, we clarify the contribution of this paper in comparison to several prior works on elastic net based subspace clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of the Geometric Properties of the Elastic Net Solution</head><p>A fundamental result that serves as the basis for the analysis of the elastic net solution in Section 2 is the next lemma. It is used to prove Theorem 2.1 and Propositions 2.1 and 2.2.</p><p>Lemma A.1 <ref type="bibr">([7, 21]</ref>). The vectorĉ ∈ IR N is the unique solution to <ref type="bibr" target="#b3">(4)</ref> if and only if it satisfies</p><formula xml:id="formula_27">(1 − λ)ĉ = T λ A ⊤ · γ(b − Aĉ) . (A.1)</formula><p>Proof. We provide a sketch of the proof for completeness. Since problem (4) is strongly convex,ĉ is the unique optimal solution if and only if it satisfies the following optimality condition:</p><formula xml:id="formula_28">A ⊤ · γ(b − Aĉ) = (1 − λ)ĉ + λz. (A.2)</formula><p>for some z ∈ ∂ ĉ 1 . Then, by taking the soft-thresholding T λ (·) on both sides of (A.2) we get (A.1). For a proof of the reverse implication, supposeĉ satisfies (A.1). For each j = 1, · · · , N , by considering the three casesĉ j &gt; 0,ĉ j = 0, andĉ j &lt; 0 separately, one can establish that the j-th row of (A.2) is satisfied when the corresponding row of (A.1) holds.</p><p>Theorem 2.1 follows trivially from this result. In the remainder of this section, we prove Propositions 2.1 and 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Proof of Proposition 2.1</head><p>Proof. Notice that c * (b, A) satisfies</p><formula xml:id="formula_29">(1 − λ)c * (b, A) = T λ A ⊤ γ(b − Ac * (b, A)) = T λ A ⊤ γ b − [A, A ′ ] c * (b, A) 0 N ′ ×1 . (A.3)</formula><p>Using the assumption that no column of A ′ is contained in ∆(b, A), it follows that</p><formula xml:id="formula_30">(1 − λ)0 N ′ ×1 = T λ A ′⊤ δ(b, A) = T λ A ′⊤ γ(b − Ac * (b, A)) = T λ A ′⊤ γ b − [A, A ′ ] c * (b, A) 0 N ′ ×1 .</formula><p>We may then combine this equality with (A.3) and define the vectorĉ :</p><formula xml:id="formula_31">= [c * (b, A) ⊤ , 0 ⊤ N ′ ×1 ] ⊤ to obtain (1 − λ)ĉ = T λ [A, A ′ ] ⊤ γ(b − [A, A ′ ]ĉ) , (A.4) thus by Lemma A.1,ĉ must equal c * (b, [A, A ′ ]).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Proof of Proposition 2.2</head><p>Proof. We prove the contrapositive; let c A ′ = 0. It then follows from</p><formula xml:id="formula_32">c A = c * (b, A) that c * (b, [A, A ′ ]) = [c * (b, A) ⊤ , 0 ⊤ ]</formula><p>, and by definition of the oracle point that</p><formula xml:id="formula_33">δ(b, A) = δ(b, [A, A ′ ])</formula><p>. Now by Theorem 2.1, we have</p><formula xml:id="formula_34">(1 − λ) c * (b, A) 0 = T λ A ⊤ A ′⊤ · δ(b, A) . (A.5)</formula><p>From the second block of equations and the definition of ∆(b, A), we have that no column of A ′ lies in the oracle region ∆(b, A), which completes the contrapositive proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Convergence for Algorithm 1 B.1. Proof of Lemma 2.1</head><p>Proof. Let us define the sets</p><formula xml:id="formula_35">Q := T k \ T k+1 , S := T k ∩ T k+1 , and R := T k+1 \ T k = ∅,</formula><p>where the fact that R is nonempty follows from the assumption T k+1 T k in the statement of Lemma 2.1. By these definitions, T k = Q ∪ S, and T k+1 = S ∪ R.</p><p>By definition, T k+1 contains all columns of A that are in ∆(b, A T k ), thus no column of A Q is in ∆(b, A T k ). By Proposition 2.1,</p><formula xml:id="formula_36">c * (b, A T k ) = c * (b, [A S , A Q ]) = c * (b, A S ) 0 , (B.1)</formula><p>in which we have assumed without loss of generality that columns of A T k are arranged in the order such that</p><formula xml:id="formula_37">A T k = [A S , A Q ]. Using (B.1), we have f (c * (b, A T k ); b, A T k ) =f c * (b, A S ) 0 ; b, [A S , A R ] ≥ min c f (c; b, [A S , A R ]) =f (c * (b, [A S , A R ]); b, [A S , A R ]) =f (c * (b, A T k+1 ); b, A T k+1 ). (B.2)</formula><p>It remains to show that the inequality in (B.2) is strict. We show this by arguing that [c * (b, A S ) ⊤ , 0 ⊤ ] ⊤ that appears on the second line of (B.2) is not an optimal solution to the optimization problem stated on the third line. Denote the solution to this optimization problem as</p><formula xml:id="formula_38">c * (b, [A S , A R ]) := c S c R , (B.3)</formula><p>where c S and c R are of appropriate sizes. By (B.1) and the definition of the oracle region, we have</p><formula xml:id="formula_39">∆(b, A S ) = ∆(b, A T k ). (B.4)</formula><p>Combining this with the facts that the columns of A T k+1 are in ∆(b, A T k ) and R ⊆ T k+1 , we know that the columns of A R are in ∆(b, A S ). Consequently, by Proposition 2.2, we must have c R = 0. This shows that [c * (b, A S ) ⊤ , 0 ⊤ ] ⊤ is not an optimal solution to the problem on the third line of (B.2) and thus the inequality in (B.2) is strict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Proof of Theorem 2.2</head><p>Proof. We first prove that Algorithm 1 terminates in a finite number of iterations. We first observe that the objective is strictly decreasing during each iteration before termination occurs (see Lemma 2.1). Since there are only finitely many different active sets, we must conclude that Algorithm 1 terminates after a finite number of iterations with T k+1 ⊂ T k . We now prove that when Algorithm 1 terminates, the output vector is optimal. Construct the vectorĉ such that</p><formula xml:id="formula_40">c T k = c * (b, A T k ) andĉ T c k = 0, in which T c k is the comple- ment of T k in {1, · · · , N }. By Theorem 2.1, for any j ∈ T k it holds that (1 − λ) · c * j (b, A T k ) = T λ (a ⊤ j · δ(b, A T k )</formula><p>). For any j / ∈ T k , by the termination condition T k+1 ⊆ T k we know j / ∈ T k+1 . Thus, by step 5, 0 = T λ (a ⊤ j · δ(b, A T k )). Consequently,ĉ satisfies the relation in (A.1) and thus is the solution, i.e.,ĉ = c * (b, A). Also, from the construction it can be seen that the support ofĉ is precisely T k+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of the Correctness of EnSC</head><p>In this section we prove the results in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Inradius</head><p>The inradius introduced in Definition 3.1 characterizes the distribution of a set of points. The next lemma can be interpreted as giving an equivalent definition of inradius for certain convex sets. The result is used in interpreting differences between Theorem 3.2 and Theorem 3.3, as well as in proving Theorem 3.1.</p><p>Lemma C.1. If {a j } N j=1 are points with unit ℓ 2 norm, then</p><formula xml:id="formula_41">r conv{±a j } N j=1 = min v =0 max j=1,··· ,N µ(a j , v). (C.1)</formula><p>Proof. Let A = [a 1 , · · · , a N ]. The right hand side of (C.1) can be written as</p><formula xml:id="formula_42">min v =0 max j=1,··· ,N µ(a j , v) = min v =0 A ⊤ v ∞ v 2 = 1/ max v =0 v 2 A ⊤ v ∞ . (C.2)</formula><p>One then quotes the relation that the inradius of a symmetric convex body is the reciprocal of the circumradius of its polar set, which is exactly the right hand side of (C.2) (see, e.g. Definition 7.2 in <ref type="bibr" target="#b35">[36]</ref> or Lemma 1 in <ref type="bibr" target="#b47">[48]</ref>).</p><p>The interpretation of Lemma C.1 is as follows: one searches for a vector v that is furthest away from all points {±a j } N j=1 , and the inradius is the coherence of this v with the closest neighbor in {a j } N j=1 . In other words, it characterizes the covering property of the points {±a j } N j=1 . If inradius is large then for any point in the space there exists an a j that is close to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Proof of Lemma 3.1</head><p>Proof. Consider the problem</p><formula xml:id="formula_43">c * (x j , X ℓ −j ) = arg min c f (c; x j , X ℓ −j ) (C.3)</formula><p>and by our notation, let ∆(x j , X ℓ −j ) be its oracle region. For the "if" part, we know from Proposition 2.1 that adding more points that are outside of the oracle region ∆(x j , X ℓ −j ) to the dictionary of (C.3) does not affect its solution. To be more specific, if it holds that x k / ∈ ∆(x j , X ℓ −j ) for all x k / ∈ S ℓ , then by Proposition 2.1 we</p><formula xml:id="formula_44">have c * (b, X −j ) = P · [c * (x j , X ℓ −j ) ⊤ , 0 ⊤ ] ⊤ , where P is some permutation matrix.</formula><p>For the "only if" part, if any x k / ∈ S ℓ is in the oracle region ∆(x j , X ℓ −j ), then Proposition 2.2 shows that the coefficient vector of c * (b, X −j ) that corresponds to points outside of S ℓ is nonzero. Therefore, the solution is not correct in identifying the l-th subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Proof of Theorem 3.1</head><p>Result Theorem 3.1 follows from the bound on the norm of the oracle point given below in Lemma C.2 and the relation κ ≥ r as revealed by Lemma C.1.</p><p>Lemma C.2. Consider problem <ref type="bibr" target="#b3">(4)</ref>. If we define κ = max j µ(a j , δ) as the coherence between the oracle point δ and its closest neighbor among the columns of A, then</p><formula xml:id="formula_45">δ 2 ≤ λκ + 1 − λ κ 2 . (C.4)</formula><p>Proof. If c * = 0, then the optimality condition (A.2) shows that A ⊤ δ ∞ ≤ λ, hence κ δ 2 ≤ λ. From this it is easy to see that (C.4) holds.</p><p>Next, we suppose that c * = 0, and assume without loss of generality that every entry in c * is positive. (If an entry of c * is zero then we can remove the corresponding column from A without affecting the quantities δ and κ. Also, if c * j &lt; 0 for some j, we can change a j to −a j so that the solution will simply have c * j changed to −c * j , which is then positive.) Since all entries of c * are positive, we may conclude that a ⊤ j δ &gt; λ for all j. We now multiply both sides of the optimality condition (A.2) by c * ⊤ to obtain</p><formula xml:id="formula_46">c * , A ⊤ δ = (1 − λ) c * 2 2 + λ c * 1 . (C.5)</formula><p>Also, by the definition of the oracle point, we have</p><formula xml:id="formula_47">Ac * , δ = b − δ/γ, δ = b, δ − δ 2 2 /γ. (C.6)</formula><p>Notice that since the left-hand-side of (C.5) and (C.6) are the same, we can equate the right-hand-sides to get</p><formula xml:id="formula_48">(1−λ) c * 2 2 + λ c * 1 = b, δ − δ 2 2 /γ ≤ δ 2 − δ 2 2 /γ. (C.7)</formula><p>We now prove a lower bound on the left-hand-side of (C.7) in terms of δ 2 . From (6) and a ⊤ j δ &gt; λ for all j, we have</p><formula xml:id="formula_49">(1 − λ) c * 2 2 + λ c * 1 ≥ (1 − λ)c 2 j + λc j = T λ (a ⊤ j δ) 2 1 − λ + λT λ (a ⊤ j δ) 1 − λ = (a ⊤ j δ − λ) · a ⊤ j δ 1 − λ (C.8)</formula><p>for all 1 ≤ j ≤ N . If we now take j to be the index that maximizes a j , δ/ δ 2 and use the definition of κ, then</p><formula xml:id="formula_50">(1 − λ) c * 2 2 + λ c * 1 ≥ (κ δ 2 − λ) · κ δ 2 1 − λ . (C.9)</formula><p>Combining (C.7) with (C.9), we get an inequality on δ 2 :</p><formula xml:id="formula_51">(κ δ 2 − λ) · κ δ 2 1 − λ ≤ δ 2 − δ 2 2 /γ. (C.10)</formula><p>This inequality gives a bound on δ 2 of</p><formula xml:id="formula_52">δ 2 ≤ λκ + 1 − λ κ 2 + (1 − λ)/γ ≤ λκ + 1 − λ κ 2 , (C.11)</formula><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Proofs of Theorem 3.3 and 3.2</head><p>Theorem 3.3 can be obtained by combining Lemma 3.1 and Theorem 3.1. Theorem 3.2 follows from Theorem 3.3 and the fact that κ j ≥ r j as revealed by Lemma C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Correctness of EnSC</head><p>In Theorem 3.2 and Theorem 3.3, we give two conditions that guarantee the correctness of the representation given by EnSC for the purpose of subspace clustering. In this section, we use synthetic experiments to verify our theoretical analysis. Specifically, we verify that as the ℓ 1 -ℓ 2 tradeoff parameter λ increases, the representation is more likely to be correct. Moreover, we examine the tightness of our bound for predicting the correctness.</p><p>For each pair of N ∈ {100, 200, 400, 800, 1600, 3200} and λ ∈ {0.99, 0.95, 0.90, 0.80, 0.60, 0.40, 0.20, 0.10}, we randomly generate subspaces and data samples as specified in the caption of Figure D.1. We then run EnSC on the generated data matrix and get the representation vectors {c j } N j=1 . In <ref type="figure" target="#fig_6">Figure 1(a)</ref> we report the percentage of the c j vectors that are correct in identifying its subspace. As can be seen, it is easier to get correct representations when λ is larger. This is consistent with our intuition: as λ becomes larger, the solution is sparser and is more likely to be correct. Moreover, this is consistent with what is predicted by our theoretical analysis, as in both Theorem 3.2 and Theorem 3.3 the condition for correctness is easier to be satisfied as λ increases.</p><p>We plot the result of Theorem 3.3 in <ref type="figure" target="#fig_6">Figure 1(b)</ref>. Specifically, for each j ∈ {1, · · · , N }, we can solve for c * (x j , X ℓ −j ) by using the ground truth labels and then compute δ(x j , X ℓ −j ) from c * (x j , X ℓ −j ) by (2.1). Consequently, all quantities in the condition of Theorem 3.3 can be computed, and consequently whether or not the condition holds. In <ref type="figure" target="#fig_6">Figure 1(b)</ref> we plot the percentage of points that satisfy the condition. Since our condition is sufficient but not necessary, we expect the percentage in <ref type="figure" target="#fig_6">Figure 1</ref>(b) to be no larger than the corresponding percentage in <ref type="figure" target="#fig_6">Figure 1(a)</ref>, and the gap between them reveals the tightness of the result of Theorem 3.3. This gap is more clearly illustrated in <ref type="figure" target="#fig_6">Figure  1</ref>(c), in which we plot selected rows from <ref type="figure" target="#fig_6">Figure 1</ref>(a) and 1(b) that correspond to λ = {0.99, 0.90, 0.60}. It can be seen that our condition becomes tighter as λ approaches 1.</p><p>Finally, notice that while the condition in Theorem 3.3 can be checked when the ground truth is known, the condition in Theorem 3.2 cannot be since it is generally NP-hard to compute the inradius r j <ref type="bibr" target="#b35">[36]</ref>. This is an advantage of Theorem 3.3, in addition to the fact that it has a weaker requirement to guarantee the correctness of EnSC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion for the Case λ = 1</head><p>As the analyses and results of this paper are for λ ∈ [0, 1), in this section we discuss the case λ = 1. It turns out that the geometric structure of the elastic net solution for λ = 1 is slightly different. As a result, many of the theorems and discussions do not apply for λ = 1, so that we need a separate discussion for most of the results.</p><p>The oracle point and oracle region. We use the same definitions of the oracle point and oracle region as before. While for λ ∈ [0, 1) the oracle point δ is unique since c * is unique due to the strong convexity of the problem, the same argument does not apply to the case λ = 1. However, we can sill establish the uniqueness of the oracle point. Proof. For λ &lt; 1, the optimization problem (4) is strongly convex, thus c * is unique. Then, by <ref type="bibr" target="#b4">(5)</ref>, δ(b, A) is unique. Introducing the dual vector v, the Lagragian function is</p><formula xml:id="formula_53">L(c, e, v) = c 1 + γ 2 e 2 2 + v, b − Ac − e , (E.2)</formula><p>and the corresponding dual problem is</p><formula xml:id="formula_54">max v b, v − 1 2γ v ⊤ v s.t. A ⊤ v ∞ ≤ 1, (E.3)</formula><p>whose objective function is strongly concave with a unique solution v * . Also, from the optimality conditions we have v * = γe * = γ(b − Ac * (b, A)) = δ(b, A), so that δ(b, A) is unique.</p><p>The geometric structure of the solution. Recall that from Theorem 2.1 we know that the oracle region contains points whose corresponding coefficients are nonzero, i.e., c * j = 0 if and only if a j ∈ ∆(b, A). For the case λ = 1, this argument no longer holds. Actually, Theorem 2.1 still holds for λ = 1, but the left-hand-side of (A.1) becomes zero, and it means that no column of A is in the oracle region ∆(b, A). To further understand the structure of the solution, we need the following result.</p><p>Theorem E.2. The solution c * = c * (b, A) to problem (4) with λ = 1 satisfies that if c * j = 0, then |a ⊤ j δ| = 1.</p><p>This result follows from the optimality condition. It means that a coefficient c * j is nonzero only if a j is on the boundary of the oracle region ∆(b, A), which we denote as ∂∆(b, A). The opposite is generally not true: if a j ∈ ∂∆(b, A), it does not necessarily mean that c * j = 0. The geometric structure of the solution is thus clear: all columns of A are outside the oracle region, but some columns of A are in ∂∆(b, A) with some of these corresponding to nonzero coefficients.</p><p>The ORGEN algorithm. Algorithm 1 needs to be revised when λ = 1. Specifically, we need an alternative step 5:</p><formula xml:id="formula_55">5": T k+1 ← {j : a j ∈ ∆(b, A T k )} ∪ S k , (E.4) where S k = {j : [c * (b, A T k )] j = 0} is the support of c * (b, A T k )</formula><p>. Notice that S k ⊆ ∂∆(b, A) when λ = 1 so that the two operands in the union in (E.4) are disjoint sets. With this modification, one can show that ORGEN converges to an optimal solution in a finite number of iterations. The proof is essentially the same as before and omitted here. In the case when the solution is not unique, the solution that ORGEN converges to depends upon the initialization T 0 as well as the specific solution given by the solver in step 3. For λ ∈ [0, 1), S k ⊆ ∆(b, A T k ) by the definition of the oracle region. Thus, the alternative step specified by (E.4) applies to any λ ∈ [0, 1]. We write this as a theorem. Theorem E.3. Algorithm 1 with the alternative step 5 specified in (E.4) converges to an optimal solution c * (b, A) in a finite number of iterations for all λ ∈ [0, 1].</p><p>Correctness of EnSC. Theorem 3.3 gives a sufficient condition for guaranteeing the correctness of EnSC when λ ∈ [0, 1). In extending the result to the case λ = 1 we need a slightly stronger condition. The difference between (E.5) and <ref type="bibr" target="#b14">(15)</ref> is that the inequality is strict in (E.5). This modification is necessary to handle the case λ = 1, for the condition <ref type="bibr" target="#b14">(15)</ref> does not exclude the case that x k / ∈ S ℓ may lie on the boundary of ∆(x j , X ℓ −j ) and yet correspond to a nonzero coefficient.</p><p>Finally, we discuss the implication of Theorem E.4 in the context of SSC. When λ = 1, condition (E.5) simplifies to</p><formula xml:id="formula_56">max k:x k / ∈S ℓ µ(x k , δ j ) &lt; κ j . (E.6)</formula><p>In <ref type="bibr" target="#b35">[36]</ref> a sufficient condition for SSC is given by</p><formula xml:id="formula_57">max k:x k / ∈S ℓ µ(x k , δ j ) &lt; r j . (E.7)</formula><p>Using the relationship r j ≤ κ j , our condition in (E.6) is a weaker requirement than that in the previous work. Specifically, condition (E.7) requires that the entire subspace S ℓ is well-covered by the columns of X ℓ −j so that r j is large. In contrast, our condition in (E.6) only requires the neighborhood of the oracle point δ j to be well-covered, i.e., that there exists a column in X ℓ −j that is close to δ j . Another advantage of our condition (E.6) is that it can be verified when the ground truth is known. In contrast, the condition in (E.7) cannot be verified since the computation of r j is generally NP-hard <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Parameters for Experiments on Real Data</head><p>For the purpose of reproducible results, we report the parameters used for all the methods in the real data experiments. TSC, OMP and NSN all have a parameter that controls the number of nonzero coefficients in the representation. This parameter is the same as the "sparsity" reported in <ref type="table">Table 2</ref>. The NSN has two additional parameters. One is the maximum subspace dimension, for which we set as the default value suggested by the original paper. The other is ǫ which controls a post-processing step. For the purpose of a fair comparison with other methods, we set ǫ = 0 which essentially disables this post-processing step. The SSC-SPAMS uses the model in (1) with r(·) = · 1 , h(·) = 1 2 · 2 2 , and γ = α · γ 0 , where α is a hyperparameter specified in <ref type="table" target="#tab_2">Table F</ref>.1 and γ 0 is the smallest value of γ such that c * (x j , X −j ) is nonzero. The parameters for the solver SPAMS are set to their default values.</p><p>For SSC-ADMM we use the code for solving the optimization problem (13) as presented in <ref type="bibr" target="#b10">[11]</ref>, with α set to the same value as for SSC-SPAMS. For LRSC we use the code for model (P3) in <ref type="bibr" target="#b38">[39]</ref>, in which the parameters τ and α are provided in <ref type="table" target="#tab_2">Table F.</ref>1. ENSC has the three key parameters λ 1 , λ 2 , and λ 3 in their model (see <ref type="bibr">[32, equation (4)</ref>]). The remaining parameters were set as suggested by the authors. For KMP we implemented <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">Algorithm 1]</ref> in which the number of iterations T is set to be 150, the parameter L is set to be 1.1 times the Lipschitz constant, and k and λ are reported in Table F.1. Finally, for the proposed algorithm EnSC-ORGEN, the parameter λ controls the trade-off between the ℓ 1 and ℓ 2 norms, and the parameter α controls the value for γ in (3) via the definition γ = αγ 0 , where γ 0 is the smallest value such that c * (x j , X −j ) is nonzero. The parameters are summarized in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Relation with prior work on EnSC</head><p>The elastic net formulation was originally proposed in <ref type="bibr" target="#b49">[50]</ref> and subsequently introduced to subspace clustering in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>. In these works, the regularization r(·) in <ref type="formula">(1)</ref> is set to be the ℓ 1 -ℓ 2 combination as in <ref type="bibr" target="#b1">(2)</ref>. For the penalty function h(·), <ref type="bibr" target="#b31">[32]</ref> proposes to use the ℓ 1 penalty, while <ref type="bibr" target="#b11">[12]</ref> uses a joint ℓ 1 -ℓ 2 penalty. Both works use existing methods for solving their optimization problem: <ref type="bibr" target="#b11">[12]</ref> uses the accelerated proximal gradient (APG) <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b31">[32]</ref> uses the linearized alternating direction method (LADM) <ref type="bibr" target="#b24">[25]</ref>.</p><p>The optimization model studied here is slightly different from these prior works since we set h(e) to be the ℓ 2 penalty as suggested in the original elastic net paper. Despite this difference in modeling the noise, all three models use the elastic net regularization. The major contributions of our work in comparison to these related works are threefold:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The interpretation for Proposition 2.1 is that the solution c * (b, A) does not change (modulo padding with additional zeros) when new columns are added to the dictionary A, as long as the new columns are not inside the oracle region ∆(b, A). From another perspective, c * (b, [A, A ′ ]) does not change if one removes columns from the dictionary [A, A ′ ] that are not in the oracle region ∆(b, [A, A ′ ]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Conceptual illustration of the ORGEN algorithm. All the dots on the unit circle illustrate the dictionary A. (a) active set T k at step k, illustrated by red dots. (b) The oracle region ∆(b, AT k ) illustrated by red arcs. (c) The new active set T k+1 illustrated in green, which is the set of indices of points that are in ∆(b, AT k ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Performance with varying N and λ: (a) λ = 0.9 and N ∈ [5000, 10 6 ]; and (b, c) N = 100, 000 and λ ∈ [0.05, 0.999].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure D. 1 .</head><label>1</label><figDesc>Correctness of the solution of EnSC for different values of λ. We generate 4 subspaces each of dimension 8 in an ambient space of dimension 20 uniformly at random. On each subspace, we sample uniformly at random an equal number of points that add up to N , which varies from 100 to 3200. We report the percentage of representations that are correct in identifying subspaces.(a, b) The percentage of correct representations for different values of λ and N as produced by experimental results and as predicted by Theorem 3.3, respectively. (c) Plots of selected rows from (a) and (b) to help clarify the difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem E. 1 .</head><label>1</label><figDesc>The oracle point δ(b, A) is unique for each choice of λ ∈ [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>For λ = 1 ,</head><label>1</label><figDesc>we rewrite problem (4) equivalently as min t. b = Ac + e. (E.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Theorem E. 4 . 2 j</head><label>42</label><figDesc>Let x j ∈ S ℓ , and δ j and κ j be defined as in Theorem 3.3. Then, for all λ ∈ [0, 1], the solution c * (x j , X −j ) is correct in identifying the subspace S ℓ ifmax k:x k / ∈S ℓ µ(x k , δ j ) &lt; κ κ j + 1−λ λ .(E.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset information.</figDesc><table><row><cell></cell><cell cols="3">N (#data) D (ambient dim.) n (#groups)</cell></row><row><cell>Coil-100</cell><cell>7,200</cell><cell>1024</cell><cell>100</cell></row><row><cell>PIE</cell><cell>11,554</cell><cell>1024</cell><cell>68</cell></row><row><cell>MNIST</cell><cell>70,000</cell><cell>500</cell><cell>10</cell></row><row><cell cols="2">CovType 581,012</cell><cell>54</cell><cell>7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table F .</head><label>F</label><figDesc>1. Parameters for experiments on real data.</figDesc><table><row><cell></cell><cell>SSC</cell><cell cols="2">LRSC</cell><cell></cell><cell>ENSC</cell><cell></cell><cell cols="2">KMP</cell><cell cols="2">EnSC-ORGEN</cell></row><row><cell></cell><cell>α</cell><cell>τ</cell><cell>α</cell><cell cols="2">λ 1 λ 2</cell><cell>λ 3</cell><cell>k</cell><cell>λ</cell><cell>λ</cell><cell>α</cell></row><row><cell>Coil-100</cell><cell>25</cell><cell>5</cell><cell>5</cell><cell cols="2">0.1 0.1</cell><cell>1</cell><cell cols="3">100 0.1 0.95</cell><cell>3</cell></row><row><cell>PIE</cell><cell cols="9">200 100 100 0.1 0.1 1000 100 0.1 0.1</cell><cell>200</cell></row><row><cell>MNIST</cell><cell>120</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.95</cell><cell>120</cell></row><row><cell>CovType</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.95</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table F</head><label>F</label><figDesc></figDesc><table /><note>.1.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Subspaces {Sκ} are independent if dim( κ Sκ) = κ dim(Sκ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://archive.ics.uci.edu/ml/datasets/Covertype</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. C. You, D. P. Robinson and R. Vi-1. We design a new active-set algorithm for solving the optimization problem. In comparison to APG and LADM that are used in the related works, our method is computationally more efficient, and is able to handle larger datasets.</p><p>2. Although using the elastic net for subspace clustering to balance correctness and connectivity is not new, we provide the first detailed argument based on a geometric interpretation of the solution of the elastic net. This deepens the understanding of the approach.</p><p>3. We provide (under general conditions) the first proof of correctness for elastic net based subspace clustering.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparse prediction with the k-support norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1466" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkagethresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral curvature clustering (SCC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale spectral clustering with landmark-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multibody factorization method for independently moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="179" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Elastic-net regularization in learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Mol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">De</forename><surname>Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complexity</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="230" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Greedy feature selection for subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Least angle regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph-oriented learning via automatic group sparsity for data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph-based learning via auto-grouped sparse regularization and kernelized extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A closed form solution to robust subspace estimation and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="586" to="597" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Trace lasso: a trace norm regularization for correlated designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-PIE. Image Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Robust subspace clustering via thresholding. CoRR, abs/1307</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bölcskei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4891</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clustering appearances of objects under varying illumination conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiscale hybrid linear models for lossy image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3655" to="3671" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Elastic-net regulariztion: error estimates and active set methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schiffler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient ksupport matrix pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structured sparse subspace clustering: A unified optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Linearized alternating direction method with adaptive penalty for low rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust subspace segmentation by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Correlation adaptive subspace segmentation by trace lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust and efficient subspace segmentation via least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph connectivity in sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nasihatkon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Columbia object image library (COIL-100)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<idno>CUCS-006-96</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Elastic net subspace clustering applied to pop/rock music structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Greedy subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scalable sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="430" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Motion segmentation in the presence of outlying, incomplete, or corrupted trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A geometric analysis of subspace clustering with outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Filtrated spectral algebraic subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tsakiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Robust Subspace Learning and Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Low rank subspace clustering (LRSC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized Principal Component Analysis (GPCA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Generalized Principal Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multiframe motion segmentation with missing data using PowerFactorization, and GPCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="105" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Noisy sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Provable subspace clustering: When LRR meets SSC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scalable sparse subspace clustering by orthogonal matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Geometric conditions for subspacesparse recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1585" to="1593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Subspace-sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<idno>abs/1507.01307</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hybrid linear modeling via local best-fit flats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="240" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
