<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MDE: Multiple Distance Embeddings for Link Prediction in Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Sadeghi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Graux</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><forename type="middle">Shariat</forename><surname>Yazdi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
						</author>
						<title level="a" type="main">MDE: Multiple Distance Embeddings for Link Prediction in Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past decade, knowledge graphs became popular for capturing structured domain knowledge. Relational learning models enable the prediction of missing links inside knowledge graphs. More specifically, latent distance approaches model the relationships among entities via a distance between latent representations. Translating embedding models (e.g., TransE) are among the most popular latent distance approaches which use one distance function to learn multiple relation patterns. However, they are mostly inefficient in capturing symmetric relations since the representation vector norm for all the symmetric relations becomes equal to zero. They also lose information when learning relations with reflexive patterns since they become symmetric and transitive. We propose the Multiple Distance Embedding model (MDE) that addresses these limitations and a framework to collaboratively combine variant latent distance-based terms. Our solution is based on two principles: 1) we use a limit-based loss instead of a margin ranking loss and, 2) by learning independent embedding vectors for each of the terms we can collectively train and predict using contradicting distance terms. We further demonstrate that MDE allows modeling relations with (anti)symmetry, inversion, and composition patterns. We propose MDE as a neural network model that allows us to map nonlinear relations between the embedding vectors and the expected output of the score function. Our empirical results show that MDE performs competitively to state-of-the-art embedding models on several benchmark datasets. <ref type="bibr" target="#b4">5</ref> The complete code and the experimental datasets are available from:</p><p>https://github.com/mlwin-de/MDE</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While machine learning methods conventionally model functions given sample inputs and outputs, a subset of Statistical Relational Learning (SRL) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref> approaches specifically aim to model "things" (entities) and relations between them. These methods usually model human knowledge which is structured in the form of multi-relational Knowledge Graphs (KG). KGs allow semantically rich queries and are used in search engines, natural language processing (NLP) and dialog systems. However, they usually miss many of the true relations <ref type="bibr" target="#b33">[34]</ref>, therefore, the prediction of missing links/relations in KGs is a crucial challenge for SRL approaches.</p><p>Practically, a KG usually consists of a set of facts. And a fact is a triple (head, relation, tail) where heads and tails are called entities. Among the SRL models, distance-based KG embeddings are popular because of their simplicity, their low number of parameters, and their efficiency on large scale datasets. Specifically, their simplicity allows integrating them into many models. Previous studies have integrated them with logical rule embeddings <ref type="bibr" target="#b9">[10]</ref>, have adopted them to encode temporal information <ref type="bibr" target="#b14">[15]</ref> and have applied them to find equivalent entities between multi-language datasets <ref type="bibr" target="#b20">[21]</ref>.</p><p>Soon after the introduction of the first multi-relational distancebased method TransE <ref type="bibr" target="#b2">[3]</ref>, it was acknowledged that it is inefficient in learning symmetric relations, since the norm of the representation vector for all the symmetric relations in the KG becomes close to zero. This means the model cannot distinguish well different symmetric relations in a KG. To extend this model many variations were studied afterwards, e.g., TransH <ref type="bibr" target="#b31">[32]</ref>, TransR <ref type="bibr" target="#b17">[18]</ref>, TransD <ref type="bibr" target="#b13">[14]</ref>, and STransE <ref type="bibr" target="#b5">[6]</ref>. Even though they solved the issue of symmetric relations, they introduced an other limitation: these models were no longer efficient in learning the inversion and composition relation patterns that originally TransE could handle.</p><p>Besides, as noted in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>, within the family of distance-based embeddings, reflexive relations are usually forced to become symmetric and transitive. In this study, we take advantage of independent vector representations of vectors that enable us to view the same relations from different aspects and put forward a translation-based model that addresses these limitations and allows the learning of all three relation patterns. In addition, we address the issue of the limitbased loss function in finding an optimal limit, and suggest an updating limit loss function to be used complementarily to the current limit-based loss function which has fixed limits. Moreover, we frame our model into a neural network structure that allows it to learn nonlinear patterns for the limits in the limit based loss, improving the generalization power of the model in link prediction tasks.</p><p>The model performs well in the empirical evaluations, competing against state-of-the-art models in link prediction benchmarks. In particular, it outperforms 5 state-of-the-art models on Countries <ref type="bibr" target="#b4">[5]</ref> benchmark which is designed to evaluate composition pattern inference and modeling.</p><p>Since our approach involves several elements that model the relations between entities as the geometric distance of vectors from different views, we dubbed it multiple-distance embeddings (MDE).</p><p>The rest of this article is structured as follows: we define background and notations in Section 2 and summarize related efforts in Section 3. Then we present the MDE model in Section 4 and describes the extensions of the model including a hyperparameter search algorithm for the loss function and a Neural Network framing of MDE in Section 5. We report on the experiments in Section 6 before concluding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Notation</head><p>Given the set of all entities E and the set of all relations R, we formally define a fact as a triple of the form (h, r, t) in which h is the head and t is the tail, h, t ∈ E and r ∈ R is a relation. A knowledge graph KG is a subset of all true facts KG ⊂ ζ and is represented by a set of triples. An embedding is a mapping from an entity or a relation to their latent representation. A latent representation is usually a (set of) vector(s), a matrix or a tensor of numbers. A relational learning model is made of an embedding function and a prediction function that given a triple (h, r, t) it determines if (h, r, t) ∈ ζ. We represent the embedding representation of an entity h with a lowercase letter h if it is a vector and with an uppercase letter H if it is a matrix. The ability to encode different patterns in the relations can show the generalization power of a model:</p><formula xml:id="formula_0">Definition 1. A relation r is symmetric (antisymmetric) if ∀x, y r(x, y) ⇒ r(y, x) ( r(x, y) ⇒ ¬r(y, x) ). Definition 2. A relation r1 is inverse to relation r2 if ∀x, y r2(x, y) ⇒ r1(y, x). Definition 3. A relation r1 is composed of relation r2 and relation r3 if ∀x, y, z r2(x, y) ∧ r3(y, z) ⇒ r1(x, z)</formula><p>3 Related Work Tensor Factorization and Multiplicative Models define the score of triples via pairwise multiplication of embeddings. DistMult <ref type="bibr" target="#b35">[36]</ref> simply multiplies the embedding vectors of a triple element by element h, r, t as the score function. Since multiplication of real numbers is symmetric, DistMult can not distinguish displacement of head relation and tail entities and therefore, it can not model antisymmetric relations.</p><p>ComplEx <ref type="bibr" target="#b30">[31]</ref> solves the issue of DistMult by the idea that the complex conjugate of the tail makes it non-symmetric. By introducing complex-valued embeddings instead of real-valued embeddings to DistMult, the score of a triple in ComplEx is Re(h diag(r)t) witht the conjugate of t and Re(.) is the real part of a complex value. ComplEx is not efficient in encoding composition rules <ref type="bibr" target="#b27">[28]</ref>. In RESCAL <ref type="bibr" target="#b24">[25]</ref> instead of a vector, a matrix represents the relation r, and performs outer products of h and t vectors to this matrix so that its score function becomes h Rt. A simplified version of RESCAL is HolE <ref type="bibr" target="#b23">[24]</ref> that defines a vector for r and performs circular correlation of h and t has been found equivalent <ref type="bibr" target="#b10">[11]</ref> to ComplEx. Another tensor factorization model is Canonical Polyadic (CP) <ref type="bibr" target="#b11">[12]</ref>. In CP decomposition, each entity e is represented by two vectors he, te ∈ R d , and each relation r has a single embedding vector vr ∈ R d . MDE is similarly based on the idea of independent vector embeddings. A study <ref type="bibr" target="#b29">[30]</ref> suggests that in CP, the independence of vectors causes the poor performance of CP in KG completion, however, we show that the independent vectors can strengthen a model if they are combined complementarily.</p><p>SimplE <ref type="bibr" target="#b15">[16]</ref> analogous to CP, trains on two sets of subject and object entity vectors. SimplE's score function, <ref type="bibr" target="#b0">1</ref> 2 he i , r, te j + 1 2 he j , r −1 , te j , is the average of two terms. The first term is similar to DistMult. However, its combination with the second term and using a second set of entity vectors allows SimplE to avoid the symmetric issue of DistMult. SimplE allows learning of symmetry, antisymmetry and inversion patterns. However, it is unable to efficiently encode composition rules, since it does not model a bijection mapping from h to t through relation r.</p><p>In Latent Distance Approaches the score function is the distance between embedding vectors of entities and relations. In the view of social network analysis, <ref type="bibr" target="#b12">[13]</ref> originally proposed distance of entities −d(h, t) as the score function for modeling uni-relational graphs where d(., .) means any arbitrary distance, such as Euclidean distance. SE <ref type="bibr" target="#b3">[4]</ref> generalizes the distance for multi-relational data by incorporating a pair of relation matrices into it. TransE <ref type="bibr" target="#b2">[3]</ref> represents relation and entities of a triple by a vector that has this relation</p><formula xml:id="formula_1">S1 = h + r − t p<label>(1)</label></formula><p>where . p is the p-norm. To better distinguish entities with complex relations, TransH <ref type="bibr" target="#b32">[33]</ref> projects the vector of head and tail to a relation-specific hyperplane. Similarly, TransR follows the idea with relation-specific spaces and extends the distance function to Mrh + r − Mrt p. RotatE <ref type="bibr" target="#b27">[28]</ref> combines translation and rotation and defines the distance of a t from tail h which is rotated the amount r as the score function of a triple −d(h • r, t) where • is Hadamard product.</p><p>Neural Network Methods train a neural network to learn the interaction of the h, r and t. ER-MLP <ref type="bibr" target="#b8">[9]</ref> is a two layer feedforward neural network considering h, r and t vectors in the input. NTN <ref type="bibr" target="#b25">[26]</ref> is neural tensor network that concatenates head h and tail t vectors and feeds them to the first layer that has r as weight. In another layer, it combines h and t with a tensor R that represents r and finally, for each relation, it defines an output layer r to represent relation embeddings. In SME <ref type="bibr" target="#b1">[2]</ref> relation r is once combined with the head h to get gu(h, r), and similarly it is combined with the tail t to get gv(t, r). SME defines a score function by the dot product of this two functions in the hidden layer. In the linear SME, g(e, r) is equal to M 1 u e + M 2 u r + bu, and in the bilinear version, it is M 1 u e • M 2 u r + bu. Here, M refers to weight matrix and b is a bias vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MDE: Multiple Distance Embeddings</head><p>The score function of MDE involves multiple terms. We first explain the intuition behind each term and then explicate a framework that we suggest to efficiently utilize them such that we benefit from their strengths and avoid their weaknesses.</p><p>Inverse Relation Learning: Inverse relations can be a strong indicator in knowledge graphs. For example, if IsP arentOf (m, c) represents that a person m is a parent of another person c, then this could imply IsChildOf (c, m) assuming that this represents the person c being the child of m. This indication is also valid in cases when this only holds in one direction, e.g. for the relations IsM otherOf and IsChildOf . In such a case, even though the actual inverse IsP arentOf may not even exist in the KG, we can still benefit from inverse relation learning. To learn the inverse of the relations, we define a score function S2 :</p><formula xml:id="formula_2">S2 = t + r − h p<label>(2)</label></formula><p>Symmetric Relations Learning: It is possible to easily check that the formulation h + r − t allows 6 learning of anti-symmetric pattern but when learning symmetric relations, r tends toward zero which limits the ability of the model in separating entities specially if symmetric relations are frequent in the KG. For learning symmetric relations, we suggest the term S3 as a score function. It learns such relations more efficiently despite it is limited in the learning of antisymmetric relations.</p><formula xml:id="formula_3">S3 = h + t − r p (3)</formula><p>Lemma 1. S1 allows modeling antisymmetry, inversion and composition patterns and S2 allows modeling symmetry patterns.</p><p>Proof. Let r1, r2, r3 be relation vector representations and ei, ej, e k are entity representations. A relation r1 between (ei, e k ) exists when a triple (ei, r1, e k ) exists and we show it by r1(ei, e k ). Formally, we have the following results:</p><p>Antisymmetric Pattern. If r1(ei, ej) and r1(ej, ei) hold, in equation 1 for S1, then:</p><formula xml:id="formula_4">ei + r1 = ej ∧ ej + r1 = ei ⇒ ei + 2r1 = ei</formula><p>Thus S1 allows encoding of relations with antisymmetric patterns.</p><p>Symmetric Pattern. If r1(ei, ej) and r1(ej, ei) hold, for S3 we have:</p><formula xml:id="formula_5">ei + ej − r1 = 0 ∧ ej + ei − r1 = 0 ⇒ ej + ei = r1</formula><p>Therefore S3 allows encoding relations with symmetric patterns. For S1 we have:</p><p>Inversion Pattern. If r1(ei, ej) and r2(ej, ei) hold, from Equation 1 we have:</p><formula xml:id="formula_6">ei + r1 = ej ∧ ej + r2 = ei ⇒ r1 = −r2</formula><p>Therefore S1 allows encoding relations with inversion patterns.</p><p>Composition Pattern. If r1(ei, e k ) , r2(ei, ej) and, r3(ej, e k ) hold, from equation 1 we have:</p><formula xml:id="formula_7">ei + r1 = e k ∧ ei + r2 = ej ∧ ej + r3 = e k ⇒ r2 + r3 = r1</formula><p>Thus S1 allows encoding relations with composition patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relieving Limitations on Learning of Reflexive Relations:</head><p>A previous study <ref type="bibr" target="#b15">[16]</ref> highlighted the common limitations of TransE, FTransE, STransE, TransH and TransR for learning reflexive relations where these translation-based models force the reflexive relations to become symmetric and transitive. To relieve these limitations, we define S4 as a score function which is similar to the score of RotatE i.e., h • r − t p but with the Hadamard operation on the tail. In contrast to RotatE which represents entities as complex vectors, S4 only holds in the real space:</p><formula xml:id="formula_8">S4 = h − r • t p<label>(4)</label></formula><p>Lemma 2. The following restrictions of translation based embeddings approaches do not apply to the S4 score function. R1: if a relation r is reflexive, on ∆ ∈ E, r it will be also symmetric on ∆. R2: if r is reflexive on ∆ ∈ E, r it will be also be transitive on ∆.</p><p>Proof. R1: For such reflexive r1, if r1(ei, ei) then r l (ej, ej). In this equation we have:</p><formula xml:id="formula_9">ei = r1ei ∧ ej = r1ej ⇒ r1 = U ⇒ ei = r1ej</formula><p>where U is unit tensor.</p><p>R2: For such reflexive r1, if r1(ei, ej) and r l (ej, e k ) then r1(ej, ei) and r l (e k , ej). In the above equation we have:</p><formula xml:id="formula_10">ei = r1ej ∧ ej = r1e k ⇒ ei = r1r1eje k ∧ ri = U ⇒ ei = eje k ⇒ ei + e k = r l</formula><p>Model Definition: To incorporate different views to the relations between entities, we define these settings for the model:</p><p>1. Using limit-based loss instead of margin ranking loss. 2. Each aggregated term in the score represents a different view of entities and relations with an independent set of embedding vectors. <ref type="bibr" target="#b2">3</ref>. In contrast to ensemble approaches that incorporate models by training independently and testing them together, MDE is based on multi-objective optimization <ref type="bibr" target="#b18">[19]</ref> that jointly minimizes the objective functions.</p><p>However, when aggregating different terms in the score function, the summation of opposite vectors can cause the norm of these vectors to diminish during the optimization. For example if S1 and S3 are added together, the minimization would lead to relation(r) vectors with zero norm value. To address this issue, we represent the same entities with independent variables in different distance functions.</p><p>Based on CP, MDE considers four vectors ei, ej, e k , e l , ∈ R d as the embedding vector of each entity e , and four vectors ri, rj, r k , r l ∈ R d for each relation r.</p><p>The score function of MDE for a triple (h, r, t) is defined as weighted sum of listed score functions:</p><formula xml:id="formula_11">fMDE = w1S i 1 + w2S j 2 + w3S k 3 + w4S l 4 − ψ<label>(5)</label></formula><p>where ψ, w1, w2, w3, w4 ∈ R are constant values. <ref type="figure" target="#fig_0">Figure 1</ref> displays the geometric illustration of the four translation terms considered in MDE. In the following, we show using ψ and limit-based loss, the combination of the terms in Equation <ref type="formula" target="#formula_11">(5)</ref> is efficient, such that if one of the terms recognises if a sample is true FMDE would also recognize it. Limit-based Loss: Because margin ranking loss minimizes the sum of error from directly comparing the score of negative to positive samples, when applying it to translation embeddings, it is possible that the score of a correct triplet is not small enough to hold the relation of the score function <ref type="bibr" target="#b37">[38]</ref>. To enforce the scores of positive triples become lower than those of negative ones, <ref type="bibr" target="#b37">[38]</ref> defines limited-based loss which minimizes the objective function such that the score for all the positive samples become less than a fixed limit. <ref type="bibr" target="#b26">[27]</ref> extends the limit-based loss so that the score of the negative samples become greater than a fixed limit. We train our model with the same loss function which is:</p><formula xml:id="formula_12">loss = β1 τ ∈T + [f (τ ) − γ1]+ + β2 τ ∈T − [γ2 − f (τ )]+<label>(6)</label></formula><p>where [.]+ = max(., 0), γ1, γ2 ∈ R + . T + , T − are the sets of positive and negative samples and β1, β2 &gt; 0 are constants denoting the importance of the positive and negative samples. This version of limit-based loss minimizes the aggregated error such that the score for the positive samples becomes less than γ1 and the score for negative samples becomes greater than γ2. To find the optimal limits for the limit-based loss, we suggest updating the limits during the training. 5 Model Extensions 5.1 Searching for the limits in the limit-based Loss</p><p>While the limit-based loss resolves the issue of margin ranking loss with distance based embeddings, it does not provide a way to find the optimal limits. Therefore the mechanism to find limits for each dataset and hyper-parameter is the try and error. To address this issue, we suggest updating the limits in the limit-based loss function during the training iterations. We denote the moving-limit loss by loss guide .</p><formula xml:id="formula_13">loss guide = lim δ,δ →γ 1 β1 τ ∈T + [f (τ ) − (γ1 − δ)]+ + β2 τ ∈T − [(γ2 − δ ) − f (τ )]+<label>(7)</label></formula><p>where the initial value of δ, δ is 0. In this formulation, we increase the δ, δ toward γ1 and γ2 during the training iterations such that the error for positive samples minimizes as much as possible. We test on the validation set after each 50 epoch and take those limits that give the best value during the tests. The details of the search for limits is explained in the algorithm below. After observing the most promising values for limits in the preset number of iterations, we stop the search and perform the training while having the δ values fixed(fixed limit-base loss) to allow the adaptive learning to reach loss values smaller than the threshold. We based this approach on the idea of adaptive learning rate <ref type="bibr" target="#b36">[37]</ref>, where the Adadelta optimizer adapts the learning rate after each iteration, therefore in the loss guided we can update the limits without stopping the training iterations. In our experiments, the variables in the algorithm, are as follows: δ0 = 0, threshold = 0.05, ξ = 0.1. 1: Initialize: δ = δ = δ0, γ1 = γ2 ∈ R + , ψ ∈ R 2: Initialize: i = 0, ξ ∈ R + , threshold ∈ R + 3: Inside training iterations: <ref type="bibr">4:</ref> if Using loss guided instead of loss limit−based then 5: loss = the result from Equation <ref type="formula" target="#formula_12">(6)</ref> Lemma 3. There exist ψ and γ1, γ2 ≥ 0 (γ1 ≥ γ2), such that only if one of the terms in fMDE estimates a fact as true, fMDE also predicts it as a true fact. Consequently, the same also holds for the capability of MDE to allow learning of different relation patterns.</p><formula xml:id="formula_14">loss + = β1 τ ∈T + [f (τ ) − (γ1 − δ)]+ 6: loss − = β2 τ ∈T − [(γ2 − δ ) − f (τ )]+</formula><p>Proof. We show there are boundaries for γ1, γ2, w1, w2, w3, w4, such that learning a fact by one of the terms in fMDE is enough to classify a fact correctly.</p><p>The case to prove is when three of the distance functions classify a fact negative N and the one distance function e.g. S2 classify it as positive P , and the case that S1 and S3 classify a fact as positive and S2 classify it as negative. We set w1 = w3 = 1/4 and w2 = 1/2 and assume that Sum is the value estimated by the score function of MDE, we have:</p><formula xml:id="formula_15">a &gt; N 2 ≥ γ2 2 ∧ γ1 2 &gt; P 2 ≥ 0 ⇒ a + γ1 2 &gt; Sum + ψ ≥ γ2 2<label>(8)</label></formula><p>There exist a = 2 and γ1 = γ2 = 2 and ψ = 1 that satisfy γ1 &gt; Sum ≥ 0 and the inequality 8.</p><p>It is notable that without the introduction of ψ and the limits γ1, γ2 from the limit-based loss, Lemma 3 does not hold and framing the model with this settings makes the efficient combination of the terms in fMDE possible. In case that future studies discover new interesting distances, this Lemma shows how to basically integrate them into MDE.</p><p>In contrast to SimplE that ties the relation vectors of two terms in the score together, MDE does not directly relate them to take advantage of the independent relation and entity vectors in combining opposite terms.</p><p>The learning of the symmetric relations is previously studied (e.g. in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28]</ref>) and <ref type="bibr" target="#b16">[17]</ref> studied the training over the inverse of relations, however providing a way to gather all these benefits in one model is a novelty of MDE. Besides, complementary modeling of different vector-based views of a knowledge graph is a novel contribution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MDE N N : MDE as a Neural Network</head><p>The score of MDE is already aggregating a multiplication of vectors to weights. We take advantage of this setting to model MDE as a layer of a neural network that allows learning the embedding vectors and multiplied weights jointly during the optimization. To create such a neural network we multiply ψ by a weight w5 and we feed the MDE score to an activation function. We call this extension of MDE as MDENN :</p><formula xml:id="formula_16">fMDE N N = F ( w1S i 1 p + w2S j 2 p + w3S k 3 p + w4S l 4 p + w5 p c − ψ)<label>(9)</label></formula><p>where F is T anhshrink activation function with the formulation</p><formula xml:id="formula_17">T anhshrink(x) = x − T anh(x)<label>(10)</label></formula><p>and w1, w2, . . . , w5 are elements of the latent vector w that are estimated during the training of the model and c and ψ are constants. Similarly we add y and z as latent vectors multiplied to the first and the second elements in the Equations 1, 2, 3 &amp; 4. For example S1 in MDENN becomes:</p><formula xml:id="formula_18">S1 = y1h + z1r − t p<label>(11)</label></formula><p>This framing of MDE reduces the number of hyper parameters. In addition, the major advantage of MDENN -in comparison to the linear combination of terms in MDE-is that the T anhshrink activation function allows the non-linear mappings between the embedding vectors and the expected target values for the loss function over positive and the negative samples.</p><p>Since T anhshrink has a range of R it allows setting large values for γ1 and γ2. For example for WN18RR we set their value to 1.9. It is notable that the classic activation functions such as sigmoid and T anh are not suitable to be used as activation functions here because they cannot converge the loss function to limit values larger than one.</p><p>To generate a non-linear loss function for MDENN , we combine the square of positive loss and the negative loss values: </p><formula xml:id="formula_19">lossMDE N N = ( τ ∈T + [f (τ ) − γ1]+) 2 + ( τ ∈T − [γ2 − f (τ )]+) 2 (12)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Datasets: We experimented on four standard datasets: WN18 and FB15k which were extracted by Bordes et al. in <ref type="bibr" target="#b2">[3]</ref> from Wordnet <ref type="bibr" target="#b19">[20]</ref> and Freebase <ref type="bibr" target="#b0">[1]</ref> respectively. We used the same train/valid/test sets as in <ref type="bibr" target="#b2">[3]</ref>. WN18 contains 40 943 entities, 18 relations and 141 442 train triples. FB15k contains 14 951 entities, 1 345 relations and 483 142 train triples. In order to test the expressiveness ability rather than relational pattern learning power of models, FB15k-237 <ref type="bibr" target="#b28">[29]</ref> and WN18RR <ref type="bibr" target="#b7">[8]</ref> exclude the triples with inverse relations from FB15k and WN18 which reduced the size of their training data to 56% and 61% respectively. <ref type="table" target="#tab_1">Table 1</ref>    <ref type="bibr" target="#b30">[31]</ref> and the results of TransR and NTN from <ref type="bibr" target="#b21">[22]</ref>, and ER-MLP from <ref type="bibr" target="#b23">[24]</ref>. The results on the inverse relation excluded datasets are from the Table13 of <ref type="bibr" target="#b27">[28]</ref> for both TransE and RotatE. And the rest are from <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b6">7</ref> .</p><p>Evaluation Settings: We evaluate the link prediction performance by ranking the score of each test triple against its versions with replaced head, and once for tail. Then we compute the hit at N (Hit@N), mean rank (MR) and mean reciprocal rank (MRR) of these rankings. We report the evaluations in the filtered setting.</p><p>Implementation: We implemented MDE in PyTorch 8 . Following <ref type="bibr" target="#b3">[4]</ref>, we generated one negative example per positive example for all the datasets. We used Adadelta <ref type="bibr" target="#b36">[37]</ref> as the optimizer and finetuned the hyperparameters on the validation dataset. The ranges of the hyperparameters are set as follows: embedding dimension 25, 50, 100, 200, batch size in range of 1024 to 1725 and iterations 50, 100, 1000, 1500, 2500, 3600. We set the initial learning rate on all datasets to 10. For MDE, the best embedding size and γ1 and γ2 and β1 and β2 values on WN18 were 50 and 1.9, 1.9, 2 and 1 respectively and for FB15k were 200, 10, 13, 1, 1. The best found embedding size and γ1 and γ2 and β1 and β2 values on FB15k-237 were 100, 9, 9, 1 and 1 respectively and for WN18RR were 50, 2, 2, 5 and 1.</p><p>We selected the coefficient of terms in (5), by grid search, with the condition that they make a convex combination, in the range 0.1 to 1.0 and testing those combinations of the coefficients where they create a convex combination. Found values are w1 = 0.16, w2 = 0.33, w3 = 0.16, w4=0.33. We also tested for the best value for ψ between {0.1, 0.2,. . . , 1.5}. We use ψ = 1.2 for the MDE experiments. We use the value 2 for p in p-norm through the paper. To regulate the loss function and to avoid over-fitting, we estimate the score function for two sets of independent vectors and we take their average in the prediction. Another advantage of this operation is the reduction of required training iterations.</p><p>For WN18RR experiment of MDENN , we use the same parameters as in MDE for γ1, γ2 and the embedding size.We use adaptive learning rate method for both MDE and MDENN in our experiments. The current framework of KG embedding model evaluations is based on the open-world assumption where the generation of an unlimited number of negative samples is possible. In this setting, it becomes debatable to consider negative sample generation as a part of the model since it significantly influences the ranking results. In particular, RotatE efficiently assimilates the effect of many negative samples in self-adversarial negative sampling technique. We verify the influence of this sampling method on the MDE results and to distinguish it we call this implementation MDE adv . For this implementation, we use Adam as the optimizer similar to RotatE. We select dimension 400, learning rate 0.0005, batch size 512 and 624 negative samples per positive sample for the test on WN18RR. For FB15k-237, we test the model with dimension 1000, learning rate 0.0005, the batch size 240 and 1224 negative samples per positive sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Relation Pattern Implicit Inference</head><p>To verify the implicit learning of relation patterns, we evaluate our model on Countries dataset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>. This dataset is curated in order to explicitly assess the ability of the link prediction models for composition pattern modeling and implicit inference. It is made from 2 relations and 272 entities, where the entities include 244 countries, 5 regions and 23 subregions. In comparison to general link prediction tasks on knowledge graphs, evaluation queries in Countries are specified only to the form locatedIn(c, ?), where, the answer is one of the five regions. The Countries dataset is made of 3 tasks, and each one requires inferring a composition pattern with increasing length and difficulty. The measure for this evaluation is usually AUC-PR. <ref type="table">Table 2</ref>, shows that our model performs significantly better than the previous models. While RotatE outperforms older models on S1 and S2, MDE gains the best result on S1 and S2 as well as S3, which is the most difficult task. We also evaluate if MDE embeddings implicitly represent different relation patters.</p><p>Symmetry pattern requires S3 term to correctly distinguish positive and negative samples for MDE. We investigate the relation em-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Countries(AUC-PR)</head><p>Model S1 S2 S3 DistMult <ref type="bibr" target="#b35">[36]</ref> 1.00 ± 0.00 0.72 ± 0.12 0.52 ± 0.07 ComplEx <ref type="bibr" target="#b30">[31]</ref> 0.97 ± 0.02 0.57 ± 0.10 0.43 ± 0.07 ConvE <ref type="bibr" target="#b7">[8]</ref> 1.00 ± 0.00 0.99 ± 0.01 0.86 ± 0.05 RotatE <ref type="bibr" target="#b27">[28]</ref> 1.00 ± 0.00 1.00 ± 0.00 0.95 ± 0.00 MDE 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 <ref type="table">Table 2</ref>: Results on the Countries datasets. Results of RotatE are taken from <ref type="bibr" target="#b27">[28]</ref> and the results of the other models are from <ref type="bibr" target="#b7">[8]</ref>.</p><p>. beddings from a 50-dimensional MDE trained on WN18. <ref type="figure" target="#fig_4">Figure 3a</ref> gives the value of different terms for a triple with symmetric relation "similar to" between the entities "pointed" and "sharpened". Since the smaller score values of MDE are suggesting that a triple is a positive sample, the smaller values of individual terms in the model would also influence the overall model to recognize a triple as positive. S3 shows the smallest value between all the terms. <ref type="figure" target="#fig_4">Figure 3b</ref> illustrates the values of terms for the negative sample (pointed, similar to, pointed) where S1 and S2 scores are low due to their incapability in recognizing a negative sample when the head and tail are the same. However, S3 adjusts the overall MDE score by producing a great number that compensates the low S1 and S2 results.</p><p>Inversion pattern requires inverse relations in S1 and S2 terms to have inverse angles. <ref type="figure" target="#fig_4">Figure 3c</ref> shows the histogram of the elements of the sum of hypernym and hyponym relations in S1. We can see from this Figure that most of the elements in this two relations have opposite values.</p><p>Composition pattern requires the embedding vectors of the composed relation to be the addition of the other two relations in S1. We train a 200-dimensional MDE model to verify the implicit inference of the composition patterns on FB15k-237. <ref type="figure" target="#fig_4">Figure 3d</ref> to 3g illustrate that most of the elements in r1 + r2 -r3 are near zero where r3 is composed of r1 and r2 relations.     <ref type="table" target="#tab_6">Table 4</ref> shows the results of the experiments on FB15k-237 and WN18RR, these results follow the same pattern as the ones reported in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Link Prediction Results</head><p>Due to the existence of hard limits in the limit-based loss, the mean rank in MDE is lower than most of the other methods. It is noticeable that the addition of independent vectors in the model does not decrease the mean rank of the model, whereas in models with high vector dimensions, the MR and MRR results are unbalanced. For example, for ComplEx and ConvE which both use a vector dimension of 200, the MRR is significant but the MR is high (which is not suitable). On a different note, RotatE mitigates this issue with the application of a high number of negative samples per positive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR</head><p>FB15k  The comparison of our model to other state-of-the-art methods in <ref type="table" target="#tab_6">Table 4</ref>, shows the competitive performance of MDE and MDE adv . It is observable that in the MDE tests with only one negative sample per positive sample and using vector sizes between 50 to 200, MDE challenges models with relatively large embedding dimensions (1000) and high number of negative samples (up to 1024). In the ablation study presented in <ref type="bibr" target="#b27">[28]</ref>, we notice that RotatE (with the margin-based ranking criterion, and without self-adversarial negative sampling) produces a Hit@10 score of 0.476 on FB15k-237, which is lower than MDE score.</p><p>The adaptation of self-adversarial negative sampling in MDE improves the Hit@10 ranking and the MRR score of the model. This improvement is more significant on the FB15k-237 rather than on the WN18RR, as there is a greater number of relations and entities in FB15k-237 and the self-adversarial negative sampling increases the coverage of different combinations of entities in the training. We also observe on the FB15-237 benchmark, that MDE adv outperforms previous models on the MRR score since it exists more relations with composition pattern in this dataset than in the WN18RR dataset.</p><p>We include each of the terms in MDE as we hypothesize that each one contributes to the generalization power of the model. Practically, we verify this approach in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Study</head><p>To better understand the role of each term in the score function of MDE, we embark two ablation experiments. First, we train MDE using one of the terms alone, and observe the link prediction performance of each term in the filtered setting. In the second experiment, we remove one of the terms at a time and test the effect of the removal of that term on the model after 100 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18RR</head><p>FB15k   <ref type="table" target="#tab_8">Table 5</ref> summarizes the results of the first experiment on WN18RR and FB15k-237. We can see that S4 outperforms the other terms while S1 and S3 perform very similar on these two datasets. Between the four terms, S2 performs the worst since most of the relations in the test datasets follow an antisymmetric pattern and S2 is not efficient in modeling them.   <ref type="table" target="#tab_10">Table 6</ref> shows the results of the second experiment. The evaluations on WN18RR and WN18 show that the removal of S4 has the most negative effect on the performance of MDE. The removal of S1 that was one of the good performing terms in the last experiment has the least effect. Nevertheless, S1 improves the MRR in the MDE. Also, when we remove S2, the MRR and Hit@10 are negatively influenced, indicating that it exists cases that S2 performs better than the other terms, although, in the individual tests, it performed the worst between all the terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this study, we created a model based on the generation of several independent vectors for each entity and relation that overrides the expressiveness restrictions of most of the embedding models. To our knowledge beside MDE and RotatE, other existing KG embedding approaches are unable to allow modeling of all the three relation patterns. We framed MDE into a Neural Network structure and validated our contributions via both theoretical proofs and empirical results.</p><p>We demonstrated that with multiple views to translation embeddings and by using independent vectors (it was previously supposed to cause poor performance <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16]</ref>), a model can perform solidly in the link prediction task. Our experimental results confirm the competitive performances of MDE in MR and Hit@10 on the benchmark datasets. Particularly, MDE outperforms all the current state-of-theart models for the benchmark of composition relation patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Geometric illustration of the translation terms considered in MDE. Time Complexity and Parameter Growth: Considering the ever growth of KGs and the expansion of the web, it is crucial that the time and memory complexity of a relational mode be minimal. Despite the limitations in expressivity, TransE is one of the popular models on large datasets due to its scalability. With O(d) time complexity (of one mini-batch), where d is the size of embedding vectors, it is more efficient than RESCAL, NTN, and the neural network models. Similar to TransE, the time complexity of MDE is O(d). Due to the additive construction of MDE, the inclusion of more distance terms keeps the time complexity linear in the size of vector embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7 :</head><label>7</label><figDesc>loss = loss + + loss − 8: if loss + = 0 &amp; γ1 ≥ ξ then 9: δ = δ + ξ 10: if loss − &gt; threshold &amp; γ2 ≥ ξ then 11: δ = δ + ξ 12: if Using loss limit−based then 13:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the possible positioning of score values for MDENN on WN18RR where the value of γ1 and γ2 is 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>shows the positioning of the score values for MDENN on WN18RR in which γ1 and γ2 is 2. The horizontal axis indicates the sample numbers and the vertical axis indicates their loss values. The score values for negative samples, f (τ ) lay on the green area and score values for the positive samples, f (τ ) lay on the red area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Prediction of each term in MDE score for a symmetric relation in a positive triple in Figure (a) and its corrupted version with the same head and tail in Figure (b). Lower values indicate that a triple is recognized as positive. Figure (c) shows the histogram diagram of the elements of two the sum of two inverse relations, hypernym and hyponym in S1. Figures (d, e, f &amp; g) show the norm of the elements in vectors r1, r2, r3 and r1+r2-r3 where r3 is composed of r1 and r2, where r1 represents /award/award category/nominees./award/award nominatio/nominated for and r2 represents /award/award nominee/award nominations./award /award nomination/nominated for and r3 represents /award/award winner/awards won./award/award honor/award winner .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>WN18</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>summarizes the statistics of these knowledge graphs.</figDesc><table><row><cell>Dataset</cell><cell>#entity</cell><cell>#relation</cell><cell>#training</cell><cell>#validation</cell><cell>#test</cell></row><row><cell>FB15k</cell><cell>14 951</cell><cell>1 345</cell><cell>483 142</cell><cell>50 000</cell><cell>59 071</cell></row><row><cell>WN18</cell><cell>40 943</cell><cell>18</cell><cell>141 442</cell><cell>5 000</cell><cell>5 000</cell></row><row><cell>FB15k-237</cell><cell>14 541</cell><cell>237</cell><cell>272 115</cell><cell>17 535</cell><cell>20 466</cell></row><row><cell>WN18RR</cell><cell>40 943</cell><cell>11</cell><cell>86 835</cell><cell>3 034</cell><cell>3 134</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of entities, relations, and triples in each division. We compare MDE with several state-of-the-art relational learning approaches. Our baselines include TransE, RESCAL, DistMult, NTN, ER-MLP, ComplEx and SimplE. We report the results of TransE, DistMult, and ComplEx from</figDesc><table /><note>Baselines:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on WN18 and FB15k. Best results are in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>summarizes our results on FB15k and WN18. It shows that MDE performs almost like RotatE and outperforms other state-ofthe-art models in MR and Hit@10 tests.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results on WN18RR and FB15k-237. Best ones are in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results of each individual term in MDE on WN18RR and FB15k-237. Best results are in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Results of MDE after 100 iterations when removing one of the terms. Best results are in bold.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Smart Data Analytics Group, University of Bonn, Germany; Fraunhofer IAIS, Germany, email: sadeghi@cs.uni-bonn.de 2 ADAPT Centre, Trinity College Dublin, Ireland, email: grauxd@tcd.ie 3 Smart Data Analytics Group, University of Bonn, Germany, email: shariat@cs.uni-bonn.de 4 Smart Data Analytics Group, University of Bonn, Germany; Fraunhofer IAIS, Germany, email: jens.lehmann@iais.fraunhofer.de</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We used the term "it allows" to imply that the encoding of such patterns do not inhibit the learning of relations having a particular pattern. Meanwhile in the literature SimplE uses "it can encode" and RotatE uses "the model infers".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Scores of ConvE on FB15k is from https://github.com/ TimDettmers/ConvE/issues/26 8 https://pytorch.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This study is partially supported by the MLwin project (Maschinelles Lernen mit Wissensgraphen, grant 01IS18050F of the Federal Ministry of Education and Research of Germany) <ref type="bibr" target="#b8">9</ref> , by Fraunhofer IAIS, and by the ADAPT Centre for Digital Content Technology funded under the SFI Research Centres Programme (Grant 13/RC/2106) and co-funded under the European Regional Development Fund. The authors would also like to thank Michael Galkin for running a part of MDE adv experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD international conference on Management of data</title>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On approximate reasoning capabilities of low-rank vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stranse: a novel embedding model of entities and relationships in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirts</forename><surname>Quoc Nguyen Dat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kairit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename><surname>Qu Lizhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="460" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Logical and relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raedt</forename><surname>Luc De</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the equivalence of holographic and complex embeddings for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05563</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent space approaches to social network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">E</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Raftery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Handcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american Statistical association</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">460</biblScope>
			<biblScope unit="page" from="1090" to="1098" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Encoding temporal information for time-aware link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingsong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2350" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4284" to="4295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00379</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Survey of multi-objective optimization methods for engineering&apos;, Structural and multidisciplinary optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Marler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasbir S</forename><surname>Arora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="369" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilingual knowledge graph embeddings for cross-lingual knowledge alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Muhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Yingtao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaniolo</forename><surname>Carlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1511" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An overview of embedding models of entities and relationships for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth Aaai conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A threeway model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bootstrapping entity alignment with knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="4396" to="4402" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge graph completion via complex tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Christopher R Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4735" to="4772" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge base completion via search-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on World wide web</title>
		<meeting>the 23rd international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="515" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning knowledge embeddings by combining limit-based scoring loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1009" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
