<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">R-C3D: Region Convolutional 3D Network for Temporal Activity Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University Boston</orgName>
								<address>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
							<email>dasabir@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University Boston</orgName>
								<address>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University Boston</orgName>
								<address>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">R-C3D: Region Convolutional 3D Network for Temporal Activity Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities, and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. The entire model is trained end-to-end with jointly optimized localization and classification losses. R-C3D is faster than existing methods (569 frames per second on a single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14. We further demonstrate that our model is a general activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on ActivityNet and Charades. Our code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Activity detection in continuous videos is a challenging problem that requires not only recognizing, but also precisely localizing activities in time. Existing state-of-the-art approaches address this task as detection by classification, i.e. classifying temporal segments generated in the form of sliding windows <ref type="bibr" target="#b17">[13,</ref><ref type="bibr" target="#b25">20,</ref><ref type="bibr" target="#b29">24,</ref><ref type="bibr" target="#b42">37]</ref> or via an external "proposal" generation mechanism <ref type="bibr" target="#b14">[10,</ref><ref type="bibr" target="#b40">35]</ref>. These approaches suffer from one or more of the following major drawbacks: they do not learn deep representations in an end-to-end fashion, but rather use hand-crafted features <ref type="bibr" target="#b38">[33,</ref><ref type="bibr" target="#b39">34]</ref>, or deep features like VGG <ref type="bibr" target="#b33">[28]</ref>, ResNet <ref type="bibr" target="#b12">[8]</ref>, C3D <ref type="bibr" target="#b37">[32]</ref> etc., learned separately on image/video classification tasks. Such offthe-shelf representations may not be optimal for localizing activities in diverse video domains, resulting in inferior performance. Furthermore, current methods' dependence <ref type="bibr">Figure 1</ref>. We propose a fast end-to-end Region Convolutional 3D Network (R-C3D) for activity detection in continuous video streams. The network encodes the frames with fully-convolutional 3D filters, proposes activity segments, then classifies and refines them based on pooled features within their boundaries. Our model improves both speed and accuracy compared to existing methods. on external proposal generation or exhaustive sliding windows leads to poor computational efficiency. Finally, the sliding-window models cannot easily predict flexible activity boundaries.</p><p>In this paper, we propose an activity detection model that addresses all of the above issues. Our Region Convolutional 3D Network (R-C3D) is end-to-end trainable and learns task-dependent convolutional features by jointly optimizing proposal generation and activity classification. Inspired by the Faster R-CNN <ref type="bibr" target="#b26">[21]</ref> object detection approach, we compute fully-convolutional 3D ConvNet features and propose temporal regions likely to contain activities, then pool features within these 3D regions to predict activity classes ( <ref type="figure">Figure 1</ref>). The proposal generation stage filters out many background segments and results in superior computational efficiency compared to sliding window models. Furthermore, proposals are predicted with respect to predefined anchor segments and can be of arbitrary length, allowing detection of flexible activity boundaries.</p><p>Convolutional Neural Network (CNN) features learned end-to-end have been successfully used for activity recognition <ref type="bibr" target="#b18">[14,</ref><ref type="bibr" target="#b32">27]</ref>, particularly in 3D ConvNets (C3D <ref type="bibr" target="#b37">[32]</ref>), which learn to capture spatio-temporal features. However, unlike the traditional usage of 3D ConvNets <ref type="bibr" target="#b37">[32]</ref> where the input is short 16-frame video chunks, our method applies full convolution along the temporal dimension to encode as many frames as the GPU memory allows. Thus, rich spatio-temporal features are automatically learned from longer videos. These feature maps are shared between the activity proposal and classification subnets to save computation time and jointly optimize features for both tasks.</p><p>Alternative activity detection approaches <ref type="bibr" target="#b8">[4,</ref><ref type="bibr" target="#b22">17,</ref><ref type="bibr" target="#b23">18,</ref><ref type="bibr" target="#b34">29,</ref><ref type="bibr" target="#b44">39]</ref> use a recurrent neural network (RNN) to encode a sequence of frame or video chunk features (e.g. VGG <ref type="bibr" target="#b33">[28]</ref>, C3D <ref type="bibr" target="#b37">[32]</ref>) and predict the activity label at each time step. However, these RNN methods can only model temporal features at a fixed granularity (e.g. per-frame CNN features or 16-frame C3D features). In order to use the same classification network to classify variable length proposals into specific activities, we extend 2D region of interest (RoI) pooling to 3D which extracts a fixed-length feature representation for these proposals. Thus, our model can utilize video features at any temporal granularity. Furthermore, some RNN-based detectors rely on direct regression to predict the temporal boundaries for each activity. As shown in object detection <ref type="bibr" target="#b11">[7,</ref><ref type="bibr" target="#b36">31]</ref> and semantic segmentation <ref type="bibr" target="#b6">[2]</ref>, object boundaries obtained using a regression-only framework are inferior compared to "proposal based detection".</p><p>We perform extensive comparisons of R-C3D to stateof-the-art activity detection methods using three publicly available benchmark datasets -THUMOS'14 <ref type="bibr" target="#b16">[12]</ref>, Activ-ityNet <ref type="bibr" target="#b13">[9]</ref> and Charades <ref type="bibr" target="#b31">[26]</ref>. We achieve new state-of-theart results on THUMOS'14 and Charades, and improved results on ActivityNet when using only C3D features.</p><p>To summarize, the main contributions of our paper are:</p><p>• an end-to-end activity detection model with combined activity proposal and classification stages that can detect arbitrary length activities; • fast detection speeds (5x faster than current methods) achieved by sharing fully-convolutional C3D features between the proposal generation and classification parts of the network; • extensive evaluations on three diverse activity detection datasets that demonstrate the general applicability of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Activity Detection There is a long history of activity recognition, or classifying trimmed video clips into fixed set of categories <ref type="bibr" target="#b15">[11,</ref><ref type="bibr" target="#b19">15,</ref><ref type="bibr" target="#b24">19,</ref><ref type="bibr" target="#b32">27,</ref><ref type="bibr" target="#b38">33,</ref><ref type="bibr" target="#b47">42]</ref>. Activity detection also needs to predict the start and end times of the activities within untrimmed and long videos. Existing activity detection approaches are dominated by models that use sliding windows to generate segments and subsequently classify them with activity classifiers trained on multiple fea-tures <ref type="bibr" target="#b17">[13,</ref><ref type="bibr" target="#b25">20,</ref><ref type="bibr" target="#b29">24,</ref><ref type="bibr" target="#b42">37]</ref>. Most of these methods have stagewise pipelines which are not trained end-to-end. Moreover, the use of exhaustive sliding windows is computationally inefficient and constrains the boundary of the detected activities to some extent.</p><p>Recently, some approaches have bypassed the need for exhaustive sliding window search to detect activities with arbitrary lengths. <ref type="bibr" target="#b8">[4,</ref><ref type="bibr" target="#b22">17,</ref><ref type="bibr" target="#b23">18,</ref><ref type="bibr" target="#b34">29,</ref><ref type="bibr" target="#b44">39]</ref> achieve this by modeling the temporal evolution of activities using RNNs or LSTMs networks and predicting an activity label at each time step. The deep action proposal model <ref type="bibr" target="#b8">[4]</ref> uses LSTM to encode C3D features of every 16-frame video chunk, and directly regresses and classifies activity segments without the extra proposal generation stage. Compared to this work, we avoid recurrent layers, encoding a large video buffer with a fully-convolutional 3D ConvNet, and use 3D RoI pooling to allow feature extraction at arbitrary proposal granularity, achieving significantly higher accuracy and speed. The method in <ref type="bibr" target="#b46">[41]</ref> tries to capture motion features at multiple resolutions by proposing a Pyramid of Score Distribution Features. However their model is not end-to-end trainable and relies on handcrafted features.</p><p>Aside from supervised activity detection, a recent work <ref type="bibr" target="#b41">[36]</ref> has addressed weakly supervised activity localization from data labeled only with video level class labels by learning attention weights on shot based or uniformly sampled proposals. The framework proposed in <ref type="bibr" target="#b27">[22]</ref> explores the uses of a language model and an activity length model for detection. Spatio-temporal activity localization <ref type="bibr" target="#b43">[38,</ref><ref type="bibr" target="#b45">40]</ref> have also been explored to some extent. We only focus on supervised temporal activity localization.</p><p>Object Detection Activity detection in untrimmed videos is closely related to object detection in images. The inspiration for our work, Faster R-CNN <ref type="bibr" target="#b26">[21]</ref>, extends R-CNN <ref type="bibr" target="#b11">[7]</ref> and Fast R-CNN <ref type="bibr" target="#b10">[6]</ref> object detection approaches, incorporating RoI pooling and a region proposal network. Compared to recent object detection models e.g., SSD <ref type="bibr" target="#b20">[16]</ref> and R-FCN <ref type="bibr" target="#b7">[3]</ref>, Faster R-CNN is a general and robust object detection framework that has been deployed on different datasets with little data augmentation effort. Like Faster R-CNN, our R-C3D model is also designed with the goal of easy deployment on varied activity detection datasets. It avoids making certain assumptions based on unique characteristics of a dataset, such as the UPC model for Activi-tyNet <ref type="bibr" target="#b23">[18]</ref> which assumes that each video contains a single activity class. We show the effectiveness of our model on three different types of activity detection datasets, the most extensive evaluation to our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We propose a Region Convolutional 3D Network (R-C3D), a novel convolutional neural network for activity detection in continuous video streams. The network, illus- <ref type="figure">Figure 2</ref>. R-C3D model architecture. The 3D ConvNet takes raw video frames as input and computes convolutional features. These are input to the Proposal Subnet that proposes candidate activities of variable length along with confidence scores. The Classification Subnet filters the proposals, pools fixed size features and then predicts activity labels along with refined segment boundaries. trated in <ref type="figure">Figure 2</ref>, consists of three components: a shared 3D ConvNet feature extractor <ref type="bibr" target="#b37">[32]</ref>, a temporal proposal stage, and an activity classification and refinement stage. To enable efficient computation and end-to-end training, the proposal and classification sub-networks share the same C3D feature maps. The proposal subnet predicts variable length temporal segments that potentially contain activities, while the classification subnet classifies these proposals into specific activity categories or background, and further refines the proposal segment boundaries. A key innovation is to extend the 2D RoI pooling in Faster R-CNN to 3D RoI pooling which allows our model to extract features at various resolutions for variable length proposals. Next, we describe the shared video feature hierarchies in Sec. 3.1, the temporal proposal subnet in Sec. 3.2 and the classification subnet in Sec. 3.3. Sections 3.4 and 3.5 detail the optimization strategy during training and testing respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Convolutional Feature Hierarchies</head><p>We use a 3D ConvNet to extract rich spatio-temporal feature hierarchies from a given input video buffer. It has been shown that both spatial and temporal features are important for representing videos, and a 3D ConvNet encodes rich spatial and temporal features in a hierarchical manner. The input to our model is a sequence of RGB video frames with dimension R 3×L×H×W . The architecture of the 3D ConvNet is taken from the C3D architecture proposed in <ref type="bibr" target="#b37">[32]</ref>. However, unlike <ref type="bibr" target="#b37">[32]</ref>, the input to our model is of variable length. We adopt the convolutional layers (conv1a to conv5b) of C3D, so a feature</p><formula xml:id="formula_0">map C conv5b ∈ R 512× L 8 × H 16 × W 16</formula><p>(512 is the channel dimension of the layer conv5b) is produced as the output of this subnetwork. We use C conv5b activations as the shared input to the proposal and classification subnets. The height (H) and width (W ) of the frames are taken as 112 each following <ref type="bibr" target="#b37">[32]</ref>. The number of frames L can be arbitrary and is only limited by memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Proposal Subnet</head><p>To allow the model to predict variable length proposals, we incorporate anchor segments into the temporal proposal sub-network. The subnet predicts potential proposal segments with respect to anchor segments and a binary label indicating whether the predicted proposal contains an activity or not. The anchor segments are pre-defined multiscale windows centered at L/8 uniformly distributed temporal locations. Each temporal location specifies K anchor segments, each at a different fixed scale. Thus, the total number of anchor segments is (L/8) * K. The same set of K anchor segments exists in different temporal locations, which ensures that the proposal prediction is temporally invariant. The anchors serve as reference activity segments for proposals at each temporal location, where the maximum number of scales K is dataset dependent.</p><p>To obtain features at each temporal location for predicting proposals with respect to these anchor segments, we first add a 3D convolutional filter with kernel size 3×3×3 on top of C conv5b to extend the temporal receptive field for the temporal proposal subnet. Then, we downsample the spatial dimensions (from H 16 × W 16 to 1 × 1) to produce a temporal only feature map C tpn ∈ R 512× L 8 ×1×1 by applying a 3D max-pooling filter with kernel size 1× H 16 × W 16 . The 512-dimensional feature vector at each temporal location in C tpn is used to predict a relative offset {δc i , δl i } to the center location and the length of each anchor segment {c i , l i }, i ∈ {1, · · · , K}. It also predicts the binary scores for each proposal being an activity or background. The proposal offsets and scores are predicted by adding two 1×1×1 convolutional layers on top of C tpn . Training: For training, we need to assign positive/negative labels to the anchor segments. Following the standard practice in object detection <ref type="bibr" target="#b26">[21]</ref>, we choose a positive label if the anchor segment 1) overlaps with some ground-truth activity with Intersection-over-Union (IoU) higher than 0.7, or 2) has the highest IoU overlap with some ground-truth activity. If the anchor has IoU overlap lower than 0.3 with all ground-truth activities, then it is given a negative label. All others are held out from training. For proposal regression, ground truth activity segments are transformed with respect to nearby anchor segments using the coordinate transformations described in Sec. 3.4. We sample balanced batches with a positive/negative ratio of 1 : 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Activity Classification Subnet</head><p>The activity classification stage has three main functions: 1) selecting proposal segments from the previous stage, 2) three-dimensional region of interest (3D RoI) pooling to extract fixed-size features for selected proposals, and 3) activity classification and boundary regression for the selected proposals based on the pooled features. Some activity proposals generated by the proposal subnet highly overlap with each other and some have a low proposal score indicating low confidence. Following the standard practice in object detection <ref type="bibr" target="#b9">[5,</ref><ref type="bibr" target="#b26">21]</ref> and activity detection <ref type="bibr" target="#b29">[24,</ref><ref type="bibr" target="#b44">39]</ref>, we employ a greedy Non-Maximum Suppression (NMS) strategy to eliminate highly overlapping and low confidence proposals. The NMS threshold is set as 0.7.</p><p>The selected proposals can be of arbitrary length. However we need to extract fixed-size features for each of them in order to use fully connected layers for further activity classification and regression. We design a 3D RoI pooling layer to extract the fixed-size volume features for each variable-length proposal from the shared convolutional features C conv5b ∈ R 512×(L/8)×7×7 (shared with the temporal proposal subnet). Specifically, in 3D RoI pooling, an input feature volume of size, say, l×h×w is divided into l s ×h s ×w s sub-volumes each with approximate size l ls × h hs × w ws , and then max pooling is performed inside each sub-volume. In our case, suppose a proposal has the feature volume of l p × 7 × 7 in C conv5b , then this feature volume will be divided into 1×4×4 grids and max pooled inside each grid. Thus, proposals of arbitrary lengths give rise to output volume features of the same size 512×1×4×4.</p><p>The output of the 3D RoI pooling is fed to a series of two fully connected layers. Here, the proposals are classified to activity categories by a classification layer and the refined start-end times for these proposals are given by a regression layer. The classification and regression layers are also two separate fully connected layers and for both of them the input comes from the aforementioned fully connected layers (after the 3D RoI pooling layer). Training: We need to assign an activity label to each proposal for training the classifier subnet. An activity label is assigned if the proposal has the highest IoU overlap with a ground-truth activity, and at the same time, the IoU overlap is greater than 0.5. A background label (no activity) is assigned to proposals with IoU overlap lower than 0.5 with all ground-truth activities. Training batches are chosen with positive/negative ratio of 1 : 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization</head><p>We train the network by optimizing both the classification and regression tasks jointly for the two subnets. The softmax loss function is used for classification, and smooth L1 loss function <ref type="bibr" target="#b10">[6]</ref> is used for regression. Specifically, the objective function is given by:</p><formula xml:id="formula_1">Loss = 1 N cls i L cls (ai, a * i ) + λ 1 Nreg i a * i Lreg(ti, t * i ) (1)</formula><p>where N cls and N reg stand for batch size and the number of anchor/proposal segments, λ is the loss trade-off parameter and is set to a value 1. i is the anchor/proposal segments index in a batch, a i is the predicted probability of the proposal or activities, a * i is the ground truth, t i = {δĉ i , δl i } represents predicted relative offset to anchor segments or proposals. t * i = {δc i , δl i } represents the coordinate transformation of ground truth segments to anchor segments or proposals. The coordinate transformations are computed as follows:</p><formula xml:id="formula_2">δc i = (c * i − c i )/l i δl i = log(l * i /l i )<label>(2)</label></formula><p>where c i and l i are the center location and the length of anchor segments or proposals while c * i and l * i denote the same for the ground truth activity segments.</p><p>In our R-C3D model, the above loss function is applied for both the temporal proposal subnet and the activity classification subnet. In the proposal subnet, the binary classification loss L cls predicts whether the proposal contains an activity or not, and the regression loss L reg optimizes the relative displacement between proposals and ground truths. In the proposal subnet the losses are activity class agnostic. For the activity classification subnet, the multiclass classification loss L cls predicts the specific activity class for the proposal, and the number of classes are the number of activities plus one for the background. The regression loss L reg optimizes the relative displacement between activities and ground truths. All four losses for the two subnets are optimized jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Prediction</head><p>Activity prediction in R-C3D consists of two steps. First, the proposal subnet generates candidate proposals and predicts the start-end time offsets as well as proposal score for each. Then the proposals are refined via NMS with threshold value 0.7. After NMS, the selected proposals are fed to the classification network to be classified into specific activity classes, and the activity boundaries of the predicted proposals are further refined by the regression layer. The boundary prediction in both proposal subnet and classification subnet is in the form of relative displacement of center point and length of segments. In order to get the start time and end time of the predicted proposals or activities, inverse coordinate transformation to Equation 2 is performed.</p><p>R-C3D accepts variable length input videos. However, to take advantage of the vectorized implementation in fast deep learning libraries, we pad the last few frames of short videos with last frame, and break long videos into buffers (limited by memory only). NMS at a lower threshold (0.1 less than the mAP evaluation threshold) is applied to the predicted activities to get the final activity predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate R-C3D on three large-scale activity detection datasets -THUMOS'14 <ref type="bibr" target="#b16">[12]</ref>, Charades <ref type="bibr" target="#b31">[26]</ref> and Activ-ityNet <ref type="bibr" target="#b13">[9]</ref>. Sections 4.1, 4.2, 4.3 provide the experimental details and evaluation results on these three datasets. Results are shown in terms of mean Average Precision -mAP@α where α denotes different Intersection over Union (IoU) thresholds, as is the common practice in the literature. Section 4.4 provides the detection speed comparison with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on THUMOS'14</head><p>THUMOS'14 activity detection dataset contains over 24 hours of video from 20 different sport activities. The training set contains 2765 trimmed videos while the validation and the test sets contain 200 and 213 untrimmed videos respectively. This dataset is particularly challenging as it consists of very long videos (up to a few hundreds of seconds) with multiple activity instances of very small duration (up to few tens of seconds). Most videos contain multiple activity instances of the same activity class. In addition, some videos contain activity segments from different classes. Experimental Setup: We divide 200 untrimmed videos from the validation set into 180 training and 20 held out videos to get the best hyperparameter setting. All 200 videos are used as the training set and the final results are reported on 213 test videos. Since the GPU memory is limited, we first create a buffer of 768 frames at 25 frames per second (fps) which means approximately 30 seconds of video. Our choice is motivated by the fact that 99.5% of all activity segments in the validation set (used here as the training set) are less than 30 seconds long. These buffers of frames act as inputs to R-C3D . We can create the buffer by sliding from the beginning of the video to the end, denoted as the 'one-way buffer'. An additional pass from the end of the video to the beginning is used to increase the amount of training data, denoted as the 'two-way buffer'. We initialize the 3D ConvNet part of our model with C3D weights trained on Sports-1M and finetuned on UCF101 released by the authors in <ref type="bibr" target="#b37">[32]</ref>. We allow all the layers of R-C3D to be trained on THUMOS'14 with a fixed learning rate of 0.0001. The number of anchor segments K chosen for this dataset is 10 with specific scale values <ref type="bibr" target="#b6">[2,</ref><ref type="bibr" target="#b8">4,</ref><ref type="bibr" target="#b9">5,</ref><ref type="bibr" target="#b10">6,</ref><ref type="bibr" target="#b12">8,</ref><ref type="bibr" target="#b13">9,</ref><ref type="bibr" target="#b14">10,</ref><ref type="bibr" target="#b16">12,</ref><ref type="bibr" target="#b18">14,</ref><ref type="bibr" target="#b20">16]</ref>. The values are chosen according to the distribution of the activity durations in the training set. At 25 fps and temporal pooling factor of 8 (C tpn downsamples the input by 8 temporally), the anchor segments correspond to segments of duration between 0.64 and 5.12 seconds 1 . Note that, the predicted proposals or activities are relative to the anchor segments but not limited to the anchor boundaries, enabling our model to detect variable-length activities. Results: As a sanity check, we first evaluate the performance of the temporal proposal subnet. A predicted proposal is marked correct if its IoU with a ground truth activity is more than 0.7, otherwise it is considered incorrect. With this binary setting, precision and recall values of the temporal proposal subnet are 85% and 83% respectively.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we present a comparative evaluation of the activity detection performance of R-C3D with existing stateof-the-art approaches in terms of mAP at IoU thresholds 0.1-0.5 (denoted as α). For both the one-way buffer setting and the two-way buffer setting we achieve new state-of-theart for all five α values. In the one-way setting, mAP@0.5 is 27.0% which is an 3.7% absolute improvement from the state-of-the-art. The two-way buffer setting further increases the mAP values at all the IoU thresholds with mAP@0.5 reaching as far as 28.9%. Our model comprehensively outperforms the current state-of-the-art by a large margin (28.9% compared to 23.3% as reported in <ref type="bibr" target="#b28">[23]</ref>).</p><p>The Average Precision (AP) for each class in THU-MOS'14 at IoU threshold 0.5 for the two-way buffer setting is shown in <ref type="table" target="#tab_1">Table 2</ref>. R-C3D outperforms the all the methods in most classes and shows significant improvement (by more than 20% absolute AP over the next best) for activities e.g., Basketball Dunk, Cliff Diving, and Javelin Throw. For some of the activities, our method is only second to the best performing ones by a very small margin (e.g., Billiards or Cricket Shot). <ref type="figure" target="#fig_0">Figure 3(a)</ref> shows some representative qualitative results from two videos in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on ActivityNet</head><p>The ActivityNet <ref type="bibr" target="#b13">[9]</ref> dataset consists of untrimmed videos and is released in three versions. We use the latest release (1.3) which has 10024, 4926 and 5044 videos containing 200 different types of activities in the train, validation and test sets respectively. Most videos contain activity instances of a single class covering a great deal of the video. Compared to THUMOS'14, this is a large-scale dataset both in terms of the number of activities involved and the amount of video. Researchers have taken part in the ActivityNet challenge <ref type="bibr" target="#b5">[1]</ref> held on this dataset. The performances of the participating teams are evaluated on test videos for which the ground truth annotations are not public. In addition to evaluating on the validation set, we show our performance on the test set after evaluating it on the challenge server. Experimental Setup: Similar to THUMOS'14, the length of the input buffer is set to 768 but, as the videos are long, we sample frames at 3 fps to fit it in the GPU memory. This makes the duration of the buffer approximately 256 seconds covering over 99.99% training activities. The considerably long activity durations prompt us to set the number of anchor segments K to be as high as 20. Specifically, we chose the following scales - <ref type="bibr" target="#b5">[1,</ref><ref type="bibr" target="#b6">2,</ref><ref type="bibr" target="#b7">3,</ref><ref type="bibr" target="#b8">4,</ref><ref type="bibr" target="#b9">5,</ref><ref type="bibr" target="#b10">6,</ref><ref type="bibr" target="#b11">7,</ref><ref type="bibr" target="#b12">8,</ref><ref type="bibr" target="#b14">10,</ref><ref type="bibr" target="#b16">12,</ref><ref type="bibr" target="#b18">14,</ref><ref type="bibr" target="#b20">16,</ref><ref type="bibr" target="#b25">20,</ref><ref type="bibr" target="#b29">24,</ref><ref type="bibr" target="#b33">28,</ref><ref type="bibr" target="#b37">32,</ref><ref type="bibr" target="#b45">40,</ref><ref type="bibr">48,</ref><ref type="bibr">56,</ref><ref type="bibr">64]</ref>. Thus the shortest and the longest anchor segments are of durations 2.7 and 170 seconds respectively covering 95.6% training activities.</p><p>Considering the vast domain difference of the activities between Sports-1M and ActivityNet, we finetune the Sports-1M pretrained 3D ConvNet model <ref type="bibr" target="#b37">[32]</ref> with the training videos of ActivityNet. We initialize the 3D Con-vNet with these finetuned weights. AcitivityNet being a large scale dataset, the training takes more epochs. As a speed-efficiency trade-off, we freeze the first two convolutional layers in our model during training. The learning rate is kept fixed at 10 −4 for first 10 epochs and is decreased to 10 −5 for the last 5 epochs. Based on the improved results on the THUMOS'14, we choose the two-way buffer setting with horizontal flipping of frames for data augmentation. Results: In <ref type="table" target="#tab_2">Table 3</ref> we show the performance of R-C3D and compare with existing published approaches. Results are shown for two different settings. In the first setting, only the training set is used for training and the performance is shown for either the validation or test data or both.</p><p>In the second setting, training is done on both training and validation sets while the performance is shown on the test set. The table shows that the proposed method does achieve a performance better than methods not using handcrafted features e.g., UPC <ref type="bibr" target="#b23">[18]</ref>. UPC is the most fair comparison as it also uses only C3D features. However, it relies on a strong assumption that each video in ActivityNet just contains one activity class. Our approach obtains an improvement of 4.3% on the validation set and 4.5% on the test set over UPC <ref type="bibr" target="#b23">[18]</ref> in terms of mAP@0.5 without any such strong assumptions. When both training and validation sets are used for training, the performance improves further by 1.6%. The ActivityNet Challenge in 2017 introduced a new evaluation metric where mAP at 10 evenly distributed thresholds between 0.5 and 0.95 are averaged to get the average mAP. Using only training data to train R-C3D, the average mAP for the validation and test set are 12.7% and 13.1% respectively. On the other hand, if both training and validation data is used during training, the average mAP for the test set increases to 16.7% showing the benefit of our end-to-end model when more data is available for training.</p><p>R-C3D falls slightly behind <ref type="bibr" target="#b34">[29]</ref> which uses LSTM based tracking and performs activity prediction using deep features as well as optical flow features from the tracked trajectories. The approach in <ref type="bibr" target="#b35">[30]</ref> also uses handcrafted motion features like MBH on top of inception and C3D features in addition to dynamic programing based post processing. However, the heavy use of an ensemble of handengineered features and dataset dependent heuristics not only stops these methods from learning in an end-to-end fashion but makes them less general across datasets. Unlike these methods, R-C3D is trainable completely end-to-end and is easily extensible to other datasets with little parameter tuning, providing better generalization performance. Our method is also capable of using hand engineered features with a possible boost to performance, and we keep <ref type="table">Table 4</ref>. Activity detection results on Charades (in percentage). We report the results using the same evaluation metric as in <ref type="bibr" target="#b30">[25]</ref>. mAP standard post-process Random <ref type="bibr" target="#b30">[25]</ref> 4. this as a future task. <ref type="figure" target="#fig_0">Figure 3(b)</ref> shows some representative qualitative results from this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Charades</head><p>Charades <ref type="bibr" target="#b31">[26]</ref> is a recently introduced dataset for activity classification and detection. The activity detection task involves daily life activities from 157 classes. The dataset consists of 7985 train and 1863 test videos. The videos are recorded by Amazon Mechanical Turk users based on provided scripts. Apart from low illumination, diversity and casual nature of the videos containing day-to-day activities, an additional challenge of this dataset is the abundance of overlapping activities, sometimes multiple activities having exactly the same start and end times (typical examples include pairs of activities like 'holding a phone' and 'playing with a phone' or 'holding a towel' and 'tidying up a towel'). Experimental Setup: For this dataset we sample frames at 5 fps, and the input buffer is set to contain 768 frames. This makes the duration of the buffer approximately 154 seconds covering all the ground truth activity segments in Charades train set. As the activity segments for this dataset are longer, we choose the number of anchor segments K to be 18 with specific scale values <ref type="bibr" target="#b5">[1,</ref><ref type="bibr" target="#b6">2,</ref><ref type="bibr" target="#b7">3,</ref><ref type="bibr" target="#b8">4,</ref><ref type="bibr" target="#b9">5,</ref><ref type="bibr" target="#b10">6,</ref><ref type="bibr" target="#b11">7,</ref><ref type="bibr" target="#b12">8,</ref><ref type="bibr" target="#b14">10,</ref><ref type="bibr" target="#b16">12,</ref><ref type="bibr" target="#b18">14,</ref><ref type="bibr" target="#b20">16,</ref><ref type="bibr" target="#b25">20,</ref><ref type="bibr" target="#b29">24,</ref><ref type="bibr" target="#b33">28,</ref><ref type="bibr" target="#b37">32,</ref><ref type="bibr" target="#b45">40,</ref><ref type="bibr">48]</ref>. So the shortest anchor segment has a duration of 1.6 seconds and the longest anchor segment has a duration of 76.8 seconds. Over 99.96% of the activities in the training set is under 76.8 seconds. For this dataset we, additionally, explored slightly different settings of the anchor segment scales, but found that our model is not very sensitive to this hyperparameter.</p><p>We first finetune the Sports-1M pretrained C3D model <ref type="bibr" target="#b37">[32]</ref> on the Charades training set at the same 5 fps and initialize the 3D ConvNet part of our model with these finetuned weights. Next, we train R-C3D end-to-end on Charades by freezing the first two convolutional layers in order to accelerate training. The learning rate is kept fixed at 0.0001 for the first 10 epochs and then decreased to 0.00001 for 5 further epochs. We augment the data by following the two-way buffer setting and horizontal flipping of frames. Results: <ref type="table">Table 4</ref> provides a comparative evaluation of the proposed model with various baseline models reported in <ref type="bibr" target="#b30">[25]</ref>. This approach <ref type="bibr" target="#b30">[25]</ref> trains a CRF based video classification model (asynchronous temporal fields) and evaluates the prediction performance on 25 equidistant frames <ref type="table">Table 5</ref>. Activity detection speed during inference. FPS S-CNN <ref type="bibr" target="#b29">[24]</ref> 60 DAP <ref type="bibr" target="#b8">[4]</ref> 134.1 R-C3D (ours on Titan X Maxwell) 569 R-C3D (ours on Titan X Pascal) 1030</p><p>by making a multi-label prediction for each of these frames.</p><p>The activity localization result is reported in terms of mAP metric on these frames. For a fair comparison, we map our activity segment prediction to 25 equidistant frames and evaluate using the same mAP evaluation metric. A second evaluation strategy proposed in this work relies on a postprocessing stage where the frame level predictions are averaged across 20 frames leading to more spatial consistency. As shown in the <ref type="table">Table 4</ref>, our model outperforms the asynchronous temporal fields model proposed in <ref type="bibr" target="#b30">[25]</ref> as well as the different baselines reported in the same paper. While the improvement over the standard method is as high as 2.8%, the improvement after the post-processing is not as high.</p><p>One possible reason could be that our end-to-end fully convolutional model captures the spatial consistency implicitly without requiring any manually-designed postprocessing.</p><p>Following the standard practice we also evaluated our model in terms of mAP@0.5 which comes out to be 9.3%. The performance is not at par with other datasets presumably because of the inherent challenges involved in Charades e.g., the low illumination indoor scenes or the multilabel nature of the data. Initialization with a better C3D classification model trained on indoor videos with these challenging conditions may further boost the performance. <ref type="figure" target="#fig_0">Figure 3</ref>(c) shows some representative qualitative results from one video in this dataset.</p><p>One of the major challenges of this dataset is the presence of a large number of temporally overlapping activities. The results show that our model is capable of handling such scenarios. This is achieved by the ability of the proposal subnet to produce possibly overlapping activity proposals and is further facilitated by region offset regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Activity Detection Speed</head><p>In this section, we compare detection speed of our model with two other state-of-the-art methods. The comparison results are shown in <ref type="table">Table 5</ref>. S-CNN <ref type="bibr" target="#b29">[24]</ref> uses a timeconsuming sliding window strategy and predicts at 60 fps. DAP <ref type="bibr" target="#b8">[4]</ref> incorporates a proposal prediction step on top of LSTM and predicts at 134.1 fps. R-C3D constructs the proposal and classification pipeline in an end-to-end fashion and these two stages share the features making it significantly faster. The speed of execution is 569 fps on a single Titan-X (Maxwell) GPU for the proposal and classification stages together. On the upgraded Titan-X (Pascal) GPU, our inference speed reaches even higher (1030 fps). One of the reasons of the speedup of R-C3D over DAP may come from (c) Charades the fact that the LSTM recurrent architecture in DAP takes time to unroll, while R-C3D directly accepts a wide range of frames as input and the convolutional features are shared by the proposal and classification subnets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce R-C3D, the first end-to-end temporal proposal classification network for activity detection. We evaluate our approach on three large-scale data sets with very diverse characteristics, and demonstrate that it can detect activities faster and more accurately than existing models based on 3D Convnets. Additional features can be incorporated into R-C3D to further boost the activity detection result. One future direction may be to integrate R-C3D with hand-engineered motion features for improved activity prediction without sacrificing speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative visualization of the predicted activities by R-C3D (best viewed in color). Figure (a) and (b) show results for two videos each in THUMOS'14 and ActivityNet. (c) shows the result for one video from Charades. Groundtruth activity segments are marked in black. Predicted activity segments are marked in green for correct predictions and in red for wrong ones. Predicted activities with IoU more than 0.5 are considered as correct. Corresponding start-end times and confidence score are shown inside brackets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Activity detection results on THUMOS'14 (in percentage). mAP at different IoU thresholds α are reported. The top three performers on the THUMOS'14 challenge leaderboard and other results reported in existing papers are shown.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>α</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">0.1 0.2 0.3 0.4 0.5</cell></row><row><cell>Karaman et al. [13]</cell><cell cols="5">4.6 3.4 2.1 1.4 0.9</cell></row><row><cell>Wang et al. [37]</cell><cell cols="5">18.2 17.0 14.0 11.7 8.3</cell></row><row><cell>Oneata et al. [20]</cell><cell cols="5">36.6 33.6 27.0 20.8 14.4</cell></row><row><cell>Heilbron et al. [10]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.5</cell></row><row><cell>Escorcia et al. [4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.9</cell></row><row><cell>Richard et al. [22]</cell><cell cols="5">39.7 35.7 30.0 23.2 15.2</cell></row><row><cell>Yeung et al. [39]</cell><cell cols="5">48.9 44.0 36.0 26.4 17.1</cell></row><row><cell>Yuan et al. [41]</cell><cell cols="5">51.4 42.6 33.6 26.1 18.8</cell></row><row><cell>Shou et al. [24]</cell><cell cols="5">47.7 43.5 36.3 28.7 19.0</cell></row><row><cell>Shou et al. [23]</cell><cell>-</cell><cell>-</cell><cell cols="3">40.1 29.4 23.3</cell></row><row><cell cols="6">R-C3D (our one-way buffer) 51.6 49.2 42.8 33.4 27.0</cell></row><row><cell cols="6">R-C3D (our two-way buffer) 54.5 51.5 44.8 35.6 28.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Per-class AP at IoU threshold α = 0.5 on THUMOS'14 (in percentage).</figDesc><table><row><cell></cell><cell cols="4">[20] [39] [24] R-C3D (ours)</cell></row><row><cell>Baseball Pitch</cell><cell>8.6</cell><cell cols="2">14.6 14.9</cell><cell>26.1</cell></row><row><cell>Basketball Dunk</cell><cell>1.0</cell><cell>6.3</cell><cell>20.1</cell><cell>54.0</cell></row><row><cell>Billiards</cell><cell>2.6</cell><cell>9.4</cell><cell>7.6</cell><cell>8.3</cell></row><row><cell>Clean and Jerk</cell><cell cols="3">13.3 42.8 24.8</cell><cell>27.9</cell></row><row><cell>Cliff Diving</cell><cell cols="3">17.7 15.6 27.5</cell><cell>49.2</cell></row><row><cell>Cricket Bowling</cell><cell>9.5</cell><cell cols="2">10.8 15.7</cell><cell>30.6</cell></row><row><cell>Cricket Shot</cell><cell>2.6</cell><cell>3.5</cell><cell>13.8</cell><cell>10.9</cell></row><row><cell>Diving</cell><cell>4.6</cell><cell cols="2">10.8 17.6</cell><cell>26.2</cell></row><row><cell>Frisbee Catch</cell><cell>1.2</cell><cell cols="2">10.4 15.3</cell><cell>20.1</cell></row><row><cell>Golf Swing</cell><cell cols="3">22.6 13.8 18.2</cell><cell>16.1</cell></row><row><cell>Hammer Throw</cell><cell cols="3">34.7 28.9 19.1</cell><cell>43.2</cell></row><row><cell>High Jump</cell><cell cols="3">17.6 33.3 20.0</cell><cell>30.9</cell></row><row><cell>Javelin Throw</cell><cell cols="3">22.0 20.4 18.2</cell><cell>47.0</cell></row><row><cell>Long Jump</cell><cell cols="3">47.6 39.0 34.8</cell><cell>57.4</cell></row><row><cell>Pole Vault</cell><cell cols="3">19.6 16.3 32.1</cell><cell>42.7</cell></row><row><cell>Shotput</cell><cell cols="3">11.9 16.6 12.1</cell><cell>19.4</cell></row><row><cell>Soccer Penalty</cell><cell>8.7</cell><cell>8.3</cell><cell>19.2</cell><cell>15.8</cell></row><row><cell>Tennis Swing</cell><cell>3.0</cell><cell>5.6</cell><cell>19.3</cell><cell>16.6</cell></row><row><cell>Throw Discus</cell><cell cols="3">36.2 29.5 24.4</cell><cell>29.2</cell></row><row><cell>Volleyball Spiking</cell><cell>1.4</cell><cell>5.2</cell><cell>4.6</cell><cell>5.6</cell></row><row><cell>mAP@0.5</cell><cell cols="3">14.4 17.1 19.0</cell><cell>28.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Detection results on ActivityNet in terms of mAP@0.5 (in percentage). The top half of the table shows performance from methods using additional handcrafted features while the bottom half shows approaches using deep features only (including ours).</figDesc><table><row><cell cols="2">Results for [29] are taken from [1]</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">train data validation</cell><cell>test</cell></row><row><cell>G. Singh et. al. [30]</cell><cell>train</cell><cell>34.5</cell><cell>36.4</cell></row><row><cell>B. Singh et. al. [29]</cell><cell>train+val</cell><cell>-</cell><cell>28.8</cell></row><row><cell>UPC [18]</cell><cell>train</cell><cell>22.5</cell><cell>22.3</cell></row><row><cell>R-C3D (ours)</cell><cell>train</cell><cell>26.8</cell><cell>26.8</cell></row><row><cell>R-C3D (ours)</cell><cell>train+val</cell><cell>-</cell><cell>28.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">2 * 8/25 = 0.64 and 16 * 8/25 = 5.12</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: We would thank Lu He for the meaningful discussions. This research was supported by the NSF IIS-1212928 grant, the National Geospatial Agency, and a hardware grant from NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">High Jump (81.8s, 87.2s, 0.78) High Jump (90s, 94.7s, 0.83) GT Cricket Bowling (10.5s, 11.9s) Cricket Shot (11.4s, 13.2s) Cricket Bowling</title>
		<imprint/>
	</monogr>
	<note>10.5s, 11.8s, 0.99) Cricket Shot (12s, 13.7s, 0.98) Ours (a) THUMOS&apos;14</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Watching/Reading/Looking at a book (0.0s, 36.3s) Opening a door</title>
		<imprint/>
	</monogr>
	<note>35.0s, 41.1s)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Closing a book (32.3s, 37.3s) Walking through a doorway</title>
		<imprint/>
	</monogr>
	<note>37.1s, 41.6s)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Grasping onto a doorknob (34.6s, 41.6s)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">Watching/Reading/Looking at a book (9.2s, 36.9s, 0.46) GT Ours Opening a book</title>
		<imprint/>
	</monogr>
	<note>18.4s, 28.7s, 0.41) Closing a book (31.5s, 36.1s, 0.32) Walking through a doorway (37.7s, 42.4s, 0.32</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<ptr target="http://activity-net.org/challenges/2016/data/anet_challenge_summary.pdf" />
	</analytic>
	<monogr>
		<title level="j">ActivitNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CPMC: Automatic Object Segmentation using Constrained Parametric Min-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1312" to="1328" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-FCN: Object Detection via Region-based Fully Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DAPs: Deep Action Proposals for Action Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part Based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast Temporal Activity Proposals for Efficient Detection of Human Actions in Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D Convolutional Neural Networks for Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS Challenge: Action Recognition with a Large Number of Classes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast Saliency Based Pooling of Fisher Encoded Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Realistic Human Actions from Movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Activity Progression in LSTMs for Activity Detection and Early Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1942" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G</forename><surname>Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08128</idno>
		<title level="m">Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond Short Snippets: Deep Networks for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The LEAR submission at Thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal Action Detection Using a Statistical Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06371</idno>
		<title level="m">Asynchronous Temporal Fields for Action Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Multi-Stream Bi-Directional Recurrent Neural Network for Fine-Grained Action Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Untrimmed Video Classification for Activity Detection: submission to ActivityNet Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01979</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video Action Detection with Relational Dynamic-Poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Actionness Estimation using Hybrid Fully Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2708" to="2717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">UntrimmedNets for Weakly Supervised Action Recognition and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Action Recognition and Detection by Combining Motion and Appearance Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV THUMOS Workshop</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to Track for Spatio-Temporal Action Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">End-toend Learning of Action Detection from Frame Glimpses in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast Action Proposals for Human Action Detection and Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal Action Localization with Pyramid of Score Distribution Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-view Action Recognition via Transferable Dictionary Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2542" to="2556" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
