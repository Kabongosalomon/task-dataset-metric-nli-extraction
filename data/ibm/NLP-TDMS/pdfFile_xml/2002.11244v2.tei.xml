<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transfer Learning from Synthetic to Real-Noise Denoising with Adaptive Instance Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonsik</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departmet of ECE</orgName>
								<orgName type="institution" key="instit1">INMC</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<country>Seoul Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Woong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departmet of ECE</orgName>
								<orgName type="institution" key="instit1">INMC</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<country>Seoul Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soh</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departmet of ECE</orgName>
								<orgName type="institution" key="instit1">INMC</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<country>Seoul Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departmet of ECE</orgName>
								<orgName type="institution" key="instit1">INMC</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<country>Seoul Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Ik</forename><surname>Cho</surname></persName>
							<email>nicho@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Departmet of ECE</orgName>
								<orgName type="institution" key="instit1">INMC</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
								<address>
									<country>Seoul Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transfer Learning from Synthetic to Real-Noise Denoising with Adaptive Instance Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-noise denoising is a challenging task because the statistics of real-noise do not follow the normal distribution, and they are also spatially and temporally changing. In order to cope with various and complex real-noise, we propose a well-generalized denoising architecture and a transfer learning scheme. Specifically, we adopt an adaptive instance normalization to build a denoiser, which can regularize the feature map and prevent the network from overfitting to the training set. We also introduce a transfer learning scheme that transfers knowledge learned from syntheticnoise data to the real-noise denoiser. From the proposed transfer learning, the synthetic-noise denoiser can learn general features from various synthetic-noise data, and the real-noise denoiser can learn the real-noise characteristics from real data. From the experiments, we find that the proposed denoising method has great generalization ability, such that our network trained with synthetic-noise achieves the best performance for Darmstadt Noise Dataset (DND) among the methods from published papers. We can also see that the proposed transfer learning scheme robustly works for real-noise images through the learning with a very small number of labeled data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image restoration tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> have achieved noticeable improvement with the development of convolutional neural network (CNN). Although most of image restoration methods work well on synthetically degraded images <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24]</ref>, they show insufficient performance on the real degradations.</p><p>Regarding the denoising methods, the networks trained with synthetic-noise (SN) do not work well for the realworld images because of the discrepancy in the distribution of SN and real-noise (RN). Specifically, CNNs <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> trained with Gaussian noise do not work well for the real-world images, because the CNNs are overfitted to the Gaussian distribution. The problem of overfitting can also be seen from a toy regression example in <ref type="figure">Fig. 1</ref>. As shown in <ref type="figure">Fig. 1(a)</ref>, the severely overfitted regression method ('w/o Regularizer') shows worse performance than a regularized method ('w/ Regularizer') on the synthetic test data. Moreover, it can be seen in <ref type="figure">Fig. 1(b)</ref> that the generalization ability is much worse when the training and test domains are different.</p><p>To better address the problem due to the different data distribution between training and test sets, two kinds of approaches have been developed: <ref type="bibr" target="#b0">(1)</ref> obtaining the pairs of RN image and corresponding near-noise-free image <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b47">48]</ref>, and <ref type="bibr" target="#b1">(2)</ref> finding more realistic noise model <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>The RN datasets enable the quantitative comparison of denoising performance on real-world images and also provide the training sets for learning-based methods. The CNNs trained with RN datasets robustly work on the realworld images, because domains of training and test set almost coincide. However, acquiring the pairs of RN images needs specialized knowledge, and the amount of provided datasets would not be enough for training a deeper CNN <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b48">49]</ref>. Furthermore, learning-based methods can be easily overfitted to a specific camera device (dataset), which cannot cover all the devices that have different characteristics such as gamma correction, color correction, and other in-camera pipelines.</p><p>For a finding more realistic noise model, CBDNet <ref type="bibr" target="#b18">[19]</ref> synthesized near-RN images by considering realistic noise models and simulating the in-camera pipeline. It generates enough dataset that simulates more than 200 camera response functions. The CBDNet shows excellent performance on RN images even though the CNN is trained with the SN. Furthermore, they showed that additional training with RN dataset improves performance. Although realistic noise modeling indeed reduces the domain discrepancy between SN and RN, there still remains a domain discrepancy to be handled. Moreover, CNN can be overfitted to a certain  noise model that is actually not a 'real' noise. From these observations, we propose a novel denoiser that is well generalized to the various RN from camera devices by employing an adaptive instance normalization (AIN) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref>. In recent CNN based methods for restoring the synthetic degradations <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b23">24]</ref>, regularization methods have not been exploited due to the small performance gain (even degrading performance). This indicates that a CNN is overfitted to the training data to get the best performance when domains of training and test set coincide <ref type="bibr" target="#b14">[15]</ref>.</p><p>On the other hand, the denoiser trained with SN needs regularization, for applying it to the RN denoising. As shown in the example of <ref type="figure">Fig. 1</ref> (a) and (b) with 'w/ Regularizer', the network needs to be generalized through the regularization. In this respect, we propose a well-regularized denoiser by adopting the AIN as a regularization method. Specifically, the affine transform parameters for the normalization of features are generated from the pixel-wise noise level. Then, the transform parameters adaptively scale and shift the feature maps according to the noise characteristics, which results in the generalization of the CNN.</p><p>Furthermore, we propose a transfer learning scheme from SN to RN denoising network to reduce the domain discrepancy between the synthetic and the real. As mentioned above, the RN dataset would not be sufficient to train a CNN, which can also be easily overfitted to a certain RN dataset. Hence, we devise a transfer learning scheme that learns the general and invariant information of denoising from the SN domain and then transfer-learns the domain-specific information from the information of RN. As can be seen in <ref type="figure">Fig. 1</ref>(c), we believe that the SN denoiser can be adapted to an RN denoiser by re-transforming normalized features. Specifically, the parameters of AIN are updated using the RN dataset. The proposed scheme based on transfer learning can be applied to any dataset that has a small number of labeled data. That is, a CNN trained with the SN is easily transferred to work for the RN removal, without the need for training the whole network with the RN.</p><p>The contribution of this work can be summarized as follows:</p><p>• We propose a novel well-generalized denoiser based on the AIN, which enables the CNN to work for various noise from many camera devices.</p><p>• We introduce a transfer learning for the denoising scheme, which learns the domain-invariant information from SN data and updates affine transform parameters of AIN for the different-domain data.</p><p>• The proposed method achieves state-of-the-art performance on the SN and RN images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The statistics of RN in standard RGB (sRGB) images depend on the properties of camera sensors and in-camera pipelines. Specifically, shot noise and readout noise are generated from the sensor, and the statistics of generated noise are changed according to the in-camera pipeline such as demosaicing, gamma correction, in-camera denoiser, white balancing, color correction, etc <ref type="bibr" target="#b37">[38]</ref>. There have been several works to approximate the RN model, including Gaussian-Poisson <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>, heteroscedastic Gaussian <ref type="bibr" target="#b19">[20]</ref>, Gaussian Mixture Model (GMM) <ref type="bibr" target="#b57">[58]</ref>, and deep leaning based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref>. Considering the camera pipeline, CBDNet <ref type="bibr" target="#b18">[19]</ref> and Unprocessing <ref type="bibr" target="#b6">[7]</ref> also considered realistic noise models. Specifically, they obtained near-RN images by adding the heteroscedastic Gaussian noise to the pseudo-raw images and feeding them to the camera pipeline. These methods can simulate more than 200 camera response functions, and thus generate noisy images having different characteristics. Moreover, CBDNet is alternately trained with the RN and SN to overcome overfitting to the noise model. We think the alternate training scheme would incur training instability due to different data distributions, and also cannot train quite different RN effectively. Thus, we introduce a new transfer learning scheme that can simply but effectively adapt SN denoiser to other RN ones by re-transforming the normalized feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed</head><p>We aim to train a robust RN denoiser, which reduces the discrepancy between the distributions of training and test sets, by proposing a novel denoiser and transfer learning. Precisely, we propose denoising architecture using the AIN, which can be well generalized to RN images. Also, we introduce a transfer learning scheme to reduce the remaining data discrepancy, which consists of two stages: (1) training a denoiser with SN dataset S = {X s , Y s } and (2) transfer learning with RN dataset T = {X r , Y r }, where X and Y are noise-free images and noisy images respectively, and the subscript s is for SN and r for RN. We use the noise model from CBDNet for generating Y s from X s with the noise level of σ(y s ) where y s ∈ Y s denotes SN image. After training SN denoiser with S, RN denoiser is trained with T (pairs of RN image y r ∈ Y r and near noise-free image x r ∈ X r ). In the transfer learning stage, domain-specific parameters are only updated to effectively preserve learned knowledge from SN data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Adaptive Instance Normalization Denoising Network</head><p>We present a novel AIN denoising network (AINDNet), where the same architecture is employed both for SN and RN denoiser. We compose AINDNet with a noise level estimator and a reconstruction network, which is presented in <ref type="figure" target="#fig_1">Fig. 2</ref>. The noise level estimator takes a noisy image y as an input and generates the estimated noise level mapσ(y) = F est (y; θ est ) where θ est denotes a training parameter of estimator. The reconstruction network takesσ(y) and y as input and generates denoised imagex = F rec (y,σ(y); θ rec ) where θ rec denotes a training parameter of reconstruction network. The reconstruction network is U-Net based architecture with AIN Residual blocks (AIN-ResBlocks).</p><p>Noise Level Estimator Estimating the noise level would not be an easy task due to the complex noise model and in-camera pipeline. In our experiment, we find that previous simple noise level estimators <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7]</ref>, which consist of five convolutions, could not accurately estimate the noise level. The main reason is that the previous estimators have a <ref type="figure">Figure 3</ref>: Illustration of the proposed AIN-ResBlock with corresponding kernel size (k), feature scale (s), and number of features (n). Note that n is linearly increasing according to s. Leaky ReLU is employed for an activation function. The Norm (red) block denotes channel-wise spatial normalization block. Average-pool scales the size ofσ(y) to be the same as that of h.</p><p>small receptive field so that it could not fully capture complex noise information. From this observation, we design a new noise level estimator with a larger receptive field by employing down/up-sampling and multi-scale estimations. Specifically, estimator produces down-scaled estimation mapσ 4 (y) ∈ R H/4×W/4×3 and original-sized estimation mapσ 1 (y) ∈ R H×W ×3 . Then, these two outputs are weight averaged to feed reconstruction network:</p><formula xml:id="formula_0">σ(y) = λ ms L(σ 4 (y)) + (1 − λ ms )σ 1 (y)<label>(1)</label></formula><p>where H, W , L(·) denotes the height and width of the image, and the linear interpolation respectively. λ ms is empirically determined to 0.8. From the weight average of multiscale estimates, we can achieve region-wisely smoothed σ(y), which follows general the characteristic of RN.</p><p>Adaptive Instance Normalization The proposed AIN-ResBlock plays two crucial roles in the proposed denoising scheme. One is regularizing the network not to be overfitted to SN images, and the other is adapting SN denoiser to RN denoiser. For this, we build AIN-ResBlcok with two convolutions and two AIN modules, which is presented in sented as</p><formula xml:id="formula_1">h new p,q,c = γ * p,q,c h p,q,c − µ c σ c + β * p,q,c<label>(2)</label></formula><p>where the variables with superscript * are generated from σ(y), and µ c and σ c denote the mean and standard deviation of h respectively, in channel c. Precisely,</p><formula xml:id="formula_2">µ c = 1 H W H p W q h p,q,c<label>(3)</label></formula><formula xml:id="formula_3">σ 2 c = 1 H W H p W q (h p,q,c − µ c ) 2 +<label>(4)</label></formula><p>where denotes the stability parameter, which prevents divide-by-zero in eq. (2), and we set = 10 −5 in our implementation. Note that γ * p,q,c and β * p,q,c can be generated pixel-wisely and thus the proposed method can process spatially variant noisy images adaptively. In another point of view, AIN module acts as feature attention <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref> with explicitly constrained information (σ(y)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transfer Learning</head><p>We propose transfer learning scheme to leverage S to accelerate the training of RN denoiser with T that has a limited number of elements (RN pairs). We expect that SN denoiser learns general and invariant feature representations and RN denoiser learns noise characteristics that cannot be fully modeled from SN data. The proposed transfer learning scheme can achieve these two merits by adapting SN denoiser to RN denoiser. For this, we focus on normalization parameter to handle different data distribution, which is inspired from other style transfer and classification tasks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40]</ref>. In these methods, transforming normalization parameters can transfer different style domain, and different domain classifications can be handled by switching the batch normalization parameters. From these observations, we try to adapt different domain denoisers by transfer-learning the normalization parameters assuming that data discrepancy between S and T can be adapted by re-transforming the normalized feature maps.</p><p>Specifically, AIN parameters of SN denoiser can be adapted pixel-wisely with conditionalσ i (y s ). Thus, AIN modules and noise level estimator are transfer-learned with RN data. Although the objective function of noise level cannot be present in T , noise level estimator can be trained with the reconstruction loss. We consider that last convolution plays a crucial role reconstructing feature maps to RGB image, hence last convolution is also updated. The overall proposed transfer learning scheme is presented in <ref type="figure">Fig. 4</ref>.</p><p>Since the proposed transfer learning scheme only updates the parts of well generalized denoiser, it can be converged with faster speed and get better performance with very few number of elements from T than training from scratch. Moreover, the proposed scheme effectively copes with multiple models, which are inevitably required due to severely different noise statics, saving lots of memory by switching specific parameters.</p><p>Training For training SN denoiser, we exploit multi-scale asymmetric loss as an estimation loss where asymmetric loss is introduced from CBDNet <ref type="bibr" target="#b18">[19]</ref> to prevent under estimation. Formally, multi-scale asymmetric loss is defined as,</p><formula xml:id="formula_4">L ms-asymm = i∈{1,4} w i |α − 1 (σi(y s )−σi(y s )&lt;0) | (5) · (σ i (y s ) − σ i (y s )). 2</formula><p>where 1, ·, and . 2 denote element-wise operations such as indicator function, multiplication, and power respectively. Hyperparameters {w 1 , w 4 , α} are empirically determined as {0.2, 0.8, 0.25}. σ 4 (y s ) is achieved from 4 × 4 average pooling σ 1 (y s ).</p><p>Then, the proposed SN denoiser is jointly trained with estimation loss and L 1 reconstruction loss as,</p><formula xml:id="formula_5">L = F (y s ; θ s ) − x s 1 1 + λ ms-asymm L ms-asymm (6)</formula><p>where θ s denotes the SN denoiser training parameter including noise level estimator and reconstruction network. λ ms-asymm denotes the weight term of noise level estimator and is empirically determined to 0.05.</p><p>For the RN denoiser, it is only trained with reconstruction loss:</p><formula xml:id="formula_6">L = F (y r ; θ r ) − x r 1 1<label>(7)</label></formula><p>where θ r denotes the RN denoiser training parameter that is transferred from θ s . Previously stated parameter such as AIN modules, estimator, and last convolution are only updated, and other parameters are fixed when training the RN denoiser. We use Adam optimizer for both SN denoiser and RN denoiser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present the results of AWGN and RN images by training a Gaussian denoiser and RN denoiser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Training Settings For the Gaussian denoiser, the training images are obtained from DIV2K <ref type="bibr" target="#b42">[43]</ref> and BSD400 <ref type="bibr" target="#b35">[36]</ref>, and noisy image is generated by AWGN model. For the RN denoiser, we train a denoiser with two step: training an SN denoiser and training the RN denoiser by transfer learning. We achieve pairs of SN images and noise-free images from Waterloo dataset <ref type="bibr" target="#b33">[34]</ref> with heteroscedastic Gaussian noise model and simulating in-camera pipelines. The RN denoiser, which is transferred from SN denoiser, is trained with SIDD training set <ref type="bibr" target="#b1">[2]</ref>. All the training images are cropped into patches of size 256 × 256.</p><p>Test Set In the AWGN experiments, we evaluate Set12 <ref type="bibr" target="#b52">[53]</ref> and BSD68 <ref type="bibr" target="#b41">[42]</ref> that are widely used for validating the AWGN denoiser. Furthermore, we adopt three datasets for real-world noisy images:</p><p>• RNI15 <ref type="bibr" target="#b26">[27]</ref> is composed of 15 real-world noisy images. Unfortunately, the ground-truth clean images are unavailable, therefore we only present qualitative results.</p><p>• DND <ref type="bibr" target="#b40">[41]</ref> provides 50 noisy images that are captured by mirrorless cameras. Since we cannot access near noise-free counterparts, the objective results (PSNR/SSIM) can be achieved by submitting the denoised images to DND site.</p><p>• SIDD <ref type="bibr" target="#b1">[2]</ref> is obtained from smartphone cameras. It provides 320 pairs of noisy images and corresponding near noise-free ones for the learning based methods where the captured scenes are mostly static. Furthermore, it provides 1280 patches for validation that has similar scenes with training set. The quantitative results (PSNR/SSIM) can be achieved by uploading the denoised image to SIDD site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with state-of-the-arts</head><p>Noise Level Estimation We evaluate an accuracy of noise level estimator on exploited noise model images. We compare the proposed noise level estimator with fully convolutional network (FCN) that are widely used <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7]</ref>. In   order to evaluate the accuracy of estimator itself, each estimator is trained with L 1 regression. The employed quantitative measurements are mean absolute error (MAE) and standard deviation (STD) of the error. We report the accuracy of each estimator in <ref type="table" target="#tab_1">Table 1</ref> where the input images are simultaneously corrupted with signal dependent noise level σ s and signal independent noise level σ c . We can find that proposed estimator gets more accurate results than previous estimator with a similar number of parameters. The results of more various noise levels will be presented in supplementary file. Furthermore, we will present the denoising performance when combined with reconstruction network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AWGN Denoising</head><p>We compare proposed denoiser on the noisy grayscale images that are corrupted by AWGN. For this, we train Gaussian denoiser in a single network that learns noise level in [0,60]. The comparisons between the proposed method and other methods are presented in <ref type="table" target="#tab_2">Table 2</ref>. We can see that the proposed denoiser achieves the best performance on Set12 where composition of Set12 is independent from training sets. On the other hand, the proposed method gets second best performance on BSD68 that consists of similar objects in BSD400 (training set). We think these results present robust generalization ability of the proposed denoising architecture for training set.</p><p>Real Noise Denoising We also investigate the proposed denoiser and transfer learning scheme on RN datasets. Processing RN image is considered very practical, but difficult, because the noises are signal dependent, spatially variant, and visualized diversely according to different in-camera pipelines. Thus, we think RN denoising is an appropriate task for showing the generalization ability of the proposed denoiser and the effects of the proposed transfer learning. For the precise comparison, we train four different denoisers according to training sets and learning methods:</p><p>• AINDNet(S): AINDNet is trained with SN images, which is proposed SN denoiser.</p><p>• AINDNet(R): AINDNet is trained with RN images.</p><p>• AINDNet+RT: All the parameters from AINDNet(S) are re-trained with RN images, which is common transfer learning scheme.</p><p>• AINDNet+TF: Specified parameters from AIND-Net(S) are updated with RN images, which is proposed RN denoiser.</p><p>Moreover, we present the geometric self-ensemble <ref type="bibr" target="#b43">[44]</ref> results denoting super script * in order to maximize potential performance of the proposed methods. Meanwhile, there have been a challenge on real image denoising <ref type="bibr" target="#b2">[3]</ref> where the SIDD is used. Our method shows lower performance than the top-ranked ones in the challenge, but it needs to be noted that the number of parameters of our network is much smaller than those in the challenge. For example, DHDN <ref type="bibr" target="#b38">[39]</ref> and DIDN <ref type="bibr" target="#b51">[52]</ref> that appeared in the challenge require about 160 M and 190 M training parameters respectively which are about 12 -15 times larger than ours. Moreover, challenge methods have been slightly overfitted to SIDD where the winning denoiser <ref type="bibr" target="#b21">[22]</ref> gets comparably lower performance (38.78 dB) on DND than our method. Therefore, we would not directly compare the proposed method with challenge methods.</p><p>The comparisons, including internal comparisons, are presented in <ref type="table" target="#tab_3">Table 3</ref> and 4. We can find that proposed methods get the best performance on DND and SIDD benchmarks. Specifically, the proposed AINDNet(S) achieves the best performance on DND benchmark, which is impressive performance that outperforms RN trained denoisers. Moreover, AINDNet(S) gets 1.5 dB and 2.4 dB gains from CBDNet on DND and SIDD respectively where employed noise models are the same. These results indicate that the proposed denoiser is not overfitted to noise model and can be well generalized to RN images. However, AINDNet(S) has inferior performance than AINDNet(R) on SIDD with big margin. The main reason is that AINDNet(R) is solely trained with SIDD training images where test set consists of similar scenes and objects in training set. In other words, AINDNet(R) can be slightly overfitted to SIDD benchmark and this phenomenon can be seen from insufficient performance on DND.</p><p>In contrast, AINDNet+RT and AINDNet+TF get satisfying performance on both DND and SIDD. Concretely, AINDNet+RT and AINDNet+TF have better performance than others, including AINDNet(R) on SIDD, which indicates that pre-training the SN images results in better performance. AINDNet+TF more likely preserves priorly learned knowledges from SN data than AINDNet+RT, so AIND-Net+TF achieves the best overall performance among compared methods.</p><p>We present visualized comparisons on SIDD and RNI15 in Figs. 5 and 6, which show that proposed methods remove noises robustly while preserving the edges. Thus, characters in output images are more apparent than in other methods' results. Furthermore, we also present visual enhancement in <ref type="figure" target="#fig_5">Fig. 7</ref> when the proposed transfer learning scheme is applied. Since RN denoiser transfer-learns characteristics of RN, AINDNet+TF successfully removes unusual noise that cannot be removed with AINDNet(S). Moreover, RN denoiser learns the properties of JPEG compression artifacts that is not priorly learned in SN denoiser, so it can also successfully reduces compression artifacts. We will also present other visualized comparisons in supplementary file.  <ref type="bibr" target="#b7">[8]</ref> Non-blind Synthetic 24.71 0.641 TNRD <ref type="bibr" target="#b11">[12]</ref> Non-blind Synthetic 24.73 0.643 BM3D <ref type="bibr" target="#b12">[13]</ref> Non-blind -25.65 0.685 WNNM <ref type="bibr" target="#b17">[18]</ref> Non-blind -25.78 0.809 KSVD <ref type="bibr" target="#b3">[4]</ref> Non </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussions</head><p>Effect of Transfer Learning with Limited RN Pairs We investigate the relation between denoising performance and the amount of RN image pairs in T , because we consider that preparation of T is quite difficult and the number of elements can also be limited. For this, we train each network with constrained image pairs from one to all (320) from SIDD <ref type="bibr" target="#b1">[2]</ref>. The average PSNR of each denoiser is presented in <ref type="table" target="#tab_6">Table 5</ref>. It can be seen that transfer learning schemes can infer great performance with the small number of real training images. It is notable that AINDNet+TF trained with 32 pairs of real data achieves better performance than RIDNet that exploits all. Thus, we can conclude that the transfer learning with SN denoiser dramatically accelerate the performance with a small number of labeled data from other domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture of Denoiser</head><p>We demonstrate the effectiveness of reconstruction network for training with S. For this, AINDNet(S) is compared with a baseline (IN + Concat), which replaces AIN module with IN and concatenated input of noisy image and noise level map <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>. Furthermore, we compare an adaptive Gaussian denoiser <ref type="bibr" target="#b23">[24]</ref> that can process spatially variant noise map by feeding gatedresidual block (Gated-ResBlock). Since it has not reported the performance on RN dataset, we train SN denoiser by replacing AIN-ResBlock to Gated-Resblock where other settings are same as AINDNet. <ref type="table" target="#tab_7">Table 6</ref> shows that the proposed AIN-ResBlock shows the best performance on RN datasets. Thus, we believe that the AIN-ResBlock is an appropriate architecture for the generalization. We will present ablation study about update variable for transfer learning in supplementary file.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a novel denoiser and transfer learning scheme of RN denoising. The proposed denoiser employs an AIN to regularize the network and also to prevent the network from overfitting to SN. The transfer learning mainly updates the AIN module using RN data to adjust data distribution. From the experimental results, we could find that the proposed denoising scheme can be well generalized to RN even if it is trained with SN. Moreover,  the transfer learning scheme can effectively adapt an SN denoiser to an RN denoiser, with very few additional training with real-noise pairs. We will make our codes publicly available at https://github.com/terryoo/AINDNet for further research and comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Transfer Learning from AWGN</head><p>We present the results of transfer-learned denoiser where AINDNet is pre-trained with AWGN and adapted to real noise (RN). For the precise comparison, we report performance of three denoisers in <ref type="table" target="#tab_8">Table 7</ref> according to training sets and learning methods:</p><p>• AINDNet(AWGN): AINDNet is trained with AWGN images.</p><p>• AINDNet(AWGN)+TF 1 : AINDNet(AWGN) is transfer learned with a single real noisy image.</p><p>• AINDNet(AWGN)+TF: AINDNet(AWGN) is transfer learned with full real noisy images (320 images).</p><p>It can be seen that proposed transfer learning scheme significantly improves the performance of synthetic noise (SN) denoisers including AWGN denoiser when the input is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">More Noise Level Estimation Results</head><p>We evaluate the accuracy of the proposed noise level estimator, where the input images are simultaneously corrupted with more diverse signal-dependent noise levels σ s and signal-independent noise levels σ c . As presented in <ref type="table" target="#tab_9">Table 8</ref>, the proposed noise level estimator achieves better accuracy with lower standard deviations of the errors in most cases. Furthermore, the proposed noise level estimator predicts quite accurate estimates when the images are corrupted with high σ s and σ c .  MAE STD MAE STD (0.04, 0.00) 0.009 0.007 0.022 0.014 (0.04, 0.02) 0.029 0.007 0.015 0.011 (0.04, 0.04) 0.050 0.006 0.009 0.009 (0.04, 0.06) 0.070 0.007 0.016 0.009 (0.08, 0.00) 0.018 0.013 0.022 0.014 (0.08, 0.02) 0.039 0.013 0.014 0.012 (0.08, 0.04) 0.059 0.014 0.012 0.011 (0.08, 0.06) 0.076 0.013 0.020 0.010 (0.12, 0.00) 0.029 0.020 0.020 0.014 (0.12, 0.02) 0.052 0.021 0.015 0.014 (0.12, 0.04) 0.071 0.020 0.017 0.014 (0.12, 0.06) 0.087 0.020 0.030 0.014 (0.16, 0.00) 0.039 0.027 0.021 0.018 (0.16, 0.02) 0.065 0.028 0.020 0.019 (0.16, 0.04) 0.076 0.027 0.021 0.019 (0.16, 0.06) 0.098 0.028 0.040 0.021 Average 0.054 0.017 0.020 0.014 # params 29.5 K 29.7 K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Ablation Study</head><p>We demonstrate the effectiveness of noise level estimator for training with S. We present performance of noise level estimators combined with reconstruction network in <ref type="table" target="#tab_10">Table 9</ref> with different objective function. Remember that L ms-asymm can generate smoothed outputs, so L T V is excluded when using L ms-asymm . We find that state-of-theart training scheme (FCN + L asymm + L T V ) infers inferior performance than proposed training scheme (Ours + L ms-asymm ). Moreover, the proposed training scheme also surpasses internal variation (Ours + L 1 + L T V ). We further investigate the relation between update parameters and performance in the transfer learning phase. For the precise comparison, we compare three variants by freezing each update parameter in <ref type="table" target="#tab_1">Table 10</ref>:</p><p>• Ours-AIN: AIN module is not updated in transfer learning stage.</p><p>• Ours-Estimator: Noise level estimator is not updated in transfer learning stage.</p><p>• Ours-LastConv: Last convolution is not updated in transfer learning stage.</p><p>It can be seen that proposed updating the noise level estimator, and last convolution contribute 0.1 -0.2 dB performance gain respectively. Fixing AIN module parameter presents even worse performance than the SN denoiser. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed denoiser. The noise level estimator and reconstruction network are U-Net based architecture, so the feature maps are down/up-sampled by average-pool/transposed convolution. We denote each scale of feature map as 1/s where s can be 1, 2, and 4. All the represented convolutions in reconstruction network are 3 × 3 kernel having 64s feature maps excluding last convolution. Feature representation of noise level estimator is also composed of 3 × 3 convolutions with 32 channels and noise level maps are achieved from 3 × 3 convolutions having 3 channel outputs. The amount of overall parameters is 13.7 M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Figure 4 :</head><label>34</label><figDesc>The AIN module affine transforms normalized feature map h ∈ R H ×W ×C of convolution by taking a conditional inputσ(y) where H × W denotes the spatial size of feature map at each scale s, and C is the number of channels. Specifically, the AIN module produces affine transform parameters such as scale (γ) and shift (β) for each pixel. Thus, every feature map is channel-wisely normalized and pixel-wisely affine transformed according to the noise level. The update process of feature map in AIN module at site (p ∈ R H , q ∈ R W , c ∈ R C ) is formally repre-Illustration of the proposed transfer learning scheme. AIN module, noise level estimator, and last convolution are only updated when learning RN data. For the better visualization, we omit the noise level estimator in thisfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, σ c ) MAE STD MAE STD (0.08, 0.02) 0.039 0.013 0.014 0.012 (0.08, 0.04) 0.059 0.014 0.012 0.011 (0.08, 0.06) 0.076 0.013 0.020 0.010 (0.12, 0.02) 0.052 0.021 0.015 0.014 (0.12, 0.04) 0.071 0.020 0.017 0.014 (0.12, 0.06) 0.087 0.020 0.030 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>The real noisy image from SIDD, and the comparison of the results. The real noisy image from RNI15, and the comparison of the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The real noisy image from RNI15, and the comparison of the results showing the effectiveness of the proposed transfer learning scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Average MAE and error STD for the images from Kodak24 where the inputs are corrupted by heteroscedastic Gaussian including in-camera pipeline.</figDesc><table><row><cell>Method</cell><cell>FCN</cell><cell>Ours</cell></row><row><cell>(σ s</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average PSNR of the denoised images, where the inputs are corrupted by AWGN with σ = 15, 25, and 50, for the images from Set12 and BSD68 datasets. (red: the best result, blue: the second best) 30.36 27.21 31.61 29.16 26.23 UNLNet [29] 32.67 30.25 27.04 31.47 28.98 26.04 FFDNET [55] 32.75 30.43 27.32 31.63 29.19 26.29 RIDNet [6] 32.91 30.60 27.43 31.81 29.34 26.40 AINDNet 32.92 30.61 27.51 31.69 29.26 26.32</figDesc><table><row><cell>Test Set</cell><cell></cell><cell>Set12</cell><cell></cell><cell></cell><cell>BSD68</cell><cell></cell></row><row><cell>Method</cell><cell>15</cell><cell>25</cell><cell>50</cell><cell>15</cell><cell>25</cell><cell>50</cell></row><row><cell>BM3D [13]</cell><cell cols="6">32.38 29.95 26.70 31.07 28.56 25.62</cell></row><row><cell>TNRD [12]</cell><cell cols="6">32.50 30.04 26.78 31.42 28.91 25.96</cell></row><row><cell>DnCNN [53]</cell><cell>32.68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average PSNR of the denoised images on the DND benchmark, we denote the environment of training, i.e., training with SN data only, RN data only, and both. * denotes geometric self-ensemble<ref type="bibr" target="#b43">[44]</ref> result. (red: the best result, blue: the second best)</figDesc><table><row><cell>Method</cell><cell cols="3">Blind/Non-blind Training Env. PSNR SSIM</cell></row><row><cell cols="2">CDnCNN-B [53] Blind</cell><cell>Synthetic</cell><cell>32.43 0.7900</cell></row><row><cell>TNRD [12]</cell><cell>Non-blind</cell><cell>Synthetic</cell><cell>33.65 0.8306</cell></row><row><cell>MLP [8]</cell><cell>Non-blind</cell><cell>Synthetic</cell><cell>34.23 0.8331</cell></row><row><cell>FFDNet [55]</cell><cell>Non-blind</cell><cell>Synthetic</cell><cell>34.40 0.8474</cell></row><row><cell>BM3D [13]</cell><cell>Non-blind</cell><cell>-</cell><cell>34.51 0.8507</cell></row><row><cell>WNNM [18]</cell><cell>Non-blind</cell><cell>-</cell><cell>34.67 0.8646</cell></row><row><cell>GCBD [10]</cell><cell>Blind</cell><cell>Synthetic</cell><cell>35.58 0.9217</cell></row><row><cell>KSVD [4]</cell><cell>Non-blind</cell><cell>-</cell><cell>36.49 0.8978</cell></row><row><cell>TWSC [50]</cell><cell>Blind</cell><cell>-</cell><cell>37.94 0.9403</cell></row><row><cell>CBDNet [19]</cell><cell>Blind</cell><cell>Synthetic</cell><cell>37.57 0.9360</cell></row><row><cell>CBDNet [19]</cell><cell>Blind</cell><cell>Real</cell><cell>37.72 0.9408</cell></row><row><cell>CBDNet [19]</cell><cell>Blind</cell><cell>All</cell><cell>38.06 0.9421</cell></row><row><cell>RIDNet [6]</cell><cell>Blind</cell><cell>Real</cell><cell>39.23 0.9526</cell></row><row><cell>AINDNet(S)</cell><cell>Blind</cell><cell>Synthetic</cell><cell>39.53 0.9561</cell></row><row><cell>AINDNet(R)</cell><cell>Blind</cell><cell>Real</cell><cell>39.16 0.9515</cell></row><row><cell>AIDNet + RT</cell><cell>Blind</cell><cell>All</cell><cell>39.21 0.9505</cell></row><row><cell>AINDNet + TF</cell><cell>Blind</cell><cell>All</cell><cell>39.37 0.9505</cell></row><row><cell>AINDNet(S)  *</cell><cell>Blind</cell><cell>Synthetic</cell><cell>39.77 0.9590</cell></row><row><cell>AINDNet(R)  *</cell><cell>Blind</cell><cell>Real</cell><cell>39.34 0.9524</cell></row><row><cell cols="2">AINDNet + RT  *  Blind</cell><cell>All</cell><cell>39.34 0.9522</cell></row><row><cell>AINDNet + TF  *</cell><cell>Blind</cell><cell>All</cell><cell>39.52 0.9522</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Average PSNR of the denoised images on the SIDD benchmark, we denote the environment of training, i.e., training with SN data only, RN data only, and both. * denotes geometric self-ensemble<ref type="bibr" target="#b43">[44]</ref> result. (red: the best result, blue: the second best)</figDesc><table><row><cell>Method</cell><cell cols="3">Blind/Non-blind Training Env. PSNR SSIM</cell></row><row><cell cols="2">CDnCNN-B [53] Blind</cell><cell>Synthetic</cell><cell>23.66 0.583</cell></row><row><cell>MLP</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Investigation of denoiser RN denoising performance according to the amount of RN dataset. The quantitative results (in average PSNR (dB)) are reported on SIDD validation dataset.</figDesc><table><row><cell></cell><cell>Num of Real Images</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>320 (full)</cell></row><row><cell></cell><cell>RIDNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.71</cell></row><row><cell></cell><cell>AINDNet(R)</cell><cell>-</cell><cell cols="7">30.36 32.19 36.94 37.70 38.14 38.66 38.70</cell><cell>38.81</cell></row><row><cell></cell><cell>AINDNet+RT</cell><cell cols="8">35.21 36.23 37.16 38.02 38.40 38.63 38.82 39.00</cell><cell>39.01</cell></row><row><cell></cell><cell>AINDNet+TF</cell><cell cols="8">35.21 36.19 37.14 37.93 38.27 38.52 38.75 38.83</cell><cell>38.90</cell></row><row><cell>(a) Noisy Image</cell><cell>(b) DnCNN-C</cell><cell></cell><cell>(c) CBDNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(d) RIDNet</cell><cell cols="4">(e) AINDNet(S) (f) AINDNet+TF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Investigation of the proposed reconstruction network when denoisers are trained with SN data. The quantitative results (in average PSNR (dB)) are reported on DND test dataset and SIDD validation dataset.</figDesc><table><row><cell>Method</cell><cell>DND SIDD</cell></row><row><cell>IN + Concat</cell><cell>38.53 34.74</cell></row><row><cell cols="2">Res-Block [24] 39.19 34.93</cell></row><row><cell>Ours</cell><cell>39.53 35.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Average PSNR of the denoised images on the SIDD validation set. 1 denotes that the number of real training noisy image is one.</figDesc><table><row><cell>Method</cell><cell>PSNR</cell></row><row><cell>RIDNet [6]</cell><cell>38.71</cell></row><row><cell>AINDNet(S)</cell><cell>35.21</cell></row><row><cell>AINDNet(AWGN)</cell><cell>26.25</cell></row><row><cell>AINDNet(R)</cell><cell>38.81</cell></row><row><cell>AINDNet(AWGN)+TF</cell><cell>38.82</cell></row><row><cell>AINDNet+TF</cell><cell>38.90</cell></row><row><cell>AINDNet(R) 1</cell><cell>30.36</cell></row><row><cell cols="2">AINDNet(AWGN)+TF 1 31.76</cell></row><row><cell>AINDNet+TF 1</cell><cell>36.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Average MAE and error STD for the images from Kodak24 where the inputs are corrupted by heteroscedastic Gaussian including in-camera pipeline.</figDesc><table><row><cell>Method</cell><cell>FCN [19]</cell><cell>Ours</cell></row><row><cell>(σ</cell><cell></cell><cell></cell></row></table><note>s , σ c )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Investigation of noise level estimator and estimation loss when denoisers are trained with SN data. The quantitative results (in average PSNR (dB)) are reported on DND test dataset and SIDD validation dataset. L asymm + L T V 39.51 34.90 Ours + L 1 + L T V 39.45 35.08 Ours + L ms-asymm 39.53 35.19</figDesc><table><row><cell>Method</cell><cell>DND SIDD</cell></row><row><cell>FCN +</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Investigation of update parameters when denoisers are transfer-learned with RN data. The quantitative results (in average PSNR (dB)) are reported on SIDD validation dataset.</figDesc><table><row><cell>Method</cell><cell>PSNR</cell></row><row><cell>Ours-AIN</cell><cell>34.60</cell></row><row><cell cols="2">Ours-Estimator 38.71</cell></row><row><cell cols="2">Ours-LastConv 38.75</cell></row><row><cell>AINDNet(S)</cell><cell>35.21</cell></row><row><cell>AINDNet+TF</cell><cell>38.90</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Noise flow: Noise modeling with conditional normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3165" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ntire 2019 challenge on real image denoising: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ksvd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Renoir-a dataset for real low-light image noise reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josue</forename><surname>Anaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="144" to="154" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real image denoising with feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with bm3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward real-world single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00523</idno>
	</analytic>
	<monogr>
		<title level="m">A new benchmark and a new model</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image blind denoising with generative adversarial network based noise modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Color image denoising via sparse 3d collaborative filtering with grouping constraint in luminancechrominance space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Suppressing model overfitting for image super-resolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Practical poissonian-gaussian noise modeling and fitting for single-image raw-data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mejdi</forename><surname>Trimeche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1737" to="1754" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">191</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shi Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise-optimal capture for high dynamic range photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Samuel W Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grdn: Grouped residual dense network for real image denoising and gan-based real-world noise modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Ryun</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptively tuning a convolutional neural network by gate process for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Woong</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Ik</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="63447" to="63456" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Agarnet: Adaptively gated jpeg compression artifacts removal network for a wide range quality factor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Woong</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Ik</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Iterative residual cnns for burst photography applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippos</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatis</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5929" to="5938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The noise clinic: a blind image denoising algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Universal denoising networks: a novel cnn architecture for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Noise2noise: Learning image restoration without clean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2971" to="2980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Universal style transfer via feature transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="386" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Practical signal-dependent noise parameter estimation from a single noisy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4361" to="4371" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Waterloo exploration database: New challenges for image quality assessment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfang</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1004" to="1016" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Noise as domain shift: Denoising medical images by unpaired image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilja</forename><surname>Manakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Rohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Schworm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kortuem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A holistic approach to cross-channel image noise modeling and its application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghyeon</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngbae</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1683" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Radiometric calibration of ccd sensors: Dark current and fixed pattern noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4730" to="4735" />
		</imprint>
	</monogr>
	<note>Proceedings. ICRA&apos;04</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Densely connected hierarchical network for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumjun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jechang</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fields of experts. International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="114" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning for general image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4091" to="4104" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhetong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02603</idno>
		<title level="m">Real-world noisy image denoising: A new benchmark</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">External prior guided internal prior learning for real-world noisy image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2996" to="3010" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A trilateral weighted sparse coding scheme for real-world image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-channel weighted nuclear norm minimization for real color image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep iterative down-up cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumjun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jechang</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for cnn-based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4608" to="4622" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3262" to="3271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">From noise modeling to blind image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
