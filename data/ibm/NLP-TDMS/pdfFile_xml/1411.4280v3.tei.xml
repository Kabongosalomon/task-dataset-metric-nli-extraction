<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Object Localization Using Convolutional Networks Figure 1: Our Model&apos;s Predicted Joint Positions on the MPII-human-pose database test-set[1]</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Object Localization Using Convolutional Networks Figure 1: Our Model&apos;s Predicted Joint Positions on the MPII-human-pose database test-set[1]</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>tompson/goroshin/ajain/lecun/bregler@cims.nyu.edu</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient 'position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPII-human-pose dataset <ref type="bibr" target="#b0">[1]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>State-of-the-art performance on the task of human-body part localization has made significant progress in recent years. This has been in part due to the success of Deep-Learning architectures -specifically Convolutional Networks (ConvNets) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5]</ref> -but also due to the availability of ever larger and more comprehensive datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20]</ref> (our model's predictions for difficult examples from <ref type="bibr" target="#b0">[1]</ref> are shown in <ref type="figure">Figure 1</ref>).</p><p>A common characteristic of all ConvNet architectures used for human body pose detection to date is that they make use of internal strided-pooling layers. These layers reduce the spatial resolution by computing a summary statistic over a local spatial region (typically a max operation in the case of the commonly used Max-Pooling layer). The main motivation behind the use of these layers is to promote invariance to local input transformations (particularly translations) since their outputs are invariant to spatial location within the pooling region. This is particularly important for image classification where local image transformations obfuscate object identity. Therefore pooling plays a vital role in preventing over-training while reducing computational complexity for classification tasks.</p><p>The spatial invariance achieved by pooling layers comes at the price of limiting spatial localization accuracy. As such, by adjusting the amount of pooling in the network, for localization tasks a trade-off is made between generalization performance, model size and spatial accuracy.</p><p>In this paper we present a ConvNet architecture for efficient localization of human skeletal joints in monocular RGB images that achieves high spatial accuracy without significant computational overhead. This model allows us to use increased amounts of pooling for computational efficiency, while retaining high spatial precision.</p><p>We begin by presenting a ConvNet architecture to perform coarse body part localization. This network outputs a low resolution, per-pixel heat-map, describing the likelihood of a joint occurring in each spatial location. We use this architecture as a platform to discuss and empirically evaluate the role of Max-pooling layers in convolutional architectures for dimensionality reduction and improving invariance to noise and local image transformations. We then present a novel network architecture that reuses hiddenlayer convolution features from the coarse heat-map regression model in order to improve localization accuracy. By jointly-training these models, we show that our model outperforms recent state-of-the-art on standard human body pose datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Following the seminal work of Felzenszwalb et al. <ref type="bibr" target="#b9">[10]</ref> on 'Deformable Part Models' (DPM) for human-body-pose estimation, many algorithms have been proposed to improve on the DPM architecture <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7]</ref>. Yang and Ramanan <ref type="bibr" target="#b22">[23]</ref> propose a mixture of templates modeled using SVMs. Johnson and Everingham <ref type="bibr" target="#b14">[15]</ref> propose more discriminative templates by using a cascade of body-part detectors.</p><p>Recently high-order DPM-based body-part dependency models have been proposed <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>. Pishchulin <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> use Poselet priors and a DPM model <ref type="bibr" target="#b2">[3]</ref> to capture spatial relationships of body-parts. In a similar work, Gkioxari et al. <ref type="bibr" target="#b10">[11]</ref> propose the Armlets approach which uses a semiglobal classifier of part configurations. Their approach exhibits good performance on real-world data, however it is demonstrated only on arms. Sapp and Taskar <ref type="bibr" target="#b19">[20]</ref> propose a multi-modal model including both holistic and local cues for coarse mode selection and pose estimation. A common characteristic to all these approaches is that they use handcrafted features (edges, contours, HoG features and color histograms), which have been shown to have poor generalization performance and discriminative power in comparison to learned features (as in this work).</p><p>Today, the best performing algorithms for many vision tasks are based on convolutional networks (ConvNets). The current state-of-the-art methods for the task of human-pose estimation in-the-wild are also built using ConvNets <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b4">5]</ref>. The model of Toshev et al. <ref type="bibr" target="#b21">[22]</ref> significantly output-performed state-of-art methods on the challenging 'FLIC' <ref type="bibr" target="#b19">[20]</ref> dataset and was competitive on the 'LSP' <ref type="bibr" target="#b15">[16]</ref> dataset. In contrast to our work, they formulate the problem as a direct (continuous) regression to joint location rather than a discrete heat-map output. However, their method performs poorly in the high-precision region and we believe that this is because the mapping from input RGB image to XY location adds unnecessary learning complexity which weakens generalization.</p><p>For example, direct regression does not deal gracefully with multi-modal outputs (where a valid joint is present in two spatial locations). Since the network is forced to produce a single output for a given regression input, the network does not have enough degrees of freedom in the output representation to afford small errors which we believe leads to over-training (since small outliers -due to for instance the presence of a valid body part -will contribute to a large error in XY).</p><p>Chen et al. <ref type="bibr" target="#b4">[5]</ref> use a ConvNet to learn a low-dimensional representation of the input image and use an image dependent spatial model and show improvement over <ref type="bibr" target="#b21">[22]</ref>. Tompson et al. <ref type="bibr" target="#b20">[21]</ref> uses a multi-resolution ConvNet architecture to perform heat-map likelihood regression which they train jointly with a graphical model network to further promote joint consistency. In similar work, Jain et al. <ref type="bibr" target="#b13">[14]</ref> also uses a multi-resolution ConvNet architecture, but they add motion features to the network input to further improve accuracy. Our Heat-Map regression model is largely inspired by both these works with improvements for better localization accuracy. The contributions of this work can be seen as an extension of the architecture of <ref type="bibr" target="#b20">[21]</ref>, where we attempt to overcome the limitations of pooling to improve the precision of the spatial locality.</p><p>In an unrelated application, Eigen et al. <ref type="bibr" target="#b7">[8]</ref> predict depth by using a cascade of coarse to fine ConvNet models. In their work the coarse model is pre-trained and the model parameters are fixed when training the fine model. By contrast, in this work we suggest a novel shared-feature architecture which enables joint training of both models to improve generalization performance and which samples a subset of the feature inputs to improve runtime performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Coarse Heat-Map Regression Model</head><p>Inspired by the work of Tompson et al. <ref type="bibr" target="#b20">[21]</ref>, we use a multi-resolution ConvNet architecture ( <ref type="figure" target="#fig_0">Figure 2</ref>) to implement a sliding window detector with overlapping contexts to produce a coarse heat-map output. Since our work is an extension of their model, we will only present a very brief overview of the architecture and explain our extensions to their model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>The coarse heat-map regression model takes as input an RGB Gaussian pyramid of 3 levels (in <ref type="figure" target="#fig_0">Figure 2</ref>   els are shown for brevity) and outputs a heat-map for each joint describing the per-pixel likelihood for that joint occurring in each output spatial location. We use an input resolution of 320x240 and 256x256 pixels for the FLIC <ref type="bibr" target="#b19">[20]</ref> and MPII <ref type="bibr" target="#b0">[1]</ref> datasets respectively. The first layer of the network is a local-contrast-normalization (LCN) layer with the same filter kernel in each of the three resolution banks.</p><p>Each LCN image is then input to a 7 stage multiresolution convolutional network (11 stages for the MPII dataset model). Due to the presence of pooling the heatmap output is at a lower resolution than the input image. It should be noted that the last 4 stages (or 3 stages for the MPII dataset model) effectively simulate a fully-connected network for a target input patch size (which is typically a much smaller context than the input image). We refer interested readers to <ref type="bibr" target="#b20">[21]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SpatialDropout</head><p>We improve the model of <ref type="bibr" target="#b20">[21]</ref> by adding an additional dropout layer before the first 1x1 convolution layer in Figure 2. The role of dropout is to improve generalization performance by preventing activations from becoming strongly correlated <ref type="bibr" target="#b11">[12]</ref>, which in turn leads to over-training. In the standard dropout implementation, network activations are "dropped-out" (by zeroing the activation for that neuron) during training with independent probability p drop . At test time all activations are used, but a gain of 1 − p drop is multiplied to the neuron activations to account for the increase in expected bias.</p><p>In initial experiments, we found that applying standard dropout (where each convolution feature map activation is "dropped-out" independently) before the 1 × 1 convolution layer generally increased training time but did not prevent over-training. Since our network is fully convolutional and natural images exhibit strong spatial correlation, the feature map activations are also strongly correlated, and in this setting standard dropout fails.</p><p>Standard dropout at the output of a 1D convolution is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. The top two rows of pixels represent the convolution kernels for feature maps 1 and 2, and the bottom row represents the output features of the previous layer. During back-propagation, the center pixel of the W 2 kernel receives gradient contributions from both f 2a and f 2b as the convolution kernel W 2 is translated over the input feature F 2 . In this example f 2b was randomly dropped out (so the activation was set to zero) while f 2a was not.</p><p>Since F 2 and F 1 are the output of a convolution layer we expect f 2a and f 2b to be strongly correlated: i.e. f 2a ≈ f 2b and de /df2a ≈ de /df 2b (where e is the error function to minimize). While the gradient contribution from f 2b is zero, the strongly correlated f 2a gradient remains. In essence, the effective learning rate is scaled by the dropout probability p, but independence is not enhanced. Instead we formulate a new dropout method which we call SpatialDropout. For a given convolution feature tensor of size n feats ×height×width, we perform only n feats dropout trials and extend the dropout value across the entire feature map. Therefore, adjacent pixels in the dropped-out feature map are either all 0 (dropped-out) or all active as illustrated in <ref type="figure">Figure 5</ref>. We have found this modified dropout implementation improves performance, especially on the FLIC dataset, where the training set size is small.  </p><formula xml:id="formula_0">F 2 F 1 W 2Pos1 W 2Pos2 W 1Pos1 W 1Pos2 f 2a f 2b f 1a f 1b ( = dropped-out) de/df 1b de/df 1a de/df 2b de/df 2a</formula><formula xml:id="formula_1">F 2 F 1 W 2Pos1 W 2Pos2 W 1Pos1 W 1Pos2 f 2a f 2b f 1a f 1b de/df 1b de/df 1a de/df 2b de/df 2a (</formula><p>= dropped-out) <ref type="figure">Figure 5</ref>: SpatialDropout after a 1D convolution layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Data Augmentation</head><p>We train the model in <ref type="figure" target="#fig_0">Figure 2</ref> by minimizing the Mean-Squared-Error (MSE) distance of our predicted heat-map to a target heat-map. The target is a 2D Gaussian of constant variance (σ ≈ 1.5 pixels) centered at the ground-truth (x, y) joint location. The objective function is:</p><formula xml:id="formula_2">E 1 = 1 N N j=1 xy H j (x, y) − H j (x, y) 2<label>(1)</label></formula><p>Where H j and H j are the predicted and ground truth heat-maps respectively for the jth joint.</p><p>During training, each input image is randomly rotated (r ∈ [−20 • , +20 • ]), scaled (s ∈ [0.5, 1.5]) and flipped (with probability 0.5) in order to improve generalization performance on the validation-set. Note that this follows the same training protocol as in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Many images contain multiple people while only a single person is annotated. To enable inference of the target person's annotations at test time, both the FLIC and MPII datasets include an approximate torso position. Since our sliding-window detector will detect all joint instances in a single frame indiscriminately, we incorporate this torso information by implementing the MRF-based spatial model of Tompson et al. <ref type="bibr" target="#b20">[21]</ref>, which formulates a tree-structured MRF over spatial locations with a random variable for each joint. The most likely joint locations are inferred (using message passing) given the noisy input distributions from the ConvNet. The ground-truth torso location is concatenated with the 14 predicted joints from the ConvNet output and these 15 joints locations are then input to the MRF. In this setup, the MRF inference step will learn to attenuate the joint activations from people for which the ground-truth torso is not anatomically viable, thus "selecting" the correct person for labeling. Interested readers should refer to <ref type="bibr" target="#b19">[20]</ref> for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Fine Heat-Map Regression Model</head><p>In essence, the goal of this work is to recover the spatial accuracy lost due to pooling of the model in Section 3.1 by using an additional ConvNet to refine the localization result of the coarse heat-map. However, unlike a standard cascade of models, as in the work of Toshev et al. <ref type="bibr" target="#b21">[22]</ref>, we reuse existing convolution features. This not only reduces the number of trainable parameters in the cascade, but also acts as a regularizer for the coarse heat-map model since the coarse and fine models are trained jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Architecture</head><p>The full system architecture is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. It consists of the heat-map-based parts model from Section 3.1 for coarse localization, a module to sample and crop the convolution features at a specified (x, y) location for each joint, as well as an additional convolutional model for fine tuning.</p><p>Joint inference from an input image is as follows: we forward-propagate (FPROP) through the coarse heat-map model then infer all joint (x, y) locations from the maximal value in each joint's heat-map. We then use this coarse (x, y) location to sample and crop the first 2 convolution layers (for all resolution banks) at each of the joint locations. We then FPROP these features through a fine heatmap model to produce a (∆x, ∆y) offset within the cropped sub-window. Finally, we add the position refinement to the coarse location to produce a final (x, y) localization for each joint. <ref type="figure" target="#fig_4">Figure 6</ref> shows the crop module functionality for a single joint. We simply crop out a window centered at the coarse joint (x, y) location in each resolution feature map, however we do so by keeping the contextual size of the window constant by scaling the cropped area at each higher resolution level. Note that back-propagation (BPROP) through this module from output feature to input feature is trivial; output gradients from the cropped image are simply added to the output gradients of the convolution stages in the coarse heat-map model at the sampled pixel locations. The fine heat-map model is a Siamese network [4] of 7 instances (14 for the MPII dataset), where the weights and biases of each module are shared (i.e. replicated across all instances and updated together during BPROP). Since the sample location for each joint is different, the convolution features do not share the same spatial context and so the convolutional sub-networks must be applied to each joint independently. However, we use parameter sharing amongst each of the 7 instances to substantially reduce the number of shared parameters and to prevent over-training. At the output of each of the 7 sub-networks we then perform a 1x1 Convolution, with no weight sharing to output a detailed-resolution heat-map for each joint. The purpose of this last layer is to perform the final detection for each joint.  Note we are potentially performing redundant computations in the Siamese network. If two cropped sub-windows overlap and since the convolutional weights are shared, the same convolution maybe applied multiple times to the same spatial locations. However, we have found in practice this is rare. Joints are infrequently co-located, and the spatial context size is chosen such that there is little overlap between cropped sub-regions (note that the context of the cropped images shown in <ref type="figure" target="#fig_3">Figures 4 and 8</ref> are exaggerated for clarity).</p><p>Each instance of the sub-network in <ref type="figure" target="#fig_6">Figure 7</ref> is a Con-vNet of 4 layers, as shown in <ref type="figure" target="#fig_7">Figure 8</ref>. Since the input images are different resolutions and originate from varying depths in the coarse heat-map model, we treat the input features as separate resolution banks and apply a similar architecture strategy as used in Section 3.1. That is we apply the same size convolutions to each bank, upscale the lowerresolution features to bring them into canonical resolution, add the activations across feature maps then apply 1x1 convolutions to the output features. It should be noted that this cascaded architecture can be extended further as is possible to have multiple cascade levels each with less and less pooling. However, in practice we have found that a single layer provides sufficient accuracy, and in particular within the level of label noise on the FLIC dataset (as we show in Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Joint Training</head><p>Before joint training, we first pre-train the coarse heatmap model of Section 3.1 by minimizing Eq 1. We then hold the parameters of the coarse model fixed and train the fine heat-map model of Section 4.1 by minimizing:</p><formula xml:id="formula_3">E 2 = 1 N N j=1 x,y G j (x, y) − G j (x, y) 2<label>(2)</label></formula><p>Where G and G are the set of predicted and ground truth heat-maps respectively for the fine heat-map model. Finally, we jointly train both models by minimizing E 3 = E 1 + λE 2 . Where λ is a constant used to trade-off the relative importance of both sub-tasks. We treat λ as another network hyper-parameter and is chosen to optimize performance over our validation set (we use λ = 0.1). Ideally, a more direct optimization function would attempt to measure the argmax of both heat-maps and therefore directly minimize the final (x, y) prediction. However, since the argmax function is not differentiable we instead reformulate the problem as a regression to a set of target heat-maps and minimize the distance to those heat-maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Our ConvNet architecture was implemented within the Torch7 <ref type="bibr" target="#b5">[6]</ref> framework and evaluation is performed on the FLIC <ref type="bibr" target="#b19">[20]</ref> and MPII-Human-Pose <ref type="bibr" target="#b0">[1]</ref>   <ref type="bibr" target="#b19">[20]</ref> and we use the PCKh measure of <ref type="bibr" target="#b0">[1]</ref> for evaluation on the MPII dataset. <ref type="figure" target="#fig_9">Figure 9</ref> shows the PCK test-set performance of our coarse heat-map model (Section 3.1) when various amounts of pooling are used within the network (keeping the number of convolution features constant). <ref type="figure" target="#fig_9">Figure 9</ref> results show quite clearly the expected effect of coarse quantization in (x, y) and therefore the impact of pooling on spatial precision; when more pooling is used the performance of detections within small distance thresholds is reduced.  For joints where the ground-truth label is ambiguous and difficult for the human mechanical-turkers to label, we do not expect our cascaded network to do better than the expected variance in the user-generated labels. To measure this variance (and thus estimate the upper bound of performance) we performed the following informal experiment:  we showed 13 users 10 random images from the FLIC training set with annotated ground-truth labels as a reference so that the users could familiarize themselves with the desired anatomical location of each joint. The users then annotated a consistent set of 10 random images from the FLIC test-set for the face, left-wrist, left-shoulder and left-elbow joints. <ref type="figure">Figure 10</ref> shows the resultant joint annotations for 2 of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 10: User generated joint annotations</head><p>To estimate joint annotation noise we calculate the standard deviation (σ) across user annotations in x for each of the 10 images separately and then average the σ across the 10 sample images to obtain an aggregate σ for each joint. Since we down-sample the FLIC images by a factor of 2 for use with our model we divide the σ values by the same down-sample ratio. The result is shown in <ref type="table" target="#tab_3">Table 1</ref>.</p><p>The histogram of the coarse heat-map model pixel error (in the x dimension) on the FLIC test-set when using an 8x internal pooling is shown in <ref type="figure" target="#fig_10">Figure 11a</ref> (for the face and shoulder joints). For demonstration purposes, we quote the error in the pixel coordinates of the input image to the network (which for FLIC is 360 × 240), not the original resolution. As expected, in these coordinates there is an approximately uniform uncertainty due to quantization of the heat-map within -4 to +4 pixels. In contrast to this, the histogram of the cascaded network is shown in <ref type="figure" target="#fig_10">Figure 11b</ref> and is close to the measured label noise 1 .</p><p>PCK performance on FLIC for face and wrist are shown in <ref type="figure" target="#fig_0">Figures 12a and 12b</ref> respectively. For the face, the per-     The performance improvement for wrist is also significant but only for the 8× and 16× pooling models. Our empirical experiments suggest that wrist detection (as one of the hardest to detect joints) requires learning features with a large amount of spatial context. This is because the wrist joint undergoes larger amounts of skeletal deformation than the shoulder or face, and typically has high input variability due to clothing and wrist accessories. Therefore, with limited convolution sizes and sampling context in the fine heat-map regression network, the cascaded network does not improve wrist accuracy beyond the coarse approximation.  To evaluate the effectiveness of the use of shared features for our cascaded network we trained a fine heat-map model (shown in <ref type="figure" target="#fig_1">Figure 13</ref>) that takes a cropped version of the input image as it's input rather than the first and second layer convolution feature maps of our coarse heat-map model. This comparison model is a greedily-trained cascade, where the coarse and fine models are trained independently. Additionally, since the network in <ref type="figure" target="#fig_3">Figure 4</ref> has a higher capacity than the comparison model, we add an additional convolution layer such that the number of trainable parameters is the same. <ref type="figure" target="#fig_3">Figure 14a</ref> shows that our 4x pooling network outperforms this comparison model on the wrist joint (we see similar performance gains for other joints not shown). We attribute this to the regularizing effect of joint training; the fine heat-map model term in the objective function prevents over-training of the coarse model and vice-versa.  We also show our model's performance with and without SpatialDropout for the wrist joint in <ref type="figure" target="#fig_3">Figure 14b</ref>. As expected we see significant perform gains in the high normalized distance region due to the regularizing effect of our  <ref type="table">Table 3</ref>: Comparison with prior-art on FLIC (PCK @ 0.05) dropout implementation and the reduction in strong heatmap outliers. <ref type="figure" target="#fig_17">Figure 15</ref> compares our detector's PCK performance averaged for the wrist and elbow joints with previous work. Our model outperforms the previous state-of-the-art results by Tompson et al. <ref type="bibr" target="#b20">[21]</ref> for large distances, due to our use of SpatialDropout. In the high precision region the cascaded network is able to out-perform all state-of-the-art by a significant margin. The PCK performance at a normalized distance of 0.05 for each joint is shown in <ref type="table">Table 3</ref>. Finally, <ref type="figure" target="#fig_4">Figure 16</ref> shows the PCKh performance of our model on the MPII human pose dataset. Similarity, <ref type="table" target="#tab_8">table 4</ref> shows a comparison of the PCKh performance of our model and previous state-of-the-art at a normalized distance of 0.5. Our model out-performs all existing methods by a considerable margin.</p><p>Since the MPII dataset provides the subject scale at testtime, in standard evaluation practice the query image is scale normalized so that the average person height is constant, thus making the detection task easier. For practical applications, a query image is run through the detector at multiple scales and typically some form of non-maximum suppression is used to aggregate activations across the resultant heat-maps. An alternative is to train the ConvNet at     <ref type="figure" target="#fig_4">Figure 16</ref> and table 4 we show the performance of our model trained on the original dataset scale (unnormalized); we show performance of this model on both the normalized and unnormalized test set. As expected, performance is degraded as the detection problem is harder. However, surprisingly this model also out performs state-of-the-art, showing that the ConvNet is able to learn some scale invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Though originally developed for the task of classification <ref type="bibr" target="#b16">[17]</ref>, Deep Convolutional Networks have been successfully applied to a multitude of other problems. In classification all variability except the object identity is suppressed. On the other hand, localization tasks such as human body pose estimation often demand a high degree of spatial precision. In this work we have shown that the precision lost due to pooling in traditional ConvNet architectures can be recovered efficiently while maintaining the computational benefits of pooling. We presented a novel cascaded architecture that combined fine and coarse scale convolutional networks, which achieved new state-of-the-art results on the FLIC <ref type="bibr" target="#b19">[20]</ref> and MPII-human-pose <ref type="bibr" target="#b0">[1]</ref> datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Multi-resolution Sliding Window Detector With Overlapping Contexts (model used on FLIC dataset)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Standard Dropout after a 1D convolution layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Overview of our Cascaded Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Crop module functionality for a single joint</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Fine heat-map model: 14 joint Siamese network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>The fine heat-map network for a single joint</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Pooling impact on FLIC test-set Average Joint Accuracy for the coarse heat-map model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Histogram of X error on FLIC test-set 4x pool 8x pool 16x pool</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Performance improvement from cascaded model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Standard cascade architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>FLIC wrist performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :</head><label>15</label><figDesc>FLIC -average PCK for wrist and elbow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>− scale norm this work 4X − scale norm (test) this work 4X − unnormalized Pishchulin et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 16 :</head><label>16</label><figDesc>MPII -average PCKh for all joints</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>datasets. The FLIC dataset consists of 3,987 training examples and 1,016 test examples of still scenes from Hollywood movies annotated with upper-body joint labels. Since the poses are predominantly front-facing and upright, FLIC is considered to be less challenging than more recent datasets. However the small number of training examples makes the dataset a good indicator for generalization performance. On the other-hand the MPII dataset is very challenging and it includes a wide variety of full-body pose annotations within the 28,821 training and 11,701 test examples. For evaluation of our model on the FLIC dataset we use the standard PCK measure proposed by</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>σ of (x, y) pixel annotations on FLIC test-set images (at 360 × 240 resolution)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">Forward-Propagation time (milli seconds) for each</cell></row><row><cell cols="6">of our FLIC trained models</cell></row><row><cell cols="6">formance improvement is significant, especially for the 8×</cell></row><row><cell cols="6">and 16× pooling part models. The FPROP time for a single</cell></row><row><cell cols="6">image (using an Nvidia-K40 GPU) for each of our models is</cell></row><row><cell cols="6">shown in Table 2; using the 8× pooling cascaded network,</cell></row><row><cell cols="6">we are able to perform close to the level of label noise with</cell></row><row><cell cols="6">a significant improvement in computation time over the 4×</cell></row><row><cell cols="2">network.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Detection rate, %</cell><cell>30 40 50 60 70 80 20 10</cell><cell></cell><cell></cell><cell cols="2">4x pool 4x pool cascade 8x pool 8x pool cascade 16x pool 16x pool cascade</cell></row><row><cell></cell><cell>0 0</cell><cell>0.05</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell></row><row><cell></cell><cell></cell><cell cols="3">Normalized distance</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparison with prior-art: MPII (PCKh @ 0.5) the original query image scale (which varies widely across the test and training sets) and thus learning scale invariance in the detection stage. This allows us to run the detector at a single scale at test time, making it more suitable for realtime applications. In</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When calculating σ for our model, we remove all outliers with error &gt; 20 and error &lt; −20. These outliers represent samples where our weak spatial model chose the wrong person's joint and so do not represent an accurate indication of the spatial accuracy of our model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This research was funded in part by Google and by the Office of Naval Research ONR Award N000141210327. We would also like the thank all the contributors to Torch7 <ref type="bibr" target="#b5">[6]</ref>, particularly Soumith Chintala, for all their hard work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>David Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Better appearance models for pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Articulated pose estimation using discriminative armlet classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning human pose estimation features with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeep: A deep learning framework using motion features for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Effective Human Pose Estimation from Inaccurate Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Join training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
