<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LUVLi Face Alignment: Estimating Landmarks&apos; Location, Uncertainty, and Visibility Likelihood</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
							<email>abhinav3663@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
							<email>tmarks@merl.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Labs (MERL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Mou</surname></persName>
							<email>wenxuanmou@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Manchester</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Labs (MERL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Labs (MERL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Labs (MERL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Koike-Akino</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Labs (MERL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
							<email>cfeng@nyu.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LUVLi Face Alignment: Estimating Landmarks&apos; Location, Uncertainty, and Visibility Likelihood</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern face alignment methods have become quite accurate at predicting the locations of facial landmarks, but they do not typically estimate the uncertainty of their predicted locations nor predict whether landmarks are visible. In this paper, we present a novel framework for jointly predicting landmark locations, associated uncertainties of these predicted locations, and landmark visibilities. We model these as mixed random variables and estimate them using a deep network trained with our proposed Location, Uncertainty, and Visibility Likelihood (LUVLi) loss. In addition, we release an entirely new labeling of a large face alignment dataset with over 19,000 face images in a full range of head poses. Each face is manually labeled with the ground-truth locations of 68 landmarks, with the additional information of whether each landmark is unoccluded, self-occluded (due to extreme head poses), or externally occluded. Not only does our joint estimation yield accurate estimates of the uncertainty of predicted landmark locations, but it also yields state-of-the-art estimates for the landmark locations themselves on multiple standard face alignment datasets. Our method's estimates of the uncertainty of predicted landmark locations could be used to automatically identify input images on which face alignment fails, which can be critical for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Uncertainty Estimation in Neural Networks</head><p>Uncertainty estimation broadly uses two types of approaches [46]: sampling-based and sampling-free. Sampling-based methods include Bayesian neural networks [67], Monte Carlo dropout <ref type="bibr" target="#b28">[29]</ref>, and bootstrap ensembles <ref type="bibr" target="#b44">[45]</ref>. They rely on multiple evaluations of the input to estimate uncertainty <ref type="bibr" target="#b45">[46]</ref>, and bootstrap ensembles also need to store several sets of weights <ref type="bibr" target="#b36">[37]</ref>. Thus, samplingbased methods work for small 1D regression problems but might not be feasible for higher-dimensional problems <ref type="bibr" target="#b36">[37]</ref>.</p><p>Sampling-free methods produce two outputs, one for the estimate and the other for the uncertainty, and optimize Gaussian log-likelihood (GLL) instead of classification and regression losses <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. <ref type="bibr" target="#b44">[45]</ref> combines the benefits of sampling-free and sampling-based methods.</p><p>Recent object detection methods have used uncertainty estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53]</ref>. Sampling-free methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> jointly estimate the four parameters of the bounding box using Gaussian log-likelihood [47], Laplacian log-likelihood [46], or both [35]. However, these methods assume the four parameters of the bounding box are independent (assume a diagonal covariance matrix). Sampling-based approaches use Monte Carlo dropout [53] and network ensembles [45] for object detection. Uncertainty estimation has also been applied to pixelwise depth regression [41], optical flow [37], pedestrian detection [5, 6, 54] and 3D vehicle detection [26].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern methods for face alignment (facial landmark localization) perform quite well most of the time, but all of them fail some percentage of the time. Unfortunately, almost all of the state-of-the-art (SOTA) methods simply output predicted landmark locations, with no assessment of whether (or how much) downstream tasks should trust these landmark locations. This is concerning, as face alignment is a key pre-processing step in numerous safety-critical ap- * Equal Contributions <ref type="figure">Figure 1</ref>: Results of our joint face alignment and uncertainty estimation on three test images. Ground-truth (green) and predicted (yellow) landmark locations are shown. The estimated uncertainty of the predicted location of each landmark is shown in blue (Error ellipse for Mahalanobis distance 1). Landmarks that are occluded (e.g., by the hand in center image) tend to have larger uncertainty. plications, including advanced driver assistance systems (ADAS), driver monitoring, and remote measurement of vital signs <ref type="bibr" target="#b56">[57]</ref>. As deep neural networks are notorious for producing overconfident predictions <ref type="bibr" target="#b32">[33]</ref>, similar concerns have been raised for other neural network technologies <ref type="bibr" target="#b45">[46]</ref>, and they become even more acute in the era of adversarial machine learning where adversarial images may pose a great threat to a system <ref type="bibr" target="#b13">[14]</ref>. However, previous work in face alignment (and landmark localization in general) has largely ignored the area of uncertainty estimation.</p><p>To address this need, we propose a method to jointly estimate facial landmark locations and a parametric probability distribution representing the uncertainty of each estimated location. Our model also jointly estimates the visibility of landmarks, which predicts whether each landmark is occluded due to extreme head pose.</p><p>We find that the choice of methods for calculating mean and covariance is crucial. Landmark locations are best obtained using heatmaps, rather than by direct regression. To estimate landmark locations in a differentiable manner using heatmaps, we do not select the location of the maximum (argmax) of each landmark's heatmap, but instead propose to use the spatial mean of the positive elements of each heatmap. Unlike landmark locations, uncertainty distribution parameters are best obtained by direct regression rather than from heatmaps. To estimate the uncertainty of the predicted locations, we add a Cholesky Estimator Network (CEN) branch to estimate the covariance matrix of a multivariate Gaussian or Laplacian probability distribution. To estimate visibility of each landmark, we add a Visibility Estimator Network (VEN). We combine these estimates using a joint loss function that we call the Location, Uncertainty and Visibility Likelihood (LUVLi) loss. Our primary goal in designing this model was to estimate uncertainty in landmark localization. In the process, not only does our method yields accurate uncertainty estimation, but it also produces SOTA landmark localization results on several face alignment datasets.</p><p>Uncertainty can be broadly classified into two categories <ref type="bibr" target="#b40">[41]</ref>: epistemic uncertainty is related to a lack of knowledge about the model that generated the observed data, and aleatoric uncertainty is related to the noise inherent in the observations, e.g., sensor or labelling noise. The ground-truth landmark locations marked on an image by human labelers would vary across multiple labelings of an image by different human labelers (or even by the same human labeler). Furthermore, this variation will itself vary across different images and landmarks (e.g., it will vary more for occluded landmarks and poorly lit images). The goal of our method is to estimate this aleatoric uncertainty.</p><p>The fact that each image only has one ground-truth labeled location per landmark makes estimating this uncertainty distribution difficult, but not impossible. To do so, we use a parametric model for the uncertainty distribution. We train a neural network to estimate the parameters of the model for each landmark of each input face image so as to maximize the likelihood under the model of the groundtruth location of that landmark (summed across all landmarks of all training faces).</p><p>The main contributions of this work are as follows:</p><p>• This is the first work to introduce the concept of parametric uncertainty estimation for face alignment. • We propose an end-to-end trainable model for the joint estimation of landmark location, uncertainty, and visibility likelihood (LUVLi), modeled as a mixed random variable.</p><p>• We compare our model using multivariate Gaussian and multivariate Laplacian probability distributions. • Our algorithm yields accurate uncertainty estimation and state-of-the-art landmark localization results on several face alignment datasets. • We are releasing a new dataset with manual labels of the locations of 68 landmarks on over 19,000 face images in a wide variety of poses, where each landmark is also labeled with one of three visibility categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Face Alignment</head><p>Early methods for face alignment were based on Active Shape Models (ASM) and Active Appearance Models (AAM) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b77">78]</ref> as well as their variations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62]</ref>. Subsequently, direct regression methods became popular due to their excellent performance. Of these, tree-based regression methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b75">76]</ref> proved particularly fast, and the subsequent cascaded regression methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b82">83]</ref> improved accuracy.</p><p>Recent approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b87">88]</ref> are all based on deep learning and can be classified into two subcategories: direct regression <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b72">73]</ref> and heatmap-based approaches. The SOTA deep methods, e.g., stacked hourglass networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b83">84]</ref> and densely connected U-nets (DU-Net) <ref type="bibr" target="#b71">[72]</ref>, use a cascade of deep networks, originally developed for human body 2D pose estimation <ref type="bibr" target="#b54">[55]</ref>. These models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref> are trained using the 2 distance between the predicted heatmap for each landmark and a proxy ground-truth heatmap that is generated by placing a symmetric Gaussian distribution with small fixed variance at the ground-truth landmark location. <ref type="bibr" target="#b47">[48]</ref> uses a larger variance for early hourglasses and a smaller variance for later hourglasses. <ref type="bibr" target="#b78">[79]</ref> employs different variations of MSE for different pixels of the proxy ground-truth heatmap. Recent works also infer facial boundary maps to improve alignment <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b80">81]</ref>. In heatmap-based methods, landmarks are estimated by the argmax of each predicted heatmap. Indirect inference through a predicted heatmap offers several advantages over direct prediction <ref type="bibr" target="#b3">[4]</ref>.</p><p>Disadvantages of Heatmap-Based Approaches. These heatmap-based methods have at least two disadvantages. First, since the goal of training is to mimic a proxy groundtruth heatmap containing a fixed symmetric Gaussian, the predicted heatmaps are poorly suited to uncertainty prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Second, they suffer from quantization errors since the heatmap's argmax is only determined to the nearest pixel <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b69">70]</ref>. To achieve sub-pixel localization for body pose estimation, <ref type="bibr" target="#b50">[51]</ref> replaces the argmax with a spatial mean over the softmax. Alternatively, for sub-pixel localization in videos, <ref type="bibr" target="#b69">[70]</ref> samples two additional points adjacent to the max of the heatmap to estimate a local peak.</p><p>Landmark Regression with Uncertainty. We have only found two other methods that estimate uncertainty of landmark regression, both developed concurrently with our approach. The first method <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> estimates face alignment uncertainty using a non-parametric approach: a kernel density network obtained by convolving the heatmaps with a fixed symmetric Gaussian kernel. The second <ref type="bibr" target="#b31">[32]</ref> performs body pose estimation with uncertainty using direct regression method (no heatmaps) to directly predict the mean and precision matrix of a Gaussian distribution. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of our LUVLi Face Alignment. The input RGB face image is passed through a DU-Net <ref type="bibr" target="#b71">[72]</ref> architecture, to which we add three additional components branching from each U-net. The first new component is a mean estimator, which computes the estimated location of each landmark as the weighted spatial mean of the positive elements of the corresponding heatmap. The second and the third new component, the Cholesky Estimator Network (CEN) and the Visibility Estimator Network (VEN), emerge from the bottleneck layer of each U-net. CEN and VEN weights are shared across all U-nets. The CEN estimates the Cholesky coefficients of the covariance matrix for each landmark location. The VEN estimates the probability of visibility of each landmark in the image, 1 meaning visible and 0 meaning not visible. For each U-net i and each landmark j, the landmark's location estimate µ ij , estimated covariance matrix Σ ij , and estimated visibility v ij are tied together by the LUVLi loss function L ij , which enables end-to-end optimization of the entire framework.</p><p>Rather than the argmax of the heatmap, we choose a mean estimator for the heatmap that is differentiable and enables sub-pixel accuracy: the weighted spatial mean of the heatmap's positive elements. Unlike the non-parametric model of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, our uncertainty prediction method is para- metric: we directly estimate the parameters of a single multivariate Laplacian or Gaussian distribution. Furthermore, our method does not constrain the Laplacian or Gaussian covariance matrix to be diagonal.</p><formula xml:id="formula_0">CEN VEN LUVLi CEN VEN vij Hij Lij LUVLi CEN VEN LUVLi L ij L T ij Mean Estimator v ij L ij H ij L ij = −(1 − v j ) ln(1 − v ij ) − v j ln( v ij ) − v j ln(P(p j |µ ij , Σ ij )) Predictions v ij µ ij = [µ ijx , µ ijy ] T Σ ij µ ij Σ ij p j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mean Estimator</head><p>Let H ij (x, y) denote the value at pixel location (x, y) of the jth landmark's heatmap from the ith U-net. The landmark's location estimate µ ij = [µ ijx , µ ijy ] T is given by first post-processing the pixels of the heatmap H ij with a function σ, then taking the weighted spatial mean of the result (See <ref type="bibr" target="#b15">(16)</ref> in the supplementary material). We considered three different functions for σ: the ReLU function (eliminates the negative values), the softmax function (makes the mean estimator a soft-argmax of the heatmap <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b84">85]</ref>), and a temperature-controlled softmax function (which, depending on the temperature setting, provides a continuum of softmax functions that range from a "hard" argmax to the uniform distribution). The ablation studies (Section 5.5) show that choosing σ to be the ReLU function yields the simplest and best mean estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LUVLi Loss</head><p>Occluded landmarks, e.g., landmarks on the far side of a profile-pose face, are common in real data. To explicitly represent visibility, we model the probability distributions of landmark locations using mixed random vari-ables. For each landmark j in an image, we denote the ground-truth (labeled) visibility by the binary variable v j ∈ {0, 1}, where 1 denotes visible, and the ground-truth location by p j . By convention, if the landmark is not visible (v j = 0), then p j = ∅, a special symbol indicating non-existence. Together, these variables are distributed according to an unknown distribution p(v j , p j ). The marginal Bernoulli distribution p(v j ) captures the probability of visibility, p(p j |v j = 1) denotes the distribution of the landmark location when it is visible, and p(p j |v j = 0) = 1 ∅ (p j ), where 1 ∅ denotes the PMF that assigns probability one to the symbol ∅.</p><p>After each U-net i, we estimate the joint distribution of the visibility v and location z of each landmark j via</p><formula xml:id="formula_1">q(v, z) = q v (v)q z (z|v), (1) where q v (v) is a Bernoulli distribution with q v (v = 1) = v ij , q v (v = 0) = 1 − v ij ,<label>(2)</label></formula><p>where v ij is the predicted probability of visibility, and q z (z|v = 1) = P(z|µ ij , Σ ij ),</p><formula xml:id="formula_2">(3) q z (z|v = 0) = ∅,<label>(4)</label></formula><p>where P denotes the likelihood of the landmark being at location z given the estimated mean µ ij and covariance Σ ij .</p><p>The LUVLi loss is the negative log-likelihood with respect to q(v, z), as given by</p><formula xml:id="formula_3">L ij = − ln q(v j , p j ) = − ln q v (v j ) − ln q z (p j |v j ) = − (1 − v j ) ln(1 − v ij ) − v j ln( v ij ) − v j ln P(p j |µ ij , Σ ij ) ,<label>(5)</label></formula><p>and thus minimizing the loss is equivalent to maximum likelihood estimation.</p><p>The terms of (5) are a binary cross entropy plus v j times the negative log-likelihood of p j with respect to P. This can be seen as an instance of multi-task learning <ref type="bibr" target="#b10">[11]</ref>, since we are predicting three things about each landmark: its location, uncertainty, and visibility. The first two terms on the right hand side of (5) can be seen as a classification loss for visibility, while the last term corresponds to a regression loss of location estimation. The sum of classification and regression losses is also widely used in object detection <ref type="bibr" target="#b38">[39]</ref>.</p><p>Minimization of negative log-likelihood also corresponds to minimizing KL-divergence, since <ref type="bibr" target="#b6">(7)</ref> where expectations are with respect to (v j , p j ) ∼ p(v j , p j ), and the entropy term E[− ln p(v j , p j )] is constant with respect to the estimate q(v j , p j ). Further, since</p><formula xml:id="formula_4">E[− ln q(v j , p j )] = E ln p(v j , p j ) q(v j , p j ) − ln p(v j , p j ) (6) = D KL (p(v j , p j ) q(v j , p j )) + E[− ln p(v j , p j )],</formula><formula xml:id="formula_5">E[− ln q(v j , p j )] = E vj ∼p(vj ) [− ln q(v j )] + p v E pj ∼p(pj |vj =1) [− ln P(p j |µ ij , Σ ij )],<label>(8)</label></formula><p>where p v := p(v j = 1) for brevity, minimizing the negative log-likelihood (LUVLi loss) is also equivalent to minimizing the combination of KL-divergences given by</p><formula xml:id="formula_6">D KL p(v j ) q(v) + p v D KL p(p j |v j =1) P(z|µ ij ,Σ ij ) (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Models for Location Likelihood</head><p>For the multivariate location distribution P, we consider two different models: Gaussian and Laplacian. Gaussian Likelihood. The 2D Gaussian likelihood is:</p><formula xml:id="formula_7">P(z|µ ij ,Σ ij ) = exp − 1 2 (z−µ ij ) T Σ −1 ij (z−µ ij ) 2π |Σ ij | .<label>(10)</label></formula><p>Substituting <ref type="formula" target="#formula_7">(10)</ref> into <ref type="formula" target="#formula_3">(5)</ref>, we have</p><formula xml:id="formula_8">L ij = −(1−v j ) ln(1− v ij )− v j ln( v ij ) +v j 1 2 log |Σ ij | T1 + v j 1 2 (p j − µ ij ) T Σ −1 ij (p j −µ ij ) T2 .<label>(11)</label></formula><p>In <ref type="formula" target="#formula_8">(11)</ref>, T 2 is the squared Mahalanobis distance, while T 1 serves as a regularization or prior term that ensures that the Gaussian uncertainty distribution does not get too large. Laplacian Likelihood. We use a 2D Laplacian likelihood <ref type="bibr" target="#b42">[43]</ref> given by:</p><formula xml:id="formula_9">P (z|µ ij ,Σ ij ) = e − 3(z−µij ) T Σ −1 ij (z−µij ) 2π 3 |Σ ij | .<label>(12)</label></formula><p>Substituting <ref type="formula" target="#formula_1">(12)</ref> in <ref type="formula" target="#formula_3">(5)</ref>, we have</p><formula xml:id="formula_10">L ij = −(1−v j ) ln(1− v ij )− v j ln( v ij ) + v j 1 2 log |Σ ij | T1 + v j 3(p j −µ ij ) T Σ −1 ij (p j −µ ij ) T2 .<label>(13)</label></formula><p>In <ref type="formula" target="#formula_10">(13)</ref>, T 2 is a scaled Mahalanobis distance, while T 1 serves as a regularization or prior term that ensures that the Laplacian uncertainty distribution does not get too large. Note that if Σ ij is the identity matrix and if all landmarks are assumed to be visible, then (11) simply reduces to the squared 2 distance, and (13) just minimizes the 2 distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Uncertainty and Visibility Estimation</head><p>Our proposed method uses heatmaps for estimating landmarks' locations, but not for estimating their uncertainty and visibility. We experimented with several methods for computing a covariance matrix directly from a heatmap, but none were accurate enough. We discuss this in Section 5.1.</p><p>Cholesky Estimator Network (CEN). We represent the uncertainty of each landmark location using a 2 × 2 covariance matrix Σ ij , which is symmetric positive definite. The three degrees of freedom of Σ ij are captured by its Cholesky decomposition: a lower-triangular matrix L ij such that Σ ij = L ij L T ij . To estimate the elements of L ij , we append a Cholesky Estimator Network (CEN) to the bottleneck of each U-net. The CEN is a fully connected linear layer whose input is the bottleneck of the U-net (128×4×4 = 2,048 dimensions) and output is an N p ×3dimensional vector, where N p is the number of landmarks (e.g., 68). As the Cholesky decomposition L ij of a covariance matrix must have positive diagonal elements, we pass the corresponding entries of the output through an ELU activation function <ref type="bibr" target="#b14">[15]</ref>, to which we add a constant to ensure the output is always positive (asymptote is negative x-axis).</p><p>Visibility Estimator Network (VEN). To estimate the visibility of the landmark v e , we add another fully connected linear layer whose input is the bottleneck of the Unet (128×4×4 = 2,048 dimensions) and output is an N pdimensional vector. This is passed through a sigmoid activation so the predicted visibility v ij is between 0 and 1.</p><p>The addition of these two fully connected layers only slightly increases the size of the original model. The loss for a single U-net is the averaged L ij across all the landmarks j = 1, ..., N p , and the total loss L for each input image is a weighted sum of the losses of all K of the U-nets:</p><formula xml:id="formula_11">L = K i=1 λ i L i , where L i = 1 N p Np j=1 L ij .<label>(14)</label></formula><p>At test time, each landmark's mean and Cholesky coefficients are derived from the Kth (final) U-net. The covariance matrix is calculated from the Cholesky coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">New Dataset: MERL-RAV</head><p>To promote future research in face alignment with uncertainty, we now introduce a new dataset with entirely new, manual labels of over 19,000 face images from the AFLW <ref type="bibr" target="#b41">[42]</ref> dataset. In addition to landmark locations, every landmark is labeled with one of three visibility classes. We call the new dataset MERL Reannotation of AFLW with Visibility (MERL-RAV).</p><p>Visibility Classification. Each landmark of every face is classified as either unoccluded, self-occluded, or externally occluded, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. Unoccluded denotes landmarks that can be seen directly in the image, with no obstructions. Self-occluded denotes landmarks that are occluded because of extreme head pose-they are occluded by another part of the face (e.g., landmarks on the far side of a profile-view face). Externally occluded denotes landmarks that are occluded by hair or an intervening object such as a cap, hand, microphone, or goggles. Human labelers are generally very bad at localizing self-occluded landmarks, so we do not provide ground-truth locations for these. We do provide ground-truth (labeled) locations for both unoccluded and externally occluded landmarks.</p><p>Relationship to Visibility in LUVLi. In Section 3, visible landmarks (v j = 1) are landmarks for which groundtruth location information is available, while invisible landmarks (v j = 0) are landmarks for which no ground-truth location information is available (p j = ∅). Thus, invisible (v j = 0) in the model is equivalent to the self-occluded landmarks in our dataset. In contrast, both unoccluded and externally occluded landmarks are considered visible (v j = 1) in our model. We choose this because human labelers are generally good at estimating the locations of externally occluded landmarks but poor at estimating the locations of self-occluded landmarks.</p><p>Existing Datasets. The most commonly used publicly available datasets for evaluation of 2D face alignment are summarized in <ref type="table" target="#tab_0">Table 1</ref>. The 300-W dataset [63-65] uses a 68-landmark system that was originally used for Multi-PIE <ref type="bibr" target="#b30">[31]</ref>. Menpo 2D [21, 74, 86] makes a hard distinction (denoted F/P) between nearly frontal faces (F) and profile faces (P). Menpo 2D uses the same landmarks as 300-W for frontal faces, but for profile faces it uses a different set of 39 landmarks that do not all correspond to the 68 landmarks in the frontal images. 300W-LP-2D <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b89">90]</ref> is a synthetic dataset created by automatically reposing 300-W faces, so it has a large number of labels, but they are noisy. The 3D model locations of self-occluded landmarks are projected onto the visible part of the face as if the face were transparent (denoted by T). The WFLW <ref type="bibr" target="#b80">[81]</ref> and AFLW-68 <ref type="bibr" target="#b58">[59]</ref> datasets do not identify which landmarks are self-occluded, but instead label self-occluded landmarks as if they were located on the visible boundary of the noseless face.</p><p>Differences from Existing Datasets. Our MERL-RAV dataset is the only one that labels every landmark using both types of occlusion (self-occlusion and external occlusion). Only one other dataset, AFLW, indicates which individual landmarks are self-occluded, but it has far fewer landmarks and does not label external occlusions. COFW and COFW-68 indicate which landmarks are externally occluded but do not have self-occlusions. Menpo 2D categorizes faces as frontal or profile, but landmarks of the two classes are incompatible. Unlike Menpo 2D, our dataset smoothly transitions from frontal to profile, with gradually more and more landmarks labeled as self-occluded.</p><p>Our dataset uses the widely adopted 68 landmarks used by 300-W, to allow for evaluation and cross-dataset comparison. Since it uses images from AFLW, our dataset has pose variation up to ±120 • yaw and ±90 • pitch. Focusing on yaw, we group the images into five pose classes: frontal,   left and right half-profile, and left and right profile. The train/test split is in the ratio of 4 : 1. <ref type="table" target="#tab_2">Table 2</ref> provides the statistics of our MERL-RAV dataset. A sample image from the dataset is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. In the figure, unoccluded landmarks are green, externally occluded landmarks are red, and self-occluded landmarks are indicated by black circles in the face schematic on the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our experiments use the datasets 300-W [63-65], 300W-LP-2D <ref type="bibr" target="#b89">[90]</ref>, Menpo 2D <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b85">86]</ref>, COFW-68 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>, AFLW-19 <ref type="bibr" target="#b41">[42]</ref>, WFLW <ref type="bibr" target="#b80">[81]</ref>, and our MERL-RAV dataset. Training and testing protocols are described in the supplementary material. On a 12 GB GeForce GTX Titan-X GPU, the inference time per image is 17 ms.</p><p>Evaluation Metrics. We use the standard metrics NME, AUC, and FR <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b78">79]</ref>. In each table, we report results using the same metric adopted in respective baselines.</p><p>Normalized Mean Error (NME). The NME is defined as:</p><formula xml:id="formula_12">NME (%) = 1 N p Np j=1 v j p j − µ Kj 2 d × 100,<label>(15)</label></formula><p>where v j , p j and µ Kj respectively denote the visibility, ground-truth and predicted location of landmark j from the Kth (final) U-net. The factor of v j is there because we cannot compute an error value for points without ground-truth location labels. Several variations of the normalizing term d are used. NME box <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b85">86]</ref> sets d to the geometric mean of the width and height of the ground-truth bounding box √ w bbox · h bbox , while NME inter-ocular <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b71">72]</ref> sets d to the distance between the outer corners of the two eyes. If a ground-truth box is not provided, the tight bounding box of the landmarks is used <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. NME diag <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b80">81]</ref> sets d as the diagonal of the bounding box.</p><p>Area Under the Curve (AUC). To compute the AUC, the cumulative distribution of the fraction of test images whose NME (%) is less than or equal to the value on the horizontal axis is first plotted. The AUC for a test set is then computed as the area under that curve, up to the cutoff NME value.</p><p>Failure Rate (FR). FR refers to the percentage of images in the test set whose NME is larger than a certain threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">300-W Face Alignment</head><p>We train on the 300-W <ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref><ref type="bibr" target="#b64">[65]</ref>, and test on 300-W, Menpo 2D <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b85">86]</ref>, and COFW-68 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>. Some of the  models are pre-trained on the 300W-LP-2D <ref type="bibr" target="#b89">[90]</ref>. Data Splits and Evaluation Metrics. There are two commonly used train/test splits for 300-W; we evaluate our method on both. Split 1: The train set contains 3,148 images and full test set has 689 images <ref type="bibr" target="#b71">[72]</ref>. Split 2: The train set includes 3,837 images and test set has 600 images <ref type="bibr" target="#b13">[14]</ref>. The model trained on Split 2 is additionally evaluated on the 6,679 near-frontal training images of Menpo 2D and 507 test images of COFW-68 <ref type="bibr" target="#b13">[14]</ref>. For Split 1, we use NME inter-ocular <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b78">79]</ref>. For Split 2, we use NME box and AUC box with 7% cutoff <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Results: Localization and Cross-Dataset Evaluation. The face alignment results for 300-W Split 1 and Split 2 are summarized in <ref type="table" target="#tab_3">Table 3</ref> and 4, respectively. <ref type="table" target="#tab_4">Table 4</ref> also shows the results of our model (trained on Split 2) on the Menpo and COFW-68 datasets, as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>. The results in <ref type="table" target="#tab_3">Table 3</ref> show that our LUVLi landmark localization is competitive with the SOTA methods on Split 1, usually one of the best two. <ref type="table" target="#tab_4">Table 4</ref> shows that LUVLi significantly outperforms the SOTA on Split 2, performing best on 5 out of the 6 cases (3 datasets × 2 metrics). This is particularly impressive on 300-W Split 2, because even though most of the other methods are pre-trained on the 300W-LP-2D dataset (as was our best method, LUVLi*), our method without pretraining still outperforms the SOTA in 2 of 6 cases. Our method performs particularly well in the cross-dataset evaluation on the more challenging COFW-68 dataset, which has multiple externally occluded landmarks.  Accuracy of Predicted Uncertainty. To evaluate the accuracy of the predicted uncertainty covariance matrix,</p><formula xml:id="formula_13">Σ Kj = Σ Kj xx Σ Kj xy Σ Kj xy Σ Kj yy</formula><p>, we compare all three unique terms of this prediction with the statistics of the residuals (2D error between the ground-truth location p j and the predicted location µ Kj ) of all landmarks in the test set. We explain how we do this for Σ Kj xx in <ref type="figure" target="#fig_3">Figure 4a</ref>. First, we bin every landmark of every test image according to the value of the predicted variance in the x-direction Σ Kj xx . Each bin is represented by one point in the scatter plot. Averaging Σ Kj xx across the N bin = 734 landmark points within each bin gives a single predicted Σ Kj xx value (horizontal axis). We next compute the residuals in the x-direction of all landmarks in the bin, and calculate the average of the squared residuals to obtain Σ xx = E(p jx −µ Kj x ) 2 for the bin. This mean squared residual error, Σ xx , is plotted on the vertical axis. If our predicted uncertainties are accurate, this residual error, Σ xx , should be roughly equal to the predicted uncertainty variance in the x-direction (horizontal axis). <ref type="figure" target="#fig_3">Figure 4</ref> shows that all three terms of our method's predicted covariance matrices are highly predictive of the actual uncertainty: the mean squared residuals (error) are strongly proportional to the predicted covariance values, as evidenced by Pearson correlation coefficients of 0.98 and 0.99. However, decreasing N bin from 734 (plotted in <ref type="figure" target="#fig_3">Figure 4)</ref> to just 36 makes the correlation coefficients decrease to 0.84, 0.80, 0.72. Thus, the predicted uncertainties are excellent after averaging but may yet have room to improve.</p><p>Uncertainty is Larger for Occluded Landmarks. The COFW-68 <ref type="bibr" target="#b29">[30]</ref> test set annotates which landmarks are externally occluded. Similar to <ref type="bibr" target="#b13">[14]</ref>, we use this to test uncertainty predictions of our model, where the square root of the determinant of the uncertainty covariance is a scalar measure of predicted uncertainty. We report the error, NME box , and average predicted uncertainty, |Σ Kj | 1/2 , in <ref type="table" target="#tab_5">Table 5</ref>. We do not use any occlusion annotation from the dataset during training. Like <ref type="bibr" target="#b13">[14]</ref>, we find that our model's predicted uncertainty is much larger for externally occluded landmarks than for unoccluded landmarks. Furthermore, our method's location estimates are more accurate (smaller NME box ) than those of <ref type="bibr" target="#b13">[14]</ref> for both occluded and unoccluded landmarks.</p><p>Heatmaps vs. Direct Regression for Uncertainty. We tried multiple approaches to estimate the uncertainty dis-   tribution from heatmaps, but none of these worked nearly as well as our direct regression using the CEN. We believe this is because in current heatmap-based networks, the resolution of the heatmap (64 × 64) is too low for accurate uncertainty estimation. This is demonstrated in <ref type="figure" target="#fig_4">Figure 5</ref>, which shows a histogram over all landmarks in 300-W Test (Split 2) of LUVLi's predicted covariance in the narrowest direction of the covariance ellipse (the smallest eigenvalue of the predicted covariance matrix). The figure shows that in most cases, the uncertainty ellipses are less wide than one heatmap pixel, which explains why heatmap-based methods are not able to accurately capture such small uncertainties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">AFLW-19 Face Alignment</head><p>On AFLW-19, we train on 20,000 images, and test on two sets: the AFLW-Full set (4,386 test images) and the AFLW-Frontal set (1,314 test images), as in <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b88">89]</ref>. <ref type="table" target="#tab_6">Table 6</ref> compares our method's localization performance with other methods that only train on AFLW-19 (without training on any 68-landmark dataset). Our proposed method outperforms not only the other uncertainty-based method KDN <ref type="bibr" target="#b13">[14]</ref>, but also all previous SOTA methods, by a significant margin on both AFLW-Full and AFLW-Frontal.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">WFLW Face Alignment</head><p>Landmark localization results for WFLW are shown in <ref type="table" target="#tab_7">Table 7</ref>. More detailed results on WFLW are in the supplementary material. Compared to the SOTA methods, LUVLi yields the second best performance on all metrics. Furthermore, while the other methods only predict landmark locations, LUVLi also estimates the prediction uncertainties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">MERL-RAV Face Alignment</head><p>Results of Landmark Localization. Results for all head poses on our MERL-RAV dataset are shown in <ref type="table" target="#tab_8">Table 8</ref>.</p><p>Results for All Visibility Classes. We analyze LUVLi's performance on all test images for all three types of landmarks in <ref type="table" target="#tab_9">Table 9</ref>. The first row is the mean value of the predicted visibility, v j , for each type of landmark. Accuracy (Visible) tests the accuracy of predicting that landmarks are visible when v j &gt; 0.5. The last two rows show the scalar measure of uncertainty, |Σ Kj | 1/2 , both unnormalized and normalized by the face box size |Σ| 0.5 box similar to NME box . Similar to results on COFW-68 in <ref type="table" target="#tab_5">Table 5</ref>, the model predicts higher uncertainty for locations of externally occluded landmarks than for unoccluded landmarks. <ref type="table" target="#tab_0">Table 10</ref> compares modifications of our approach on Split 2. <ref type="table" target="#tab_0">Table 10</ref> shows that computing the loss only on the last U-net performs worse than computing loss on all U-nets, perhaps because of the vanishing gradient problem <ref type="bibr" target="#b79">[80]</ref>. Moreover, LUVLi's log-likelihood loss without visibility outperforms using MSE loss on the landmark lo- cations (which is equivalent to setting all Σ ij = I). We also find that the loss with Laplacian likelihood (13) outperforms the one with Gaussian likelihood <ref type="bibr" target="#b10">(11)</ref>. Training from scratch is slightly inferior to first training the base DU-Net architecture before fine-tuning the full LUVLi network, consistent with previous observations that the model does not have strongly supervised pixel-wise gradients through the heatmap during training <ref type="bibr" target="#b55">[56]</ref>. Regarding the method for estimating the mean, using heatmaps is more effective than direct regression (Direct) from each U-net bottleneck, consistent with previous observations that neural networks have difficulty predicting continuous real values <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b55">56]</ref>. As described in Section 3.1, in addition to ReLU, we compared two other functions for σ: softmax, and a temperaturescaled softmax (τ -softmax). Results for temperature-scaled softmax and ReLU are essentially tied, but the former is more complicated and requires tuning a temperature parameter, so we chose ReLU for our LUVLi model. Finally, reducing the number of U-nets from 8 to 4 increases test speed by about 2× with minimal decrease in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we present LUVLi, a novel end-to-end trainable framework for jointly estimating facial landmark locations, uncertainty, and visibility. This joint estimation not only provides accurate uncertainty predictions, but also yields state-of-the-art estimates of the landmark locations on several datasets. We show that the predicted uncertainty distinguishes between unoccluded and externally occluded landmarks without any supervision for that task. In addition, the model achieves sub-pixel accuracy by taking the spatial mean of the ReLU'ed heatmap, rather than the arg max. We also introduce a new dataset containing manual labels of over 19,000 face images with 68 landmarks, which also labels every landmark with one of three visibility classes. Although our implementation is based on the DU-Net architecture, our framework is general enough to be applied to a variety of architectures for simultaneous estimation of landmark location, uncertainty, and visibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LUVLi Face Alignment: Estimating Landmarks' Location, Uncertainty, and Visibility Likelihood</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Implementation Details</head><p>Images are cropped using the detector bounding boxes provided by the dataset and resized to 256 × 256. Images with no detector bounding box are initialized by adding 5% uniform noise to the location of each edge of the tight bounding box around the landmarks, as in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Training. We modified the PyTorch <ref type="bibr" target="#b57">[58]</ref> code for DU-Net <ref type="bibr" target="#b71">[72]</ref>, keeping the number of U-nets K = 8 as in <ref type="bibr" target="#b71">[72]</ref>. Unless otherwise stated, we use the 2D Laplacian likelihood (12) as our landmark location likelihood, and therefore we use (13) as our final loss function. All U-nets have equal weights λ i = 1 in <ref type="bibr" target="#b13">(14)</ref>. For all datasets, visibility v j = 1 is assigned to unoccluded landmarks (those that are not labeled as occluded) and to landmarks that are labeled as externally occluded. Visibility v j = 0 is assigned to landmarks that are labeled as self-occluded and landmarks whose labels are missing.</p><p>Training images for 300-W Split 1 are augmented randomly using scaling (0.75 − 1.25), rotation (−30 • , −30 • ) and color jittering (0.6, 1.4) as in <ref type="bibr" target="#b71">[72]</ref>, while those from 300-W Split 2, AFLW-19, WFLW-98 and MERL-RAV datasets are augmented randomly using scaling (0.8 − 1.2), rotation (−50 • , 50 • ), color jittering (0.6, 1.4), and random occlusion, as in <ref type="bibr" target="#b6">[7]</ref>.</p><p>The RMSprop optimizer is used as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b71">72]</ref>, with batch size 24. Training from scratch takes 100 epochs and starts with learning rate 2.5 × 10 −4 , which is divided by 5, 2, and 2 at epochs 30, 60, and 90 respectively <ref type="bibr" target="#b71">[72]</ref>. When we initialize from pretrained weights, we finetune for 50 epochs using the LUVLi loss: 20 with learning rate 10 −4 , followed by 30 with learning rate 2×10 −5 . We consider the model saved in the last epoch as our final model.</p><p>Testing. Whereas heatmap based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b71">72]</ref> adjust their pixel output with a quarter-pixel offset in the direction from the highest response to the second highest response, we use the spatial mean as the landmark location without carrying out any adjustment nor shifting the heatmap even by a quarter of a pixel. We do not need to implement a sub-pixel shift, because our spatial mean over the ReLUed heatmaps already performs sub-pixel location prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Mean</head><p>The spatial mean µ ij of each of the heatmap is defined as</p><formula xml:id="formula_14">µ ij = µ ijx µ ijy = x,y σ H ij (x, y) x y x,y σ H ij (x, y)<label>(16)</label></formula><p>where σ H ij (x, y) denotes the output of post-processing the heatmap pixel with a function σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2. Additional Experiments and Results</head><p>We now provide additional results evaluating our system's performance in terms of both localization and uncertainty estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1. System Trained on 300-W A2.1.1 Training</head><p>For Split 1, we initialized using the pre-trained DU-Net model available from the authors of <ref type="bibr" target="#b71">[72]</ref>, then fine-tuned on the 300-W training set (Split 1) using our proposed architecture and LUVLi loss. For Split 2, for the experiments in which we pre-trained on 300W-LP-2D, we pre-trained from scratch on 300W-LP-2D using heatmaps (using the original DU-Net architecture and loss). We then fine-tuned on the 300-W training set (Split 2) using our proposed architecture and LUVLi loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1.2 Comparison with KDN [13]</head><p>To compare directly with Chen et al. <ref type="bibr" target="#b12">[13]</ref>, in <ref type="figure" target="#fig_5">Figure 6</ref> we plot normalized mean error (NME) vs. predicted uncertainty (rank, from smallest to largest), as in <ref type="figure">Figure 1</ref> of <ref type="bibr" target="#b12">[13]</ref>. (We obtained the predicted uncertainty and NME data of <ref type="bibr" target="#b12">[13]</ref> from the authors.) The figure shows that for our method as well as for <ref type="bibr" target="#b12">[13]</ref>, there is a strong trend that higher predicted uncertainties correspond to larger location errors. However, the errors of our method are significantly smaller than the errors produced by <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1.3 Verifying Predicted Uncertainty Distributions</head><p>For every image, for each landmark j, our network predicts a mean µ Kj and a covariance matrix Σ Kj . We can view this as our network predicting that a human labeler of that image will effectively select the landmark location p j for  that image from the Laplacian distribution from (12) with mean µ Kj and covariance Σ Kj :</p><formula xml:id="formula_15">p j ∼ P (z|µ Kj ,Σ Kj ) = e − 3(z−µ Kj ) T Σ −1 Kj (z−µ Kj ) 2π 3 |Σ Kj | .</formula><p>(17) If we had multiple labels (e.g., ground-truth landmark locations from multiple human labelers) for a single landmark in one image, then it would be straightforward to evaluate how well our method's predicted probability distribution matches the distribution of labeled landmark locations. Unfortunately, face alignment datasets only have a single ground-truth location for each landmark in each image. This makes it difficult, but not impossible, to evaluate how well the human labels for images in the test set fit our method's predicted uncertainty distributions. We propose the following method for verifying the predicted probability distributions.</p><p>Suppose we transform the ground-truth location of a landmark, p j , using the predicted mean and covariance for that landmark as follows:</p><formula xml:id="formula_16">p j = Σ −0.5 Kj (p j − µ Kj ).<label>(18)</label></formula><p>If our method's predictions are correct, then from <ref type="formula">(17)</ref>,</p><formula xml:id="formula_17">p j ∼ P (z|µ Kj ,Σ Kj ). Hence, p j is drawn from the trans- formed distribution P (z ), where z = Σ −0.5 Kj (z − µ Kj ): p j ∼ P (z |0, I) = e − √ 3z T z 2π/3 .<label>(19)</label></formula><p>After this simple transformation (transforming the labeled ground-truth location p j of each landmark using its predicted mean and covariance), we have transformed our network's prediction about p j into a prediction about p j that is much easier to evaluate, because the distribution in <ref type="formula" target="#formula_17">(19)</ref> is simply a standard 2D Laplacian distribution-it no longer depends on the predicted mean and covariance.</p><p>Thus, our method predicts that after the transformation <ref type="bibr" target="#b17">(18)</ref>, every ground-truth landmark location p j is drawn from the same standard 2D Laplacian distribution <ref type="bibr" target="#b18">(19)</ref>. Now that we have an entire population of transformed labels that our model predicts are all drawn from the same distribution, it is easy to verify whether the labels fit our model's predictions. <ref type="figure" target="#fig_6">Figure 7</ref> shows a scatter plot of the transformed locations, p j , for all landmarks in all test images of 300-W (Split 2). We plot the histogram of the marginalized landmark locations (x-or y-coordinate of p j ) in orange above and to the right of the plot, and overlay the marginal pdf of the standard Laplacian <ref type="bibr" target="#b18">(19)</ref> in black. The excellent match between the transformed landmark locations and the standard Laplacian distribution indicates that our model's predicted uncertainty distributions are quite accurate. Since Kullback-Leibler (KL) divergence is invariant to affine transformations like the one in (18), we can evaluate the KL-divergence (printed at the top of the scatterplot) between the standard 2D Laplacian distribution <ref type="bibr" target="#b18">(19)</ref> and the distribution of the transformed landmark locations (using their 2D histograms) as a numerical measure of how well the predictions of our model fit the distribution of labeled locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1.4 Relationship to Variation Among Human Labelers on Multi-PIE</head><p>We test our Split 2 model on 812 frontal face images of all subjects from the Multi-PIE dataset <ref type="bibr" target="#b30">[31]</ref>, then compute the mean of the uncertainty ellipses predicted by our model across all 812 images. To compute the mean, we first normalize the location of each landmark using the inter-ocular distance, as in <ref type="bibr" target="#b62">[63]</ref>, and also normalize the covariance matrix by the square of the inter-ocular distance. We then take the average of the normalized locations across all faces to obtain the mean landmark location. The covariance matrices are averaged across all faces using the log-meanexponential technique. The mean location and covariance matrix of each landmark (averaged across all faces) is then used to plot the results which are shown on the right in <ref type="figure" target="#fig_7">Figure 8</ref>. We compare our model predictions with <ref type="figure" target="#fig_4">Figure 5</ref> of <ref type="bibr" target="#b62">[63]</ref>, shown on the left of <ref type="figure" target="#fig_7">Figure 8</ref>. To create that figure, <ref type="bibr" target="#b62">[63]</ref> tasked three different human labelers with annotating the same frontal face images from the Multi-PIE database of 80 different subjects in frontal pose with neutral expression. For each landmark, they plotted the the covariance of the label locations across the three labelers using an ellipse. Note the similarity between our model's predicted uncertainties (on the right of <ref type="figure" target="#fig_7">Figure 8</ref> and the covariance across human labelers (on the left of <ref type="figure" target="#fig_7">Figure 8</ref>), especially around the eyes, nose, and mouth. Around the outside edge of the face, note that our model predicts that label locations will vary primarily in the direction parallel to the edge, which is precisely the pattern observed across human labelers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1.5 Sample Uncertainty Ellipses on Multi-PIE</head><p>To illustrate how the predicted uncertainties output by our method vary across different subjects from Multi-PIE, in <ref type="figure" target="#fig_8">Figure 9</ref> we overlay our model's mean uncertainty predictions (in blue, copied from right side of <ref type="figure" target="#fig_7">Figure 8</ref>) with our model's predicted uncertainties of some of the individual Multi-PIE face images (in various colors). To simplify the figure, we plot all landmarks except for the eyes, nose, and mouth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1.6 Laplacian vs. Gaussian Likelihood</head><p>We have described two versions of our model: one whose loss function (13) uses a 2D Laplacian probability distribution <ref type="bibr" target="#b11">(12)</ref>, and another whose loss function (11) uses a 2D Gaussian probability distribution <ref type="bibr" target="#b9">(10)</ref>. We now discuss the question of which of these two models performs better.</p><p>The numerical comparisons are shown in <ref type="table" target="#tab_0">Table 11</ref>. The numbers in the first two columns of the table were also presented in the ablation studies <ref type="table" target="#tab_0">table, Table 10</ref>.</p><p>Comparing the Predicted Locations. If we consider only the errors of the predicted landmark locations, the first two columns of <ref type="table" target="#tab_0">Table 11</ref> show that the Laplacian model is slightly better: The Laplacian model has a smaller value of NME box and a larger value of AUC 7 box . Comparing the Predicted Uncertainties. To compare the two models' predicted uncertainties as well as their predicted locations, we consider the probability distributions over landmark locations that are predicted by each model. We want to know which model's predicted probability distributions better explain the ground-truth locations of the landmarks in the test images. In other words, we want to know which model assigns a higher likelihood to the ground-truth landmark locations (i.e., which model yields a lower negative log-likelihood on the test data). We compute the negative log-likelihood of the ground-truth locations p j from the last hourglass using (13) for the Lapla-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2. WFLW Face Alignment</head><p>Data Splits and Implementation Details. The training set consists of 7,500 images, while the test set consists of 2,500 images. In <ref type="table" target="#tab_0">Table 13</ref>, we report results on the entire test set (All), which we also reported in <ref type="table" target="#tab_7">Table 7</ref>. In <ref type="table" target="#tab_0">Table 13</ref>, we additionally report results on several subsets of the test set: large head pose (326 images), facial expression (314 images), illumination (698 images), make-up (206 images), occlusion (736 images), and blur (773 images). The images are cropped using the detector bounding boxes provided by <ref type="bibr" target="#b67">[68]</ref> and resized to 256 × 256.</p><p>We first train the images with the heatmaps on proxy ground-truth heatmaps, then finetune using our proposed LUVLi loss. NME inter-ocular , AUC 10 inter-ocular , and FR 10</p><p>inter-ocular are used as evaluation metrics, as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b78">79]</ref>. We report AUC and FR with cutoff 10% as in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b78">79]</ref>.</p><p>Results of Facial Landmark Localization. <ref type="table" target="#tab_0">Table 13</ref> compares our method's landmark localization results with those of other state-of-the-art methods on the WFLW dataset. Our method performs performs in the top two methods on all the metrics. Importantly, all of the other methods only predict landmark locations-they do not predict the uncertainty of their estimated landmark locations. Not only does our method place in the top two on all three landmark localization metrics, but our method also accurately predicts its own uncertainty of landmark localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.3. MERL-RAV Face Alignment</head><p>We next define a modified version of the evaluation metric NME that may be more appropriate for face images with extreme head pose. Whereas NME as defined in <ref type="bibr" target="#b14">(15)</ref> divides by the total number of landmarks N p , the modified NME instead divides by the number of visible landmarks. This metric, which we call NME vis , computes the mean across only the visible (unoccluded and externally occluded) landmarks:</p><formula xml:id="formula_18">NME vis (%) = 1 j v j Np j=1 v j p j − µ Kj 2 d × 100,<label>(20)</label></formula><p>If all of the facial landmarks are visible, then this reduces to our previous definition of NME <ref type="bibr" target="#b14">(15)</ref>.</p><p>We define NME vis box as the special case of NME vis in which the normalization d is set to the geometric mean of the width and height of the ground-truth bounding box √ w bbox · h bbox , as in NME box <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b85">86]</ref>. Results for all head poses on MERL-RAV dataset using the metric NME vis box are shown in <ref type="table" target="#tab_0">Table 12</ref>. We also repeat the NME box numbers from <ref type="table" target="#tab_8">Table 8</ref>. Clearly, the NME vis box and NME box numbers are very close for the frontal subsets but are different for half-profile and profile subsets. This is because half-profile and (especially) profile face images have fewer visible landmarks (more self-occluded landmarks), which causes the denominator in <ref type="bibr" target="#b19">(20)</ref> to be smaller for these images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.4. Additional Qualitative Results</head><p>In <ref type="figure">Figure 10</ref>, we show example results on images from four datasets on which we tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.5. Video Demo of LUVLi</head><p>We include a short demo video of our LUVLi model that was trained on our new MERL-RAV dataset. The video demonstrates our method's ability to predict landmarks' visibility (i.e., whether they are self-occluded) as well as their locations and uncertainty. We take a simple face video of a person turning his head from frontal to profile pose and run our method on each frame independently. Overlaid on each frame of video, we plot each estimated landmark location in yellow, and plot the predicted uncertainty as a blue ellipse. To indicate the predicted visibility of each landmark, we modulate the transparency of the landmark (of the yellow dot and blue ellipse). Landmarks whose predicted visibility is close to 1 are shown as fully opaque, while landmarks whose predicted visibility is close to zero are fully transparent (are not shown). Landmarks with intermediate predicted visibilities are shown as partially transparent.</p><p>In the video, notice that as the face approaches the profile pose, points on the far edge of the face begin to disappear, because the method correctly predicts that they are not visible (are self-occluded) when the face is in profile pose. <ref type="table" target="#tab_0">Table 13</ref>: NME inter-ocular and AUC 10 inter-ocular comparison between our proposed method and the state-of-the-art landmark localization methods on the WFLW dataset.</p><p>[Key: Best, Second best; (w/DA) = uses more data; (w/B) = uses boundary; (↓) = smaller is better; (↑) = larger is better] A2.6. Examples from our MERL-RAV Dataset <ref type="figure">Figure 11</ref> shows several sample images from our MERL-RAV dataset. The ground-truth labels are overlaid on the images. On each image, unoccluded landmarks are shown in green, externally occluded landmarks are shown in red, and self-occluded landmarks are indicated by black circles in the face schematic to the right of the image. <ref type="figure">Figure 10</ref>: Results of our LUVLi face alignment on example face images from four face datasets. Top row: 300-W. Second row: AFLW-19. Third row: WFLW. Bottom row: MERL-RAV. Ground-truth (green) and predicted (yellow) landmark locations are shown. The estimated uncertainty of the predicted location of each landmark is shown in blue (Error ellipse for Mahalanobis distance 1). In the MERL-RAV images (bottom row), the predicted visibility of each landmark controls its transparency. In particular, the predicted locations of landmarks with predicted visibility close to zero (such the points on the far side of the profile face in the third image of the bottom row) are 100% transparent (not shown). <ref type="figure">Figure 11</ref>: Sample images from our MERL-RAV dataset with unoccluded landmarks shown in green, externally occluded landmarks shown in red, and self-occluded landmarks indicated by black circles in the face schematic on the right of each image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our LUVLi method. From each Unet of a DU-Net, we append a shared Cholesky Estimator Network (CEN) and Visibility Estimator Network (VEN) to the bottleneck layer and apply a mean estimator to the heatmap. The figure shows the joint estimation of location, uncertainty, and visibility of the landmarks performed for each U-net i and landmark j. The landmark has groundtruth (labeled) location p j and visibility v j ∈ {0, 1}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Unoccluded, externally occluded, and self-occluded landmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) variance of x (b) variance of y (c) covariance of x, y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Mean squared residual error vs. predicted covariance matrix for all landmarks in 300-W Test (Split 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Histogram of the smallest eigenvalue of Σ Kj .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Average NME vs sorted uncertainty, averaged across landmarks in an image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Scatter plot of transformed ground-truth locations, p j = Σ −0.5 Kj (p j − µ Kj ), for 300-W Test (Split 2). The histograms (orange) of their x and y coordinates are very close to the the marginal pdf (black curves) of the Standard Laplacian distribution P (z |0, I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Variation across three human labelers<ref type="bibr" target="#b62">[63]</ref> (left) versus uncertainties computed by our proposed method on frontal images of Multi-PIE dataset (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Our model's uncertainty predictions for some individual frontal face images from the Multi-PIE dataset (various colors), overlaid with the mean uncertainty predictions across all frontal Multi-PIE faces (blue, copied fromFigure 8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of face alignment datasets. [Key: Self Occ= Self-Occlusions, Ext Occ= External Occlusions]</figDesc><table><row><cell>Dataset</cell><cell cols="4">#train #test #marks Profile Self Ext</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Images Occ Occ</cell></row><row><cell>COFW [8]</cell><cell cols="2">1,345 507</cell><cell>29</cell><cell></cell></row><row><cell>COFW-68 [30]</cell><cell>-</cell><cell>507</cell><cell>68</cell><cell></cell></row><row><cell>300-W [63-65]</cell><cell cols="2">3,837 600</cell><cell>68</cell><cell></cell></row><row><cell cols="4">Menpo 2D [21, 74, 86] 7,564 7,281 68/39</cell><cell>F/P</cell></row><row><cell>300W-LP-2D [90]</cell><cell>61,225</cell><cell>-</cell><cell>68</cell><cell>T</cell></row><row><cell>WFLW [81]</cell><cell cols="2">7,500 2,500</cell><cell>98</cell><cell></cell></row><row><cell>AFLW [42]</cell><cell cols="2">20,000 4,386</cell><cell>21</cell><cell></cell></row><row><cell>AFLW-19 [89]</cell><cell cols="2">20,000 4,386</cell><cell>19</cell><cell></cell></row><row><cell>AFLW-68 [59]</cell><cell cols="2">20,000 4,386</cell><cell>68</cell><cell></cell></row><row><cell>MERL-RAV (Ours)</cell><cell cols="2">15,449 3,865</cell><cell>68</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of our new dataset for face alignment.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>NME inter-ocular on 300-W Common, Challenge, and Full datasets (Split 1). [Key: Best, Second best]</figDesc><table><row><cell></cell><cell cols="3">NME inter-ocular (%)(↓)</cell></row><row><cell></cell><cell cols="2">Common Challenge</cell><cell>Full</cell></row><row><cell>SAN [23]</cell><cell>3.34</cell><cell>6.60</cell><cell>3.98</cell></row><row><cell>AVS [59]</cell><cell>3.21</cell><cell>6.49</cell><cell>3.86</cell></row><row><cell>DAN [44]</cell><cell>3.19</cell><cell>5.24</cell><cell>3.59</cell></row><row><cell>LAB (w/B) [81]</cell><cell>2.98</cell><cell>5.19</cell><cell>3.49</cell></row><row><cell>Teacher [24]</cell><cell>2.91</cell><cell>5.91</cell><cell>3.49</cell></row><row><cell>DU-Net (Public code) [72]</cell><cell>2.97</cell><cell>5.53</cell><cell>3.47</cell></row><row><cell>DeCaFa (More data) [20]</cell><cell>2.93</cell><cell>5.26</cell><cell>3.39</cell></row><row><cell>HR-Net [68]</cell><cell>2.87</cell><cell>5.15</cell><cell>3.32</cell></row><row><cell>HG-HSLE [91]</cell><cell>2.85</cell><cell>5.03</cell><cell>3.28</cell></row><row><cell>AWing [79]</cell><cell>2.72</cell><cell>4.52</cell><cell>3.07</cell></row><row><cell>LUVLi (Ours)</cell><cell>2.76</cell><cell>5.16</cell><cell>3.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>NME box and AUC 7 box comparisons on 300-W Test (Split 2), Menpo 2D and COFW-68 datasets.[Key: Best, Second best, * = Pretrained on 300W-LP-2D]</figDesc><table><row><cell></cell><cell cols="3">NME box (%) (↓)</cell><cell cols="3">AUC 7 box (%) (↑)</cell></row><row><cell></cell><cell cols="6">300-W Menpo COFW 300-W Menpo COFW</cell></row><row><cell cols="2">SAN* [23] in [14] 2.86</cell><cell>2.95</cell><cell>3.50</cell><cell>59.7</cell><cell>61.9</cell><cell>51.9</cell></row><row><cell>2D-FAN* [7]</cell><cell>2.32</cell><cell>2.16</cell><cell>2.95</cell><cell>66.5</cell><cell>69.0</cell><cell>57.5</cell></row><row><cell>KDN [13]</cell><cell>2.49</cell><cell>2.26</cell><cell>-</cell><cell>67.3</cell><cell>68.2</cell><cell>-</cell></row><row><cell>Softlabel* [14]</cell><cell>2.32</cell><cell>2.27</cell><cell>2.92</cell><cell>66.6</cell><cell>67.4</cell><cell>57.9</cell></row><row><cell>KDN* [14]</cell><cell cols="3">2.21 2.01 2.73</cell><cell cols="3">68.3 71.1 60.1</cell></row><row><cell>LUVLi (Ours)</cell><cell>2.24</cell><cell>2.18</cell><cell>2.75</cell><cell cols="2">68.3 70.1</cell><cell>60.8</cell></row><row><cell cols="4">LUVLi* (Ours) 2.10 2.04 2.57</cell><cell cols="3">70.2 71.9 63.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>NME box and uncertainty |Σ Kj | 1/2 on unoccluded and externally occluded landmarks of COFW-68 dataset. [Key: Best] Externally Occluded NME box |Σ| 1/2 NME box |Σ| 1/2</figDesc><table><row><cell></cell><cell cols="2">Unoccluded</cell><cell></cell><cell></cell></row><row><cell>Softlabel [14]</cell><cell>2.30</cell><cell>5.99</cell><cell>5.01</cell><cell>7.32</cell></row><row><cell>KDN [14]</cell><cell>2.34</cell><cell>1.63</cell><cell>4.03</cell><cell>11.62</cell></row><row><cell cols="2">LUVLi (Ours) 2.15</cell><cell>9.31</cell><cell>4.00</cell><cell>32.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>NME and AUC on the AFLW-19 dataset (previous results are quoted from<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b67">68]</ref>). [Key: Best, Second best]</figDesc><table><row><cell></cell><cell cols="2">NME diag</cell><cell cols="2">NME box AUC 7 box</cell></row><row><cell></cell><cell cols="2">Full Frontal</cell><cell>Full</cell><cell>Full</cell></row><row><cell>CFSS [88]</cell><cell cols="2">3.92 2.68</cell><cell>-</cell><cell>-</cell></row><row><cell>CCL [89]</cell><cell cols="2">2.72 2.17</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">DAC-CSR [28] 2.27 1.81</cell><cell>-</cell><cell>-</cell></row><row><cell>LLL [61]</cell><cell>1.97</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SAN [23]</cell><cell cols="2">1.91 1.85</cell><cell>4.04</cell><cell>54.0</cell></row><row><cell>DSRN [52]</cell><cell>1.86</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">LAB (w/o B) [81] 1.85 1.62</cell><cell>-</cell><cell>-</cell></row><row><cell>HR-Net [68]</cell><cell cols="2">1.57 1.46</cell><cell>-</cell><cell>-</cell></row><row><cell>Wing [27]</cell><cell>-</cell><cell>-</cell><cell>3.56</cell><cell>53.5</cell></row><row><cell>KDN [14]</cell><cell>-</cell><cell>-</cell><cell>2.80</cell><cell>60.3</cell></row><row><cell>LUVLi (Ours)</cell><cell cols="2">1.39 1.19</cell><cell>2.28</cell><cell>68.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>WFLW-All dataset results for NME inter-ocular , AUC 10 inter-ocular , and FR 10 inter-ocular . [Key: Best, Second best]</figDesc><table><row><cell></cell><cell cols="3">NME(%) (↓) AUC 10 (↑) FR 10 (%) (↓)</cell></row><row><cell>CFSS [88]</cell><cell>9.07</cell><cell>0.366</cell><cell>20.56</cell></row><row><cell>DVLN [82]</cell><cell>10.84</cell><cell>0.456</cell><cell>10.84</cell></row><row><cell>LAB (w/B) [81]</cell><cell>5.27</cell><cell>0.532</cell><cell>7.56</cell></row><row><cell>Wing [27]</cell><cell>5.11</cell><cell>0.554</cell><cell>6.00</cell></row><row><cell>DeCaFa (w/DA) [20]</cell><cell>4.62</cell><cell>0.563</cell><cell>4.84</cell></row><row><cell>AVS [59]</cell><cell>4.39</cell><cell>0.591</cell><cell>4.08</cell></row><row><cell>AWing [79]</cell><cell>4.36</cell><cell>0.572</cell><cell>2.84</cell></row><row><cell>LUVLi (Ours)</cell><cell>4.37</cell><cell>0.577</cell><cell>3.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>NME box and AUC 7</figDesc><table><row><cell></cell><cell></cell><cell cols="3">box comparisons on MERL-</cell></row><row><cell cols="2">RAV dataset. [Key: Best]</cell><cell></cell><cell></cell></row><row><cell>Metric (%)</cell><cell>Method</cell><cell cols="3">All Frontal Half-Profile Profile</cell></row><row><cell>NME box (↓)</cell><cell cols="2">DU-Net [72] 1.99 LUVLi (Ours) 1.61 1.74 1.89</cell><cell>2.50 1.79</cell><cell>1.92 1.25</cell></row><row><cell>AUC 7 box (↑)</cell><cell cols="2">DU-Net [72] 71.80 73.25 LUVLi (Ours) 77.08 75.33</cell><cell>64.78 74.69</cell><cell>72.79 82.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>MERL-RAV results on three types of landmarks.</figDesc><table><row><cell></cell><cell cols="3">Self-Occluded Unoccluded Externally Occluded</cell></row><row><cell>Mean v j</cell><cell>0.13</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell>Accuracy (Visible)</cell><cell>0.88</cell><cell>0.99</cell><cell>0.99</cell></row><row><cell>NME box |Σ| 0.5</cell><cell>--</cell><cell>1.60 9.28</cell><cell>3.53 34.41</cell></row><row><cell>|Σ| 0.5 box (×10 −4 )</cell><cell>-</cell><cell>1.87</cell><cell>7.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Ablation studies using our method trained on 300W-LP-2D and then fine-tuned on 300-W (Split 2). Menpo 300-W Menpo Supervision All HGs → Last HG 2.32 2.16 67.7 70.8</figDesc><table><row><cell cols="2">Change from LUVLi model:</cell><cell>NME box (%) AUC 7 box (%)</cell></row><row><cell cols="3">Changed From → To LUVLi→MSE Lap+vis→Gauss+No-vis 2.15 2.07 69.6 71.6 2.25 2.10 68.0 71.0 300-W Loss Lap+vis → Gauss+vis 2.13 2.05 69.8 71.8</cell></row><row><cell></cell><cell>Lap+vis → Lap+No-vis</cell><cell>2.10 2.05 70.1 71.8</cell></row><row><cell>Initialization</cell><cell cols="2">LP-2D wts→300-W wts 2.24 2.18 68.3 70.1 LP-2D wts→ Scratch 2.32 2.26 67.2 69.4</cell></row><row><cell>Mean Estimator</cell><cell>Heatmap → Direct ReLU → softmax ReLU → τ -softmax</cell><cell>4.32 3.99 41.3 47.5 2.37 2.19 66.4 69.8 2.10 2.04 70.1 71.8</cell></row><row><cell cols="2">No of HG 8 → 4</cell><cell>2.14 2.07 69.5 71.5</cell></row><row><cell>-</cell><cell cols="2">LUVLi (our best model) 2.10 2.04 70.2 71.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Comparison of our model with Laplacian likelihood vs. with Gaussian likelihood, on 300-W Test (Split 2).[Key: (↑) = higher is better;(↓) = lower is better ]</figDesc><table><row><cell>Likelihood</cell><cell cols="2">NME box (%) (↓) AUC 7 box (%) (↑)</cell><cell>NLL (↓)</cell></row><row><cell>Laplacian</cell><cell>2.10</cell><cell>70.1</cell><cell>0.51</cell></row><row><cell>Gaussian</cell><cell>2.13</cell><cell>69.8</cell><cell>0.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>NME vis box comparisons on MERL-RAV dataset. [Key: Best]</figDesc><table><row><cell>Metric (%)</cell><cell>Method</cell><cell cols="3">All Frontal Half-Profile Profile</cell></row><row><cell>NME vis box (↓)</cell><cell cols="2">DU-Net [72] 2.27 1.91 LUVLi (Ours) 1.84 1.75</cell><cell>2.77 1.99</cell><cell>3.10 2.03</cell></row><row><cell>NME box (↓)</cell><cell cols="2">DU-Net [72] 1.99 1.89 LUVLi (Ours) 1.61 1.74</cell><cell>2.50 1.79</cell><cell>1.92 1.25</cell></row><row><cell cols="5">cian model and (11) for the Gaussian model. The results, in</cell></row><row><cell cols="5">the last column of Table 11, show that the Laplacian model</cell></row><row><cell cols="5">gives a lower negative log-likelihood. In other words, the</cell></row><row><cell cols="5">ground-truth landmark locations have a higher likelihood</cell></row><row><cell cols="5">under our Laplacian model. We conclude that the learned</cell></row><row><cell cols="5">Laplacian model explains the human labels better than the</cell></row><row><cell cols="2">learned Gaussian model.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Lisha Chen from RPI for providing results from their method and Zhiqiang Tang and Shijie Geng from Rutgers University for providing their pretrained models on 300-W (Split 1). We would also like to thank Adrian Bulat and Georgios Tzimiropoulos from the University of Nottingham for detailed discussions on getting bounding boxes for 300-W (Split 2). We also had very useful discussions with Peng Gao from Chinese University of Hong Kong on the loss functions and Moitreya Chatterjee from University of Illinois Urbana-Champaign. We are also grateful to Maitrey Mehta from the University of Utah who volunteered for the demo. We also acknowledge anonymous reviewers for their feedback that helped in shaping the final manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully automatic pose-invariant face recognition via 3D pose normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohith</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incremental face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular video-based trailer coupler detection using multiplexer convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousef</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wende</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monoloco: Monocular 3D pedestrian localization and uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term on-board prediction of people in traffic scenes under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2D &amp; 3D face alignment problem? (and a dataset of 230,000 3D facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gradient descent optimization of smoothed information retrieval metrics. Information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingrui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kernel density network for quantifying regression uncertainty in face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshops</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Face alignment with kernel density deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust and accurate shape model fitting using random forest regression voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Ionita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Sauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Active shape models-their training and application. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">View-based active appearance models. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">De-CaFA: Deep convolutional cascade for face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Dapogny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Menpo benchmark for multi-pose 2D and 3D facial landmark localisation and tracking. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Style aggregated network for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Teacher supervises students how to learn from partially labeled images for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3D vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ITSC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic attention-controlled cascaded shape regression exploiting training data augmentation and fuzzy-set sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Zhen-Hua Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08347</idno>
		<title level="m">Occlusion coherence: Detecting and localizing occluded faces</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-PIE. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured aleatoric uncertainty in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">BayesOD: A bayesian approach for uncertainty estimation in deep object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03838</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bounding box regression with uncertainty for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fitting a single active appearance model simultaneously to multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Uncertainty estimates and multi-hypotheses networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozgun</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Galesso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osama</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A survey of deep learningbased object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09408</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A largescale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Asymmetric multivariate laplace distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaz</forename><surname>Kozubowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Podgórski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Laplace distribution and generalizations</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep alignment network: A convolutional neural network for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Trzcinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Uncertainty estimation for deep neural object detectors in safety-critical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Evaluating and calibrating uncertainty prediction in regression tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liran</forename><surname>Gispan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11659</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<title level="m">Rethinking on multi-stage networks for human pose estimation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Discriminative face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Video-based face model fitting using adaptive active appearance model. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">2D/3D pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Vassilis Athitsos, and Heng Huang. Direct shape regression networks for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiantong</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Benchmarking sampling-based probabilistic object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimity</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feras</forename><surname>Dayoub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Relaxed softmax: Efficient confidence auto-calibration for safe pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiden</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<title level="m">Numerical coordinate regression with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SparsePPG: towards driver monitoring using camera-based vital signs estimation in near-infrared</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewa</forename><surname>Nowara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashok</forename><surname>Veeraraghavany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Shengju Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Laplace landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A multi-view nonlinear active shape model using kernel PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahaogang</forename><surname>Psarrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 300 faces in-the-wild challenge: Database and results. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Epameinondas</forename><surname>Antonakos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A semi-automatic methodology for facial landmark annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Accurate regression procedures for active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Laumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02731</idno>
		<title level="m">A comprehensive guide to bayesian convolutional neural network with variational inference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Adaptive active appearance model with incremental learning. Pattern recognition letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewon</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daijin</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Towards highly accurate and stable face alignment for high-resolution videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Quantized densely connected U-Nets for efficient landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Towards efficient U-Nets: A coupled and quantized approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihalis</forename><surname>Nicolaou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Epameinondas Antonakos, and Stefanos Zafeiriou</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Robust face alignment using a mixture of invariant experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salil</forename><surname>Tambe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning on lie groups for invariant detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Project-out cascaded regression with an application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Optimization problems for fast AAM fitting in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Adaptive wing loss for robust face alignment via heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Look at boundary: A boundary-aware face alignment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yici</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Leveraging intra and interdataset variations for robust face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The Menpo facial landmark localisation challenge: A step towards the solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Coarse-to-fine auto-encoder networks (CFAN) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Face alignment by coarse-to-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning robust facial landmark detection via hierarchical structured ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
