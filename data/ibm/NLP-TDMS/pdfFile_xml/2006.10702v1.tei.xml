<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Recognition under a Noisy and Fine-grained Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Cui</surname></persName>
							<email>cuicheng0101@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">National Internet Emergency Center(CNCERT)</orgName>
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Internet Emergency Center(CNCERT)</orgName>
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Internet Emergency Center(CNCERT)</orgName>
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Internet Emergency Center(CNCERT)</orgName>
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Internet Emergency Center(CNCERT)</orgName>
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Internet Emergency Center(CNCERT)</orgName>
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Internet Emergency Center(CNCERT)</orgName>
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmei</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Internet Emergency Center(CNCERT)</orgName>
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongji</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Internet Emergency Center(CNCERT)</orgName>
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Internet Emergency Center(CNCERT)</orgName>
								<address>
									<settlement>Baidu</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Recognition under a Noisy and Fine-grained Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Simi-Supervised Recognition Challenge-FGVC7 is a challenging fine-grained recognition competition. One of the difficulties of this competition is how to use unlabeled data. We adopted pseudo-tag data mining to increase the amount of training data. The other one is how to identify similar birds with a very small difference, especially those have a relatively tiny main-body in examples. We combined generic image recognition and fine-grained image recognition method to solve the problem. All generic image recognition models were training using PaddleClas 1 . Using the combination of two different ways of deep recognition models, we finally won the third place in the competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Simi-Supervised Recognition Challenge-FGVC7 focused on learning from partially labeled data, which is a form of semi-supervised learning. The dataset is designed to expose some of the challenges encountered in a realistic setting, such as the fine-grained similarity between classes, significant class imbalance, and domain mismatch between the labeled and unlabeled data. This challenge is part of the FGVC7 workshop in CVPR 2020.</p><p>The data set is divided into the following three parts, the first is labeled data, a total of 5959 pictures, the second is unlabeled data whose categories belong to the labeled data, a total of 26640 pictures, and the third is unlabeled data but its category is out of the labeled data, a total of 122,208 pictures. The labeled data is divided into a training set of 3959 pictures and a validation set of 2000 pictures. The training data is unbalanced, while the validation data balanced. In addition, there is a total of 8,000 pictures in the test set, including 4,000 public data and 4,000 private data.</p><p>In the traditional fine-grained recognition tasks, most common fine-grained recognition methods are used. How- <ref type="figure">Figure 1</ref>. Diagram of the overall training framework. For the in-class dataset (labeled and unlabeled), we trained the labeled data using basic classification and fine-grained classification algorithms with various optimization methods, the different outputs were merged together to do the pseudo-labeling on in-class dataset. The processing pipeline we called ensemble modeling as shown in <ref type="figure">Figure 2</ref>. The pseudo-labeling process iterates several times to enlarge the pseudo-label samples from in-class set gradually. To effectively utilize the out-of-class dataset, we clustered the data into 10k classes then train a classification model for a good pretraining start. The model was finetuned with labeled data and then used for mining in-class dataset as before. Finally we fused two pseudo-labeling dataset to build it as our final training set. In practice, we tried different mining and fusion ways with minor parameter changes to get final results. ever, in this competition, fine-grained method can not solve this problem well due to the small amount of training data and small targets with complex backgrounds in examples. In addition to the fine-grained method, a lot of generic identification models were adopted. And we used PaddleClas to train these models, mainly for the reasons as follows:</p><p>(1) PaddleClas is a toolset for image classification tasks prepared for the industry and academia written in Pad-dlePaddle 2 . It helps users train better computer vision models and apply them in real scenarios.</p><p>(2) Based on Image-Net1k dataset, PaddleClas provides 23 series of image classification networks such as ResNet, ResNet vd, Res2Net, HRNet, and MobileNetV3 with brief introductions, reproduction configurations and training tricks.</p><p>(3) PaddleClas provides a Simple Semi-supervised Label Distillation method (SSLD). With this method, different models on Image-Net1k validation dataset have 3% absolute improvement(Top1 accuracy). For example, ResNet50 and ResNet101 achieved top-1 accuracy of 83% and 83.7%, respectively.</p><p>(4) PaddleClas provides the reproduction of the 8 data augmentation algorithms and the evaluation of the effect in a unified environment. We can use data augmentation such as Mixup, Cutmix, Cutout, RandomAugment, etc conveniently.</p><p>In this paper, we will introduce our solution to this competition. Firstly, we trained several specific deep models with the generic image recognition method. Secondly, these models were used to mine more examples from the in-class data through voting strategy. At the same time, we clustered the out-of-class unlabeled data to 10k class to produce a pretrained model on the bird data. Then, we used this pretrained model to train a new model on the train-validation dataset and the new model was used to fuse the mined data. The fused data was fixed as our final training set. Finally, we retrained a generic image recognition models and a finegrained model with highest accuracy on this final training set, and merged the results of the two best models of different schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Solution</head><p>We now describe our approach in detail, which consists of the follwing models and main steps. The whole pipline is show in <ref type="figure">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data selection</head><p>There are only 3959 labeled images provided, so the main difficulty lies in the utilization of unlabeled data. We've tried many ways to label in-class data and out-class data, including some semi-supervised methods: FixMatch <ref type="bibr" target="#b0">[1]</ref> and MixMatch <ref type="bibr" target="#b1">[2]</ref>, but their results are not ideal. Take FixMatch as an example, we used out-of-class data as unlabeled data while merging the train-set, validation-set and inclass samples whose classification score larger than a specific threshold as labeled data. However the accuracy get lower than training with pseudo labeled in-class data. So we adopt the pseudo label method finally. Firstly, we used various basic classification and fine-grained classification algorithms to train labeled data, the details is described below. Secondly, we assemble models through the top-1 voting fusion and verify the accuracy on the validation set. Specifically, we select the model combination with the highest accuracy to label the in-class data. Lastly, we select the inclass data with high confidence and add them to the labeled data set. This whole process iterated several times until the accuracy of merged model converged.</p><p>We clustered the out-of-class 120k data into 10k classes. Then, we trained the models using the 10k-class data and the top1 accuracy reached 92%. We used this model as the pretrained model to train the train-validation dataset, which had a better performance than using ImageNet pretrained model. After the training, we used this model's prediction result on in-class data to intersect the result of the data mined by the ImageNet pretrained model training. The intersection part is the in-class training data set we finally choose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generic image recognition method</head><p>In terms of the selection of backbone, we found that the model with high accuracy in ImageNet will also have high accuracy in 3959 training sets and 2000 validation sets, so we followed this principle and selected the ImageNet pretrained model with high accuracy: SENet154 vd, Res2Net200 vd 26w 4s, DPN107, DPN131, HRNet W64 C, ResNet200 vd, ResNeXt152 Vd 64x4d, InceptionV4, Xception65, DenseNet161, DenseNet264 <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, etc. The experimental results show that their merged results achieved good results. We conducted experiments on model training tricks on 3959 training sets and 2000 validation sets. Some conclusions are as follows:</p><p>(1) Using a complex model network (such as ResNet200 vd, ResNeXt152 vd 64x4d) is about 5˜10 points higher than a simple network such as ResNet50 vd;</p><p>(2) In large scale dataset, epoch=50 is a more suitable parameter;</p><p>(3) Through grid-search, batch size=128, learning rate=0.025 is the best parameter combination;</p><p>(4) Cutmix improves about 2 points which is better than other data augmentation methods (such as mixup). If the data set is large, cutmix may be unhelpful; <ref type="bibr" target="#b4">(5)</ref> There is no obvious help using dropout. While using dropblock in the model will increase by about 1 point; <ref type="bibr" target="#b5">(6)</ref> Adjusting the training and validation crop size from 224 to 448 will increase by about 4 points;</p><p>(7) Increasing model regularization can improve the accuracy. The epsilon value of label smooth <ref type="bibr" target="#b9">[10]</ref> is better to be 0.2 for small models such as ResNet50 and 0.3 for large models such as SE ResNet152; <ref type="bibr" target="#b7">(8)</ref> In the model test phase, test time augmentation (TTA) is adopted. Through three data preprocessing methods: Resize+CenterCrop, Resize+RandomCrop, Resize+RandomCrop+RandomHorizontalFlip, each model can output 3 results, which will increase by 0.3˜0.5 Points;</p><p>(9) During model testing, using the 144-crop method will increase it by about 1 point, but it takes more time.</p><p>(10) The 'Fix strategy' is very effective in postprocessing, which can increase by more than 1 point <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure">Figure 2</ref>. In the generic image recognition method , multiple generic recognition models are trained, and then the precision is further improved through 'Fix strategy' <ref type="bibr" target="#b10">[11]</ref>. The model trained by 'Fix strategy' is fused, and the fused model is trained again by 'Fix strategy', in which the data of training 'Fix strategy' is 5959 train+val dataset.</p><p>Based on the above conclusions, we trained a total of 8 models and fused the results. Finally, as a result of the fusion, we reached 16.6% on the public list and 16.0% on the private list. The fusion strategy is mainly to add the weighted FC results before sotfmax, the weight is determined by its accuracy, and the higher the accuracy of the model, the greater the weight. At the same time, in the process of fusion, we also used the train-validation data of 5959 to perform the 'Fix strategy' <ref type="bibr" target="#b10">[11]</ref>. The detailed operation is show in <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Fine-grained recognition method</head><p>In this competition, we tried many advanced image recognition methods, but after several iterations, the accuracy reached the bottleneck, and it's difficult to be improved. On the one hand, we think that the amount of data with labels is very small (3959), while the effect of general data augmentation methods will reach the ceiling soon, such as RandomCrop, RandomHorizontalFlip, ColorJitter and so on; on the other hand, we find that there are high intraclass variances and low inter-class variances by analyzing bad cases, which is a great challenge to the generic image recognition. At this time, we turn our attention to the finegrained classification algorithm. We use Weakly Supervised Data Augmentation Network(WS-DAN) <ref type="bibr" target="#b11">[12]</ref> and Destruction and Construction Learning for Fine-grained Image Recognition(DCL) <ref type="bibr" target="#b12">[13]</ref> here.</p><p>With WSDAN, we implemented fine-grained data augmentation. Specifically, for each training image, we first generate attention maps to represent the object's discriminative parts by weakly supervised learning, then we augment the image under the guidance of these attention maps, including attention cropping and attention dropping. WS-DAN improves the classification accuracy in two aspects: first, images can be seen better since more discriminative parts' features will be extracted; then, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common finegrained visual classification datasets also prove the effectiveness of WS-DAN.</p><p>DCL is also a data augmentation method for fine-grained recognition, it enhances the difficulty of fine-grained recognition and exercises the classification model to acquire expert knowledge. Besides the standard classification backbone network, another "destruction and construction" stream is introduced to carefully "destruct" and then "reconstruct" the input image, for learning discriminative regions and features. More specifically, for "destruction", we first partition the input image into local regions and then shuffle them by a Region Confusion Mechanism (RCM). To correctly recognize these destructed images, the classification network has to pay more attention to discriminative regions for spotting the differences. For "construction", a region alignment network, which tries to restore the original spatial layout of local regions, is followed to model the semantic correlation among local regions, experimental results also show its effectiveness.</p><p>We further optimize WSDAN and DCL by the following methods:</p><p>(1) About tricks for weakly supervised learning, we find that label smooth, warmup + Learning rate with cosine decay and 144-crop prediction can improved the results. Besides, larger batch size and image crop size can increase by about 1.5 points;</p><p>(2) With the increase of iterations of inclass data, the number of trusted pseudo labels is increasing, which can significantly improve the model result by 10 points or more;</p><p>(3) We try to use different SOTA backbones to train WS-DAN, such as ResNeSt269 <ref type="bibr" target="#b13">[14]</ref> and EfficientNet-B7 <ref type="bibr" target="#b14">[15]</ref>, the individual model output didn't show up any exciting result, but by combining these results, the accuracy can be further improved about 1.5 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">The combination of generic image recognition and fine-grained recognition</head><p>The combination of generic image recognition method and fine-grained recognition pipeline is very important to improve the recognition effect of this challenge dataset. We train several basic generic classification models using PaddleClas toolbox from PaddlePaddle and specific finegrained models with Pytorch framework respectively. We assembled various models' results by merging the last fc layer output before the softmax activatation and this operation will give us more robust classification results.</p><p>In the prediction phase, we first locate the target objects by our attention visualization method. Some visualization samples are shown in <ref type="figure">Figure 3</ref>. Then, general or <ref type="figure">Figure 3</ref>. Visualization of attention region among long-shot, medium-shot and close-shot bird images. We found attention region area (especially fine-grained model) is helpful for us to judge the shoot type of the image. If it is a long-shot image, we give a higher confidence to the generic model, otherwise we give more confidence to fine-grained model when we do the model fusion.</p><p>fine-grained recognition model or combination of them is sclected to classify different samples under direction of the scale of targets. For example, for the very small targets, especially those hidden in the surrounding environment, or almost integrated with the environment, which makes our naked eyes almost unable to identify, generic image recognition will pay more attention to the environmental information in the image, which also plays a very good role in the correct recognition process. For the middle large objects, we give a higher confidence to the fine-grained image recognition model while for the very large objects, we give a higher confidence to the generic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Result</head><p>The following <ref type="table">Table 1</ref> lists the training results of some generic image recognition models. We did not submit the results of all models, so the results only listed the results of Res2Net200 vd 26w 4s and multi-model fusion.</p><p>The following <ref type="table" target="#tab_0">Table 2</ref> lists some of the training results of fine-grained models. In order to obtain better results, we expanded the resolution to 600x600.</p><p>The following <ref type="table">Table 3</ref> lists the results of adjusting the fusion thresholds of different models. The fusion v3 method is our final result. Through the meticulous combination of generic and fine-grained models, we got the third place in the Semi-Supervised Recognition Competition of FGVC7.  <ref type="table">Table 3</ref>. The results of the combination of generic image recognition and fine-grained recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we introduced our method of mining data, gave some conclusions and some tricks for training generic image recognition models in detail, and at the same time, we introduced methods and conclusions for training finegrained image recognition models. Also, we described the combination method of the generic image recognition model results fusion and the fine-grained image recognition model results fusion. In the end, we got the third place in the Semi-Supervised Recognition Competition of FGVC7.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Results of different fine-grained models on the test dataset.</figDesc><table><row><cell>Model</cell><cell>Training</cell><cell>Test</cell><cell></cell><cell>Public Private</cell></row><row><cell></cell><cell>resolu-</cell><cell>resolu-</cell><cell></cell></row><row><cell></cell><cell>tion</cell><cell>tion</cell><cell></cell></row><row><cell>SENet154 vd</cell><cell cols="4">448x448 576x576 --</cell><cell>--</cell></row><row><cell>Res2Net200 vd</cell><cell cols="4">448x448 576x576 18.5% 17.5%</cell></row><row><cell>DenseNet264</cell><cell cols="4">448x448 576x576 --</cell><cell>--</cell></row><row><cell>DenseNet161</cell><cell cols="4">448x448 576x576 --</cell><cell>--</cell></row><row><cell>ResNet200 vd</cell><cell cols="4">448x448 576x576 --</cell><cell>--</cell></row><row><cell>Inception V4</cell><cell cols="4">448x448 576x576 --</cell><cell>--</cell></row><row><cell>Xception65</cell><cell cols="4">448x448 576x576 --</cell><cell>--</cell></row><row><cell>HRNet W64 C</cell><cell cols="4">448x448 576x576 --</cell><cell>--</cell></row><row><cell cols="2">Combine 8 model --</cell><cell cols="3">576x576 16.6% 16.0%</cell></row><row><cell cols="5">Table 1. Results of different generic image recognition models on</cell></row><row><cell>the test dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>model(WS-</cell><cell>Training</cell><cell cols="2">Testing</cell><cell>Public Private</cell></row><row><cell>DAN)</cell><cell>resolu-</cell><cell>resolu-</cell><cell></cell></row><row><cell></cell><cell>tion</cell><cell>tion</cell><cell></cell></row><row><cell>ResNet152</cell><cell cols="4">600x600 600x600 20.1% 20.3%</cell></row><row><cell>Res2Net101</cell><cell cols="4">600x600 600x600 18.9% 19.5%</cell></row><row><cell>SENet154</cell><cell cols="4">600x600 600x600 --</cell><cell>--</cell></row><row><cell cols="2">Combine 3 model --</cell><cell cols="3">600x600 16.7% 16.8%</cell></row><row><cell>Fusion method</cell><cell></cell><cell></cell><cell cols="2">Public</cell><cell>Private</cell></row><row><cell>Fusion method v1</cell><cell></cell><cell></cell><cell cols="2">14.2%</cell><cell>14.3%</cell></row><row><cell>Fusion method v2</cell><cell></cell><cell></cell><cell cols="2">13.3%</cell><cell>13.0%</cell></row><row><cell cols="3">Fusion method v3(final result)</cell><cell cols="2">12.9% 12.9%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/PaddlePaddle/Paddle</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>In ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning for image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dual path networks[C]//Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4467" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4696" to="4705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06423</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09891</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Destruction and construction learning for finegrained image recognition in CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-Attention Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
