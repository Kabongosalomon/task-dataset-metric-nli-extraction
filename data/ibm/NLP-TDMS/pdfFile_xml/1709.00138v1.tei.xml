<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Shot Text Detector with Regional Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Science Foundation Center for Big Learning</orgName>
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qile</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Science Foundation Center for Big Learning</orgName>
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Science Foundation Center for Big Learning</orgName>
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Single Shot Text Detector with Regional Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel single-shot text detector that directly outputs word-level bounding boxes in a natural image. We propose an attention mechanism which roughly identifies text regions via an automatically learned attentional map. This substantially suppresses background interference in the convolutional features, which is the key to producing accurate inference of words, particularly at extremely small sizes. This results in a single model that essentially works in a coarse-to-fine manner. It departs from recent FCNbased text detectors which cascade multiple FCN models to achieve an accurate prediction. Furthermore, we develop a hierarchical inception module which efficiently aggregates multi-scale inception features. This enhances local details, and also encodes strong context information, allowing the detector to work reliably on multi-scale and multiorientation text with single-scale images. Our text detector achieves an F-measure of 77% on the ICDAR 2015 benchmark, advancing the state-of-the-art results in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>. Demo is available at: http://sstd.whuang.org/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reading text in the wild has attracted increasing attention in computer vision community, as shown in recent work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref>. It has numerous potential applications in image retrieval, industrial automation, robot navigation and scene understanding. Recent work focuses on text detection in natural images, which remains a challenging problem <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b1">2]</ref>. The main difficulty lies in a vast diversity in text scale, orientation, illumination, and font, which often come with a highly complicated background.</p><p>Previous works in text detection have been dominated by bottom-up approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b7">8]</ref>, which often contain <ref type="figure">Figure 1</ref>: Illustrations of text attention mechanism in the proposed one-shot text detector. Our model automatically learns a rough text regional attention which is used to highlight text information in the convolutional features. This makes it possible to achieve accurate word-level predictions in one shot. Text can be precisely separated and accurately predicted at the word level in challenging cases. multiple sequential steps, including character or text component detection, followed by character classification or filtering, text line construction and word splitting. Character detection and filtering steps play a key role in such bottomup approaches. Previous methods typically identify character or text component candidates using connected component based approaches (e.g., stroke width <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> or extremal region <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33]</ref>), or sliding window methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6]</ref>. However, both groups of methods commonly suffer from two main limitations which significantly reduce their efficiencies and performance. First, text detection is built on identification of individual characters or components, making it difficult to explore regional context information. This often results in a low recall where ambiguous characters are easily discarded. It also leads to a reduction in precision, by generating a large number of false detections. Second, multiple sequential steps make the system highly complicated, and errors are easily accumulated in the later steps.</p><p>Deep learning technologies have advanced the performance of text detection considerably <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref>. A number of recent approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref> were built on Fully Convolutional Networks (FCN) <ref type="bibr" target="#b18">[19]</ref>, by producing pixel-wise prediction of text or non-text. We refer this group of methods as pixel-based text detectors, which cast previous character-based detections into the problem of text semantic segmentation. This allows them to explore rich regional context information, resulting in a stronger capability for detecting ambiguous text, and also reducing the number of false detections substantially. In spite of effectively identifying rough text regions, these FCN-based approaches fail to produce accurate word-level predictions with a single model. The main challenge is to precisely identify individual words from a detected rough region of text. As indicated in <ref type="bibr" target="#b27">[28]</ref>, the task of text detection often requires a higher localization accuracy than general object detection. To improve the accuracy, a coast-to-fine detection pipeline was developed, by cascading two FCN models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>. The second FCN produces word-or character-level predictions on a cropped text region detected by the first one. This inevitably increases system complexity: (i) it is highly heuristic yet complicated to correctly crop out regions of texts, words or characters from a predicted heatmap <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>; (ii) multiple bottom-up steps are still required for constructing text lines/words <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Recently, another group of text detectors was developed for direct prediction of text bounding boxes, by extending from the state-of-the-art object detectors, such as Faster R-CNN <ref type="bibr" target="#b22">[23]</ref> and SSD <ref type="bibr" target="#b16">[17]</ref>. They all aim to predict text boxes directly by sliding a window through the convolutional features <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24]</ref>, and we refer them as boxbased text detectors. The box-based text detectors are often trained by simply using bounding-box annotations, which may be too coarse (high-level) to provide a direct and detailed supervision, compared to the pixel-based approaches where a text mask is provided. This makes the models difficult to learn sufficient word information in details, leading to accuracy loss in one-shot prediction of words, particularly for those small-scale ones. Therefore, they may again come up with multiple post-processing steps to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions</head><p>These related approaches inspire current work that aims to directly estimate word-level bounding boxes in one shot. We cast the cascaded FCNs detectors into a single model by introducing a new attention module, which enables a di-rect mask supervision that explicitly encodes detailed text information in training, and functions on an implicit text region detection in testing. This elegantly bridges the gap between the pixel-based approaches and the box-based text detectors, resulting in a single-shot model that essentially works in a coarse-to-fine manner. We develop a hierarchical inception module which further enhances the convolutional features. The main contributions of the paper are three-fold.</p><p>First, we propose a novel text attention module by introducing a new auxiliary loss, built upon the aggregated inception convolutional features. It explicitly encodes strong text-specific information using a pixel-wise text mask, allowing the model to learn rough top-down spatial attention on text regions. This text regional attention significantly suppresses background interference in the convolutional features, which turns out to reduce false detections and also highlight challenging text patterns.</p><p>Second, we develop a hierarchical inception module which efficiently aggregates multi-scale inception features. An inception architecture with dilated convolutions <ref type="bibr" target="#b33">[34]</ref> is applied to each convolutional layer, enabling the model to capture multi-scale image content. The multi-layer aggregations further enhance local detailed information and encode rich context information, resulting in stronger deep features for word prediction.</p><p>Third, the proposed text-specific modules are seamlessly incorporated into the SSD framework, which elegantly customizes it towards fast, accurate and single-short text detection. This results in a powerful text detector that works reliably on multi-scale and multi-orientation text with singlescale inputs. It obtains state-of-the-art performance on the standard ICDAR 2013 and ICDAR 2015 benchmarks, with about 0.13s running time on an image of 704 × 704.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Previous works on scene text detection mainly focus on bottom-up approaches, which detect characters or text components from images by using hand-crafted features <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref> or sliding window methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. They often involve pixel-level binary classification of text/non-text and generate a text salience map. Then multiple bottom-up steps are designed to group text-related pixels into characters, which are further formed character pairs, and then text lines. Each step may be followed by a text/non-text filter or classifier. Most of these steps are built on the heuristic or hand-crafted features, e.g., gradient, stroke width <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>, covariance descriptor <ref type="bibr" target="#b8">[9]</ref>, etc. These bottom-up approaches are complicated, and identification of individual characters using low-level features is neither robust nor reliable.</p><p>Deep learning technologies have significantly advanced the performance of text detection in the past years <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7]</ref>. These approaches essentially work in a sliding window fashion, with two key developments: (i) they lever- <ref type="figure">Figure 2</ref>: Our single-shot text detector contains three main parts: a convolutional part, a text-specific part, and a word box prediction part. We propose the text-specific part which comprises a Text Attention Module (TAM) and a Hierarchical Inception Module (HIM). The TAM introduces a new pixel-wise supervision of text, allowing the model to automatically learn text attention map which identifies rough text regions. The HIM aggregates multi-layer inception modules, and enhances the convolutional features towards text task.</p><p>age deep features, jointly learned with a classifier, to enable strong representation of text; (ii) sharing convolutional mechanism <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6]</ref> was applied for reducing the computational cost remarkably. With these two improvements, a number of Fully Convolutional Network (FCN) <ref type="bibr" target="#b18">[19]</ref> based approaches have been proposed <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref>. They compute pixel-wise semantic estimations of text or non-text, resulting in a fast text detector able to explore rich regional context information. However, these pixel-based text detectors are difficult to provide sufficient localization accuracy by using a single model. Furthermore, accurate segmentation of text from a predicted heatmap is complicated, and often requires a number of heuristic post-processing steps.</p><p>Our work also relates to most recent approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24]</ref> which are extended from the state-of-the-art object detectors, such as SSD <ref type="bibr" target="#b16">[17]</ref> and Fast R-CNN <ref type="bibr" target="#b22">[23]</ref>. These approaches all aim to predict text bounding boxes from the convolutional features. Liao et. al. <ref type="bibr" target="#b15">[16]</ref> proposed a TextBox by extending the SSD model for text detection, but their performance is limited by the SSD architecture which is designed for general object detection. Deep Matching Prior Network (DMPNet) was proposed in <ref type="bibr" target="#b17">[18]</ref>, by introducing quadrilateral sliding windows to handle multi-orientation text. Accurate text localization is achieved by using multi-step coarse-to-fine detection with post adjustments. Tian et. al. <ref type="bibr" target="#b27">[28]</ref> proposed a Connectionist Text Proposal Network (CTPN) which detects a text line by predicting a sequence of fine-scale text components. The CTPN is difficult to work on multi-orientation text and requires bottom-up steps to group text components into text lines. Gupta et. al. <ref type="bibr" target="#b3">[4]</ref> developed a Fully Convolutional Regression Network (FCRN) which predicts word bounding boxes in an image. However, the FCRN requires threestage post-processing steps which reduce the efficiency of the system considerably. For example, the post-processing steps take about 1.2s/image, comparing to 0.07s/image for bounding box estimations. Our work differs distinctly from these approaches by proposing two text-specific modules. It has a number of appealing properties that advance over these related methods: (i) it is a single-shot detector that directly outputs word bounding boxes, filling the gap between semantic text segmentation and direct regression of word boxes; (ii) it is highly efficient, and does not require any bottom-up or heuristic post-processing step; (iii) it works reliably on multi-orientation text; (iv) it is fast yet accurate, and significantly outperforms those related approaches on the standard ICDAR 2015 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We present details of the proposed single-shot text detector, which directly outputs word-level bounding boxes without post-processing, except for a simple NMS. Our detector is composed of three main parts: a convolutional component, a text-specific component, and a box prediction component. The convolutional and box prediction components mainly inherit from SSD detector <ref type="bibr" target="#b16">[17]</ref>. We propose the text-specific component which contains two new modules: a text attention module and a hierarchical inception module. The convolutional architecture of the SSD is extended from the 16-layer VGGNet <ref type="bibr" target="#b24">[25]</ref>, by replacing the fully-connected (FC) layers with several convolutional layers <ref type="bibr" target="#b16">[17]</ref>. The proposed modules can be easily incorporated into the convolutional component and box prediction component of the SSD, resulting in an end-to-end trainable model, as shown in <ref type="figure">Fig. 2</ref>. The two text-specific modules elegantly customize the SSD framework towards accurate word detection. Compared to most recent methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref>, we show experimentally that our particular designs provide a more principled solution that generalizes better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Text Attention Mechanism</head><p>Our attention module is able to automatically learn rough spatial regions of text from the convolutional features. This attention to text is then directly encoded back into the convolutional features, where text-related features are strongly enhanced by reducing background interference in the convolutional maps, as shown in <ref type="figure" target="#fig_0">Fig. 3</ref>.</p><p>The attention module is built on the Aggregated Inception Feature (AIF) (described in Sect. 3.2). It generates a pixel-wise probability heatmap which indicates the text probability at each pixel location. This probability heatmap is referred as the attention map which has an identical size of an input image and will be downsampled for each prediction layer. The attention module includes two 3 × 3 convolutional layers with pad 1, one deconvolution (upsampling with bilinear operation) layer which connects the AIF to the attention map. Then the attention map is generated by using a softmax activation function on the de-convoluted features. Specifically, given an input image of 512 × 512, we get the first-layer AIF features, F AIF1 ∈ R 64×64×512 . The atten-tion map, α + ∈ R 512×512 , is computed as,</p><formula xml:id="formula_0">D AIF1 = deconv 3×3 (F AIF1 ), (1) D AIF1 = conv 1×1 (D AIF1 ), (2) α = sof tmax(D AIF1 ).<label>(3)</label></formula><p>where D AIF1 ∈ R 512×512×512 is the de-convoluted feature maps, which are further projected to 2-channel maps, D AIF1 ∈ R 512×512×2 using 1 × 1 kernels, followed by a softmax function. Then the positive part of the softmax maps, α + , is the attention map, indicating pixel-wise possibility of text. The attention map is further encoded into the AIF by simply resizing it as with spatial size,</p><formula xml:id="formula_1">α + = resize(α + ),<label>(4)</label></formula><formula xml:id="formula_2">F AIF1 =α + F AIF1 .<label>(5)</label></formula><p>whereα + ∈ R 64×64 is the resized attention map, and indicates element-wise dot production across all channel of the AIF maps.F AIF1 is the resulted feature maps with encoded text regional attention. The AIFs with and without text attention are shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, where text information is clearly presented when the attention is encoded. The text attentional information is learned automatically in the training process. We introduce an auxiliary loss which provides a direct and detailed supervision of text via a binary mask that indicates text or non-text at each pixel location. A softmax function is used to optimize this attention map toward the provided text mask, explicitly encoding strong text information into the attention module. Notice that the proposed attention module is formulated in a unified framework which is trained end to end by allowing for computing back-propagations through all layers.</p><p>The proposed attention module sets this work apart from both previous pixel-wise and box-wise text detectors. It elegantly handles the main limitations of both groups of methods, resulting in an efficient single-shot text detector that produces accurate word-level text detection. Efficiency of the proposed attention module is demonstrated in <ref type="figure" target="#fig_1">Fig. 4</ref>, where detection results by a baseline model and a withattention model are presented. Obviously, the proposed attention module improves the performance at three aspects: (i) it reduces the number of false detections; (ii) it allows the model to detect more ambiguous texts; (iii) it improves the word-level detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Inception Module</head><p>In a CNN model, convolutional features in a lower layer often focus on local image details, while the features in a deeper layer generally capture more high-level abstracted information. The SSD detector predicts object bounding boxes at multi-layer convolutional maps, allowing it to localize multi-scale objects from a single-scale input. Texts  often have large variations in scale with significantly different aspect ratios, making the single-scale text detection highly challenging. In recent work <ref type="bibr" target="#b27">[28]</ref>, a layer-wise RNN is incorporated into a convolutional layer, allowing the detector to explore rich context information thought the whole line. This RNN-based text detector is powerful to detect near-horizontal text lines but is difficult to work reliably on multi-orientation texts.</p><p>Inspired by the inception architecture of GoogleNet <ref type="bibr" target="#b25">[26]</ref>, we propose a hierarchical inception module able to aggregate stronger convolutional features. First, we develop a similar inception module which is applied to each convolutional layer, as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. This allows it to capture richer context information by using multi-scale receptive fields. Second, we aggregate the inception features from multiple layers and generate final Aggregated Inception Features (AIF).</p><p>Details of inception building block are described in <ref type="figure" target="#fig_2">Fig.  5</ref>. It is applied to those convolutional layers which are used to predict word bounding boxes. The convolutional maps in a layer are processed through four different convolutional operations: 1 × 1-conv, 3 × 3-conv, 3 × 3-pool with 1 × 1conv, and 5 × 5-conv. The 5 × 5-conv is decomposed into 1 × 5 and 5 × 1 convolution layers. Dilated convolutions <ref type="bibr" target="#b33">[34]</ref>, which support an exponential expansion of receptive field without loss of resolution or coverage, are applied. Each convolutional operation reduces the number of feature channels to 128. The final inception features are generated by simply concatenating four 128-channel features, resulting in 512-channel inception features. By using mul- Motivated from HyperNet <ref type="bibr" target="#b4">[5]</ref>, we further enhance the convolutional features by aggregating multi-layer inception features, which generates final AIFs at three key convolutional layers, as shown in <ref type="figure">Fig. 2</ref>. Each AIF is computed by fusing the inception features of current layer with two directly adjacent layers. Down-sampling and up-sampling are applied to the lower layer and higher layer, respectively. These sampling operations ensure same feature resolutions for three inception features, which are combined together using channel concatenation.</p><p>The proposed hierarchical inception module is related to the skip architecture developed in <ref type="bibr" target="#b18">[19]</ref>, which combines multi-layer convolutional features for handling multi-scale objects. We propose a two-step aggregation approach by leveraging efficient layer-wise inception module. This results in a stronger AIF, with richer local details encoded and a more powerful multi-scale capability. Both improvements are important for text task, allowing our model to identify very small-scale text and work reliably on the multi-scale text which often has a wider range of scales than the general objects. Compared to layer-wise RNN method developed in <ref type="bibr" target="#b27">[28]</ref>, the AIF encodes more local detailed information and generalizes better to multi-orientation text. The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Word Prediction Module</head><p>The proposed text-specific modules are directly incorporated into the SSD framework. It can be trained by simply following the SSD, with slight modifications on the box prediction part. As in <ref type="bibr" target="#b16">[17]</ref>, we use a softmax function for binary classification of text or non-text, and apply the smoothl 1 loss for regressing 5 parameters for each word bounding box, including a parameter for box orientation. Our model predicts N word bounding boxes at each spatial location of the inception or AIF maps. The predictions are computed through all inception and AIF maps shown in <ref type="figure">Fig. 2</ref>. N is the number of the pre-defined default word boxes, which can be pre-computed. A default box is defined by its scale and aspect ratio. Since different inception maps have various sizes of receptive fields, the scale of a default box is varied over different inception or AIF maps. Its length is measured by the number of pixels in the input image. To better match the default boxes with ground truth ones, we use three different scales for each layer, and the scales for all inception and AIF layers are listed in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Aspect ratios of a default box are set in a wide range (in <ref type="table" target="#tab_0">Table 1</ref>), due to the significant variances of them for text. Same aspect ratios are used in all layers. In order to handle multi-orientation text, a scale can be the width or height of a default box. This results in 45 default boxes in total for each layer, which allow the detector to handle large shape variances of text. The center of a default box is identical to the current spatial location of the inception/convolution maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>Our methods are evaluated on three standard benchmarks, the ICDAR 2013 <ref type="bibr" target="#b13">[14]</ref>, ICDAR 2015 <ref type="bibr" target="#b14">[15]</ref> and COCO-Text dataset <ref type="bibr" target="#b28">[29]</ref>. The effectiveness of each proposed component is investigated by producing exploration studies. Full results are compared with the state-of-the-art performance on the three benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Implementation Details</head><p>Datasets. The ICDAR 2013 <ref type="bibr" target="#b13">[14]</ref> consists of 229 training images and 233 testing images, with word-level annotations provided. It is the standard benchmark for evaluating near-horizontal text detection. We use two standard evaluation protocols: the new ICDAR13 standard <ref type="bibr" target="#b14">[15]</ref> and the DetEval <ref type="bibr" target="#b13">[14]</ref>. Our results were obtained by uploading the predicted bounding boxes to the official evaluation system. The ICDAR 2015 (Incidental Scene Text Challenge 4) <ref type="bibr" target="#b14">[15]</ref> was collected by using Google Glass and it has 1,500 images in total: 1,000 images for training and the remained 500 images for testing. This new benchmark was designed for evaluating multi-orientation text detection. Word-level annotations are provided and each word is labeled by the coordinates of its four corners in a clockwise manner. This dataset is more challenging and has images with arbitrary orientations, motion blur, and low-resolution text. We evaluate our results based on the online evaluation system <ref type="bibr" target="#b14">[15]</ref>. The COCO-Text <ref type="bibr" target="#b28">[29]</ref> is the largest text detection dataset, which has 63,686 annotated images in total: 43,686 for the training and the rest 20,000 for testing.</p><p>Training datasets. Our training samples were collected from the training sets of the ICDAR 2013 and ICDAR 2015. We also added images harvested from Internet as the training data and manually labelled them with word level annotation. We have 13, 090 training images in total. We did not use the training data from the COCO-Text.</p><p>Implementation details. Our detection network is trained end-to-end by using the mini-batch stochastic gradient descent algorithm, where the batch size is set to 32, with a momentum of 0.9. We initialize our model using the pre-trained model in <ref type="bibr" target="#b16">[17]</ref>. The learning rate is set to 0.001 and is decayed to its 1 10 after 15, 000 iterations. The model is trained by fixing the first four convolutional layers, and the training is stopped when the loss no longer decreases.</p><p>Data augmentation is used by following the SSD <ref type="bibr" target="#b16">[17]</ref>. We take a similar strategy by randomly sampling a patch from an image and set the minimum Jaccard overlap with ground truth word bounding boxes to {0.1, 0.3, 0.5, 0.7, 0.9}. The sampled patches are then resized to 704 × 704 and are randomly mirrored with color distortion. The NMS threshold and confidence threshold are set to 0.3 and 0.7, respectively. Our methods were implemented using Caffe <ref type="bibr" target="#b12">[13]</ref>, with TITAN X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploration Study</head><p>We evaluate the effectiveness of the proposed text attentional module (TAM) and hierarchical inception module (HIM) individually. We compare them with our baseline model, which is extended from the SSD framework by simply modifying the word prediction module (as described in Sect. 3.3). Experimental results on the ICDAR 2013 are compared in <ref type="table" target="#tab_1">Table 2</ref>, in the terms of precision, recall  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Recall Precision F-measure SSD <ref type="bibr" target="#b16">[17]</ref> 0.60 0.80 0.68 Textboxes <ref type="bibr" target="#b15">[16]</ref> 0  <ref type="bibr" target="#b15">[16]</ref>, which is also extended from the SSD. Second, the proposed TAM and HIM both achieve large improvements in recall, indicating that the proposed text-specific modules can increase model accuracy and improve its capability for identifying challenging words, as shown in <ref type="figure" target="#fig_1">Fig. 4 and 6</ref>. Third, by incorporating both TAM and HIM into a single model, the final single-shot detector further improves the performance in both recall and precision, obtaining a final F-measure of 0.87. This suggests that both TAM and HIM can compensate for each other in our model. Notice that the performance on the ICDAR2013 is saturated, and the improvement obtained by each component is significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons with state-of-the-art methods</head><p>Qualitative results. Detection results on a number of very challenging images are demonstrated in <ref type="figure" target="#fig_4">Fig. 7</ref>, where our detector is able to correctly identify many extremely challenging words, some of which are even difficult for hu-man. It is important to point out that the word-level detection by our detector is particularly accurate, even for those very small-scale words that are closed to each other.</p><p>On the ICDAR 2013. We compare our performance against recently published results in <ref type="table" target="#tab_3">Table 3</ref>. All our results are reported on single-scale test images. On the ICDAR 2013 which is designed for near-horizontal text detection, our detector achieves the state-of-the-art performance on both evaluation standards. By using the ICDAR 2013 standard, which focuses on word-level evaluation, our method obtains an F-measure of 0.87, outperforming all other methods compared, including recent FCRN <ref type="bibr" target="#b3">[4]</ref>, CTPN <ref type="bibr" target="#b27">[28]</ref>, and TextBoxes <ref type="bibr" target="#b15">[16]</ref>. By using the DetEval standard, our method is comparable with the CTPN, and again has substantial improvements over the others. It has to point out that the DetEval standard allows for text-line level evaluation as well, and is less strict than the ICDAR 2013 standard. Our method encourages a more accurate detection in word level, resulting in better performance in the ICDAR 2013 standard.</p><p>In addition, we further evaluate our method for end-toend word spotting on the ICDAR 2013 by directly combining our detector with recent word recognition model presented in <ref type="bibr" target="#b5">[6]</ref>. We obtain an accuracy of 0.83 on generic case, which is comparable to recent results: 0.79 in <ref type="bibr" target="#b10">[11]</ref> and 0.85 in <ref type="bibr" target="#b3">[4]</ref>.</p><p>On the ICDAR 2015. The ability to work on multiorientation text is verified on the ICDAR 2015. Our method obtains an F-measure of 0.77, which is a remarkable improvement over 0.61 achieved by CTPN <ref type="bibr" target="#b27">[28]</ref>. This clearly indicates that our method generalizes much better to multiorientation text than the CTPN. On the ICDAR 2015, our method got the state-of-the-art performance in terms of recall, precision, and F-measure, surpassing recently published results in <ref type="bibr" target="#b17">[18]</ref> (0.71 F-measure) by a large margin.  In all implementations, our method obtains a higher recall than the others. This suggests that the proposed textspecific modules enable our detector with stronger capability for detecting extremely challenging text in a high wordlevel accuracy, including the very small-scale, significantlyslant or highly-ambiguous words, as shown in <ref type="figure" target="#fig_4">Fig. 7</ref>.</p><p>On the COCO-Text. We further evaluate our detector on the COCO-Text <ref type="bibr" target="#b28">[29]</ref>, which is a large-scale text dataset. We achieve state-of-the-art performance with an F-score of 0.37, which improves recent state-of-the-art result <ref type="bibr" target="#b31">[32]</ref> slightly. This demonstrates strong generalization ability of our method to work practically on large-scale images in the unseen scenarios. Again, our method achieves a significantly higher recall than all compared approaches.</p><p>Running time. We compare running time of various methods on the ICDAR 2013 (in <ref type="table" target="#tab_3">Table 3</ref>). Our detector achieves running time of 0.13s/image using a single GPU, which is slightly faster than CTPN using 0.14s/image. It is sightly slower than TextBoxes <ref type="bibr" target="#b15">[16]</ref>, but has substantial performance improvements. In addition, the TextBoxes was not tested on the multi-orientation text. Besides, FCRN <ref type="bibr" target="#b3">[4]</ref> predicts word boxes at 0.07s/image, but it takes 1.2s/image for post-processing the generated boxes. All recent CNN-based approaches are compared on GPU time, which has become the main stream with success of deep learning technologies. The fastest CPU based text detector is FASText <ref type="bibr" target="#b0">[1]</ref>, using 0.15s/image. But its performance is significantly lower than recent CNN-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a fast yet accurate text detector that predicts word-level bounding boxes in one shot. We proposed a novel text attention mechanism which encodes strong supervised information of text in training. This enables the model to automatically learn a text attentional map that implicitly identifies rough text regions in testing, allowing it to work essentially in a coarse-to-fine manner. We developed a hierarchical inception module that efficiently aggregates multi-scale inception features. This further enhances convolutional features by encoding more local details and stronger context information. Both text-specific developments result in a powerful text detector that works reliably on the multi-scale and multi-orientation text. Our method achieved new state-of-the-art results on the ICDAR 2013, ICDAR 2015 and COCO-Text benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Text attention module. It computes a text attention map from Aggregated Inception Features (AIFs). The attention map indicates rough text regions and is further encoded into the AIFs. The attention module is trained by using a pixel-wise binary mask of text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>We compare detection results of the baseline model and the model with our text attention module (TAM), which enables the detector with stronger capability for identifying extremely challenging text with a higher word-level accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Inception module. The convolutional maps are processed through four different convolutional operations, with Dilated convolutions<ref type="bibr" target="#b33">[34]</ref> applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Comparisons of baseline model and Hierarchical Inception Module (HIM) model. The HIM allows the detector to handle extremely challenging text, and also improves word-level detection accuracy. tiple convolutional operations with channel concatenation, the inception features have multi-scale receptive fields and thus can focus on image content in a wide range of scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Detection results by the proposed single-shot text detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Scales and aspect rations of the default box applied for each AIF or Inception layer.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Scales of Default Box</cell></row><row><cell cols="4">AIF-1 AIF-2 AIF-3 Inc-4 Inc-5 Inc-6 Inc-7</cell></row><row><cell>7.7</cell><cell>38.4</cell><cell>69.1</cell><cell>102.4 194.6 286.7 378.9</cell></row><row><cell>17.9</cell><cell>48.6</cell><cell>79.4</cell><cell>133.1 225.3 317.4 409.6</cell></row><row><cell>28.2</cell><cell>58.9</cell><cell>89.6</cell><cell>163.8 256.0 348.2 440.3</cell></row><row><cell cols="4">Aspect Ratios: {0.5, 1, 2, 3, 5, 7, 9, 11}</cell></row><row><cell cols="4">effectiveness of the proposed hierarchical inception module</cell></row><row><cell cols="3">is demonstrated in Fig. 6.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Exploration study on the ICDAR 2013 dataset, based on the new ICDAR13 standard.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of the state-of-the-art results on the ICDAR 2013 and ICDAR 2015. The results are reported in the terms of Recall (R), Precision (P) and F-measure (F)</figDesc><table><row><cell></cell><cell cols="2">ICDAR 2013 dataset</cell><cell></cell><cell></cell><cell></cell><cell cols="2">ICDAR 2015 dataset</cell><cell></cell></row><row><cell>Method</cell><cell>ICDAR Standard R P F</cell><cell>R</cell><cell>DetEval P</cell><cell>F</cell><cell cols="2">Time(s) Method</cell><cell>R</cell><cell>P</cell><cell>F</cell></row><row><cell>SSD [17]</cell><cell cols="4">0.60 0.80 0.68 0.60 0.80 0.69</cell><cell>0.10</cell><cell>Deep2Text-MO</cell><cell cols="3">0.32 0.50 0.39</cell></row><row><cell>Yin [33]</cell><cell cols="4">0.66 0.88 0.76 0.69 0.89 0.78</cell><cell>0.43</cell><cell>HUST MCLAB</cell><cell cols="3">0.44 0.38 0.41</cell></row><row><cell>Neumann [20]</cell><cell>0.72 0.82 0.77</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.40</cell><cell>AJOU</cell><cell cols="3">0.47 0.47 0.47</cell></row><row><cell>Neumann [21]</cell><cell>0.71 0.82 0.76</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.40</cell><cell>NJU-Text</cell><cell cols="3">0.36 0.73 0.48</cell></row><row><cell>FASText [1]</cell><cell>0.69 0.84 0.77</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.15</cell><cell>CASIA USTB</cell><cell cols="3">0.40 0.62 0.48</cell></row><row><cell>Zhang [35]</cell><cell cols="4">0.74 0.88 0.80 0.76 0.88 0.82</cell><cell>60</cell><cell>StradVision1</cell><cell cols="3">0.46 0.53 0.50</cell></row><row><cell>TextFlow [27]</cell><cell>0.76 0.85 0.80</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.94</cell><cell>StradVision2</cell><cell cols="3">0.37 0.77 0.50</cell></row><row><cell>Text-CNN [8]</cell><cell cols="4">0.73 0.93 0.82 0.76 0.93 0.84</cell><cell>4.6</cell><cell cols="4">MCLAB FCN [36] 0.43 0.71 0.54</cell></row><row><cell>FCRN [4]</cell><cell cols="4">0.76 0.94 0.84 0.76 0.92 0.83</cell><cell>1.27</cell><cell>CTPN [28]</cell><cell cols="3">0.52 0.74 0.61</cell></row><row><cell>TextBoxes [16]</cell><cell cols="4">0.74 0.86 0.80 0.74 0.88 0.81</cell><cell>0.09</cell><cell>Yao et. al.[32]</cell><cell cols="3">0.57 0.72 0.64</cell></row><row><cell>CTPN [28]</cell><cell cols="4">0.73 0.93 0.82 0.83 0.93 0.88</cell><cell>0.14</cell><cell>DMPNet [18]</cell><cell cols="3">0.68 0.73 0.71</cell></row><row><cell cols="5">Proposed method 0.86 0.88 0.87 0.86 0.89 0.88</cell><cell>0.13</cell><cell>Proposed method</cell><cell cols="3">0.73 0.80 0.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on the COCO-text dataset</figDesc><table><row><cell cols="3">COCO-text dataset</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Recall Precision F-score</cell></row><row><cell>Baseline model C</cell><cell>0.05</cell><cell>0.19</cell><cell>0.07</cell></row><row><cell>Baseline model B</cell><cell>0.11</cell><cell>0.90</cell><cell>0.19</cell></row><row><cell>Baseline model A</cell><cell>0.23</cell><cell>0.84</cell><cell>0.36</cell></row><row><cell>Yao [32]</cell><cell>0.27</cell><cell>0.43</cell><cell>0.33</cell></row><row><cell>Proposed method</cell><cell>0.31</cell><cell>0.46</cell><cell>0.37</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported in part by National Science Foundation (CNS-1624782, OAC-1229576, CCF-1128805), National Institutes of Health (R01-GM110240), Industrial Collaboration Project (Y5Z0371001), National Natural Science Foundation of China (U1613211, 61503367) and Guangdong Research Program (2015B010129013, 2015A030310289).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fastext: Efficient unconstrained scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno>ICCV. 8</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Canny text detector: Fast and robust scene text localization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
		<idno>CVPR. 1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. 1, 2, 3</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>CVPR. 5</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading scene text in deep convolutional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. 1</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Accurate text localization in natural image with cascaded convolutional text network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09423</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text-attentional convolutional neural networks for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2529" to="2541" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text localization in natural images using stroke feature transform and text covariance descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolutional neural networks induced mser trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>ACM MM. 6</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<idno>ICDAR. 6</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<idno>ICDAR. 6</idno>
		<title level="m">Icdar 2015 competition on robust reading</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>In AAAI. 2, 3, 4, 6</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>In ECCV. 2, 3, 6</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. 1, 2, 3, 4</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient scene text localization and recognition with local character refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno>ICDAR. 8</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time lexicon-free scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1872" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matas</surname></persName>
		</author>
		<idno>CVPR. 1</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ICLR. 3</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>CVPR. 5</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>In ECCV. 1, 2, 3, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno>CVPR. 2</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09002</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust text detection in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="970" to="983" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Symmetry-based text line detection in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno>CVPR. 8</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-oriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In CVPR. 1, 2, 3, 8</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A text detection system for natural scenes with convolutional feature learning and cascaded classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
