<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abduallah</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
							<email>mohamed.elhoseiny@kaust.edu.sa</email>
							<affiliation key="aff1">
								<orgName type="department">KAUST</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Claudel</surname></persName>
							<email>christian.claudel@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Better machine understanding of pedestrian behaviors enables faster progress in modeling interactions between agents such as autonomous vehicles and humans. Pedestrian trajectories are not only influenced by the pedestrian itself but also by interaction with surrounding objects. Previous methods modeled these interactions by using a variety of aggregation methods that integrate different learned pedestrians states. We propose the Social Spatio-Temporal Graph Convolutional Neural Network (Social-STGCNN), which substitutes the need of aggregation methods by modeling the interactions as a graph. Our results show an improvement over the state of art by 20% on the Final Displacement Error (FDE) and an improvement on the Average Displacement Error (ADE) with 8.5 times less parameters and up to 48 times faster inference speed than previously reported methods. In addition, our model is data efficient, and exceeds previous state of the art on the ADE metric with only 20% of the training data. We propose a kernel function to embed the social interactions between pedestrians within the adjacency matrix. Through qualitative analysis, we show that our model inherited social behaviors that can be expected between pedestrians trajectories. Code is available at https://github. com/abduallahmohamed/Social-STGCNN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Predicting pedestrian trajectories is of major importance for several applications including autonomous driving and surveillance systems. In autonomous driving, an accurate prediction of pedestrians trajectories enables the controller to plan ahead the motion of the vehicle in an adversarial environment. For example, it is a critical component for collision avoidance systems or emergency braking systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref>. In surveillance systems, forecasting ** Equal advising.  pedestrian trajectories is critical in helping identifying suspicious activities <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b19">20]</ref>. The trajectory of a pedestrian is challenging to predict, due to the complex interactions between the pedestrian with the environment. Objects potentially influencing the trajectory of a pedestrian include physical obstacles such as trees or roads, and moving objects including vehicles and other pedestrians. According to <ref type="bibr" target="#b18">[19]</ref>, 70% of pedestrians tend to walk in groups. The interactions between pedestrians are mainly driven by common sense and social conventions. The complexity of pedestrian trajectory prediction comes from different social behaviors such as walking in parallel with others, within a group, collision avoidance and merging from different directions into a specific point. Another source of complexity is the randomness of the motion, given that the target destination and intended path of the pedestrian are unknown.</p><p>The social attributes of pedestrian motions encouraged researchers in this area to focus on inventing deep methods to model social interactions between pedestrians. In the Social-LSTM <ref type="bibr" target="#b0">[1]</ref> article, deep learning based model is applied to predict the pedestrians trajectories by modeling each pedestrian trajectory via a recurrent deep model. The outputs of recurrent models are made to interact with each other via a pooling layer. Several articles <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref> followed this direction. Social-LSTM <ref type="bibr" target="#b0">[1]</ref> modeled the pedestrian trajec-tories as a bi-variate Gaussian distribution, while some of others aimed at predicting deterministic trajectories. Another direction is to use Generative Adversarial Networks (GANs) for this task, assuming that the distribution of trajectories is multi-modal. Several articles <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b12">13]</ref> used GANs to predict distributions of future trajectories. For these models, generators are designed using recurrent neural networks, and again, aggregation methods are relied upon to extract the social interactions between pedestrians. We argue that a limitation of earlier articles comes from the use of recurrent architectures, which are parameter inefficient and expensive in training <ref type="bibr" target="#b2">[3]</ref>. We overcome this limitation through the use of temporal convolutional architectures.</p><p>In addition to the limitation of recurrent architectures, aggregation layers used in earlier works can also limit their performance. The aggregation layer takes the hidden states of the recurrent units as inputs. It is expected to assimilate a global representation of the scene, since each recurrent unit models a pedestrian trajectory. However, there are two issues within this type of aggregation. First, the aggregation in feature states is neither intuitive nor direct in modelling interactions between people, as the physical meaning of feature states is difficult to interpret. Second, since the aggregation mechanisms are usually based on heuristics like pooling, they could fail in modeling interactions between pedestrians correctly. For example, the pooling operation is known to be leaky in information <ref type="bibr" target="#b25">[26]</ref>. In order to directly capture the interactions between pedestrians and predict future paths from these, the recent article social-BiGAT <ref type="bibr" target="#b9">[10]</ref> relies on a graph representation to model social interactions. As the topology of graphs is a natural way to represent social interactions between pedestrians in a scene, we argue that it is a more direct, intuitive and efficient way to model pedestrians interactions than aggregation based methods. We also argue that social-BiGAT did not make the most of the graph representation, since they used it only as a pooling mechanism for recurrent units states. Social-STGCNN benefits more from graph representation through modeling the scene with as spatio-temporal graph and performs on it.</p><p>We designed Social-STGCNN to overcome the two aforementioned limitations. First, we model the pedestrians trajectories from the start as a spatio-temporal graph to replace the aggregation layers. The graph edges model the social interactions between the pedestrians. We propose a weighted adjacency matrix in which the kernel function quantitatively measure the influence between pedestrians. To address issues associated with recurrent units, our model manipulates over the spatio-temporal graph using a graph Convolutional Neural Networks (CNN)s and a temporal CNNs. This allows our model to predict the whole sequence in a single shot. Due to the above design, our model outperforms previous models in terms of prediction accuracy, parameters size, inference speed and data efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The recent interest in autonomous driving has lead to increasing focus on pedestrian trajectory prediction. Recently, new deep models are making promising progresses on this task. In this section, we give a brief review of related work. Human trajectory prediction using deep models Social-LSTM <ref type="bibr" target="#b0">[1]</ref> is one of the earliest deep model focusing on pedestrian trajectory prediction. Social-LSTM uses a recurrent network to model the motion of each pedestrian, then they aggregated the recurrent outputs using a pooling mechanism and predict the trajectory afterwards. Social-LSTM assumes the pedestrian trajectory follow a bi-variate Gaussian distribution, in which we follow this assumption in our model. Later works such as Peek Into The Future (PIF) <ref type="bibr" target="#b13">[14]</ref> and State-Refinement LSTM (SR-LSTM) <ref type="bibr" target="#b29">[30]</ref> extends <ref type="bibr" target="#b0">[1]</ref> with visual features and new pooling mechanisms to improve the prediction precision. It is noticeable that SR-LSTM <ref type="bibr" target="#b29">[30]</ref> weighs the contribution of each pedestrian to others via a weighting mechanism. It is similar to the idea in Social-BiGAT <ref type="bibr" target="#b9">[10]</ref> which uses an attention mechanism to weigh the contribution of the recurrent states that represent the trajectories of pedestrians. Based on the assumption that pedestrian trajectories follow multi-modal distributions, Social-GAN <ref type="bibr" target="#b5">[6]</ref> extends Social LSTM <ref type="bibr" target="#b0">[1]</ref> into a Recurrent Neural Network (RNN) based generative model. Sophie <ref type="bibr" target="#b22">[23]</ref> used a CNNs to extract the features from the scene as a whole then a two way attention mechanism is used per pedestrian. Later on, Sophie concatenates the attention outputs with the visual CNN outputs then a Long Short Term Memory (LSTM) autoencoder based generative model is used to generate the future trajectories. The work CGNS <ref type="bibr" target="#b12">[13]</ref> is similar to Sophie <ref type="bibr" target="#b22">[23]</ref> in terms of the architecture but they used a Gated Recurrent Units(GRU)s instead of LSTMs. We notice that most previous works were circulating around two ideas, model each pedestrian motion using a recurrent net and combine the recurrent nets using a pooling mechanism. Recent work Social-BiGAT <ref type="bibr" target="#b9">[10]</ref> relies on graph attention networks to model the social interactions between pedestrians. The LSTM outputs are fed to the graph in Social-BiGAT. One key difference between our model Social-STGCNN and Social-BiGAT is that we directly model pedestrian trajectories as a graph from the beginning, where we give meaningful values for vertices. Recent Advancements in Graph CNNs Graph CNNs were introduced by <ref type="bibr" target="#b7">[8]</ref> which extends the concept of CNNs into graphs. The Convolution operation defined over graphs is a weighted aggregation of target node attributes with the attributes of its neighbor nodes. It is similar to CNNs but the convolution operation is taken over the adjacency matrix of the graphs. The works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref> extend the graph CNNs to other applications such as matrix completion and Variational Auto Encoders. One of the development related to our work is the ST-GCNN <ref type="bibr" target="#b26">[27]</ref>. ST-GCNN is a spatio-temporal Graph CNN that was originally designed to solve skeleton-based action recognition problem. Even though the architecture itself was designed to work on a classification task, we adapt it to suit our problem. In our work, ST-GCNNs extract both spatial and temporal information from the graph creating a suitable embedding. We then operate on this embedding to predict the trajectories of pedestrians. Details are shown in section 4. Temporal Convolutional Neural Networks (TCNs) Starting from <ref type="bibr" target="#b2">[3]</ref>, the argue between the usage of Recurrent Neural Networks (RNN)s versus the usage of temporal CNNs for sequential data modeling is highlighted. Introduced by <ref type="bibr" target="#b2">[3]</ref>, Temporal Convolutional Neural Networks(TCNs) take a stacked sequential data as input and predict a sequence as a whole. This could alleviate the problem of error accumulating in sequential predictions made by RNNs. What is more, TCNs are smaller in size compared to RNNs. We were inspired by TCNs and designed a temporal CNN model that extends the capabilities of ST-GCNNs. More details about this are in the model description section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>Given a set of N pedestrians in a scene with their corresponding observed positions tr n o , n ∈ {1, . . . , N } over a time period T o , we need to predict the upcoming trajectories tr n p over a future time horizon T p . For a pedestrian n, we write the corresponding trajectory to be predicted as tr n p = { p n t = (x n t , y n t ) | t ∈ {1, . . . , T p }}, where (x n t , y n t ) are random variables describing the probability distribution of the location of pedestrian n at time t, in the 2D space. We make the assumption that (x n 4. The Social-STGCNN Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Description</head><p>The Social-STGCNN model consists of two main parts: the Spatio-Temporal Graph Convolution Neural Network (ST-GCNN) and the Time-Extrapolator Convolution Neural Network (TXP-CNN). The ST-GCNN conducts spatiotemporal convolution operations on the graph representation of pedestrian trajectories to extract features. These features are a compact representation of the observed pedestrian trajectory history. TXP-CNN takes these features as inputs and predicts the future trajectories of all pedestrians as a whole. We use the name Time-Extrapolator because TXP-CNNs are expected to extrapolate future trajectories through convolution operation. <ref type="figure">Figure 2</ref> illustrates the overview of the model. Graph Representation of Pedestrian Trajectories We first introduce the construction of the graph representation of pedestrian trajectories. We start by constructing a set of spatial graphs G t representing the relative locations of pedestrians in a scene at each time step t. G t is defined as</p><formula xml:id="formula_0">G t = (V t , E t ), where V t = {v i z (l+1) = σ( k h=1 k w=1 (p(z (l) , h, w)).w (l) (h, w)) (3)</formula><p>where k is the kernel size and p(.) is the sampling function which aggregates the information of neighbors centering around z <ref type="bibr" target="#b4">[5]</ref> and σ is an activation function and (l) indicates layer l.</p><p>The graph convolution operation is defined as:</p><formula xml:id="formula_1">v i(l+1) = σ( 1 Ω v j(l) ∈B(v i(l) ) p(v i(l) , v j(l) ).w(v i(l) , v j(l) )) (4) where 1 Ω is a normalization term, B(v i ) = {v j |d(v i , v j ) ≤ D} is the neighbor set of vertices v i and d(v i , v j )</formula><p>denotes the shortest path connecting v i and v j . Note that Ω is the cardinality of the neighbor set. Interested readers are referred to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref> for more detailed explanations and reasoning. <ref type="figure">Figure 2</ref>. The Social-STGCNN Model. Given T frames, we construct the spatio-temporal graph representing G = (V, A). Then G is forwarded through the Spatio-Temporal Graph Convolution Neural Networks (ST-GCNNs) creating a spatio-temporal embedding. Following this, the TXP-CNNs predicts future trajectories. P is the dimension of pedestrian position, N is the number of pedestrians, T is the number of time steps andP is the dimensions of the embedding coming from ST-GCNN.</p><p>Spatio-Temporal Graph Convolution Neural Network (ST-GCNNs) ST-GCNNs extends spatial graph convolution to spatio-temporal graph convolution by defining a new graph G whose attributes are the set of the attributes of G t . G incorporates the spatio-temporal information of pedestrian trajectories. It is worth noticing that the topology of G 1 , . . . , G T is the same, while different attributes are assigned to v i t when t varies. Thus, we define G</p><formula xml:id="formula_2">as (V, E), in which V = {v i | i ∈ {1, . . . , N }} and E = {e ij | ∀i, j ∈ {1, . . . , N }}. The attributes of ver- tex v i in G is the set of v i t , ∀t ∈ {0, . . . , T }.</formula><p>In addition, the weighted adjacency matrix A corresponding to G is the set of {A 1 , . . . , A T }. We denote the embedding resulting from ST-GCNN asV .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time-Extrapolator Convolution Neural Network (TXP-CNN)</head><p>The functionality of ST-GCNN is to extract spatiotemporal node embedding from the input graph. However, our objective is to predict further steps in the future. We also aim to be a stateless system and here where the TXP-CNN comes to play. TXP-CNN operates directly on the temporal dimension of the graph embeddingV and expands it as a necessity for prediction. Because TXP-CNN depends on convolution operations on feature space, it is less in parameters size compared to recurrent units. A property to note regards TXP-CNN layer that it is not a permutation invariant as changes in the graph embedding right before TXP-CNN leads to different results. Other than this, if the order of pedestrians is permutated starting from the input to Social-STGCNN then the predictions are invariant.</p><p>Overall, there are two main differences between Social-STGCNN and ST-GCNN <ref type="bibr" target="#b26">[27]</ref>. First, Social-STGCNN constructs the graph in a totally different way from ST-GCNN with a novel kernel function. Second, beyond the spatiotemporal graph convolution layers, we added the flexibility in manipulating the time dimension using the TXP-CNN. ST-GCNN was originally designed for classification. By using TXP-CNN, our model was able to utilize the graph embedding originating from ST-GCNN to predict the futuree trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementing Social-STGCNN</head><p>Several steps are necessary to implement the model correctly. We first normalize the adjacency matrix for the ease of learning. The adjacency matrix A is a stack of {A 1 , . . . , A T }, we symmetrically normalize each A t using the following form <ref type="bibr" target="#b7">[8]</ref> A</p><formula xml:id="formula_3">t = Λ − 1 2 tÂt Λ − 1 2 t</formula><p>whereÂ t = A t + I and Λ t is the diagonal node degree matrix ofÂ t . We useÂ and Λ to denote the stack ofÂ t and Λ t respectively. The normalization of adjacency is essential for the graph CNN to work properly, as outlined in <ref type="bibr" target="#b7">[8]</ref>. We denote the vertices values at time step t and network layer l as V</p><formula xml:id="formula_4">(l) t . Suppose V (l) is the stack of V (l)</formula><p>t . With the above definitions, we can now implement the ST-GCNN layers defined in equation 4 as follows.:</p><formula xml:id="formula_5">f (V (l) , A) = σ(Λ − 1 2Â Λ − 1 2 V (l) W (l) )<label>(5)</label></formula><p>where W (l) is the matrix of trainable parameters at layer l.</p><p>After applying the ST-GCNN, we have features that compactly represent the graph. The TXP-CNN receives features V and treats the time dimension as feature channels. The TXP-CNN is made up of a series of residual connected CNNs. Only the first layer in TXP-CNN does not have a residual connection as it receivesV from the ST-GCNNs, in which they differ in terms of the dimensions of the observed samples and the samples to be predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Datasets and Evaluation Metrics</head><p>The model is trained on two human trajectory prediction datasets: ETH <ref type="bibr" target="#b20">[21]</ref> and UCY <ref type="bibr" target="#b10">[11]</ref>. ETH contains two scenes named ETH and HOTEL, while UCY contains three scenes named ZARA1, ZARA2 and UNIV. The trajectories in datasets are sampled every 0.4 seconds. Our method of training follows the same strategy as Social-LSTM <ref type="bibr" target="#b0">[1]</ref>. In Social-LSTM, the model was trained on a portion of a specific dataset and tested against the rest and validated versus the other four datasets. When being evaluated, the model observes the trajectory of 3.2 seconds which corresponds to 8 frames and predicts the trajectories for the next 4.8 seconds that are 12 frames.</p><p>Two metrics are used to evaluate model performance: the Average Displacement Error (ADE) <ref type="bibr" target="#b20">[21]</ref> defined in equation 6 and the Final Displacement Error (FDE) <ref type="bibr" target="#b0">[1]</ref> defined in equation 7. Intuitively, ADE measures the average prediction performance along the trajectory, while the FDE considers only the prediction precision at the end points. Since Social-STGCNN generates a bi-variate Gaussian distribution as the prediction, to compare a distribution with a certain target value, we follow the evaluation method used in Social-LSTM <ref type="bibr" target="#b0">[1]</ref> in which 20 samples are generated based on the predicted distribution. Then the ADE and FDE are computed using the closest sample to the ground truth. This method of evaluation were adapted by several works such as Social-GAN <ref type="bibr" target="#b5">[6]</ref> and many more.</p><formula xml:id="formula_6">ADE = n∈N t∈Tp p n t − p n t 2 N × T p (6) FDE = n∈N p n t − p n t 2 N , t = T p<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Results Analysis</head><p>Model configuration and training setup Social-STGCNN is composed of a series of ST-GCNN layers followed by TXP-CNN layers. We use PReLU <ref type="bibr" target="#b6">[7]</ref> as the activation function σ across our model. We set a training batch size of 128 and the model was trained for 250 epochs using Stochastic Gradient Descent (SGD). The initial learning rate is 0.01, and changed to 0.002 after 150 epochs. According to our ablation study in table 6, the best model to use has one ST-GCNN layer and five TXP-CNN layers. Furthermore, it is noticeable that when the number of ST-GCNN layers increases, the model performance decreases. Apparently, this problem of going deep using graph CNN was noticed by the work in <ref type="bibr" target="#b11">[12]</ref>, in which they proposed a method to solve it. Unfortunately, their solution does not extend to temporal graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Ablation Study of Kernel Function</head><p>In this section, our objective is to find a suitable kernel function to construct the weighted adjacency matrix. The weighted adjacency matrix A t is a representation of the graph edges attributes. The kernel function maps attributes at v i t and v j t to a value a ij t attached to e ij t . In the implementation of Social-STGCNN , A t weights the vertices contribu- tions to each other in the convolution operations. The kernel function can thus be considered as a prior knowledge about the social relations between pedestrians. A straightforward idea in designing the kernel function is to use the distance measured by the L 2 norm defined in equation 8 between pedestrians to model their impacts to each other. However, this is against the intuition that the pedestrians tend to be influenced more by closer ones. To overcome this, we use similarity measure between the pedestrians. One of the proposals is to use the inverse of L 2 norm as defined in equation <ref type="bibr" target="#b9">10</ref>. The term is added in denominator to ensure numerical stability. Another candidate function is the Gaussian Radial Basis Function <ref type="bibr" target="#b24">[25]</ref>, shown in equation 9. We compare the performance of these kernel functions through experiments. The case that all the values in adjacency matrix between different nodes are set to one is used as a baseline. According to results listed in table 6.1, the best performance comes from a ij sim,t defined in function 2. The difference between functions 10 and 2 exists in the case where v i t − v j t 2 = 0. In function 2, we set a ij sim,t = 0 when v i t − v j t 2 = 0 because it is assumed that the two pedestrians can be viewed as the same person when they stay together. Without it, the model will have an ambiguity in the relationship between pedestrians. For this, we use a ij sim,t in the definition of the adjacency matrix in all of our experiments.</p><formula xml:id="formula_7">a ij L2,t = v i t − v j t 2 (8) a ij exp,t = exp (− v i t − v j t 2 ) σ (9) a ij sim ,t = 1 v i t − v j t 2 +<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Quantitative Analysis</head><p>The performance of Social-STGCNN is compared with other models on ADE/FDE metrics in Ground truth Observed Prediction <ref type="figure">Figure 3</ref>. Qualitative analysis of Social-STGCNN . We compare models trained with different kernel functions (Kernel 1: equation 8 and Kernel 2: equation 2) versus previous models. Social-GAN <ref type="bibr" target="#b5">[6]</ref> is taken as a baseline for the comparison. Illustration scenes are from the ETH <ref type="bibr" target="#b20">[21]</ref> and UCY <ref type="bibr" target="#b10">[11]</ref> datasets. We used the pre-trained Social-GAN model provided by <ref type="bibr" target="#b5">[6]</ref>. A variety of scenarios are shown: two individuals walking in parallel (1)(2), two persons meeting from the same direction (3), two persons meeting from different directions (4) and one individual meeting another group of pedestrians from an angle <ref type="bibr" target="#b4">(5)</ref>. For each case, the dashed line is the true trajectory that the pedestrians are taking and the color density is the predicted trajectory distribution.</p><p>Social-STGCNN outperforms all previous methods on the two metrics. The previous state of art on the FDE metric is SR-LSTM <ref type="bibr" target="#b29">[30]</ref> with an error of 0.94. Our model has an error of 0.75 on the FDE metric which is about 20% less than the state of the art. The results in qualitative analysis explains how Social-STGCNN encourages social behaviors that enhanced the FDE metric. For the ADE metric, Social-STGCNN is slightly better than the state-of-art SR-LSTM by 2%. Also, it is better than the previous generative methods with an improvement ranging in between 63% compared to S-LSTM <ref type="bibr" target="#b0">[1]</ref> and 4% compared to PIF <ref type="bibr" target="#b13">[14]</ref>. Interestingly, our model without the vision signal that contains scene context outperforms methods that utilized it such as SR-LSTM, PIF and Sophie.</p><p>Inference speed and model size S-GAN-P <ref type="bibr" target="#b5">[6]</ref> previously had the smallest model size with 46.3k parameters. The size of Social-STGCNN is 7.6K parameters only which is about one sixth of the number of parameters in S-GAN-P. In terms of inference speed, S-GAN-P was previously the fastest method with an inference time of 0.0968 seconds per inference step. The inference time of our model is 0.002 seconds per inference step which is about 48 × faster than S-GAN-P. <ref type="table">Table 6</ref> lists out the speed comparisons between our model and publicly available models which we could bench-mark against. We achieved these results because we overcame the two limitations of previous methods which used recurrent architecture and aggregation mechanisms via the design of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Efficiency</head><p>In this section, we evaluate if the efficiency in model size leads to a better efficiency in learning from   <ref type="bibr" target="#b29">[30]</ref> 64.9K (8.5x) 0.1578 (78.9x) S-GAN-P <ref type="bibr" target="#b5">[6]</ref> 46.3K (6.1x) 0.0968 (48.4x) PIF <ref type="bibr" target="#b13">[14]</ref> 360.3K (47x) 0.1145 (57.3x)</p><p>Social-STGCNN 7.6K 0.0020  fewer samples of the data. We ran a series of experiments where 5%, 10%, 20% and 50% of the training data. The training data were randomly selected. Once selected, we fed the same data to train different models. Social-GAN is employed as a comparison baseline because it has least trainable parameters amongst previous deep models. <ref type="figure">Figure 6.2</ref> shows the data learning efficiency experiments results with mean and error. We notice that our model exceeds the state of the art on the FDE metric when only 20% of training data is used. Also, Social-STGCNN exceeds the performance of Social-GAN on the ADE metric when trained only on with 20% of the training data. The results also show that S-GAN-P did not improve much in performance with more training data, unlike the present model. It is an interesting phenomenon that S-GAN-P does not absorb more training data. We assume that this behavior is due to the fact that GANs are data efficient because they can learn a distribution from few training samples. However, the training of GANs can easily fall into the problem of mode collapse. In comparison, the data efficiency of our model comes from the parameter efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Qualitative Analysis</head><p>The quantitative analysis section shows that Social-STGCNN outperforms previous state-of-art in terms of ADE/FDE metrics. We now qualitatively analyze how Social-STGCNN captures the social interactions between pedestrians and takes that into consideration when predicting the distributions. We show cases in which Social-STGCNN successfully predicts collision free trajectories between pedestrians coming from different angles, maintains parallel walking, and correctly predicts the outcome of situations where person meets with a group of pedestrians.</p><p>We qualitatively compare the prediction results between Social-GAN <ref type="bibr" target="#b5">[6]</ref>, Social-STGCNN with L 2 norm (equation 8) as the kernel function and Social-STGCNN with inverse L 2 norm (equation 2) as the kernel function.</p><p>Parallel walking In scenarios one and two in figure 3, two pedestrians are walking in parallel. Usually, when people are walking in parallel they are tightly connected to each other and their momentum will be preserved in the future. The predictions by Social-STGCNN and Social-GAN all show that these two pedestrians will keep walking in parallel in the future. However, the predicted density by Social-STGCNN closely matches with the ground truth trajectory unlike the deviation we see in Social-GAN.</p><p>Using our proposed kernel function a sim,t defined in equation 2 for weighted adjacency matrix helps us model the social influences between pedestrians better than using the regular L 2 norm kernel function defined in equation 8. It is shown in scenes one and two that the model with a sim,t preforms much better in maintaining the relative location between people walking side by side. In scene five, similar behavior is observed. Collision avoidance Scenario three and Scenario four in <ref type="figure">figure 3</ref> are scenarios in which two pedestrians are heading towards similar or opposite directions. A collision could happen if they maintain their momentum. In scenario 3, two pedestrians are walking towards a similar direction. The forecast by Social-GAN acts linearly based on the momentum of the pedestrians and may lead to a collision. In the forecast of Social-STGCNN , we notice that the trajectories are adjusted slightly such that they both avoid collision and align well with the observed momentum of pedestrians. As a result, Social-STGCNN matches better with ground truth. In scenario four, Social-GAN fails to avoid the collision, while ours shows a realistic collision free path prediction. Individual meeting a group A more complex scenario is case five in figure 3, in which one person meets a group of parallel walking individuals. Our model suggests that the group of people still walk in parallel while adjusting their heading direction to avoid collision. In this case, although neither our model nor Social-GAN capture the ground truth trajectory very well, the predicted distribution by our model still makes sense from the social interaction point of view.</p><p>Diversity in samples and social behaviors In order to understand in detail how Social-STGCNN generates samples, we plot the samples generated from predicted bi-variate Gaussian distributions. There are two different scenarios in <ref type="figure" target="#fig_3">figure 4</ref>. In the first scene, three people meet from opposite directions. In the other scene, two people merge at an angle. Several patterns of samples could be generated by the predicted distributions. In column two in <ref type="figure" target="#fig_3">figure 4</ref>, the generated samples adjusts the advancing direction to avoid possible collisions in both scenes. Another social attribute of pedestrians is to slow down or accelerate to avoid crash. Samples in the third column in <ref type="figure" target="#fig_3">figure 4</ref> capture this attribute. This analysis shows that our samples encode different expected social behaviors of pedestrians. However, some samples show undesired behaviors such as collision or divergence in the last column. More cases of these undesired behaviors are in the last row of figure4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this article, we showed that a proper graph-based spatio-temporal setup for pedestrian trajectory prediction improves over previous methods on several key aspects, including prediction error, computational time and number of parameters. By applying a specific kernel function in the weighted adjacency matrix together with our model design, Social-STGCNN outperforms state-of-art models over a number of publicly available datasets. We also showed that our configuration results in a data-efficient model and can learn from few data samples. We also qualitatively analyze the performance of Social-STGCNN under situations such as collision avoidance, parallel walking and individual meeting a group. In these situations, Social-STGCNN tend to provide more realistic path forecasts than several other reported methods. Furthermore, Social-STGCNN is extremely efficient computationally, dividing the number of required parameters by a factor of 8.5, and boosting the inference speed by up to 48 × comparing to previous models. In the future, we intend to extend Social-STGCNN to multi-modal settings that involve other moving objects including bicycles, cars and pedestrians.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Pedestrian future trajectories prediction using the Social-STGCNN model. The social interactions between pedestrians and their temporal dynamics are represented by a spatio-temporal graph. We predict the future trajectories in a single pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The first column is the ground truth, while the other columns illustrate samples from our model. The first two rows show two different scenarios where pedestrians merge into a direction or meet from opposite directions. The second and third columns show changes in speed or direction in samples from our model. The last column shows undesired behaviors. The last row show failed samples. Parameters count Inference time S-LSTM [1] 264K (35x) 1.1789 (589x) SR-LSTM-2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Model performance versus shrinked training dataset. The x-axis shows several randomly samples shrink percentages. The shade represents errors. The same shrinked data were used across the models. The figure shows our performance versus Social-GAN which is the closest model in terms of parameter size to ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>.47 / 0.78 0.47 / 0.84 0.44 / 0.75 0.48 / 0.87 3 0.59 / 1.02 0.52 / 0.92 0.54 / 0.93 0.54 / 0.92 5 0.62 / 1.07 0.57 / 0.98 0.59 / 1.02 0.59 / 0.98 7 0.75 / 1.28 0.75 / 1.27 0.62 / 1.07 0.75 /1.28 Ablation study of the Social-STGCNN model. The first row corresponds to the number of TXP-CNN layers. The first column from the left corresponds to the number of ST-GCNN layers. We show the effect of different configurations of Social-STGCNN on the ADE/FDE metric. The best setting is to use one layer for ST-GCNN and five layers for TXP-CNN.</figDesc><table><row><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell></row><row><cell>1 0</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>table 2 .Table 2 .</head><label>22</label><figDesc>.94 0.39 / 0.72 0.82 / 1.59 0.62 / 1.21 0.77 / 1.48 0.79 / 1.59 SR-LSTM-2 * [30] 0.63 / 1.25 0.37 / 0.74 0.51 / 1.10 0.41 / 0.90 0.32 / 0.70 0.45 / 0.94 S-LSTM [1] 1.09 / 2.35 0.79 / 1.76 0.67 / 1.40 0.47 / 1.00 0.56 / 1.17 0.72 / 1.54 S-GAN-P [6] 0.87 / 1.62 0.67 / 1.37 0.76 / 1.52 0.35 / 0.68 0.42 / 0.84 0.61 / 1.21 ADE / FDE metrics for several methods compared to Social-STGCNN are shown.The models with * mark are non-probabilistic. The rest of models used the best amongst 20 samples for evaluation. All models takes as an input 8 frames and predicts the next 12 frames. We notice that Social-STGCNN have the best average error on both ADE and FDE metrics. The lower the better.</figDesc><table><row><cell>Overall,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Parameters size and inference time of different models compared to ours. The lower the better. Models were bench-marked using Nvidia GTX1080Ti GPU. The inference time is the average of several single inference steps. We notice that Social-STGCNN has the least parameters size compared and the least inference time compared to others. The text in blue show how many times our model is faster than others.</figDesc><table><row><cell cols="2">Kernel function ADE / FDE</cell></row><row><cell>a ij L2,t</cell><cell>0.48 / 0.84</cell></row><row><cell>a ij exp,t</cell><cell>0.50 / 0.84</cell></row><row><cell>a ij sim ,t</cell><cell>0.48 / 0.88</cell></row><row><cell>Just ones</cell><cell>0.49 / 0.79</cell></row><row><cell>a ij sim,t</cell><cell>0.44 / 0.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The effect of different kernel functions for the adjacency matrix At over the Social-STGCNN performance.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , y n t ) follows bi-variate Gaussian distribution such that p n t ∼ N (µ n t , σ n t , ρ n t ). Besides, we denote the predicted trajectory asp n t which follows the estimated bi-variate distribution N (μ n t ,σ n t ,ρ n t ). Our model is trained to minimize the negative log-likelihood, which defined as:L n (W) = − Tp t=1 log(P((p n t |μ n t ,σ n t ,ρ n t ))(1)in which W includes all the trainable parameters of the model, µ n t is the mean of the distribution,σ n t is the variances and ρ n t is the correlation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t | ∀i ∈ {1, . . . , N }} is the set of vertices of the graph G t . The observed location (x i t , y i t ) is the attribute of v i t . E t is the set of edges within graph G t which is expressed as E t = {e ij t | ∀i, j ∈ {1, . . . , N }}. e ij t = 1 if v i tand v j t are connected, e ij t = 0 otherwise. In order to model how strongly two nodes could influence with each other, we attach a value a ij t , which is computed by some kernel function for each e ij t . a ij t s are organized into the weighted adjacency matrix A t . We introduce a ij sim,t as a kernel function to be used within the adjacency matrix A t . a ij sim,t is defined in equation 2. We discuss the details of A t kernel function later in section 6.1.a ij sim,t = 1/ v i t − v j t 2 , v i t − v j t 2 = 0 0 , Otherwise.(2)Graph Convolution Neural Network With the graph representation of pedestrian trajectories, we introduce the spatial convolution operation defined on graphs. For convolution operations defined on 2D grid maps or feature maps, the convolution operation is shown in equation 3.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary: More Qualitative Results</head><p>Good Obs GT sgan linear ours Good Good Good Bad <ref type="figure">Figure 6</ref>. Comparison of averaged trajectories predictions per model. Scenes were taken from the ETH dataset <ref type="bibr" target="#b20">[21]</ref>.</p><p>We compare our performance against linear model and Social-GAN <ref type="bibr" target="#b0">[1]</ref> predictions. Unlike the linear or Social-GAN predictions, in which they diverge and do not account for the variation in the pedestrian motion. Social-STGCNN is able to track and align precisely with the ground truth. This can be seen in the first four rows in figure 6. The last row shows cases where our averaged trajectory fails.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intention-aware online pomdp planning for autonomous driving in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee Sun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 ieee international conference on robotics and automation (icra)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="454" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Social gan: Socially acceptable trajectories with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2255" to="2264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational graph autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Social-bigat: Multimodal trajectory forecasting using bicycle-gan and graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martín-Martín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Hamid Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03395</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crowds by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiorgos</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01631</idno>
		<title level="m">Conditional generative neural system for probabilistic trajectory prediction</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Peeking into the future: Predicting future person activities and locations in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5725" to="5734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">People tracking with human motion predictions from social forces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Luber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gian</forename><forename type="middle">Diego</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Tipaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="464" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wee Sun Lee, and Dinesh Manocha. Porca: Modeling and planning for autonomous driving among many pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanfu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panpan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3418" to="3425" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scene-lstm: A model for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huynh</forename><surname>Manh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gita</forename><surname>Alaghband</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04018</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collision position predicting device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Morotomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Katoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">558</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The walking behaviour of pedestrian social groups and its impact on crowd dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Moussaïd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niriaska</forename><surname>Perozo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Garnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Theraulaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10047</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Identifying and tracking pedestrians based on sensor fusion and motion stability predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basam</forename><surname>Musleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Otamendi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturo</forename><surname>José M A Armingol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De La Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="8028" to="8053" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You&apos;ll never walk alone: Modeling social behavior for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Motion planning and control of autonomous driving intelligence system based on risk potential optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pongsathorn</forename><surname>Raksincharoensak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Nagai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automotive Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">AVEC14</biblScope>
			<biblScope unit="page" from="53" to="60" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sophie: An attentive gan for predicting paths compliant to social and physical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1349" to="1358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A primer on kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kernel methods in computational biology</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="35" to="70" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wavelet pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pedestrian detection and tracking in far infrared images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Yasuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noboru</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Aoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="125" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Stochastic trajectory prediction with social graph network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10233</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sr-lstm: State refinement for lstm towards pedestrian trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12085" to="12094" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
